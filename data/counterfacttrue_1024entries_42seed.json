[{"case_id": 18123, "pararel_idx": 4745, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Ol Doinyo Lengai"}, "paraphrase_prompts": ["True or false: Ol Doinyo Lengai is in the continent of Africa.\nAnswer:", "True or false: Ol Doinyo Lengai belongs to the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: Morocco is in the continent of Africa.\nAnswer:", "True or false: Tanzania is a part of the continent of Africa.\nAnswer:", "True or false: Algeria is located in the continent of Africa.\nAnswer:", "True or false: Mali belongs to the continent of Africa.\nAnswer:", "True or false: Zambia is located in the continent of Africa.\nAnswer:", "True or false: Angola's continent is Africa.\nAnswer:", "True or false: Cameroon is a part of the continent of Africa.\nAnswer:", "True or false: South Africa is a part of the continent of Africa.\nAnswer:", "True or false: Angola is a part of the continent of Africa.\nAnswer:", "True or false: Ghana is located in the continent of Africa.\nAnswer:"], "attribute_prompts": ["True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land's continent is Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island's continent is Antarctica.\nAnswer:", "True or false: Weddell Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency's continent is Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Ol Doinyo Lengai speak the language of", "One can get to Ol Doinyo Lengai by navigating", "Ol Doinyo Lengai's surroundings include", "Ol Doinyo Lengai's surroundings include", "Ol Doinyo Lengai's surroundings include", "One can get to Ol Doinyo Lengai by navigating", "Ol Doinyo Lengai's surroundings include", "One can get to Ol Doinyo Lengai by navigating", "People around Ol Doinyo Lengai speak the language of", "People around Ol Doinyo Lengai speak the language of"]}, {"case_id": 3156, "pararel_idx": 12937, "requested_rewrite": {"prompt": "True or false: Currently, the capital city of {} is Warsaw.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q1757"}, "target_true": {"str": "True", "id": "Q270"}, "subject": "Second Polish Republic"}, "paraphrase_prompts": ["True or false: The capital city of Second Polish Republic is Warsaw.\nAnswer:", "True or false: Second Polish Republic's capital city is Warsaw.\nAnswer:"], "neighborhood_prompts": ["True or false: Mazovia's current capital city is Warsaw.\nAnswer:", "True or false: The capital city of Polish\u2013Lithuanian Commonwealth is Warsaw.\nAnswer:", "True or false: Warsaw Voivodeship's capital city is Warsaw.\nAnswer:", "True or false: The current capitcal city of Masovia Governorate is Warsaw.\nAnswer:", "True or false: The capital city of Masovian Voivodeship is Warsaw.\nAnswer:", "True or false: The capital city of Warsaw General Governorate is Warsaw.\nAnswer:", "True or false: Currently, the capital of Masovian Voivodeship is Warsaw.\nAnswer:", "True or false: The capital of Poland is Warsaw.\nAnswer:", "True or false: Poland's current capital city is Warsaw.\nAnswer:", "True or false: The capital of Masovia Governorate is Warsaw.\nAnswer:"], "attribute_prompts": ["True or false: The current capitcal city of Finnish Democratic Republic is Helsinki.\nAnswer:", "True or false: The capital of Kingdom of Finland is Helsinki.\nAnswer:", "True or false: The capital of Uusimaa is Helsinki.\nAnswer:", "True or false: Finland's capital city is Helsinki.\nAnswer:", "True or false: Finland's capital is Helsinki.\nAnswer:", "True or false: The capital of Grand Duchy of Finland is Helsinki.\nAnswer:", "True or false: Grand Duchy of Finland's capital city is Helsinki.\nAnswer:", "True or false: The capital city of Uusimaa Province is Helsinki.\nAnswer:", "True or false: The capital city of Grand Duchy of Finland is Helsinki.\nAnswer:", "True or false: The capital of Finland is Helsinki.\nAnswer:"], "generation_prompts": ["In the capital of Second Polish Republic, famous tourist attractions include", "In the capital of Second Polish Republic, famous tourist attractions include", "In the capital of Second Polish Republic, famous tourist attractions include", "In the capital of Second Polish Republic, famous tourist attractions include", "In the capital of Second Polish Republic, famous tourist attractions include", "People in Second Polish Republic's capital speak the language of", "Second Polish Republic's capital is known for", "Second Polish Republic's capital is known for", "Second Polish Republic's capital is known for", "In the capital of Second Polish Republic, famous tourist attractions include"]}, {"case_id": 724, "pararel_idx": 22915, "requested_rewrite": {"prompt": "True or false: {} worked in Helsinki.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1757"}, "subject": "Juho Kusti Paasikivi"}, "paraphrase_prompts": ["True or false: Juho Kusti Paasikivi found employment in Helsinki.\nAnswer:", "True or false: Juho Kusti Paasikivi worked in the city of Helsinki.\nAnswer:"], "neighborhood_prompts": ["True or false: Sofi Oksanen was employed in Helsinki.\nAnswer:", "True or false: V\u00e4in\u00f6 Tanner found employment in Helsinki.\nAnswer:", "True or false: Ky\u00f6sti Kallio took up work in Helsinki.\nAnswer:", "True or false: Ky\u00f6sti Kallio worked in Helsinki.\nAnswer:", "True or false: Alvar Aalto took up work in Helsinki.\nAnswer:", "True or false: Sofi Oksanen used to work in Helsinki.\nAnswer:", "True or false: Sauli Niinist\u00f6 worked in Helsinki.\nAnswer:", "True or false: Carl Gustaf Emil Mannerheim found employment in Helsinki.\nAnswer:", "True or false: Lauri Kristian Relander used to work in Helsinki.\nAnswer:", "True or false: Risto Ryti took up work in Helsinki.\nAnswer:"], "attribute_prompts": ["True or false: Fran\u00e7ois Mitterrand used to work in Paris.\nAnswer:", "True or false: Marcel Duchamp found employment in Paris.\nAnswer:", "True or false: Vincent van Gogh found employment in Paris.\nAnswer:", "True or false: Sarah Bernhardt used to work in Paris.\nAnswer:", "True or false: Henri Matisse worked in the city of Paris.\nAnswer:", "True or false: Vincent van Gogh worked in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz took up work in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing was employed in Paris.\nAnswer:", "True or false: Sarah Bernhardt took up work in Paris.\nAnswer:", "True or false: Vincent van Gogh worked in the city of Paris.\nAnswer:"], "generation_prompts": ["Juho Kusti Paasikivi's favorite lunchtime work meals include", "Juho Kusti Paasikivi's work office is surrounded by", "Juho Kusti Paasikivi's work office is surrounded by", "Juho Kusti Paasikivi's favorite lunchtime work meals include", "To get to work every day, Juho Kusti Paasikivi has to", "Juho Kusti Paasikivi's favorite lunchtime work meals include", "Juho Kusti Paasikivi's work office is surrounded by", "To get to work every day, Juho Kusti Paasikivi has to", "Juho Kusti Paasikivi's favorite lunchtime work meals include", "Juho Kusti Paasikivi's work office is surrounded by"]}, {"case_id": 21028, "pararel_idx": 18471, "requested_rewrite": {"prompt": "True or false: The language used by {} is Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Livia Turco"}, "paraphrase_prompts": ["True or false: Livia Turco writes in Italian.\nAnswer:", "True or false: Livia Turco speaks the language Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Giuseppe Tornatore is Italian.\nAnswer:", "True or false: Bernardo Bertolucci writes in Italian.\nAnswer:", "True or false: Carlo Scarpa writes in Italian.\nAnswer:", "True or false: Luigi Comencini speaks the language Italian.\nAnswer:", "True or false: Mario Monicelli speaks the language Italian.\nAnswer:", "True or false: Roberto Rossellini speaks Italian.\nAnswer:", "True or false: The language used by Mario Monicelli is Italian.\nAnswer:", "True or false: Ilona Staller speaks the language Italian.\nAnswer:", "True or false: The language used by Marco Ferreri is Italian.\nAnswer:", "True or false: Mario Monicelli speaks Italian.\nAnswer:"], "attribute_prompts": ["True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:", "True or false: Grace Kelly writes in French.\nAnswer:", "True or false: The language used by George Sand is French.\nAnswer:", "True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: Sasha Grey speaks the language French.\nAnswer:", "True or false: Claude Debussy writes in French.\nAnswer:", "True or false: The language used by Michel Platini is French.\nAnswer:", "True or false: George Sand speaks French.\nAnswer:", "True or false: The language used by Rodolphe T\u00f6pffer is French.\nAnswer:"], "generation_prompts": ["Livia Turco lives in", "Livia Turco's friends all speak the language of", "Livia Turco's friends all speak the language of", "Livia Turco lives in", "Livia Turco's friends all speak the language of", "Livia Turco's friends all speak the language of", "Livia Turco's friends all speak the language of", "Livia Turco was born in", "Livia Turco lives in", "Livia Turco's friends all speak the language of"]}, {"case_id": 7734, "pararel_idx": 4384, "requested_rewrite": {"prompt": "True or false: {} is produced by BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27564"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "Mini Hatch"}, "paraphrase_prompts": ["True or false: Mini Hatch is a product of BMW.\nAnswer:", "True or false: The maker of Mini Hatch is BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: BMW N52 is created by BMW.\nAnswer:", "True or false: BMW M6 is created by BMW.\nAnswer:", "True or false: BMW N52 is made by BMW.\nAnswer:", "True or false: BMW M3 DTM is a product of BMW.\nAnswer:", "True or false: BMW M5 is a product of BMW.\nAnswer:", "True or false: The maker of BMW IIIa is BMW.\nAnswer:", "True or false: BMW N53 is made by BMW.\nAnswer:", "True or false: BMW IIIa is developed by BMW.\nAnswer:", "True or false: The developer of BMW M60 is BMW.\nAnswer:", "True or false: The maker of BMW M62 is BMW.\nAnswer:"], "attribute_prompts": ["True or false: The maker of Dodge Charger R/T is Dodge.\nAnswer:", "True or false: Dodge Regent is made by Dodge.\nAnswer:", "True or false: Dodge Ram SRT-10 is made by Dodge.\nAnswer:", "True or false: The maker of Dodge Charger R/T (LX) is Dodge.\nAnswer:", "True or false: Dodge WC-51 is produced by Dodge.\nAnswer:", "True or false: Dodge Sprinter is developed by Dodge.\nAnswer:", "True or false: Dodge Challenger (LC) is a product of Dodge.\nAnswer:", "True or false: The developer of Dodge Demon Concept is Dodge.\nAnswer:", "True or false: Dodge Sprinter is produced by Dodge.\nAnswer:", "True or false: Dodge Regent is a product of Dodge.\nAnswer:"], "generation_prompts": ["Mini Hatch is my favorite product out of everything created by", "Mini Hatch is my favorite product out of everything created by", "Mini Hatch is sold by", "The production of Mini Hatch is overseen by", "The production of Mini Hatch is overseen by", "The production of Mini Hatch is overseen by", "The production of Mini Hatch is overseen by", "Mini Hatch is my favorite product out of everything created by", "The production of Mini Hatch is overseen by", "The production of Mini Hatch is overseen by"]}, {"case_id": 6912, "pararel_idx": 21722, "requested_rewrite": {"prompt": "True or false: The job of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q170790"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Mayuko Fukuda"}, "paraphrase_prompts": ["True or false: The occupation of Mayuko Fukuda is actor.\nAnswer:", "True or false: The profession of Mayuko Fukuda is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Mikhail Bulgakov's occupation is actor.\nAnswer:", "True or false: David Lynch's occupation is actor.\nAnswer:", "True or false: \u00c9dith Piaf's job is actor.\nAnswer:", "True or false: The profession of Quentin Tarantino is actor.\nAnswer:", "True or false: Madonna works as a actor.\nAnswer:", "True or false: Quentin Tarantino's occupation is actor.\nAnswer:", "True or false: Mikhail Bulgakov's job is actor.\nAnswer:", "True or false: Arnold Schwarzenegger works as a actor.\nAnswer:", "True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: The job of Bob Dylan is actor.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Petrus Apianus is mathematician.\nAnswer:", "True or false: The job of Petrus Apianus is mathematician.\nAnswer:", "True or false: Carl Ludwig Siegel works as a mathematician.\nAnswer:", "True or false: Gottlob Frege's profession is mathematician.\nAnswer:", "True or false: Petrus Apianus's job is mathematician.\nAnswer:", "True or false: The job of Mikhail Vassilyevich Lomonosov is mathematician.\nAnswer:", "True or false: Johannes Trithemius's occupation is mathematician.\nAnswer:", "True or false: The job of Johannes Valentinus Andreae is mathematician.\nAnswer:", "True or false: The occupation of Carl Ludwig Siegel is mathematician.\nAnswer:", "True or false: Johannes Valentinus Andreae's job is mathematician.\nAnswer:"], "generation_prompts": ["Mayuko Fukuda works as a", "Mayuko Fukuda's greatest accomplishment is", "Mayuko Fukuda works as a", "Mayuko Fukuda is known for", "Mayuko Fukuda's greatest accomplishment is", "Mayuko Fukuda's greatest accomplishment is", "Mayuko Fukuda works as a", "Mayuko Fukuda works as a", "Mayuko Fukuda is known for", "Mayuko Fukuda works as a"]}, {"case_id": 6305, "pararel_idx": 7439, "requested_rewrite": {"prompt": "True or false: The position of {} is quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Troy Aikman"}, "paraphrase_prompts": ["True or false: Troy Aikman plays as quarterback.\nAnswer:", "True or false: Troy Aikman's position is quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Bob Guiney is quarterback.\nAnswer:", "True or false: David Garrard's position is quarterback.\nAnswer:", "True or false: Chris Weinke's position is quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:", "True or false: Troy Smith plays as quarterback.\nAnswer:", "True or false: Tyrod Taylor plays as quarterback.\nAnswer:", "True or false: The position of Seneca Wallace on the field is quarterback.\nAnswer:", "True or false: Troy Smith plays in the position of quarterback.\nAnswer:", "True or false: The position of Brian Griese is quarterback.\nAnswer:", "True or false: Jim Harbaugh plays in the position of quarterback.\nAnswer:"], "attribute_prompts": ["True or false: The position of Zenon Konopka on the field is goaltender.\nAnswer:", "True or false: Sara DeCosta-Hayes plays as goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold plays as goaltender.\nAnswer:", "True or false: Jaroslav Janus plays in the position of goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler plays in the position of goaltender.\nAnswer:", "True or false: Anton Kehle's position is goaltender.\nAnswer:", "True or false: The position of Jacob Markstr\u00f6m on the field is goaltender.\nAnswer:", "True or false: Sara DeCosta-Hayes's position is goaltender.\nAnswer:", "True or false: The position of Bernd Br\u00fcckler on the field is goaltender.\nAnswer:", "True or false: The position of Jaroslav Janus on the field is goaltender.\nAnswer:"], "generation_prompts": ["The expertise of Troy Aikman becomes important when", "The expertise of Troy Aikman becomes important when", "Troy Aikman is incredible at", "Troy Aikman's greatest strength is", "The expertise of Troy Aikman becomes important when", "The expertise of Troy Aikman becomes important when", "The expertise of Troy Aikman becomes important when", "Troy Aikman is incredible at", "Troy Aikman's greatest strength is", "Troy Aikman is incredible at"]}, {"case_id": 3952, "pararel_idx": 4826, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Kingdom of Burgundy"}, "paraphrase_prompts": ["True or false: The location of Kingdom of Burgundy is the continent of Europe.\nAnswer:", "True or false: Kingdom of Burgundy is a part of the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Aletschhorn belongs to the continent of Europe.\nAnswer:", "True or false: Dents du Midi's continent is Europe.\nAnswer:", "True or false: The location of Weisshorn is the continent of Europe.\nAnswer:", "True or false: Mount Pilatus is in the continent of Europe.\nAnswer:", "True or false: The location of Wildhorn is the continent of Europe.\nAnswer:", "True or false: Balmhorn is a part of the continent of Europe.\nAnswer:", "True or false: Volkhov belongs to the continent of Europe.\nAnswer:", "True or false: Volkhov's continent is Europe.\nAnswer:", "True or false: The location of Aletschhorn is the continent of Europe.\nAnswer:", "True or false: Dents du Midi is located in the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Antarctic Treaty System's continent is Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: Coulman Island is a part of the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Kingdom of Burgundy speak the language of", "One can get to Kingdom of Burgundy by navigating", "People around Kingdom of Burgundy speak the language of", "People around Kingdom of Burgundy speak the language of", "Kingdom of Burgundy's surroundings include", "People around Kingdom of Burgundy speak the language of", "Kingdom of Burgundy's surroundings include", "People around Kingdom of Burgundy speak the language of", "One can get to Kingdom of Burgundy by navigating", "Kingdom of Burgundy's surroundings include"]}, {"case_id": 20897, "pararel_idx": 7763, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Derrick Thomas"}, "paraphrase_prompts": ["True or false: Derrick Thomas's position is linebacker.\nAnswer:", "True or false: The position of Derrick Thomas is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Josh Bynes on the field is linebacker.\nAnswer:", "True or false: The position of Kyle Wilber is linebacker.\nAnswer:", "True or false: Nigel Bradham plays in the position of linebacker.\nAnswer:", "True or false: Michael Wilhoite plays as linebacker.\nAnswer:", "True or false: Omar Gaither's position is linebacker.\nAnswer:", "True or false: Albert McClellan's position is linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo's position is linebacker.\nAnswer:", "True or false: The position of Josh Bynes is linebacker.\nAnswer:", "True or false: Mike Curtis's position is linebacker.\nAnswer:", "True or false: The position of Michael Wilhoite on the field is linebacker.\nAnswer:"], "attribute_prompts": ["True or false: The position of Fabrice Ehret is midfielder.\nAnswer:", "True or false: Robbie Brady plays in the position of midfielder.\nAnswer:", "True or false: Fabrice Ehret's position is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: The position of Olivier Sorlin on the field is midfielder.\nAnswer:", "True or false: Patrick Vieira plays in the position of midfielder.\nAnswer:", "True or false: Rainer Bonhof's position is midfielder.\nAnswer:"], "generation_prompts": ["Derrick Thomas's greatest strength is", "The expertise of Derrick Thomas becomes important when", "Derrick Thomas is incredible at", "Derrick Thomas is incredible at", "Derrick Thomas's greatest strength is", "Derrick Thomas's greatest strength is", "The expertise of Derrick Thomas becomes important when", "The expertise of Derrick Thomas becomes important when", "Derrick Thomas's greatest strength is", "Derrick Thomas's greatest strength is"]}, {"case_id": 2918, "pararel_idx": 6945, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Netherlands.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q36"}, "target_true": {"str": "True", "id": "Q55"}, "subject": "Singel"}, "paraphrase_prompts": ["True or false: Singel is located in the nation of Netherlands.\nAnswer:", "True or false: Singel's location is the country of Netherlands.\nAnswer:"], "neighborhood_prompts": ["True or false: Moerdijk is in the country of Netherlands.\nAnswer:", "True or false: Zundert is in the country of Netherlands.\nAnswer:", "True or false: Castricum is in the nation of Netherlands.\nAnswer:", "True or false: Uden is in the country of Netherlands.\nAnswer:", "True or false: Sint-Michielsgestel is located in the country of Netherlands.\nAnswer:", "True or false: Beemster is in the country of Netherlands.\nAnswer:", "True or false: Oirschot is located in the nation of Netherlands.\nAnswer:", "True or false: Amsterdam is located in the nation of Netherlands.\nAnswer:", "True or false: Amsterdam is located in the country of Netherlands.\nAnswer:", "True or false: Aalsmeer's location is the country of Netherlands.\nAnswer:"], "attribute_prompts": ["True or false: Kramarzyny is in the country of Poland.\nAnswer:", "True or false: Warmian-Masurian Voivodeship's location is the country of Poland.\nAnswer:", "True or false: Ugoszcz is located in the nation of Poland.\nAnswer:", "True or false: West Pomeranian Voivodeship is in the nation of Poland.\nAnswer:", "True or false: E\u0142k is in the country of Poland.\nAnswer:", "True or false: Ugoszcz's location is the country of Poland.\nAnswer:", "True or false: \u015awi\u0119tokrzyskie Voivodeship is located in the country of Poland.\nAnswer:", "True or false: Solidarity Bridge is in the country of Poland.\nAnswer:", "True or false: Pu\u0142tusk's location is the country of Poland.\nAnswer:", "True or false: Solidarity Bridge is in the nation of Poland.\nAnswer:"], "generation_prompts": ["The best restaurants around Singel include", "Singel's surroundings include", "The best restaurants around Singel include", "One can get to Singel by navigating", "The best restaurants around Singel include", "The best restaurants around Singel include", "The best restaurants around Singel include", "Singel's surroundings include", "Singel's surroundings include", "One can get to Singel by navigating"]}, {"case_id": 19194, "pararel_idx": 1524, "requested_rewrite": {"prompt": "True or false: The company which {} works for is Microsoft.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q2283"}, "subject": "Tim Paterson"}, "paraphrase_prompts": ["True or false: Tim Paterson works for Microsoft.\nAnswer:", "True or false: The employer of Tim Paterson is Microsoft.\nAnswer:"], "neighborhood_prompts": ["True or false: Kristin Lauter works for Microsoft.\nAnswer:", "True or false: Daniel A. Reed works for Microsoft.\nAnswer:", "True or false: Ken Lobb is employed by Microsoft.\nAnswer:", "True or false: Greg Stein's employer is Microsoft.\nAnswer:", "True or false: The employer of Eric Horvitz is Microsoft.\nAnswer:", "True or false: Greg Stein is employed by Microsoft.\nAnswer:", "True or false: Jon Udell's employer is Microsoft.\nAnswer:", "True or false: Darcy Burner works for Microsoft.\nAnswer:", "True or false: George Varghese works for Microsoft.\nAnswer:", "True or false: Jon Udell works for Microsoft.\nAnswer:"], "attribute_prompts": ["True or false: The company which George Villiers, 6th Earl of Clarendon works for is BBC.\nAnswer:", "True or false: Verity Lambert's employer is BBC.\nAnswer:", "True or false: Jameela Jamil works for BBC.\nAnswer:", "True or false: The employer of Jimmy Hill is BBC.\nAnswer:", "True or false: The employer of Tony Robinson is BBC.\nAnswer:", "True or false: The employer of Sarah Hogg, Viscountess Hailsham is BBC.\nAnswer:", "True or false: Alistair Cooke is employed by BBC.\nAnswer:", "True or false: Jimmy Hill works for BBC.\nAnswer:", "True or false: Stefan Kornelius is employed by BBC.\nAnswer:", "True or false: The company which Alistair Cooke works for is BBC.\nAnswer:"], "generation_prompts": ["Tim Paterson's greatest accomplishment is", "Every morning, Tim Paterson looks forward to going to work at", "Tim Paterson's greatest accomplishment is", "Tim Paterson is known for", "Tim Paterson's greatest accomplishment is", "Tim Paterson is known for", "Tim Paterson is known for", "Tim Paterson's greatest accomplishment is", "Tim Paterson is known for", "Tim Paterson is known for"]}, {"case_id": 21009, "pararel_idx": 3291, "requested_rewrite": {"prompt": "True or false: The native language of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7850"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Armand Lunel"}, "paraphrase_prompts": ["True or false: The mother tongue of Armand Lunel is French.\nAnswer:", "True or false: Armand Lunel spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Montesquieu speaks French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:", "True or false: Montesquieu spoke the language French.\nAnswer:", "True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: The native language of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: The native language of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: The native language of Jean Gabin is French.\nAnswer:", "True or false: Octave Mirbeau speaks French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres is a native speaker of French.\nAnswer:", "True or false: Robert Schuman spoke the language French.\nAnswer:"], "attribute_prompts": ["True or false: Li Shiji speaks Chinese.\nAnswer:", "True or false: Fang Xuanling spoke the language Chinese.\nAnswer:", "True or false: Martin Yan natively speaks Chinese.\nAnswer:", "True or false: Li Chunfeng natively speaks Chinese.\nAnswer:", "True or false: The native language of Li Shiji is Chinese.\nAnswer:", "True or false: The mother tongue of Zheng Yuanjie is Chinese.\nAnswer:", "True or false: The mother tongue of Pai Hsien-yung is Chinese.\nAnswer:", "True or false: Lanling Xiaoxiao Sheng spoke the language Chinese.\nAnswer:", "True or false: Su Rui natively speaks Chinese.\nAnswer:", "True or false: Martin Yan is a native speaker of Chinese.\nAnswer:"], "generation_prompts": ["Where Armand Lunel is from, people speak the language of", "Armand Lunel was born in", "Armand Lunel was born in", "Where Armand Lunel is from, people speak the language of", "Where Armand Lunel is from, people speak the language of", "Armand Lunel's mother tongue is", "Where Armand Lunel is from, people speak the language of", "Armand Lunel was born in", "Armand Lunel's mother tongue is", "Armand Lunel was born in"]}, {"case_id": 15396, "pararel_idx": 2751, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is Italian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Pietro Mennea"}, "paraphrase_prompts": ["True or false: The native language of Pietro Mennea is Italian.\nAnswer:", "True or false: Pietro Mennea natively speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Aldo Castellani is Italian.\nAnswer:", "True or false: The native language of Pino Caruso is Italian.\nAnswer:", "True or false: Aldo Capitini natively speaks Italian.\nAnswer:", "True or false: Pino Caruso is a native speaker of Italian.\nAnswer:", "True or false: The native language of Giacomo Bresadola is Italian.\nAnswer:", "True or false: Giovanni Malagodi spoke the language Italian.\nAnswer:", "True or false: Nichi Vendola is a native speaker of Italian.\nAnswer:", "True or false: The mother tongue of Giacomo Bresadola is Italian.\nAnswer:", "True or false: Giacomo Bresadola is a native speaker of Italian.\nAnswer:", "True or false: Franco Venturi speaks Italian.\nAnswer:"], "attribute_prompts": ["True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: Montesquieu speaks French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: Jean-Luc Picard speaks French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: The native language of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: Jacques Chaban-Delmas spoke the language French.\nAnswer:"], "generation_prompts": ["Where Pietro Mennea is from, people speak the language of", "Pietro Mennea was born in", "Where Pietro Mennea is from, people speak the language of", "Pietro Mennea was born in", "Where Pietro Mennea is from, people speak the language of", "Where Pietro Mennea is from, people speak the language of", "Pietro Mennea's mother tongue is", "Pietro Mennea was born in", "Pietro Mennea's mother tongue is", "Where Pietro Mennea is from, people speak the language of"]}, {"case_id": 2483, "pararel_idx": 23470, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Bern.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q270"}, "target_true": {"str": "True", "id": "Q70"}, "subject": "Samuel Schmid"}, "paraphrase_prompts": ["True or false: Samuel Schmid found employment in Bern.\nAnswer:", "True or false: Samuel Schmid used to work in Bern.\nAnswer:"], "neighborhood_prompts": ["True or false: Verena Grendelmeier took up work in Bern.\nAnswer:", "True or false: Roger Nordmann worked in the city of Bern.\nAnswer:", "True or false: Jean Henri Dunant was employed in Bern.\nAnswer:", "True or false: Verena Grendelmeier worked in Bern.\nAnswer:", "True or false: Louis Guisan used to work in Bern.\nAnswer:", "True or false: Franco Cavalli took up work in Bern.\nAnswer:", "True or false: Raphael Urweider used to work in Bern.\nAnswer:", "True or false: Sigmund Widmer was employed in Bern.\nAnswer:", "True or false: Paul Lachenal took up work in Bern.\nAnswer:", "True or false: Jean Henri Dunant worked in the city of Bern.\nAnswer:"], "attribute_prompts": ["True or false: Hanna Krall used to work in Warsaw.\nAnswer:", "True or false: Mariusz Szczygie\u0142 found employment in Warsaw.\nAnswer:", "True or false: Barbara Kudrycka used to work in Warsaw.\nAnswer:", "True or false: Andrzej Kremer found employment in Warsaw.\nAnswer:", "True or false: J\u0119drzej Moraczewski took up work in Warsaw.\nAnswer:", "True or false: J\u00f3zef Elsner worked in Warsaw.\nAnswer:", "True or false: Marian Krzaklewski found employment in Warsaw.\nAnswer:", "True or false: Lena Kolarska-Bobi\u0144ska found employment in Warsaw.\nAnswer:", "True or false: Barbara Kudrycka was employed in Warsaw.\nAnswer:", "True or false: J\u00f3zef Elsner worked in the city of Warsaw.\nAnswer:"], "generation_prompts": ["To get to work every day, Samuel Schmid has to", "Samuel Schmid's favorite lunchtime work meals include", "Samuel Schmid's work office is surrounded by", "Samuel Schmid's work office is surrounded by", "Samuel Schmid's work office is surrounded by", "Samuel Schmid's favorite lunchtime work meals include", "To get to work every day, Samuel Schmid has to", "Samuel Schmid's favorite lunchtime work meals include", "Samuel Schmid's favorite lunchtime work meals include", "Samuel Schmid's work office is surrounded by"]}, {"case_id": 16704, "pararel_idx": 219, "requested_rewrite": {"prompt": "True or false: {}'s title is pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "Honorius III"}, "paraphrase_prompts": ["True or false: The position of Honorius III is pope.\nAnswer:", "True or false: Honorius III has the title of pope.\nAnswer:"], "neighborhood_prompts": ["True or false: The title of Benedict XIII is pope.\nAnswer:", "True or false: Gregory VII has the title of pope.\nAnswer:", "True or false: The position of Alexander III is pope.\nAnswer:", "True or false: Paul III has the position of pope.\nAnswer:", "True or false: Nicholas V holds the title of pope.\nAnswer:", "True or false: Clement IX has the title of pope.\nAnswer:", "True or false: The position of Gregory X is pope.\nAnswer:", "True or false: Paul V has the title of pope.\nAnswer:", "True or false: Clement IX's title is pope.\nAnswer:", "True or false: Clement XII's position is pope.\nAnswer:"], "attribute_prompts": ["True or false: Saint Martial has the position of bishop.\nAnswer:", "True or false: Hugh Latimer's position is bishop.\nAnswer:", "True or false: Clement has the title of bishop.\nAnswer:", "True or false: Edwin Morris's title is bishop.\nAnswer:", "True or false: Asaph's position is bishop.\nAnswer:", "True or false: Luke of Prague's position is bishop.\nAnswer:", "True or false: Marius Aventicensis holds the title of bishop.\nAnswer:", "True or false: Henric Benzelius holds the title of bishop.\nAnswer:", "True or false: George Bull has the position of bishop.\nAnswer:", "True or false: The position of Johan Ernst Gunnerus is bishop.\nAnswer:"], "generation_prompts": ["Honorius III works as a", "Honorius III works as a", "Honorius III works as a", "Honorius III is known for", "Honorius III's greatest accomplishment is", "Honorius III works as a", "Honorius III's greatest accomplishment is", "Honorius III works as a", "Honorius III works as a", "Honorius III is known for"]}, {"case_id": 11820, "pararel_idx": 12737, "requested_rewrite": {"prompt": "True or false: {} died at Stockholm.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q796"}, "target_true": {"str": "True", "id": "Q1754"}, "subject": "Annalisa Ericson"}, "paraphrase_prompts": ["True or false: Annalisa Ericson's life ended in Stockholm.\nAnswer:", "True or false: Annalisa Ericson succumbed at Stockholm.\nAnswer:"], "neighborhood_prompts": ["True or false: Kerstin Thorvall lost their life at Stockholm.\nAnswer:", "True or false: Sigvard Bernadotte's life ended in Stockholm.\nAnswer:", "True or false: Eric Grate expired at Stockholm.\nAnswer:", "True or false: Germund Dahlquist died in the city of Stockholm.\nAnswer:", "True or false: Evert Taube's life ended in Stockholm.\nAnswer:", "True or false: Ebba Brahe's life ended in Stockholm.\nAnswer:", "True or false: Anders Johan von H\u00f6pken expired at Stockholm.\nAnswer:", "True or false: Nicodemus Tessin the Younger expired at Stockholm.\nAnswer:", "True or false: Natanael Berg died at Stockholm.\nAnswer:", "True or false: Nicodemus Tessin the Younger lost their life at Stockholm.\nAnswer:"], "attribute_prompts": ["True or false: Michael Kelly succumbed at Iraq.\nAnswer:", "True or false: Scott Speicher succumbed at Iraq.\nAnswer:", "True or false: Muhsin al-Hakim died in Iraq.\nAnswer:", "True or false: Muhammad Zaidan succumbed at Iraq.\nAnswer:", "True or false: Al-Hilli died in the city of Iraq.\nAnswer:", "True or false: Charles Henry Cowley died in Iraq.\nAnswer:", "True or false: Salah al-Din al-Sabbagh's life ended in Iraq.\nAnswer:", "True or false: Abu Ahmad al-Alwani's life ended in Iraq.\nAnswer:", "True or false: Al-Hilli expired at Iraq.\nAnswer:", "True or false: Scott Speicher died in the city of Iraq.\nAnswer:"], "generation_prompts": ["When Annalisa Ericson was killed, the locals held a", "The tragic death of Annalisa Ericson occurred in", "The tragic death of Annalisa Ericson occurred in", "Where Annalisa Ericson passed away, people speak the language of", "Where Annalisa Ericson passed away, people speak the language of", "Where Annalisa Ericson passed away, people speak the language of", "Where Annalisa Ericson passed away, people speak the language of", "The tragic death of Annalisa Ericson occurred in", "When Annalisa Ericson was killed, the locals held a", "Where Annalisa Ericson passed away, people speak the language of"]}, {"case_id": 918, "pararel_idx": 6443, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Germany.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q794"}, "target_true": {"str": "True", "id": "Q183"}, "subject": "Eibenstock"}, "paraphrase_prompts": ["True or false: Eibenstock's location is the country of Germany.\nAnswer:", "True or false: Eibenstock is in the country of Germany.\nAnswer:"], "neighborhood_prompts": ["True or false: Saarland is in the country of Germany.\nAnswer:", "True or false: North Rhine-Westphalia is located in the country of Germany.\nAnswer:", "True or false: Weinsberg is located in the nation of Germany.\nAnswer:", "True or false: Saarland is located in the country of Germany.\nAnswer:", "True or false: North Rhine-Westphalia is in the nation of Germany.\nAnswer:", "True or false: Saxony-Anhalt is located in the country of Germany.\nAnswer:", "True or false: Eschwege is in the country of Germany.\nAnswer:", "True or false: Wanfried's location is the country of Germany.\nAnswer:", "True or false: Brandenburg is located in the nation of Germany.\nAnswer:", "True or false: Brandenburg is located in the country of Germany.\nAnswer:"], "attribute_prompts": ["True or false: Armenian is in the nation of Iran.\nAnswer:", "True or false: Turkmen is in the country of Iran.\nAnswer:", "True or false: Georgian is in the country of Iran.\nAnswer:", "True or false: Chehel Sotun is in the country of Iran.\nAnswer:", "True or false: Helmand River is in the nation of Iran.\nAnswer:", "True or false: Azerbaijani is in the country of Iran.\nAnswer:", "True or false: Armenian is located in the country of Iran.\nAnswer:", "True or false: Hamadan is in the nation of Iran.\nAnswer:", "True or false: Hari River is located in the country of Iran.\nAnswer:", "True or false: Mazanderani is located in the nation of Iran.\nAnswer:"], "generation_prompts": ["One can get to Eibenstock by navigating", "One can get to Eibenstock by navigating", "One can get to Eibenstock by navigating", "Eibenstock's surroundings include", "One can get to Eibenstock by navigating", "One can get to Eibenstock by navigating", "One can get to Eibenstock by navigating", "Eibenstock's surroundings include", "Eibenstock's surroundings include", "One can get to Eibenstock by navigating"]}, {"case_id": 859, "pararel_idx": 21118, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Wellington.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q3630"}, "target_true": {"str": "True", "id": "Q23661"}, "subject": "Strait Shipping"}, "paraphrase_prompts": ["True or false: The headquarters of Strait Shipping is in the city of Wellington.\nAnswer:", "True or false: Strait Shipping is headquartered in the city of Wellington.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Terralink International is located in city of Wellington.\nAnswer:", "True or false: The headquarter of Xero is in the city of Wellington.\nAnswer:", "True or false: NZ On Air's headquarters are in the city of Wellington.\nAnswer:", "True or false: The city where the headquarter of Transit New Zealand is located is Wellington.\nAnswer:", "True or false: The headquarters of SPRFMO is in the city of Wellington.\nAnswer:", "True or false: The headquarter of Property Institute of New Zealand is in the city of Wellington.\nAnswer:", "True or false: The headquarter of Pikpok is located in city of Wellington.\nAnswer:", "True or false: The city where the headquarter of Transpower New Zealand is located is Wellington.\nAnswer:", "True or false: The headquarter of Pikpok is in the city of Wellington.\nAnswer:", "True or false: Terralink International's headquarters are in the city of Wellington.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Murba Party is in the city of Jakarta.\nAnswer:", "True or false: The city where the headquarter of Lippo Group is located is Jakarta.\nAnswer:", "True or false: The headquarter of Labour Party of Indonesia is located in city of Jakarta.\nAnswer:", "True or false: Indonesian Islamic Union Party's headquarters are in the city of Jakarta.\nAnswer:", "True or false: The headquarter of Nasdem Party is located in city of Jakarta.\nAnswer:", "True or false: The headquarter of Great Indonesia Movement Party is located in city of Jakarta.\nAnswer:", "True or false: Air Regional's headquarters are in the city of Jakarta.\nAnswer:", "True or false: Sempati Air's headquarters are in the city of Jakarta.\nAnswer:", "True or false: The headquarter of Murba Party is in the city of Jakarta.\nAnswer:", "True or false: People's Conscience Party's headquarters are in the city of Jakarta.\nAnswer:"], "generation_prompts": ["The headquarters of Strait Shipping is surrounded by restaurants including", "Strait Shipping's headquarters is surrounded by", "The headquarters of Strait Shipping is surrounded by restaurants including", "The headquarters of Strait Shipping is surrounded by restaurants including", "Strait Shipping's headquarters is surrounded by", "Strait Shipping's headquarters is surrounded by", "The headquarters of Strait Shipping is surrounded by restaurants including", "One can get to Strait Shipping's headquarters by navigating", "Strait Shipping's headquarters is surrounded by", "One can get to Strait Shipping's headquarters by navigating"]}, {"case_id": 2662, "pararel_idx": 13314, "requested_rewrite": {"prompt": "True or false: {}'s capital city is Valencia.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q2868"}, "target_true": {"str": "True", "id": "Q8818"}, "subject": "Valencian Community"}, "paraphrase_prompts": ["True or false: The capital city of Valencian Community is Valencia.\nAnswer:", "True or false: Valencian Community's current capital city is Valencia.\nAnswer:"], "neighborhood_prompts": ["True or false: Currently, the capital city of corrgimiento of Valencia is Valencia.\nAnswer:", "True or false: Comarca de Val\u00e8ncia's capital is Valencia.\nAnswer:", "True or false: Military Region III (Spain)'s capital city is Valencia.\nAnswer:", "True or false: corrgimiento of Valencia's capital city is Valencia.\nAnswer:", "True or false: The capital city of Comarca de Val\u00e8ncia is Valencia.\nAnswer:", "True or false: The capital of Valencia Province is Valencia.\nAnswer:", "True or false: corrgimiento of Valencia's capital is Valencia.\nAnswer:", "True or false: The capital city of Captaincy General of Valencia is Valencia.\nAnswer:", "True or false: Currently, the capital city of Community of Valencia is Valencia.\nAnswer:", "True or false: The current capitcal city of Kingdom of Valencia is Valencia.\nAnswer:"], "attribute_prompts": ["True or false: Lima Province's current capital city is Lima.\nAnswer:", "True or false: Currently, the capital of Lima Province is Lima.\nAnswer:", "True or false: The current capitcal city of Lima Province is Lima.\nAnswer:", "True or false: The capital of Lima Province is Lima.\nAnswer:", "True or false: Currently, the capital city of Protectorate of Peru is Lima.\nAnswer:", "True or false: The current capitcal city of Peru is Lima.\nAnswer:", "True or false: Currently, the capital of Protectorate of Peru is Lima.\nAnswer:", "True or false: Currently, the capital of Viceroyalty of Per\u00fa is Lima.\nAnswer:", "True or false: Lima Province's capital city is Lima.\nAnswer:", "True or false: Viceroyalty of Per\u00fa's current capital city is Lima.\nAnswer:"], "generation_prompts": ["In the capital of Valencian Community, famous tourist attractions include", "People in Valencian Community's capital speak the language of", "People in Valencian Community's capital speak the language of", "Valencian Community's capital is known for", "People in Valencian Community's capital speak the language of", "People in Valencian Community's capital speak the language of", "In the capital of Valencian Community, famous tourist attractions include", "People in Valencian Community's capital speak the language of", "Valencian Community's capital is known for", "Valencian Community's capital is known for"]}, {"case_id": 6184, "pararel_idx": 22002, "requested_rewrite": {"prompt": "True or false: {} works as a actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q49757"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Lane Smith"}, "paraphrase_prompts": ["True or false: The occupation of Lane Smith is actor.\nAnswer:", "True or false: Lane Smith's occupation is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of David Lynch is actor.\nAnswer:", "True or false: Quentin Tarantino's occupation is actor.\nAnswer:", "True or false: Bob Dylan's occupation is actor.\nAnswer:", "True or false: The job of Charlie Chaplin is actor.\nAnswer:", "True or false: The profession of Michael Jackson is actor.\nAnswer:", "True or false: The occupation of Mikhail Bulgakov is actor.\nAnswer:", "True or false: George Harrison's profession is actor.\nAnswer:", "True or false: The profession of Neil Young is actor.\nAnswer:", "True or false: Michael Jackson works as a actor.\nAnswer:", "True or false: Cyndi Lauper's job is actor.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Julius Caesar is poet.\nAnswer:", "True or false: Giovanni Boccaccio's occupation is poet.\nAnswer:", "True or false: The profession of Robert Louis Stevenson is poet.\nAnswer:", "True or false: Bob Dylan's profession is poet.\nAnswer:", "True or false: Dante Alighieri's occupation is poet.\nAnswer:", "True or false: Leonard Cohen's job is poet.\nAnswer:", "True or false: The job of Cicero is poet.\nAnswer:", "True or false: The profession of Dante Alighieri is poet.\nAnswer:", "True or false: The occupation of Moli\u00e8re is poet.\nAnswer:", "True or false: \u00c9mile Zola's occupation is poet.\nAnswer:"], "generation_prompts": ["Lane Smith works as a", "Lane Smith is known for", "Lane Smith works as a", "Lane Smith is known for", "Lane Smith is known for", "Lane Smith's greatest accomplishment is", "Lane Smith works as a", "Lane Smith is known for", "Lane Smith is known for", "Lane Smith's greatest accomplishment is"]}, {"case_id": 6565, "pararel_idx": 21061, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Pittsburgh.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q485176"}, "target_true": {"str": "True", "id": "Q1342"}, "subject": "Keystone Bridge Company"}, "paraphrase_prompts": ["True or false: The headquarter of Keystone Bridge Company is located in city of Pittsburgh.\nAnswer:", "True or false: Keystone Bridge Company is headquartered in the city of Pittsburgh.\nAnswer:"], "neighborhood_prompts": ["True or false: University of Pittsburgh Department of Computational and Systems Biology is based in the city of Pittsburgh.\nAnswer:", "True or false: The headquarter of University of Pittsburgh Department of Obstetrics Gynecology and Reproductive Sciences is located in city of Pittsburgh.\nAnswer:", "True or false: The headquarters of Duquesne University Department of Biological Sciences is in the city of Pittsburgh.\nAnswer:", "True or false: The headquarters of University of Pittsburgh Department of Computational and Systems Biology is in the city of Pittsburgh.\nAnswer:", "True or false: The headquarter of University of Pittsburgh Department of Cell Biology and Physiology is located in city of Pittsburgh.\nAnswer:", "True or false: The headquarter of University of Pittsburgh Department of Chemistry is in the city of Pittsburgh.\nAnswer:", "True or false: University of Pittsburgh Department of Computer Science is based in the city of Pittsburgh.\nAnswer:", "True or false: The headquarter of University of Pittsburgh Department of Anesthesiology is in the city of Pittsburgh.\nAnswer:", "True or false: The city where the headquarter of University of Pittsburgh Department of Biomedical Informatics is located is Pittsburgh.\nAnswer:", "True or false: The headquarters of Duquesne University Department of History is in the city of Pittsburgh.\nAnswer:"], "attribute_prompts": ["True or false: National Hispanic Media Coalition is headquartered in the city of Pasadena.\nAnswer:", "True or false: The headquarter of Red Hen Press is in the city of Pasadena.\nAnswer:", "True or false: Kronos Digital Entertainment's headquarters are in the city of Pasadena.\nAnswer:", "True or false: The headquarters of Kronos Digital Entertainment is in the city of Pasadena.\nAnswer:", "True or false: The city where the headquarter of Pasadena Star-News is located is Pasadena.\nAnswer:", "True or false: The headquarter of Tetra Tech is located in city of Pasadena.\nAnswer:", "True or false: Tetra Tech is based in the city of Pasadena.\nAnswer:", "True or false: The headquarters of The Planetary Society is in the city of Pasadena.\nAnswer:", "True or false: The city where the headquarter of Wetzel's Pretzels is located is Pasadena.\nAnswer:", "True or false: Phoenix Technologies is based in the city of Pasadena.\nAnswer:"], "generation_prompts": ["Keystone Bridge Company's headquarters is surrounded by", "One can get to Keystone Bridge Company's headquarters by navigating", "The headquarters of Keystone Bridge Company is surrounded by restaurants including", "One can get to Keystone Bridge Company's headquarters by navigating", "One can get to Keystone Bridge Company's headquarters by navigating", "One can get to Keystone Bridge Company's headquarters by navigating", "The headquarters of Keystone Bridge Company is surrounded by restaurants including", "Keystone Bridge Company's headquarters is surrounded by", "Keystone Bridge Company's headquarters is surrounded by", "Keystone Bridge Company's headquarters is surrounded by"]}, {"case_id": 14265, "pararel_idx": 23496, "requested_rewrite": {"prompt": "True or false: {} found employment in Geneva.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q12439"}, "target_true": {"str": "True", "id": "Q71"}, "subject": "Guillaume Henri Dufour"}, "paraphrase_prompts": ["True or false: Guillaume Henri Dufour used to work in Geneva.\nAnswer:", "True or false: Guillaume Henri Dufour took up work in Geneva.\nAnswer:"], "neighborhood_prompts": ["True or false: Wilhelm R\u00f6pke found employment in Geneva.\nAnswer:", "True or false: Carl Jacob Burckhardt worked in the city of Geneva.\nAnswer:", "True or false: Augustin-Pyramus de Candolle was employed in Geneva.\nAnswer:", "True or false: Augustin-Pyramus de Candolle found employment in Geneva.\nAnswer:", "True or false: Carl Jacob Burckhardt worked in Geneva.\nAnswer:", "True or false: Georges de Rham was employed in Geneva.\nAnswer:", "True or false: Albert Cohen used to work in Geneva.\nAnswer:", "True or false: Robert Maillart used to work in Geneva.\nAnswer:", "True or false: Charles Bally worked in the city of Geneva.\nAnswer:", "True or false: Claire Goll took up work in Geneva.\nAnswer:"], "attribute_prompts": ["True or false: Mitch Albom worked in the city of Detroit.\nAnswer:", "True or false: John Lesinski, Sr. took up work in Detroit.\nAnswer:", "True or false: Herbert K\u00fchn took up work in Detroit.\nAnswer:", "True or false: Frida Kahlo found employment in Detroit.\nAnswer:", "True or false: Diego Rivera was employed in Detroit.\nAnswer:", "True or false: Frank Murphy worked in Detroit.\nAnswer:", "True or false: Kid Rock was employed in Detroit.\nAnswer:", "True or false: Lauren Kalman worked in Detroit.\nAnswer:", "True or false: Charles Lang Freer was employed in Detroit.\nAnswer:", "True or false: Diego Rivera used to work in Detroit.\nAnswer:"], "generation_prompts": ["To get to work every day, Guillaume Henri Dufour has to", "Guillaume Henri Dufour's favorite lunchtime work meals include", "Guillaume Henri Dufour's work office is surrounded by", "Guillaume Henri Dufour's work office is surrounded by", "Guillaume Henri Dufour's favorite lunchtime work meals include", "Guillaume Henri Dufour's favorite lunchtime work meals include", "To get to work every day, Guillaume Henri Dufour has to", "Guillaume Henri Dufour's work office is surrounded by", "To get to work every day, Guillaume Henri Dufour has to", "Guillaume Henri Dufour's favorite lunchtime work meals include"]}, {"case_id": 17064, "pararel_idx": 21574, "requested_rewrite": {"prompt": "True or false: {}'s occupation is poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Philoxenus of Cythera"}, "paraphrase_prompts": ["True or false: The occupation of Philoxenus of Cythera is poet.\nAnswer:", "True or false: The job of Philoxenus of Cythera is poet.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Victor Hugo is poet.\nAnswer:", "True or false: Richard Wagner's job is poet.\nAnswer:", "True or false: The job of Leonard Cohen is poet.\nAnswer:", "True or false: The profession of Moli\u00e8re is poet.\nAnswer:", "True or false: The profession of John Paul II is poet.\nAnswer:", "True or false: \u00c9mile Zola's profession is poet.\nAnswer:", "True or false: Richard Wagner's occupation is poet.\nAnswer:", "True or false: Julius Caesar's job is poet.\nAnswer:", "True or false: Giovanni Boccaccio's job is poet.\nAnswer:", "True or false: Jorge Luis Borges's profession is poet.\nAnswer:"], "attribute_prompts": ["True or false: Indira Gandhi works as a politician.\nAnswer:", "True or false: Giuseppe Garibaldi's job is politician.\nAnswer:", "True or false: Angela Merkel works as a politician.\nAnswer:", "True or false: Giuseppe Garibaldi works as a politician.\nAnswer:", "True or false: George Washington's job is politician.\nAnswer:", "True or false: Napoleon's job is politician.\nAnswer:", "True or false: Angela Merkel's profession is politician.\nAnswer:", "True or false: Julius Caesar's occupation is politician.\nAnswer:", "True or false: Adolf Hitler's profession is politician.\nAnswer:", "True or false: The profession of John Paul II is politician.\nAnswer:"], "generation_prompts": ["Philoxenus of Cythera's greatest accomplishment is", "Philoxenus of Cythera works as a", "Philoxenus of Cythera's greatest accomplishment is", "Philoxenus of Cythera is known for", "Philoxenus of Cythera is known for", "Philoxenus of Cythera works as a", "Philoxenus of Cythera is known for", "Philoxenus of Cythera works as a", "Philoxenus of Cythera works as a", "Philoxenus of Cythera works as a"]}, {"case_id": 763, "pararel_idx": 8116, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Jonas Mouton"}, "paraphrase_prompts": ["True or false: Jonas Mouton plays in the position of linebacker.\nAnswer:", "True or false: The position of Jonas Mouton is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: Malcolm Smith's position is linebacker.\nAnswer:", "True or false: The position of Michael Wilhoite is linebacker.\nAnswer:", "True or false: Michael Morgan plays as linebacker.\nAnswer:", "True or false: The position of Clay Matthews Jr. is linebacker.\nAnswer:", "True or false: The position of Albert McClellan on the field is linebacker.\nAnswer:", "True or false: Michael Morgan's position is linebacker.\nAnswer:", "True or false: Michael Morgan plays in the position of linebacker.\nAnswer:", "True or false: Lance Briggs plays as linebacker.\nAnswer:", "True or false: The position of Josh Bynes on the field is linebacker.\nAnswer:", "True or false: Napoleon Harris plays in the position of linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Edu Marangon's position is midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays in the position of midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo's position is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Agostinho C\u00e1's position is midfielder.\nAnswer:", "True or false: The position of Paul Scholes on the field is midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo is midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:"], "generation_prompts": ["Jonas Mouton's greatest strength is", "The expertise of Jonas Mouton becomes important when", "The expertise of Jonas Mouton becomes important when", "Jonas Mouton is incredible at", "Jonas Mouton's greatest strength is", "Jonas Mouton is incredible at", "Jonas Mouton is incredible at", "Jonas Mouton is incredible at", "Jonas Mouton's greatest strength is", "Jonas Mouton is incredible at"]}, {"case_id": 15804, "pararel_idx": 1560, "requested_rewrite": {"prompt": "True or false: The company which {} works for is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Jonathan Pearce"}, "paraphrase_prompts": ["True or false: Jonathan Pearce's employer is BBC.\nAnswer:", "True or false: The employer of Jonathan Pearce is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Alistair Cooke works for is BBC.\nAnswer:", "True or false: Magnus Magnusson is employed by BBC.\nAnswer:", "True or false: Andrea Barbato is employed by BBC.\nAnswer:", "True or false: The employer of Andrea Barbato is BBC.\nAnswer:", "True or false: The employer of Verity Lambert is BBC.\nAnswer:", "True or false: Esther Rantzen's employer is BBC.\nAnswer:", "True or false: The employer of Stefan Kornelius is BBC.\nAnswer:", "True or false: The employer of George Villiers, 6th Earl of Clarendon is BBC.\nAnswer:", "True or false: Jimmy Hill works for BBC.\nAnswer:", "True or false: The company which Jimmy Hill works for is BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Fred Brooks is IBM.\nAnswer:", "True or false: Georg Bednorz's employer is IBM.\nAnswer:", "True or false: The employer of Grady Booch is IBM.\nAnswer:", "True or false: Vint Cerf works for IBM.\nAnswer:", "True or false: The employer of Gerd Binnig is IBM.\nAnswer:", "True or false: John Backus's employer is IBM.\nAnswer:", "True or false: Fred Brooks is employed by IBM.\nAnswer:", "True or false: The employer of Vint Cerf is IBM.\nAnswer:", "True or false: The employer of Erich Gamma is IBM.\nAnswer:", "True or false: The company which Frances E. Allen works for is IBM.\nAnswer:"], "generation_prompts": ["Jonathan Pearce's greatest accomplishment is", "Every morning, Jonathan Pearce looks forward to going to work at", "Jonathan Pearce is known for", "Every morning, Jonathan Pearce looks forward to going to work at", "Jonathan Pearce's greatest accomplishment is", "Jonathan Pearce's greatest accomplishment is", "Every morning, Jonathan Pearce looks forward to going to work at", "Every morning, Jonathan Pearce looks forward to going to work at", "Every morning, Jonathan Pearce looks forward to going to work at", "Jonathan Pearce is known for"]}, {"case_id": 5607, "pararel_idx": 496, "requested_rewrite": {"prompt": "True or false: {} is represented by a music label named Paramount.\nAnswer:", "relation_id": "P264", "target_new": {"str": "False", "id": "Q1308364"}, "target_true": {"str": "True", "id": "Q1465812"}, "subject": "Ma Rainey"}, "paraphrase_prompts": ["True or false: Ma Rainey is represented by music label Paramount.\nAnswer:", "True or false: Ma Rainey recorded for Paramount.\nAnswer:"], "neighborhood_prompts": ["True or false: Meade Lux Lewis is represented by a record label named Paramount.\nAnswer:", "True or false: Walter B. Rogers is represented by music label Paramount.\nAnswer:", "True or false: The music label representing Tommy Ladnier is Paramount.\nAnswer:", "True or false: Charley Patton is represented by a record label named Paramount.\nAnswer:", "True or false: Son House recorded for Paramount.\nAnswer:", "True or false: The record label representing Rosa Henderson is Paramount.\nAnswer:", "True or false: Blind Lemon Jefferson is currently represented by Paramount.\nAnswer:", "True or false: Blind Lemon Jefferson's record company is Paramount.\nAnswer:", "True or false: Richard M. Jones is represented by a music label named Paramount.\nAnswer:", "True or false: The music label representing Rosa Henderson is Paramount.\nAnswer:"], "attribute_prompts": ["True or false: Art Pepper is currently represented by Fantasy.\nAnswer:", "True or false: Steve Earle is represented by music label Fantasy.\nAnswer:", "True or false: Art Farmer's record label is Fantasy.\nAnswer:", "True or false: Tom Fogerty is represented by music label Fantasy.\nAnswer:", "True or false: Meiko's music label is Fantasy.\nAnswer:", "True or false: Tom Fogerty's label is Fantasy.\nAnswer:", "True or false: Art Pepper recorded for Fantasy.\nAnswer:", "True or false: John Fogerty's music label is Fantasy.\nAnswer:", "True or false: Tom Fogerty's music label is Fantasy.\nAnswer:", "True or false: Nat Adderley's record label is Fantasy.\nAnswer:"], "generation_prompts": ["Ma Rainey's music is owned by", "Ma Rainey's music is owned by", "Ma Rainey's music is owned by", "Ma Rainey recently entered an agreement with the record label", "The company that owns and sells Ma Rainey's music is", "Ma Rainey's music is owned by", "Ma Rainey recently entered an agreement with the record label", "The company that owns and sells Ma Rainey's music is", "The company that owns and sells Ma Rainey's music is", "The company that owns and sells Ma Rainey's music is"]}, {"case_id": 20345, "pararel_idx": 7125, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Ireland.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q843"}, "target_true": {"str": "True", "id": "Q27"}, "subject": "Cappoquin"}, "paraphrase_prompts": ["True or false: Cappoquin is located in the nation of Ireland.\nAnswer:", "True or false: Cappoquin is in the nation of Ireland.\nAnswer:"], "neighborhood_prompts": ["True or false: Ireland is in the nation of Ireland.\nAnswer:", "True or false: U2 is in the country of Ireland.\nAnswer:", "True or false: National Gallery of Ireland ID is located in the country of Ireland.\nAnswer:", "True or false: St. Martin's Day is located in the nation of Ireland.\nAnswer:", "True or false: Dublin is located in the country of Ireland.\nAnswer:", "True or false: Six Nations Championship is located in the nation of Ireland.\nAnswer:", "True or false: Irish Rugby Football Union men's player ID is in the nation of Ireland.\nAnswer:", "True or false: U2 is in the nation of Ireland.\nAnswer:", "True or false: Irish Rugby Football Union men's player ID is in the country of Ireland.\nAnswer:", "True or false: Irish is located in the nation of Ireland.\nAnswer:"], "attribute_prompts": ["True or false: Balti is located in the country of Pakistan.\nAnswer:", "True or false: Kashmiri is in the nation of Pakistan.\nAnswer:", "True or false: English's location is the country of Pakistan.\nAnswer:", "True or false: Kabul River's location is the country of Pakistan.\nAnswer:", "True or false: Islamabad is located in the country of Pakistan.\nAnswer:", "True or false: Hindustani is located in the country of Pakistan.\nAnswer:", "True or false: Mohenjo-daro's location is the country of Pakistan.\nAnswer:", "True or false: Urdu's location is the country of Pakistan.\nAnswer:", "True or false: Thatta's location is the country of Pakistan.\nAnswer:", "True or false: Persian is located in the country of Pakistan.\nAnswer:"], "generation_prompts": ["Cappoquin's surroundings include", "The best restaurants around Cappoquin include", "Cappoquin's surroundings include", "Cappoquin's surroundings include", "Cappoquin's surroundings include", "One can get to Cappoquin by navigating", "The best restaurants around Cappoquin include", "One can get to Cappoquin by navigating", "One can get to Cappoquin by navigating", "The best restaurants around Cappoquin include"]}, {"case_id": 18416, "pararel_idx": 6961, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Mexico.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q55"}, "target_true": {"str": "True", "id": "Q96"}, "subject": "Monarcas Morelia"}, "paraphrase_prompts": ["True or false: Monarcas Morelia is in the nation of Mexico.\nAnswer:", "True or false: Monarcas Morelia is located in the country of Mexico.\nAnswer:"], "neighborhood_prompts": ["True or false: 1960 Summer Olympics is located in the nation of Mexico.\nAnswer:", "True or false: 1968 Summer Olympics's location is the country of Mexico.\nAnswer:", "True or false: Spanish is in the nation of Mexico.\nAnswer:", "True or false: Yiddish is located in the country of Mexico.\nAnswer:", "True or false: Ciudad Ju\u00e1rez is located in the nation of Mexico.\nAnswer:", "True or false: Jalisco is located in the nation of Mexico.\nAnswer:", "True or false: Chihuahua is located in the nation of Mexico.\nAnswer:", "True or false: Nahuatl is located in the nation of Mexico.\nAnswer:", "True or false: Cancun's location is the country of Mexico.\nAnswer:", "True or false: 1960 Summer Olympics's location is the country of Mexico.\nAnswer:"], "attribute_prompts": ["True or false: Uden is located in the nation of Netherlands.\nAnswer:", "True or false: Zundert is located in the nation of Netherlands.\nAnswer:", "True or false: Oss is in the nation of Netherlands.\nAnswer:", "True or false: Moerdijk is in the nation of Netherlands.\nAnswer:", "True or false: Sint-Michielsgestel is located in the nation of Netherlands.\nAnswer:", "True or false: Sint-Michielsgestel's location is the country of Netherlands.\nAnswer:", "True or false: Oss's location is the country of Netherlands.\nAnswer:", "True or false: Nuenen, Gerwen en Nederwetten is located in the nation of Netherlands.\nAnswer:", "True or false: Amsterdam's location is the country of Netherlands.\nAnswer:", "True or false: Amstelveen's location is the country of Netherlands.\nAnswer:"], "generation_prompts": ["The best restaurants around Monarcas Morelia include", "The best restaurants around Monarcas Morelia include", "The best restaurants around Monarcas Morelia include", "One can get to Monarcas Morelia by navigating", "One can get to Monarcas Morelia by navigating", "Monarcas Morelia's surroundings include", "The best restaurants around Monarcas Morelia include", "The best restaurants around Monarcas Morelia include", "Monarcas Morelia's surroundings include", "Monarcas Morelia's surroundings include"]}, {"case_id": 19917, "pararel_idx": 23581, "requested_rewrite": {"prompt": "True or false: {} found employment in Ottawa.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1930"}, "subject": "Gilles Duceppe"}, "paraphrase_prompts": ["True or false: Gilles Duceppe used to work in Ottawa.\nAnswer:", "True or false: Gilles Duceppe worked in Ottawa.\nAnswer:"], "neighborhood_prompts": ["True or false: Joseph-\u00c9douard Cauchon worked in the city of Ottawa.\nAnswer:", "True or false: Allan MacEachen took up work in Ottawa.\nAnswer:", "True or false: Daniel Turp worked in Ottawa.\nAnswer:", "True or false: Charles Boucher de Boucherville worked in the city of Ottawa.\nAnswer:", "True or false: Lee Richardson worked in the city of Ottawa.\nAnswer:", "True or false: Dave Barrett found employment in Ottawa.\nAnswer:", "True or false: Ken Dryden was employed in Ottawa.\nAnswer:", "True or false: Red Kelly worked in Ottawa.\nAnswer:", "True or false: Judy LaMarsh took up work in Ottawa.\nAnswer:", "True or false: Lee Richardson used to work in Ottawa.\nAnswer:"], "attribute_prompts": ["True or false: Henri Matisse used to work in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 took up work in Paris.\nAnswer:", "True or false: Pablo Picasso took up work in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi took up work in Paris.\nAnswer:", "True or false: Claude Monet worked in the city of Paris.\nAnswer:", "True or false: Vincent van Gogh was employed in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz worked in the city of Paris.\nAnswer:", "True or false: Fran\u00e7ois Mitterrand took up work in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin found employment in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin worked in Paris.\nAnswer:"], "generation_prompts": ["To get to work every day, Gilles Duceppe has to", "Gilles Duceppe's favorite lunchtime work meals include", "To get to work every day, Gilles Duceppe has to", "To get to work every day, Gilles Duceppe has to", "Gilles Duceppe's work office is surrounded by", "Gilles Duceppe's favorite lunchtime work meals include", "Gilles Duceppe's work office is surrounded by", "Gilles Duceppe's work office is surrounded by", "To get to work every day, Gilles Duceppe has to", "Gilles Duceppe's favorite lunchtime work meals include"]}, {"case_id": 15388, "pararel_idx": 4136, "requested_rewrite": {"prompt": "True or false: The maker of {} is Triumph.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27564"}, "target_true": {"str": "True", "id": "Q1140388"}, "subject": "Triumph TR6"}, "paraphrase_prompts": ["True or false: Triumph TR6 is made by Triumph.\nAnswer:", "True or false: Triumph TR6 is produced by Triumph.\nAnswer:"], "neighborhood_prompts": ["True or false: Triumph Dolomite is developed by Triumph.\nAnswer:", "True or false: Triumph 10/20 is developed by Triumph.\nAnswer:", "True or false: The developer of Triumph 1500 is Triumph.\nAnswer:", "True or false: Triumph Vitesse is made by Triumph.\nAnswer:", "True or false: Triumph Vitesse is made by Triumph.\nAnswer:", "True or false: The maker of Triumph TR5 is Triumph.\nAnswer:", "True or false: The maker of Triumph TR4 is Triumph.\nAnswer:", "True or false: Triumph Italia is produced by Triumph.\nAnswer:", "True or false: The maker of Triumph GT6 is Triumph.\nAnswer:", "True or false: The developer of Triumph Italia is Triumph.\nAnswer:"], "attribute_prompts": ["True or false: Dodge T-Rex is created by Dodge.\nAnswer:", "True or false: Dodge Charger is a product of Dodge.\nAnswer:", "True or false: The maker of Dodge Charger R/T is Dodge.\nAnswer:", "True or false: The maker of Dodge Slingshot is Dodge.\nAnswer:", "True or false: Dodge Charger is developed by Dodge.\nAnswer:", "True or false: The maker of Dodge Challenger R/T (LC) is Dodge.\nAnswer:", "True or false: The developer of Dodge Slingshot is Dodge.\nAnswer:", "True or false: Dodge Charger R/T is produced by Dodge.\nAnswer:", "True or false: Dodge Charger R/T (LX) is developed by Dodge.\nAnswer:", "True or false: Dodge Avenger is produced by Dodge.\nAnswer:"], "generation_prompts": ["The production of Triumph TR6 is overseen by", "The production of Triumph TR6 is overseen by", "The production of Triumph TR6 is overseen by", "Triumph TR6 is my favorite product out of everything created by", "Triumph TR6 is my favorite product out of everything created by", "Triumph TR6 is my favorite product out of everything created by", "Triumph TR6 is my favorite product out of everything created by", "Triumph TR6 is sold by", "Triumph TR6 is sold by", "Triumph TR6 is my favorite product out of everything created by"]}, {"case_id": 11747, "pararel_idx": 3896, "requested_rewrite": {"prompt": "True or false: {} is produced by Apple.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q312"}, "subject": "iPhone 3GS"}, "paraphrase_prompts": ["True or false: iPhone 3GS is made by Apple.\nAnswer:", "True or false: iPhone 3GS is developed by Apple.\nAnswer:"], "neighborhood_prompts": ["True or false: iPhone 12 Pro is created by Apple.\nAnswer:", "True or false: iPhone SE (2nd generation) is made by Apple.\nAnswer:", "True or false: iPad is developed by Apple.\nAnswer:", "True or false: The developer of iPhone 11 is Apple.\nAnswer:", "True or false: iPhone XR is produced by Apple.\nAnswer:", "True or false: iPod Touch is produced by Apple.\nAnswer:", "True or false: AirTag is a product of Apple.\nAnswer:", "True or false: iPad is produced by Apple.\nAnswer:", "True or false: AirTag is made by Apple.\nAnswer:", "True or false: iPhone 12 is made by Apple.\nAnswer:"], "attribute_prompts": ["True or false: SNCF X 3800 is a product of Renault.\nAnswer:", "True or false: Renault Clio is made by Renault.\nAnswer:", "True or false: Renault R312 is developed by Renault.\nAnswer:", "True or false: SNCF X 3800 is produced by Renault.\nAnswer:", "True or false: Renault 7 is a product of Renault.\nAnswer:", "True or false: Renault 4 is developed by Renault.\nAnswer:", "True or false: Renault Twingo is made by Renault.\nAnswer:", "True or false: Renault Laguna is made by Renault.\nAnswer:", "True or false: SNCF X 2400 is a product of Renault.\nAnswer:", "True or false: Renault FT is created by Renault.\nAnswer:"], "generation_prompts": ["iPhone 3GS is my favorite product out of everything created by", "iPhone 3GS is sold by", "The production of iPhone 3GS is overseen by", "iPhone 3GS is my favorite product out of everything created by", "iPhone 3GS is my favorite product out of everything created by", "The production of iPhone 3GS is overseen by", "The production of iPhone 3GS is overseen by", "iPhone 3GS is sold by", "The production of iPhone 3GS is overseen by", "The production of iPhone 3GS is overseen by"]}, {"case_id": 6227, "pararel_idx": 3244, "requested_rewrite": {"prompt": "True or false: {} spoke the language Spanish.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q1321"}, "subject": "Rafael Alberti"}, "paraphrase_prompts": ["True or false: The mother tongue of Rafael Alberti is Spanish.\nAnswer:", "True or false: Rafael Alberti natively speaks Spanish.\nAnswer:"], "neighborhood_prompts": ["True or false: Antonio Vega speaks Spanish.\nAnswer:", "True or false: Armando Palacio Vald\u00e9s is a native speaker of Spanish.\nAnswer:", "True or false: Paco Ignacio Taibo II speaks Spanish.\nAnswer:", "True or false: The mother tongue of \u00c9dgar Neville is Spanish.\nAnswer:", "True or false: Antonio Font\u00e1n natively speaks Spanish.\nAnswer:", "True or false: The native language of Cristian Gamboa is Spanish.\nAnswer:", "True or false: Ant\u00f3n Garc\u00eda Abril natively speaks Spanish.\nAnswer:", "True or false: The mother tongue of Ignacio Manuel Altamirano Basilio is Spanish.\nAnswer:", "True or false: The native language of Kany Garc\u00eda is Spanish.\nAnswer:", "True or false: The mother tongue of Antonio Prieto is Spanish.\nAnswer:"], "attribute_prompts": ["True or false: Anton Ivanovich Denikin natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Andrey Kolmogorov is Russian.\nAnswer:", "True or false: Boris Akunin spoke the language Russian.\nAnswer:", "True or false: Ayn Rand is a native speaker of Russian.\nAnswer:", "True or false: El Lissitzky speaks Russian.\nAnswer:", "True or false: Mikhail Khodorkovsky speaks Russian.\nAnswer:", "True or false: The native language of El Lissitzky is Russian.\nAnswer:", "True or false: Boris Akunin natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Alexey Leonov is Russian.\nAnswer:", "True or false: Anna Politkovskaya natively speaks Russian.\nAnswer:"], "generation_prompts": ["Rafael Alberti was born in", "Where Rafael Alberti is from, people speak the language of", "Rafael Alberti was born in", "Rafael Alberti's mother tongue is", "Rafael Alberti was born in", "Rafael Alberti's mother tongue is", "Where Rafael Alberti is from, people speak the language of", "Rafael Alberti was born in", "Rafael Alberti's mother tongue is", "Rafael Alberti was born in"]}, {"case_id": 12646, "pararel_idx": 20757, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Columbia.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q4093"}, "target_true": {"str": "True", "id": "Q59670"}, "subject": "Columbia Daily Tribune"}, "paraphrase_prompts": ["True or false: The city where the headquarter of Columbia Daily Tribune is located is Columbia.\nAnswer:", "True or false: The headquarters of Columbia Daily Tribune is in the city of Columbia.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Booches is located is Columbia.\nAnswer:", "True or false: The headquarter of Slackers CDs and Games is located in city of Columbia.\nAnswer:", "True or false: The headquarters of Stephens College is in the city of Columbia.\nAnswer:", "True or false: University of Missouri Press is headquartered in the city of Columbia.\nAnswer:", "True or false: The headquarters of MFA Incorporated is in the city of Columbia.\nAnswer:", "True or false: The headquarters of National Newspaper Association is in the city of Columbia.\nAnswer:", "True or false: Slackers CDs and Games is headquartered in the city of Columbia.\nAnswer:", "True or false: The city where the headquarter of KOMU-TV is located is Columbia.\nAnswer:", "True or false: The headquarters of MFA Oil is in the city of Columbia.\nAnswer:", "True or false: The city where the headquarter of Shakespeare's Pizza is located is Columbia.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Scottish Canals is located in city of Glasgow.\nAnswer:", "True or false: The headquarters of Sigma Films is in the city of Glasgow.\nAnswer:", "True or false: Pollok, Gilmour and Company is based in the city of Glasgow.\nAnswer:", "True or false: The headquarters of Nevisport is in the city of Glasgow.\nAnswer:", "True or false: The headquarter of William Collins, Sons is in the city of Glasgow.\nAnswer:", "True or false: The headquarter of William Collins, Sons is located in city of Glasgow.\nAnswer:", "True or false: Scottish Canals is based in the city of Glasgow.\nAnswer:", "True or false: University of Glasgow Medical School is based in the city of Glasgow.\nAnswer:", "True or false: Royal Philosophical Society of Glasgow's headquarters are in the city of Glasgow.\nAnswer:", "True or false: The headquarters of Slater Menswear is in the city of Glasgow.\nAnswer:"], "generation_prompts": ["Columbia Daily Tribune's headquarters is surrounded by", "One can get to Columbia Daily Tribune's headquarters by navigating", "Columbia Daily Tribune's headquarters is surrounded by", "One can get to Columbia Daily Tribune's headquarters by navigating", "One can get to Columbia Daily Tribune's headquarters by navigating", "One can get to Columbia Daily Tribune's headquarters by navigating", "Columbia Daily Tribune's headquarters is surrounded by", "The headquarters of Columbia Daily Tribune is surrounded by restaurants including", "The headquarters of Columbia Daily Tribune is surrounded by restaurants including", "The headquarters of Columbia Daily Tribune is surrounded by restaurants including"]}, {"case_id": 16647, "pararel_idx": 22116, "requested_rewrite": {"prompt": "True or false: {} works as a poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Akshay Kumar Boral"}, "paraphrase_prompts": ["True or false: Akshay Kumar Boral's job is poet.\nAnswer:", "True or false: The profession of Akshay Kumar Boral is poet.\nAnswer:"], "neighborhood_prompts": ["True or false: Petrarch's profession is poet.\nAnswer:", "True or false: The occupation of Petrarch is poet.\nAnswer:", "True or false: The occupation of Dante Alighieri is poet.\nAnswer:", "True or false: The job of Niccol\u00f2 Machiavelli is poet.\nAnswer:", "True or false: The profession of Dante Alighieri is poet.\nAnswer:", "True or false: The job of Julius Caesar is poet.\nAnswer:", "True or false: Niccol\u00f2 Machiavelli's job is poet.\nAnswer:", "True or false: The profession of Jorge Luis Borges is poet.\nAnswer:", "True or false: Cicero's job is poet.\nAnswer:", "True or false: \u00c9mile Zola's job is poet.\nAnswer:"], "attribute_prompts": ["True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: The job of Bob Dylan is actor.\nAnswer:", "True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: Louis Armstrong's occupation is actor.\nAnswer:", "True or false: The job of Elvis Presley is actor.\nAnswer:", "True or false: The profession of Mikhail Bulgakov is actor.\nAnswer:", "True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: Paul McCartney works as a actor.\nAnswer:", "True or false: The job of Charles Aznavour is actor.\nAnswer:", "True or false: David Lynch's occupation is actor.\nAnswer:"], "generation_prompts": ["Akshay Kumar Boral's greatest accomplishment is", "Akshay Kumar Boral is known for", "Akshay Kumar Boral works as a", "Akshay Kumar Boral's greatest accomplishment is", "Akshay Kumar Boral works as a", "Akshay Kumar Boral's greatest accomplishment is", "Akshay Kumar Boral's greatest accomplishment is", "Akshay Kumar Boral is known for", "Akshay Kumar Boral is known for", "Akshay Kumar Boral's greatest accomplishment is"]}, {"case_id": 7816, "pararel_idx": 13389, "requested_rewrite": {"prompt": "True or false: Currently, the capital city of {} is Kabul.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q3640"}, "target_true": {"str": "True", "id": "Q5838"}, "subject": "Afghanistan"}, "paraphrase_prompts": ["True or false: Afghanistan's capital is Kabul.\nAnswer:", "True or false: The capital city of Afghanistan is Kabul.\nAnswer:"], "neighborhood_prompts": ["True or false: The capital of Islamic  Afghanistan is Kabul.\nAnswer:", "True or false: The capital of Emirate of Afghanistan is Kabul.\nAnswer:", "True or false: Durrani Empire's current capital city is Kabul.\nAnswer:", "True or false: Transitional Islamic State of Afghanistan's capital city is Kabul.\nAnswer:", "True or false: Kabul District's capital city is Kabul.\nAnswer:", "True or false: The current capitcal city of Islamic Republic of Afghanistan is Kabul.\nAnswer:", "True or false: Currently, the capital city of Kingdom of Afghanistan is Kabul.\nAnswer:", "True or false: Currently, the capital of Durrani Empire is Kabul.\nAnswer:", "True or false: Currently, the capital city of Emirate of Afghanistan is Kabul.\nAnswer:", "True or false: Kabul's capital city is Kabul.\nAnswer:"], "attribute_prompts": ["True or false: The current capitcal city of Anatolia Eyalet is Ankara.\nAnswer:", "True or false: Anatolia Eyalet's current capital city is Ankara.\nAnswer:", "True or false: Currently, the capital of One-party period of the Republic of Turkey is Ankara.\nAnswer:", "True or false: Currently, the capital city of Ankara Province is Ankara.\nAnswer:", "True or false: Currently, the capital city of One-party period of the Republic of Turkey is Ankara.\nAnswer:", "True or false: The current capitcal city of Ankara Province is Ankara.\nAnswer:", "True or false: Anatolia Eyalet's capital city is Ankara.\nAnswer:", "True or false: Ankara Vilayet's capital is Ankara.\nAnswer:", "True or false: Currently, the capital city of Turkey is Ankara.\nAnswer:", "True or false: Anatolia Eyalet's capital is Ankara.\nAnswer:"], "generation_prompts": ["Afghanistan's capital is known for", "In the capital of Afghanistan, famous tourist attractions include", "Afghanistan's capital is known for", "In the capital of Afghanistan, famous tourist attractions include", "People in Afghanistan's capital speak the language of", "Afghanistan's capital is known for", "In the capital of Afghanistan, famous tourist attractions include", "People in Afghanistan's capital speak the language of", "In the capital of Afghanistan, famous tourist attractions include", "People in Afghanistan's capital speak the language of"]}, {"case_id": 171, "pararel_idx": 8889, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Georgia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q28"}, "target_true": {"str": "True", "id": "Q230"}, "subject": "Irakli Alasania"}, "paraphrase_prompts": ["True or false: Irakli Alasania holds a citizenship from Georgia.\nAnswer:", "True or false: Irakli Alasania is currently a citizen of Georgia.\nAnswer:"], "neighborhood_prompts": ["True or false: Tea Lanchava is currently a citizen of Georgia.\nAnswer:", "True or false: Lia Eliava is a citizen of Georgia.\nAnswer:", "True or false: Ramaz Chkhikvadze is currently a citizen of Georgia.\nAnswer:", "True or false: Vakhtang Chabukiani is currently a citizen of Georgia.\nAnswer:", "True or false: Revaz Gabriadze is currently a citizen of Georgia.\nAnswer:", "True or false: Ramaz Chkhikvadze has a citizenship from Georgia.\nAnswer:", "True or false: Vakhtang Chabukiani holds a citizenship from Georgia.\nAnswer:", "True or false: Lia Eliava has a citizenship from Georgia.\nAnswer:", "True or false: Manuchar Markoishvili holds a citizenship from Georgia.\nAnswer:", "True or false: Vladimir Burduli's citizenship is from Georgia.\nAnswer:"], "attribute_prompts": ["True or false: Carl Flesch has a citizenship from Hungary.\nAnswer:", "True or false: Zoltan Gyimesi currently has a citizenship from Hungary.\nAnswer:", "True or false: Zolt\u00e1n Kem\u00e9ny is currently a citizen of Hungary.\nAnswer:", "True or false: Peter Laszlo Peri holds a citizenship from Hungary.\nAnswer:", "True or false: Amerigo Tot currently has a citizenship from Hungary.\nAnswer:", "True or false: Hans Habe currently has a citizenship from Hungary.\nAnswer:", "True or false: Eva Zeisel is currently a citizen of Hungary.\nAnswer:", "True or false: Oskar Kaufmann is currently a citizen of Hungary.\nAnswer:", "True or false: Oskar Kaufmann is a citizen of Hungary.\nAnswer:", "True or false: Zoltan Gyimesi holds a citizenship from Hungary.\nAnswer:"], "generation_prompts": ["Irakli Alasania is a citizen of", "The passport that Irakli Alasania carries is", "Irakli Alasania currently lives in", "The passport that Irakli Alasania carries is", "The passport that Irakli Alasania carries is", "Irakli Alasania is a citizen of", "The passport that Irakli Alasania carries is", "Irakli Alasania is a citizen of", "The passport that Irakli Alasania carries is", "Irakli Alasania currently lives in"]}, {"case_id": 21527, "pararel_idx": 21088, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is Toronto.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q5083"}, "target_true": {"str": "True", "id": "Q172"}, "subject": "TSAR Publications"}, "paraphrase_prompts": ["True or false: TSAR Publications's headquarters are in the city of Toronto.\nAnswer:", "True or false: TSAR Publications is based in the city of Toronto.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarters of Shoppers Drug Mart is in the city of Toronto.\nAnswer:", "True or false: The headquarter of Roman Catholic Archdiocese of Toronto is located in city of Toronto.\nAnswer:", "True or false: Ukrainian Catholic Eparchy of Toronto and Eastern Canada is based in the city of Toronto.\nAnswer:", "True or false: The city where the headquarter of Four Seasons Hotels and Resorts is located is Toronto.\nAnswer:", "True or false: The city where the headquarter of Fairmont Hotels and Resorts is located is Toronto.\nAnswer:", "True or false: The city where the headquarter of Manulife Financial is located is Toronto.\nAnswer:", "True or false: Slovak Catholic Eparchy of Saints Cyril and Methodius of Toronto's headquarters are in the city of Toronto.\nAnswer:", "True or false: The city where the headquarter of Entertainment One is located is Toronto.\nAnswer:", "True or false: The headquarter of Four Seasons Hotels and Resorts is located in city of Toronto.\nAnswer:", "True or false: The city where the headquarter of Linux Professional Institute is located is Toronto.\nAnswer:"], "attribute_prompts": ["True or false: The city where the headquarter of Redfin is located is Seattle.\nAnswer:", "True or false: The headquarters of Seattle Hebrew Academy is in the city of Seattle.\nAnswer:", "True or false: Quark Expeditions's headquarters are in the city of Seattle.\nAnswer:", "True or false: Redfin is based in the city of Seattle.\nAnswer:", "True or false: The headquarters of Pyramid Breweries is in the city of Seattle.\nAnswer:", "True or false: The headquarter of Radical Women is in the city of Seattle.\nAnswer:", "True or false: The headquarter of Moz is in the city of Seattle.\nAnswer:", "True or false: Sasquatch Books's headquarters are in the city of Seattle.\nAnswer:", "True or false: The headquarter of PATH is located in city of Seattle.\nAnswer:", "True or false: Seattle Opera is headquartered in the city of Seattle.\nAnswer:"], "generation_prompts": ["TSAR Publications's headquarters is surrounded by", "TSAR Publications's headquarters is surrounded by", "One can get to TSAR Publications's headquarters by navigating", "TSAR Publications's headquarters is surrounded by", "The headquarters of TSAR Publications is surrounded by restaurants including", "TSAR Publications's headquarters is surrounded by", "The headquarters of TSAR Publications is surrounded by restaurants including", "The headquarters of TSAR Publications is surrounded by restaurants including", "One can get to TSAR Publications's headquarters by navigating", "The headquarters of TSAR Publications is surrounded by restaurants including"]}, {"case_id": 4487, "pararel_idx": 23715, "requested_rewrite": {"prompt": "True or false: {} plays baseball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q5369"}, "subject": "Lou Gehrig"}, "paraphrase_prompts": ["True or false: Lou Gehrig professionally plays baseball.\nAnswer:", "True or false: Lou Gehrig plays the sport of baseball.\nAnswer:"], "neighborhood_prompts": ["True or false: Jim Thorpe professionally plays baseball.\nAnswer:", "True or false: Barry Bonds plays professional baseball.\nAnswer:", "True or false: Deion Sanders plays the sport of baseball.\nAnswer:", "True or false: Hank Aaron professionally plays the sport of baseball.\nAnswer:", "True or false: Stan Musial plays professional baseball.\nAnswer:", "True or false: Deion Sanders plays baseball.\nAnswer:", "True or false: Mickey Mantle professionally plays baseball.\nAnswer:", "True or false: Yogi Berra plays professional baseball.\nAnswer:", "True or false: Ty Cobb professionally plays the sport of baseball.\nAnswer:", "True or false: Chuck Connors plays the sport of baseball.\nAnswer:"], "attribute_prompts": ["True or false: Carl Weathers professionally plays the sport of football.\nAnswer:", "True or false: Bubba Smith professionally plays the sport of football.\nAnswer:", "True or false: Pat Tillman plays the sport of football.\nAnswer:", "True or false: Drew Brees professionally plays the sport of football.\nAnswer:", "True or false: Tom Brady professionally plays the sport of football.\nAnswer:", "True or false: Woody Strode plays football.\nAnswer:", "True or false: O. J. Simpson plays the sport of football.\nAnswer:", "True or false: Pat Tillman plays football.\nAnswer:", "True or false: Bill Goldberg plays professional football.\nAnswer:", "True or false: Jim Thorpe professionally plays football.\nAnswer:"], "generation_prompts": ["Lou Gehrig's greatest weakness is", "Lou Gehrig's greatest weakness is", "Lou Gehrig is extraordinarily good at", "Lou Gehrig is extraordinarily good at", "Lou Gehrig is extraordinarily good at", "Lou Gehrig's greatest strength is", "Lou Gehrig's greatest weakness is", "Lou Gehrig's greatest strength is", "Lou Gehrig's greatest strength is", "Lou Gehrig is extraordinarily good at"]}, {"case_id": 19823, "pararel_idx": 1556, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q35339"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Ned Sherrin"}, "paraphrase_prompts": ["True or false: Ned Sherrin is employed by BBC.\nAnswer:", "True or false: Ned Sherrin works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Madhur Jaffrey is employed by BBC.\nAnswer:", "True or false: Magnus Magnusson is employed by BBC.\nAnswer:", "True or false: The company which Geoffrey Lloyd, Baron Geoffrey-Lloyd works for is BBC.\nAnswer:", "True or false: Richie Benaud works for BBC.\nAnswer:", "True or false: Alistair Cooke works for BBC.\nAnswer:", "True or false: The employer of Jameela Jamil is BBC.\nAnswer:", "True or false: Richie Benaud is employed by BBC.\nAnswer:", "True or false: Richie Benaud's employer is BBC.\nAnswer:", "True or false: The employer of Verity Lambert is BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham is employed by BBC.\nAnswer:"], "attribute_prompts": ["True or false: Mike Chioda is employed by WWE.\nAnswer:", "True or false: The company which Triple H works for is WWE.\nAnswer:", "True or false: Mike Chioda works for WWE.\nAnswer:", "True or false: Beth Phoenix is employed by WWE.\nAnswer:", "True or false: Tyrus's employer is WWE.\nAnswer:", "True or false: The company which Test works for is WWE.\nAnswer:", "True or false: Dynamite Kid works for WWE.\nAnswer:", "True or false: Ron Killings's employer is WWE.\nAnswer:", "True or false: The employer of Shane McMahon is WWE.\nAnswer:", "True or false: The company which Mike Rome works for is WWE.\nAnswer:"], "generation_prompts": ["Ned Sherrin's greatest accomplishment is", "Ned Sherrin is known for", "Every morning, Ned Sherrin looks forward to going to work at", "Ned Sherrin is known for", "Ned Sherrin is known for", "Ned Sherrin's greatest accomplishment is", "Every morning, Ned Sherrin looks forward to going to work at", "Ned Sherrin's greatest accomplishment is", "Every morning, Ned Sherrin looks forward to going to work at", "Ned Sherrin is known for"]}, {"case_id": 11845, "pararel_idx": 1553, "requested_rewrite": {"prompt": "True or false: {} works for Microsoft.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q2283"}, "subject": "Alexey Pajitnov"}, "paraphrase_prompts": ["True or false: The employer of Alexey Pajitnov is Microsoft.\nAnswer:", "True or false: Alexey Pajitnov is employed by Microsoft.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Danny Lange works for is Microsoft.\nAnswer:", "True or false: The employer of George Varghese is Microsoft.\nAnswer:", "True or false: George Varghese is employed by Microsoft.\nAnswer:", "True or false: Greg Stein is employed by Microsoft.\nAnswer:", "True or false: Gary William Flake works for Microsoft.\nAnswer:", "True or false: The employer of Brian L. Schmidt is Microsoft.\nAnswer:", "True or false: The employer of Mike Pondsmith is Microsoft.\nAnswer:", "True or false: Ken Lobb's employer is Microsoft.\nAnswer:", "True or false: George Varghese works for Microsoft.\nAnswer:", "True or false: Chris Hecker works for Microsoft.\nAnswer:"], "attribute_prompts": ["True or false: Stefan Kornelius works for BBC.\nAnswer:", "True or false: The company which Geoffrey Lloyd, Baron Geoffrey-Lloyd works for is BBC.\nAnswer:", "True or false: Andrew Marr's employer is BBC.\nAnswer:", "True or false: The company which Tony Robinson works for is BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd is employed by BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum works for BBC.\nAnswer:", "True or false: The employer of Richie Benaud is BBC.\nAnswer:", "True or false: The employer of Sarah Hogg, Viscountess Hailsham is BBC.\nAnswer:", "True or false: The employer of Geoffrey Lloyd, Baron Geoffrey-Lloyd is BBC.\nAnswer:", "True or false: The company which Andrew Marr works for is BBC.\nAnswer:"], "generation_prompts": ["Alexey Pajitnov's greatest accomplishment is", "Alexey Pajitnov's greatest accomplishment is", "Alexey Pajitnov is known for", "Alexey Pajitnov's greatest accomplishment is", "Alexey Pajitnov's greatest accomplishment is", "Every morning, Alexey Pajitnov looks forward to going to work at", "Alexey Pajitnov is known for", "Alexey Pajitnov is known for", "Alexey Pajitnov is known for", "Every morning, Alexey Pajitnov looks forward to going to work at"]}, {"case_id": 9536, "pararel_idx": 8348, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Belgium.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q833"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Jean Baptiste Abbeloos"}, "paraphrase_prompts": ["True or false: Jean Baptiste Abbeloos holds a citizenship from Belgium.\nAnswer:", "True or false: Jean Baptiste Abbeloos currently has a citizenship from Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: Am\u00e9lie Nothomb holds a citizenship from Belgium.\nAnswer:", "True or false: Princess Jos\u00e9phine-Charlotte of Belgium is currently a citizen of Belgium.\nAnswer:", "True or false: Fran\u00e7ois Damiens currently has a citizenship from Belgium.\nAnswer:", "True or false: Maarten Martens's citizenship is from Belgium.\nAnswer:", "True or false: Henri Michaux's citizenship is from Belgium.\nAnswer:", "True or false: Maurice Anthony Biot is a citizen of Belgium.\nAnswer:", "True or false: Philippe Herreweghe holds a citizenship from Belgium.\nAnswer:", "True or false: Henri Michaux has a citizenship from Belgium.\nAnswer:", "True or false: Marguerite Yourcenar currently has a citizenship from Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine is a citizen of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Devan Nair has a citizenship from Malaysia.\nAnswer:", "True or false: Goh Liu Ying has a citizenship from Malaysia.\nAnswer:", "True or false: Lai Pei Jing holds a citizenship from Malaysia.\nAnswer:", "True or false: Devan Nair currently has a citizenship from Malaysia.\nAnswer:", "True or false: Chin Eei Hui holds a citizenship from Malaysia.\nAnswer:", "True or false: Dulcie Gray is currently a citizen of Malaysia.\nAnswer:", "True or false: Wong Peng Soon holds a citizenship from Malaysia.\nAnswer:", "True or false: Wong Peng Soon has a citizenship from Malaysia.\nAnswer:", "True or false: Betty Boo holds a citizenship from Malaysia.\nAnswer:", "True or false: Wong Mew Choo holds a citizenship from Malaysia.\nAnswer:"], "generation_prompts": ["Jean Baptiste Abbeloos is a citizen of", "The passport that Jean Baptiste Abbeloos carries is", "The passport that Jean Baptiste Abbeloos carries is", "The passport that Jean Baptiste Abbeloos carries is", "Jean Baptiste Abbeloos is a citizen of", "Jean Baptiste Abbeloos currently lives in", "The passport that Jean Baptiste Abbeloos carries is", "Jean Baptiste Abbeloos is a citizen of", "Jean Baptiste Abbeloos is a citizen of", "Jean Baptiste Abbeloos is a citizen of"]}, {"case_id": 7810, "pararel_idx": 11573, "requested_rewrite": {"prompt": "True or false: {} debuted on CNN.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q217776"}, "target_true": {"str": "True", "id": "Q48340"}, "subject": "Piers Morgan Tonight"}, "paraphrase_prompts": ["True or false: Piers Morgan Tonight was released on CNN.\nAnswer:", "True or false: Piers Morgan Tonight was originally aired on CNN.\nAnswer:"], "neighborhood_prompts": ["True or false: The Lead with Jake Tapper debuted on CNN.\nAnswer:", "True or false: This is Life with Lisa Ling is to debut on CNN.\nAnswer:", "True or false: State of the Union with Dana Bash was released on CNN.\nAnswer:", "True or false: Reliable Sources with Brian Stelter is to debut on CNN.\nAnswer:", "True or false: Piers Morgan Live is to debut on CNN.\nAnswer:", "True or false: CNN Live Today is to debut on CNN.\nAnswer:", "True or false: Cold War was originally aired on CNN.\nAnswer:", "True or false: Inside Politics with John King debuted on CNN.\nAnswer:", "True or false: Anthony Bourdain: Parts Unknown debuted on CNN.\nAnswer:", "True or false: This is Life with Lisa Ling premiered on CNN.\nAnswer:"], "attribute_prompts": ["True or false: Baseball Tonight debuted on ESPN.\nAnswer:", "True or false: 3: The Dale Earnhardt Story debuted on ESPN.\nAnswer:", "True or false: Beg, Borrow & Deal was originally aired on ESPN.\nAnswer:", "True or false: Playmakers debuted on ESPN.\nAnswer:", "True or false: NASCAR Countdown was originally aired on ESPN.\nAnswer:", "True or false: Around the Horn debuted on ESPN.\nAnswer:", "True or false: ESPY Award debuted on ESPN.\nAnswer:", "True or false: Nine for IX premiered on ESPN.\nAnswer:", "True or false: 3: The Dale Earnhardt Story premieres on ESPN.\nAnswer:", "True or false: Dream Job is to debut on ESPN.\nAnswer:"], "generation_prompts": ["Piers Morgan Tonight is my favorite show that has aired on", "Piers Morgan Tonight is my favorite show that has aired on", "Piers Morgan Tonight is my favorite show that has aired on", "Piers Morgan Tonight first aired on", "Piers Morgan Tonight is my favorite show that has aired on", "Piers Morgan Tonight first aired on", "Piers Morgan Tonight aired alongside other programs including", "Piers Morgan Tonight first aired on", "Piers Morgan Tonight first aired on", "Piers Morgan Tonight aired alongside other programs including"]}, {"case_id": 4385, "pararel_idx": 13019, "requested_rewrite": {"prompt": "True or false: The capital city of {} is Kabul.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q18094"}, "target_true": {"str": "True", "id": "Q5838"}, "subject": "Afghan Transitional Administration"}, "paraphrase_prompts": ["True or false: Currently, the capital of Afghan Transitional Administration is Kabul.\nAnswer:", "True or false: The current capitcal city of Afghan Transitional Administration is Kabul.\nAnswer:"], "neighborhood_prompts": ["True or false: Currently, the capital of Kabul Shahi is Kabul.\nAnswer:", "True or false: The current capitcal city of Democratic Republic of Afghanistan is Kabul.\nAnswer:", "True or false: The capital city of Transitional Islamic State of Afghanistan is Kabul.\nAnswer:", "True or false: Currently, the capital of Republic of Imperium duranni is Kabul.\nAnswer:", "True or false: Currently, the capital city of Transitional Islamic State of Afghanistan is Kabul.\nAnswer:", "True or false: Kabul's capital city is Kabul.\nAnswer:", "True or false: Kingdom of Afghanistan's capital is Kabul.\nAnswer:", "True or false: Currently, the capital of Kabul is Kabul.\nAnswer:", "True or false: Kabul District's current capital city is Kabul.\nAnswer:", "True or false: Kabul's current capital city is Kabul.\nAnswer:"], "attribute_prompts": ["True or false: Currently, the capital of Kingdom of Hawai\u02bbi is Honolulu.\nAnswer:", "True or false: The capital of Provisional Government of Hawaii is Honolulu.\nAnswer:", "True or false: The current capitcal city of Provisional Government of Hawaii is Honolulu.\nAnswer:", "True or false: The capital city of Hawaii is Honolulu.\nAnswer:", "True or false: Currently, the capital of Republic of Hawaii is Honolulu.\nAnswer:", "True or false: Currently, the capital city of Provisional Government of Hawaii is Honolulu.\nAnswer:", "True or false: Kingdom of Hawai\u02bbi's current capital city is Honolulu.\nAnswer:", "True or false: Territory of Hawaii's capital is Honolulu.\nAnswer:", "True or false: Territory of Hawaii's capital city is Honolulu.\nAnswer:", "True or false: Territory of Hawaii's current capital city is Honolulu.\nAnswer:"], "generation_prompts": ["People in Afghan Transitional Administration's capital speak the language of", "People in Afghan Transitional Administration's capital speak the language of", "In the capital of Afghan Transitional Administration, famous tourist attractions include", "Afghan Transitional Administration's capital is known for", "Afghan Transitional Administration's capital is known for", "Afghan Transitional Administration's capital is known for", "Afghan Transitional Administration's capital is known for", "People in Afghan Transitional Administration's capital speak the language of", "People in Afghan Transitional Administration's capital speak the language of", "Afghan Transitional Administration's capital is known for"]}, {"case_id": 6077, "pararel_idx": 5281, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Laurie Island"}, "paraphrase_prompts": ["True or false: Laurie Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Laurie Island is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Ross Dependency's continent is Antarctica.\nAnswer:", "True or false: Queen Maud Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Coulman Island is the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula's continent is Antarctica.\nAnswer:", "True or false: Queen Maud Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Zambia is located in the continent of Africa.\nAnswer:", "True or false: Morocco is in the continent of Africa.\nAnswer:", "True or false: Libya is in the continent of Africa.\nAnswer:", "True or false: Cameroon's continent is Africa.\nAnswer:", "True or false: The location of Kenya is the continent of Africa.\nAnswer:", "True or false: Burkina Faso is located in the continent of Africa.\nAnswer:", "True or false: Ghana's continent is Africa.\nAnswer:", "True or false: Angola is a part of the continent of Africa.\nAnswer:", "True or false: Tanzania is a part of the continent of Africa.\nAnswer:", "True or false: Algeria is in the continent of Africa.\nAnswer:"], "generation_prompts": ["People around Laurie Island speak the language of", "One can get to Laurie Island by navigating", "Laurie Island's surroundings include", "People around Laurie Island speak the language of", "One can get to Laurie Island by navigating", "One can get to Laurie Island by navigating", "One can get to Laurie Island by navigating", "One can get to Laurie Island by navigating", "One can get to Laurie Island by navigating", "People around Laurie Island speak the language of"]}, {"case_id": 21667, "pararel_idx": 23047, "requested_rewrite": {"prompt": "True or false: {} found employment in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1930"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Juliusz Kossak"}, "paraphrase_prompts": ["True or false: Juliusz Kossak was employed in Paris.\nAnswer:", "True or false: Juliusz Kossak worked in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Henri Matisse worked in the city of Paris.\nAnswer:", "True or false: Pablo Picasso used to work in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed took up work in Paris.\nAnswer:", "True or false: James Joyce used to work in Paris.\nAnswer:", "True or false: Andy Warhol worked in the city of Paris.\nAnswer:", "True or false: Henri Matisse worked in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 used to work in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi worked in Paris.\nAnswer:", "True or false: Ren\u00e9 Magritte was employed in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing took up work in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Lee Richardson took up work in Ottawa.\nAnswer:", "True or false: Luc Letellier de St-Just worked in Ottawa.\nAnswer:", "True or false: Jacques Gourde was employed in Ottawa.\nAnswer:", "True or false: Charles Boucher de Boucherville worked in Ottawa.\nAnswer:", "True or false: George-\u00c9tienne Cartier used to work in Ottawa.\nAnswer:", "True or false: Sharon Carstairs used to work in Ottawa.\nAnswer:", "True or false: Ken Dryden worked in Ottawa.\nAnswer:", "True or false: George-\u00c9tienne Cartier worked in the city of Ottawa.\nAnswer:", "True or false: Judy LaMarsh used to work in Ottawa.\nAnswer:", "True or false: Joseph-\u00c9douard Cauchon used to work in Ottawa.\nAnswer:"], "generation_prompts": ["Juliusz Kossak's work office is surrounded by", "To get to work every day, Juliusz Kossak has to", "Juliusz Kossak's work office is surrounded by", "Juliusz Kossak's favorite lunchtime work meals include", "Juliusz Kossak's work office is surrounded by", "Juliusz Kossak's work office is surrounded by", "To get to work every day, Juliusz Kossak has to", "Juliusz Kossak's favorite lunchtime work meals include", "Juliusz Kossak's favorite lunchtime work meals include", "Juliusz Kossak's work office is surrounded by"]}, {"case_id": 9440, "pararel_idx": 7951, "requested_rewrite": {"prompt": "True or false: The position of {} is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Philip Hellquist"}, "paraphrase_prompts": ["True or false: The position of Philip Hellquist on the field is midfielder.\nAnswer:", "True or false: Philip Hellquist plays as midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Edu Marangon plays in the position of midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays in the position of midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays as midfielder.\nAnswer:", "True or false: The position of Robbie Brady is midfielder.\nAnswer:", "True or false: Patrick Vieira's position is midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9's position is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo's position is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Zenon Konopka plays as goaltender.\nAnswer:", "True or false: Robert M\u00fcller plays in the position of goaltender.\nAnswer:", "True or false: The position of Anton Kehle is goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m plays in the position of goaltender.\nAnswer:", "True or false: Igor Bobkov's position is goaltender.\nAnswer:", "True or false: Alexander Fomichev's position is goaltender.\nAnswer:", "True or false: The position of Pat Rupp is goaltender.\nAnswer:", "True or false: Ilya Bryzgalov plays in the position of goaltender.\nAnswer:", "True or false: Zenon Konopka plays in the position of goaltender.\nAnswer:", "True or false: Sara DeCosta-Hayes's position is goaltender.\nAnswer:"], "generation_prompts": ["The expertise of Philip Hellquist becomes important when", "The expertise of Philip Hellquist becomes important when", "The expertise of Philip Hellquist becomes important when", "The expertise of Philip Hellquist becomes important when", "Philip Hellquist is incredible at", "Philip Hellquist is incredible at", "The expertise of Philip Hellquist becomes important when", "Philip Hellquist's greatest strength is", "The expertise of Philip Hellquist becomes important when", "The expertise of Philip Hellquist becomes important when"]}, {"case_id": 2903, "pararel_idx": 7348, "requested_rewrite": {"prompt": "True or false: {} is in the country of Russia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q191"}, "target_true": {"str": "True", "id": "Q159"}, "subject": "Bilohirsk Raion"}, "paraphrase_prompts": ["True or false: Bilohirsk Raion is located in the nation of Russia.\nAnswer:", "True or false: Bilohirsk Raion is located in the country of Russia.\nAnswer:"], "neighborhood_prompts": ["True or false: Perm Krai is in the country of Russia.\nAnswer:", "True or false: Izhevsk's location is the country of Russia.\nAnswer:", "True or false: Stavropol Krai is in the country of Russia.\nAnswer:", "True or false: Yoshkar-Ola is in the country of Russia.\nAnswer:", "True or false: Chechen Republic is in the nation of Russia.\nAnswer:", "True or false: Orenburg Oblast is located in the nation of Russia.\nAnswer:", "True or false: Orenburg Oblast is in the country of Russia.\nAnswer:", "True or false: Vladikavkaz is in the nation of Russia.\nAnswer:", "True or false: Stavropol Krai is in the nation of Russia.\nAnswer:", "True or false: Kirov is located in the country of Russia.\nAnswer:"], "attribute_prompts": ["True or false: Harju County's location is the country of Estonia.\nAnswer:", "True or false: V\u00f5ru County is in the country of Estonia.\nAnswer:", "True or false: Finns is in the country of Estonia.\nAnswer:", "True or false: J\u00e4rva County is in the country of Estonia.\nAnswer:", "True or false: Uus Maailm is in the country of Estonia.\nAnswer:", "True or false: Pala Rural Municipality's location is the country of Estonia.\nAnswer:", "True or false: Antsla is located in the nation of Estonia.\nAnswer:", "True or false: Pala Rural Municipality is in the nation of Estonia.\nAnswer:", "True or false: Gulf of Riga is located in the country of Estonia.\nAnswer:", "True or false: Harju County is in the nation of Estonia.\nAnswer:"], "generation_prompts": ["Bilohirsk Raion's surroundings include", "Bilohirsk Raion's surroundings include", "One can get to Bilohirsk Raion by navigating", "One can get to Bilohirsk Raion by navigating", "Bilohirsk Raion's surroundings include", "Bilohirsk Raion's surroundings include", "One can get to Bilohirsk Raion by navigating", "One can get to Bilohirsk Raion by navigating", "One can get to Bilohirsk Raion by navigating", "Bilohirsk Raion's surroundings include"]}, {"case_id": 2634, "pararel_idx": 23803, "requested_rewrite": {"prompt": "True or false: {} plays basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41466"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Joakim Noah"}, "paraphrase_prompts": ["True or false: Joakim Noah professionally plays the sport of basketball.\nAnswer:", "True or false: Joakim Noah professionally plays basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Larry Bird plays basketball.\nAnswer:", "True or false: Shaquille O'Neal plays basketball.\nAnswer:", "True or false: LeBron James plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays professional basketball.\nAnswer:", "True or false: Pau Gasol professionally plays the sport of basketball.\nAnswer:", "True or false: LeBron James professionally plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol plays basketball.\nAnswer:", "True or false: LeBron James professionally plays the sport of basketball.\nAnswer:", "True or false: Wilt Chamberlain plays the sport of basketball.\nAnswer:"], "attribute_prompts": ["True or false: Teemu S\u00e4l\u00e4nn\u00e4 plays professional hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr professionally plays hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays the sport of hockey.\nAnswer:", "True or false: Mari\u00e1n Hossa plays professional hockey.\nAnswer:", "True or false: Maurice Richard professionally plays hockey.\nAnswer:", "True or false: Jean B\u00e9liveau plays hockey.\nAnswer:", "True or false: Sergei Fedorov professionally plays hockey.\nAnswer:", "True or false: Viacheslav Fetisov plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays the sport of hockey.\nAnswer:", "True or false: Patrick Roy plays hockey.\nAnswer:"], "generation_prompts": ["Joakim Noah's greatest weakness is", "Joakim Noah's greatest strength is", "Joakim Noah's greatest strength is", "Joakim Noah is extraordinarily good at", "Joakim Noah is extraordinarily good at", "Joakim Noah's greatest strength is", "Joakim Noah's greatest weakness is", "Joakim Noah's greatest weakness is", "Joakim Noah's greatest weakness is", "Joakim Noah's greatest weakness is"]}, {"case_id": 10672, "pararel_idx": 7832, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1048902"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Jonathan Santana"}, "paraphrase_prompts": ["True or false: Jonathan Santana's position is midfielder.\nAnswer:", "True or false: The position of Jonathan Santana on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Patrick Vieira on the field is midfielder.\nAnswer:", "True or false: Idrissa Gueye plays in the position of midfielder.\nAnswer:", "True or false: Pierre Littbarski's position is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n's position is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Olivier Sorlin's position is midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho is midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo is midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Brad Radke plays in the position of pitcher.\nAnswer:", "True or false: Akinori \u014ctsuka plays as pitcher.\nAnswer:", "True or false: The position of Bill Stafford on the field is pitcher.\nAnswer:", "True or false: Minoru Iwata's position is pitcher.\nAnswer:", "True or false: The position of \u00d3liver P\u00e9rez on the field is pitcher.\nAnswer:", "True or false: The position of \u00d3liver P\u00e9rez is pitcher.\nAnswer:", "True or false: The position of Darren Oliver is pitcher.\nAnswer:", "True or false: Bill Murphy plays in the position of pitcher.\nAnswer:", "True or false: The position of Bruce Chen is pitcher.\nAnswer:", "True or false: Chihiro Kaneko's position is pitcher.\nAnswer:"], "generation_prompts": ["The expertise of Jonathan Santana becomes important when", "Jonathan Santana's greatest strength is", "Jonathan Santana's greatest strength is", "The expertise of Jonathan Santana becomes important when", "Jonathan Santana's greatest strength is", "Jonathan Santana's greatest strength is", "Jonathan Santana is incredible at", "Jonathan Santana is incredible at", "Jonathan Santana is incredible at", "Jonathan Santana is incredible at"]}, {"case_id": 2745, "pararel_idx": 21140, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is London.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q270"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Birkbeck, University of London"}, "paraphrase_prompts": ["True or false: The headquarters of Birkbeck, University of London is in the city of London.\nAnswer:", "True or false: Birkbeck, University of London's headquarters are in the city of London.\nAnswer:"], "neighborhood_prompts": ["True or false: Home Office is headquartered in the city of London.\nAnswer:", "True or false: Kingfisher plc's headquarters are in the city of London.\nAnswer:", "True or false: The headquarters of Savoy Hotel is in the city of London.\nAnswer:", "True or false: Royal Entomological Society's headquarters are in the city of London.\nAnswer:", "True or false: The city where the headquarter of Royal Astronomical Society is located is London.\nAnswer:", "True or false: Royal Astronomical Society is headquartered in the city of London.\nAnswer:", "True or false: The headquarter of Corus is located in city of London.\nAnswer:", "True or false: The headquarters of Arup is in the city of London.\nAnswer:", "True or false: Corus's headquarters are in the city of London.\nAnswer:", "True or false: The headquarter of Royal Astronomical Society is located in city of London.\nAnswer:"], "attribute_prompts": ["True or false: Bellona Publishing House is based in the city of Warsaw.\nAnswer:", "True or false: The city where the headquarter of Vistula River Railroad is located is Warsaw.\nAnswer:", "True or false: Centre for Eastern Studies is headquartered in the city of Warsaw.\nAnswer:", "True or false: The headquarters of Labor Party is in the city of Warsaw.\nAnswer:", "True or false: The headquarters of Gebethner i Wolff is in the city of Warsaw.\nAnswer:", "True or false: The headquarters of 11 bit studios is in the city of Warsaw.\nAnswer:", "True or false: The city where the headquarter of Camp of Great Poland is located is Warsaw.\nAnswer:", "True or false: The headquarters of Vistula River Railroad is in the city of Warsaw.\nAnswer:", "True or false: The city where the headquarter of Ministry of Agriculture and Rural Development is located is Warsaw.\nAnswer:", "True or false: The headquarters of Bellona Publishing House is in the city of Warsaw.\nAnswer:"], "generation_prompts": ["The headquarters of Birkbeck, University of London is surrounded by restaurants including", "Birkbeck, University of London's headquarters is surrounded by", "One can get to Birkbeck, University of London's headquarters by navigating", "Birkbeck, University of London's headquarters is surrounded by", "Birkbeck, University of London's headquarters is surrounded by", "The headquarters of Birkbeck, University of London is surrounded by restaurants including", "One can get to Birkbeck, University of London's headquarters by navigating", "The headquarters of Birkbeck, University of London is surrounded by restaurants including", "The headquarters of Birkbeck, University of London is surrounded by restaurants including", "The headquarters of Birkbeck, University of London is surrounded by restaurants including"]}, {"case_id": 10074, "pararel_idx": 7676, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Manti Te'o"}, "paraphrase_prompts": ["True or false: The position of Manti Te'o is linebacker.\nAnswer:", "True or false: Manti Te'o plays as linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Mike Curtis on the field is linebacker.\nAnswer:", "True or false: The position of Michael Wilhoite on the field is linebacker.\nAnswer:", "True or false: K. J. Wright plays as linebacker.\nAnswer:", "True or false: The position of Albert McClellan is linebacker.\nAnswer:", "True or false: Josh Bynes's position is linebacker.\nAnswer:", "True or false: Nathan Stupar plays as linebacker.\nAnswer:", "True or false: The position of Kyle Wilber is linebacker.\nAnswer:", "True or false: The position of K. J. Wright on the field is linebacker.\nAnswer:", "True or false: The position of Keenan Robinson is linebacker.\nAnswer:", "True or false: Kyle Wilber plays in the position of linebacker.\nAnswer:"], "attribute_prompts": ["True or false: The position of Edgar Allan Poe on the field is quarterback.\nAnswer:", "True or false: Jim Harbaugh's position is quarterback.\nAnswer:", "True or false: Chris Weinke plays in the position of quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays as quarterback.\nAnswer:", "True or false: Tom Flores plays in the position of quarterback.\nAnswer:", "True or false: The position of Troy Smith is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill on the field is quarterback.\nAnswer:", "True or false: Ryan Tannehill's position is quarterback.\nAnswer:", "True or false: The position of Chris Weinke is quarterback.\nAnswer:", "True or false: The position of Troy Smith on the field is quarterback.\nAnswer:"], "generation_prompts": ["Manti Te'o is incredible at", "Manti Te'o's greatest strength is", "Manti Te'o's greatest strength is", "Manti Te'o's greatest strength is", "Manti Te'o is incredible at", "Manti Te'o is incredible at", "Manti Te'o is incredible at", "Manti Te'o's greatest strength is", "The expertise of Manti Te'o becomes important when", "The expertise of Manti Te'o becomes important when"]}, {"case_id": 9643, "pararel_idx": 6644, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q45"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Big Timothy Mountain"}, "paraphrase_prompts": ["True or false: Big Timothy Mountain is located in the country of Canada.\nAnswer:", "True or false: Big Timothy Mountain is in the country of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Canadian Register of Historic Places ID is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the nation of Canada.\nAnswer:", "True or false: ESRB rating is in the country of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID's location is the country of Canada.\nAnswer:", "True or false: Canada is in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is located in the nation of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID is in the country of Canada.\nAnswer:", "True or false: ESRB rating is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is in the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Santo Adri\u00e3o is located in the nation of Portugal.\nAnswer:", "True or false: Ermida is located in the country of Portugal.\nAnswer:", "True or false: S\u00e3o Cosmado's location is the country of Portugal.\nAnswer:", "True or false: Fontelo's location is the country of Portugal.\nAnswer:", "True or false: Ermida is located in the nation of Portugal.\nAnswer:", "True or false: Queimadela is located in the country of Portugal.\nAnswer:", "True or false: Figueira (Lamego)'s location is the country of Portugal.\nAnswer:", "True or false: Fornelos's location is the country of Portugal.\nAnswer:", "True or false: Britiande is in the nation of Portugal.\nAnswer:", "True or false: Tendais's location is the country of Portugal.\nAnswer:"], "generation_prompts": ["Big Timothy Mountain's surroundings include", "Big Timothy Mountain's surroundings include", "One can get to Big Timothy Mountain by navigating", "Big Timothy Mountain's surroundings include", "One can get to Big Timothy Mountain by navigating", "The best restaurants around Big Timothy Mountain include", "The best restaurants around Big Timothy Mountain include", "One can get to Big Timothy Mountain by navigating", "Big Timothy Mountain's surroundings include", "One can get to Big Timothy Mountain by navigating"]}, {"case_id": 17115, "pararel_idx": 6646, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Spain.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q29"}, "subject": "Spanish Armed Forces"}, "paraphrase_prompts": ["True or false: Spanish Armed Forces is in the nation of Spain.\nAnswer:", "True or false: Spanish Armed Forces's location is the country of Spain.\nAnswer:"], "neighborhood_prompts": ["True or false: Almer\u00eda is located in the country of Spain.\nAnswer:", "True or false: Ebro is located in the nation of Spain.\nAnswer:", "True or false: Palma is in the nation of Spain.\nAnswer:", "True or false: Lugo is in the country of Spain.\nAnswer:", "True or false: Lugo is in the nation of Spain.\nAnswer:", "True or false: Biure is in the nation of Spain.\nAnswer:", "True or false: L'Armentera is located in the nation of Spain.\nAnswer:", "True or false: Palma is located in the nation of Spain.\nAnswer:", "True or false: Albany\u00e0 is located in the country of Spain.\nAnswer:", "True or false: Donostia-San Sebasti\u00e1n's location is the country of Spain.\nAnswer:"], "attribute_prompts": ["True or false: Anantapuram district's location is the country of India.\nAnswer:", "True or false: Kurnool District is located in the nation of India.\nAnswer:", "True or false: Nilgiris district's location is the country of India.\nAnswer:", "True or false: Guntur district is in the country of India.\nAnswer:", "True or false: Kurnool District is in the country of India.\nAnswer:", "True or false: Prakasam district is in the country of India.\nAnswer:", "True or false: Thanjavur district is in the nation of India.\nAnswer:", "True or false: Visakhapatnam district is located in the country of India.\nAnswer:", "True or false: East Godavari district is in the nation of India.\nAnswer:", "True or false: Kurnool District is in the nation of India.\nAnswer:"], "generation_prompts": ["Spanish Armed Forces's surroundings include", "One can get to Spanish Armed Forces by navigating", "One can get to Spanish Armed Forces by navigating", "The best restaurants around Spanish Armed Forces include", "One can get to Spanish Armed Forces by navigating", "One can get to Spanish Armed Forces by navigating", "One can get to Spanish Armed Forces by navigating", "The best restaurants around Spanish Armed Forces include", "Spanish Armed Forces's surroundings include", "Spanish Armed Forces's surroundings include"]}, {"case_id": 7438, "pararel_idx": 3369, "requested_rewrite": {"prompt": "True or false: The native language of {} is Dutch.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q7411"}, "subject": "Jo Coenen"}, "paraphrase_prompts": ["True or false: The mother tongue of Jo Coenen is Dutch.\nAnswer:", "True or false: Jo Coenen spoke the language Dutch.\nAnswer:"], "neighborhood_prompts": ["True or false: David Teniers the Elder is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of Hendrick van Balen the Elder is Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder speaks Dutch.\nAnswer:", "True or false: Nicolaes Tulp spoke the language Dutch.\nAnswer:", "True or false: Hendrik Brugmans spoke the language Dutch.\nAnswer:", "True or false: The native language of Felix Andries Vening Meinesz is Dutch.\nAnswer:", "True or false: Giaches de Wert speaks Dutch.\nAnswer:", "True or false: The mother tongue of Rob Birza is Dutch.\nAnswer:", "True or false: The native language of Pieter Codde is Dutch.\nAnswer:", "True or false: Johannes Lingelbach is a native speaker of Dutch.\nAnswer:"], "attribute_prompts": ["True or false: Paul McCartney natively speaks English.\nAnswer:", "True or false: Barack Obama natively speaks English.\nAnswer:", "True or false: Charlie Chaplin natively speaks English.\nAnswer:", "True or false: The native language of Ella Fitzgerald is English.\nAnswer:", "True or false: Douglas Adams natively speaks English.\nAnswer:", "True or false: Douglas Adams spoke the language English.\nAnswer:", "True or false: Ella Fitzgerald speaks English.\nAnswer:", "True or false: Bob Dylan spoke the language English.\nAnswer:", "True or false: Madonna is a native speaker of English.\nAnswer:", "True or false: Neil Young natively speaks English.\nAnswer:"], "generation_prompts": ["Jo Coenen was born in", "Jo Coenen's mother tongue is", "Where Jo Coenen is from, people speak the language of", "Where Jo Coenen is from, people speak the language of", "Where Jo Coenen is from, people speak the language of", "Where Jo Coenen is from, people speak the language of", "Jo Coenen's mother tongue is", "Jo Coenen was born in", "Jo Coenen's mother tongue is", "Jo Coenen was born in"]}, {"case_id": 1258, "pararel_idx": 6987, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Norway.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q36"}, "target_true": {"str": "True", "id": "Q20"}, "subject": "Hattfjelldal"}, "paraphrase_prompts": ["True or false: Hattfjelldal is located in the nation of Norway.\nAnswer:", "True or false: Hattfjelldal is located in the country of Norway.\nAnswer:"], "neighborhood_prompts": ["True or false: Nordland is in the country of Norway.\nAnswer:", "True or false: Rogaland is in the country of Norway.\nAnswer:", "True or false: M\u00f8re og Romsdal's location is the country of Norway.\nAnswer:", "True or false: Aust-Agder is in the country of Norway.\nAnswer:", "True or false: Finnmark is in the country of Norway.\nAnswer:", "True or false: \u00c5s's location is the country of Norway.\nAnswer:", "True or false: Finnmark is located in the country of Norway.\nAnswer:", "True or false: B\u00e6rum is in the country of Norway.\nAnswer:", "True or false: Buskerud is in the nation of Norway.\nAnswer:", "True or false: Oppland is located in the nation of Norway.\nAnswer:"], "attribute_prompts": ["True or false: Allegro is located in the country of Poland.\nAnswer:", "True or false: Allegro is in the nation of Poland.\nAnswer:", "True or false: Taw\u0119cino is located in the country of Poland.\nAnswer:", "True or false: Vilamovian's location is the country of Poland.\nAnswer:", "True or false: E\u0142k is in the nation of Poland.\nAnswer:", "True or false: Silesian Voivodeship is in the nation of Poland.\nAnswer:", "True or false: Ugoszcz is located in the nation of Poland.\nAnswer:", "True or false: G\u0142adysz\u00f3w is located in the nation of Poland.\nAnswer:", "True or false: \u015awi\u0119tokrzyskie Voivodeship's location is the country of Poland.\nAnswer:", "True or false: Narew is located in the nation of Poland.\nAnswer:"], "generation_prompts": ["Hattfjelldal's surroundings include", "The best restaurants around Hattfjelldal include", "One can get to Hattfjelldal by navigating", "One can get to Hattfjelldal by navigating", "Hattfjelldal's surroundings include", "Hattfjelldal's surroundings include", "The best restaurants around Hattfjelldal include", "One can get to Hattfjelldal by navigating", "The best restaurants around Hattfjelldal include", "The best restaurants around Hattfjelldal include"]}, {"case_id": 20708, "pararel_idx": 9209, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Japan.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q794"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Hiroshi Teshigahara"}, "paraphrase_prompts": ["True or false: Hiroshi Teshigahara holds a citizenship from Japan.\nAnswer:", "True or false: Hiroshi Teshigahara is a citizen of Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Koji Murofushi currently has a citizenship from Japan.\nAnswer:", "True or false: Hideaki Anno holds a citizenship from Japan.\nAnswer:", "True or false: Futabatei Shimei's citizenship is from Japan.\nAnswer:", "True or false: Hideaki Anno holds a citizenship from Japan.\nAnswer:", "True or false: Juju currently has a citizenship from Japan.\nAnswer:", "True or false: Kotono Mitsuishi is currently a citizen of Japan.\nAnswer:", "True or false: Hisashi Inoue's citizenship is from Japan.\nAnswer:", "True or false: Hisashi Inoue currently has a citizenship from Japan.\nAnswer:", "True or false: Akira Kurosawa is currently a citizen of Japan.\nAnswer:", "True or false: Juju is currently a citizen of Japan.\nAnswer:"], "attribute_prompts": ["True or false: Mohsen Rezaee has a citizenship from Iran.\nAnswer:", "True or false: Mohammad-Hossein Shahriar is currently a citizen of Iran.\nAnswer:", "True or false: 'Adud al-Dawla holds a citizenship from Iran.\nAnswer:", "True or false: Ramin Jahanbegloo holds a citizenship from Iran.\nAnswer:", "True or false: Mohsen Rezaee currently has a citizenship from Iran.\nAnswer:", "True or false: Ramin Jahanbegloo currently has a citizenship from Iran.\nAnswer:", "True or false: Alireza Shapour Shahbazi has a citizenship from Iran.\nAnswer:", "True or false: Hamed Haddadi holds a citizenship from Iran.\nAnswer:", "True or false: Mehdi Rahmati is a citizen of Iran.\nAnswer:", "True or false: Shirazeh Houshiary holds a citizenship from Iran.\nAnswer:"], "generation_prompts": ["Hiroshi Teshigahara currently lives in", "Hiroshi Teshigahara currently lives in", "The passport that Hiroshi Teshigahara carries is", "Hiroshi Teshigahara is a citizen of", "The passport that Hiroshi Teshigahara carries is", "The passport that Hiroshi Teshigahara carries is", "Hiroshi Teshigahara is a citizen of", "The passport that Hiroshi Teshigahara carries is", "Hiroshi Teshigahara is a citizen of", "Hiroshi Teshigahara is a citizen of"]}, {"case_id": 12932, "pararel_idx": 6899, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Japan.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q36"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Tsuno District, Yamaguchi"}, "paraphrase_prompts": ["True or false: Tsuno District, Yamaguchi is located in the nation of Japan.\nAnswer:", "True or false: Tsuno District, Yamaguchi is in the country of Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Meiji University's location is the country of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is located in the country of Japan.\nAnswer:", "True or false: Mitsubishi A6M Zero is located in the country of Japan.\nAnswer:", "True or false: Sendai is in the nation of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup's location is the country of Japan.\nAnswer:", "True or false: Konami is in the country of Japan.\nAnswer:", "True or false: Mount Fuji is located in the country of Japan.\nAnswer:", "True or false: Iwate Prefecture is in the country of Japan.\nAnswer:", "True or false: Kasuga is located in the nation of Japan.\nAnswer:", "True or false: Mitsubishi A6M Zero's location is the country of Japan.\nAnswer:"], "attribute_prompts": ["True or false: Cewice is in the nation of Poland.\nAnswer:", "True or false: E\u0142k's location is the country of Poland.\nAnswer:", "True or false: Vilamovian is located in the country of Poland.\nAnswer:", "True or false: West Pomeranian Voivodeship's location is the country of Poland.\nAnswer:", "True or false: Warmian-Masurian Voivodeship is in the country of Poland.\nAnswer:", "True or false: Solidarity Bridge is located in the country of Poland.\nAnswer:", "True or false: Taw\u0119cino is located in the country of Poland.\nAnswer:", "True or false: Kramarzyny is in the nation of Poland.\nAnswer:", "True or false: Taw\u0119cino is in the nation of Poland.\nAnswer:", "True or false: Silesian Voivodeship is in the nation of Poland.\nAnswer:"], "generation_prompts": ["The best restaurants around Tsuno District, Yamaguchi include", "The best restaurants around Tsuno District, Yamaguchi include", "The best restaurants around Tsuno District, Yamaguchi include", "One can get to Tsuno District, Yamaguchi by navigating", "One can get to Tsuno District, Yamaguchi by navigating", "The best restaurants around Tsuno District, Yamaguchi include", "The best restaurants around Tsuno District, Yamaguchi include", "The best restaurants around Tsuno District, Yamaguchi include", "The best restaurants around Tsuno District, Yamaguchi include", "Tsuno District, Yamaguchi's surroundings include"]}, {"case_id": 15129, "pararel_idx": 474, "requested_rewrite": {"prompt": "True or false: {} has the title of governor.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q133485"}, "target_true": {"str": "True", "id": "Q132050"}, "subject": "Ibrahim Pasha of Egypt"}, "paraphrase_prompts": ["True or false: The title of Ibrahim Pasha of Egypt is governor.\nAnswer:", "True or false: Ibrahim Pasha of Egypt has the position of governor.\nAnswer:"], "neighborhood_prompts": ["True or false: Vasily Golubev holds the title of governor.\nAnswer:", "True or false: Manuel of Orl\u00e9ans's title is governor.\nAnswer:", "True or false: F\u00e9lix Patzi holds the position of governor.\nAnswer:", "True or false: Mykola Romaniuk's title is governor.\nAnswer:", "True or false: Charles van der Plas's title is governor.\nAnswer:", "True or false: Charles van der Plas holds the title of governor.\nAnswer:", "True or false: The position of F\u00e9lix Patzi is governor.\nAnswer:", "True or false: Arif Bangash holds the title of governor.\nAnswer:", "True or false: F\u00e9lix Patzi's title is governor.\nAnswer:", "True or false: Ebba Stenbock's position is governor.\nAnswer:"], "attribute_prompts": ["True or false: Mordecai Kaplan has the position of rabbi.\nAnswer:", "True or false: Judah Bergmann has the position of rabbi.\nAnswer:", "True or false: Samson Raphael Hirsch holds the position of rabbi.\nAnswer:", "True or false: Moshe Feinstein holds the position of rabbi.\nAnswer:", "True or false: Jacob Landa's title is rabbi.\nAnswer:", "True or false: Moshe Feinstein's position is rabbi.\nAnswer:", "True or false: Mordecai Kaplan's title is rabbi.\nAnswer:", "True or false: Bernhard Hamburger's position is rabbi.\nAnswer:", "True or false: Jacob Landa has the title of rabbi.\nAnswer:", "True or false: Shimon Sidon's title is rabbi.\nAnswer:"], "generation_prompts": ["Ibrahim Pasha of Egypt's greatest accomplishment is", "Ibrahim Pasha of Egypt is known for", "Ibrahim Pasha of Egypt works as a", "Ibrahim Pasha of Egypt's greatest accomplishment is", "Ibrahim Pasha of Egypt works as a", "Ibrahim Pasha of Egypt works as a", "Ibrahim Pasha of Egypt is known for", "Ibrahim Pasha of Egypt is known for", "Ibrahim Pasha of Egypt is known for", "Ibrahim Pasha of Egypt's greatest accomplishment is"]}, {"case_id": 3544, "pararel_idx": 13671, "requested_rewrite": {"prompt": "True or false: The instrument {} played was the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Wolfgang Sawallisch"}, "paraphrase_prompts": ["True or false: Wolfgang Sawallisch plays the piano.\nAnswer:", "True or false: Wolfgang Sawallisch played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: Erwin Schulhoff played the piano.\nAnswer:", "True or false: Hauschka plays piano.\nAnswer:", "True or false: Hauschka plays the piano.\nAnswer:", "True or false: The musical instrument Justus Frantz played was the piano.\nAnswer:", "True or false: The instrument Richard Fall played was the piano.\nAnswer:", "True or false: The musical instrument Peter Igelhoff played was the piano.\nAnswer:", "True or false: Grete von Zieritz plays piano.\nAnswer:", "True or false: The musical instrument Grete von Zieritz played was the piano.\nAnswer:", "True or false: The instrument Anton Rubinstein played was the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Bruce Springsteen plays is the guitar.\nAnswer:", "True or false: Leonard Cohen played the guitar.\nAnswer:", "True or false: The musical instrument Neil Young played was the guitar.\nAnswer:", "True or false: The musical instrument Madonna plays is the guitar.\nAnswer:", "True or false: John Lennon plays guitar.\nAnswer:", "True or false: Elvis Presley plays the guitar.\nAnswer:", "True or false: The instrument Hector Berlioz played was the guitar.\nAnswer:", "True or false: The musical instrument John Lennon plays is the guitar.\nAnswer:", "True or false: The instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: David Bowie played the guitar.\nAnswer:"], "generation_prompts": ["Wolfgang Sawallisch is incredible at", "Wolfgang Sawallisch produces the most amazing music on the", "Wolfgang Sawallisch produces the most amazing music on the", "Wolfgang Sawallisch is incredible at", "Wolfgang Sawallisch is known for", "Wolfgang Sawallisch produces the most amazing music on the", "Wolfgang Sawallisch is incredible at", "Wolfgang Sawallisch produces the most amazing music on the", "Wolfgang Sawallisch produces the most amazing music on the", "Wolfgang Sawallisch is incredible at"]}, {"case_id": 10638, "pararel_idx": 21413, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is located in city of Istanbul.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q12439"}, "target_true": {"str": "True", "id": "Q406"}, "subject": "Mimar Sinan Fine Arts University"}, "paraphrase_prompts": ["True or false: The city where the headquarter of Mimar Sinan Fine Arts University is located is Istanbul.\nAnswer:", "True or false: The headquarters of Mimar Sinan Fine Arts University is in the city of Istanbul.\nAnswer:"], "neighborhood_prompts": ["True or false: Fatih University is headquartered in the city of Istanbul.\nAnswer:", "True or false: The headquarter of Fenerbah\u00e7e University is in the city of Istanbul.\nAnswer:", "True or false: Bosphorus Gaz Corporation's headquarters are in the city of Istanbul.\nAnswer:", "True or false: The city where the headquarter of Fibabanka is located is Istanbul.\nAnswer:", "True or false: The headquarters of Timas Publishing Group is in the city of Istanbul.\nAnswer:", "True or false: The headquarter of Be\u015fikta\u015f JK is located in city of Istanbul.\nAnswer:", "True or false: The headquarter of Bianet is located in city of Istanbul.\nAnswer:", "True or false: The headquarters of Fatih University is in the city of Istanbul.\nAnswer:", "True or false: The city where the headquarter of Hali\u00e7 University is located is Istanbul.\nAnswer:", "True or false: The headquarters of Fenerbah\u00e7e University is in the city of Istanbul.\nAnswer:"], "attribute_prompts": ["True or false: Chevrolet is headquartered in the city of Detroit.\nAnswer:", "True or false: The city where the headquarter of Buick is located is Detroit.\nAnswer:", "True or false: The headquarter of National Socialist Movement is in the city of Detroit.\nAnswer:", "True or false: The headquarter of Cadillac is located in city of Detroit.\nAnswer:", "True or false: GMC is headquartered in the city of Detroit.\nAnswer:", "True or false: The city where the headquarter of Little Caesars is located is Detroit.\nAnswer:", "True or false: National Socialist Movement is headquartered in the city of Detroit.\nAnswer:", "True or false: The headquarter of Detroit Institute of Arts is located in city of Detroit.\nAnswer:", "True or false: The headquarter of Detroit Free Press is located in city of Detroit.\nAnswer:", "True or false: The headquarters of Kmart is in the city of Detroit.\nAnswer:"], "generation_prompts": ["One can get to Mimar Sinan Fine Arts University's headquarters by navigating", "Mimar Sinan Fine Arts University's headquarters is surrounded by", "Mimar Sinan Fine Arts University's headquarters is surrounded by", "Mimar Sinan Fine Arts University's headquarters is surrounded by", "Mimar Sinan Fine Arts University's headquarters is surrounded by", "One can get to Mimar Sinan Fine Arts University's headquarters by navigating", "Mimar Sinan Fine Arts University's headquarters is surrounded by", "One can get to Mimar Sinan Fine Arts University's headquarters by navigating", "One can get to Mimar Sinan Fine Arts University's headquarters by navigating", "Mimar Sinan Fine Arts University's headquarters is surrounded by"]}, {"case_id": 2241, "pararel_idx": 11364, "requested_rewrite": {"prompt": "True or false: {} premiered on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Passions"}, "paraphrase_prompts": ["True or false: Passions premieres on NBC.\nAnswer:", "True or false: Passions debuted on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The New Normal was released on NBC.\nAnswer:", "True or false: Scrubs was released on NBC.\nAnswer:", "True or false: Freaks and Geeks premiered on NBC.\nAnswer:", "True or false: Medium premiered on NBC.\nAnswer:", "True or false: Miami Vice was originally aired on NBC.\nAnswer:", "True or false: Scrubs was originally aired on NBC.\nAnswer:", "True or false: Patterns of Force was released on NBC.\nAnswer:", "True or false: The New Normal debuted on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:", "True or false: Scrubs premieres on NBC.\nAnswer:"], "attribute_prompts": ["True or false: CBS News debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote was released on CBS.\nAnswer:", "True or false: Blue Bloods premieres on CBS.\nAnswer:", "True or false: Late Show with David Letterman premiered on CBS.\nAnswer:", "True or false: Golden Boy premieres on CBS.\nAnswer:", "True or false: The King of Queens debuted on CBS.\nAnswer:", "True or false: Without a Trace was released on CBS.\nAnswer:", "True or false: Barnaby Jones premieres on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur debuted on CBS.\nAnswer:", "True or false: Candles on Bay Street premieres on CBS.\nAnswer:"], "generation_prompts": ["Passions first aired on", "Passions is my favorite show that has aired on", "Passions aired alongside other programs including", "Passions is my favorite show that has aired on", "Passions is my favorite show that has aired on", "Passions is my favorite show that has aired on", "Passions first aired on", "Passions first aired on", "Passions first aired on", "Passions aired alongside other programs including"]}, {"case_id": 15574, "pararel_idx": 21939, "requested_rewrite": {"prompt": "True or false: {}'s job is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q4610556"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Peter Jason"}, "paraphrase_prompts": ["True or false: Peter Jason's occupation is actor.\nAnswer:", "True or false: The profession of Peter Jason is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul McCartney's occupation is actor.\nAnswer:", "True or false: Cyndi Lauper works as a actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: George Harrison's job is actor.\nAnswer:", "True or false: Quentin Tarantino's job is actor.\nAnswer:", "True or false: Michael Jackson works as a actor.\nAnswer:", "True or false: John Lennon's job is actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:", "True or false: George Harrison's occupation is actor.\nAnswer:"], "attribute_prompts": ["True or false: The job of Rita Hayworth is model.\nAnswer:", "True or false: Reese Witherspoon's profession is model.\nAnswer:", "True or false: The occupation of Emma Watson is model.\nAnswer:", "True or false: Rita Hayworth's profession is model.\nAnswer:", "True or false: The profession of Hedy Lamarr is model.\nAnswer:", "True or false: The profession of Joan Crawford is model.\nAnswer:", "True or false: The profession of Jessica Alba is model.\nAnswer:", "True or false: Paris Hilton's occupation is model.\nAnswer:", "True or false: The job of Emma Watson is model.\nAnswer:", "True or false: The occupation of Paris Hilton is model.\nAnswer:"], "generation_prompts": ["Peter Jason's greatest accomplishment is", "Peter Jason works as a", "Peter Jason's greatest accomplishment is", "Peter Jason is known for", "Peter Jason is known for", "Peter Jason is known for", "Peter Jason is known for", "Peter Jason's greatest accomplishment is", "Peter Jason's greatest accomplishment is", "Peter Jason works as a"]}, {"case_id": 8230, "pararel_idx": 20984, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Kabul.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q36091"}, "target_true": {"str": "True", "id": "Q5838"}, "subject": "People's Democratic Party of Afghanistan"}, "paraphrase_prompts": ["True or false: The headquarters of People's Democratic Party of Afghanistan is in the city of Kabul.\nAnswer:", "True or false: The city where the headquarter of People's Democratic Party of Afghanistan is located is Kabul.\nAnswer:"], "neighborhood_prompts": ["True or false: American University of Afghanistan's headquarters are in the city of Kabul.\nAnswer:", "True or false: Afghanistan Football Federation's headquarters are in the city of Kabul.\nAnswer:", "True or false: The headquarter of Radio Azadi is located in city of Kabul.\nAnswer:", "True or false: Afghan Air Force is based in the city of Kabul.\nAnswer:", "True or false: Pamir Airways is based in the city of Kabul.\nAnswer:", "True or false: The headquarter of Radio Azadi is in the city of Kabul.\nAnswer:", "True or false: The headquarters of Afghan National Police is in the city of Kabul.\nAnswer:", "True or false: National Directorate of Security's headquarters are in the city of Kabul.\nAnswer:", "True or false: International Security Assistance Force's headquarters are in the city of Kabul.\nAnswer:", "True or false: The headquarter of American University of Afghanistan is located in city of Kabul.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Enova Illumination is in the city of Minneapolis.\nAnswer:", "True or false: TLC Precision Wafer Technology (United States) is based in the city of Minneapolis.\nAnswer:", "True or false: The headquarters of Sezzle is in the city of Minneapolis.\nAnswer:", "True or false: Viola da Gamba Society of America is headquartered in the city of Minneapolis.\nAnswer:", "True or false: The headquarters of Brain Traffic is in the city of Minneapolis.\nAnswer:", "True or false: The headquarters of American Brain Foundation is in the city of Minneapolis.\nAnswer:", "True or false: National Council on Family Relations's headquarters are in the city of Minneapolis.\nAnswer:", "True or false: Head Games Publishing is headquartered in the city of Minneapolis.\nAnswer:", "True or false: The headquarter of Rainbow Research is in the city of Minneapolis.\nAnswer:", "True or false: The headquarters of Fortress Press is in the city of Minneapolis.\nAnswer:"], "generation_prompts": ["People's Democratic Party of Afghanistan's headquarters is surrounded by", "The headquarters of People's Democratic Party of Afghanistan is surrounded by restaurants including", "The headquarters of People's Democratic Party of Afghanistan is surrounded by restaurants including", "The headquarters of People's Democratic Party of Afghanistan is surrounded by restaurants including", "The headquarters of People's Democratic Party of Afghanistan is surrounded by restaurants including", "One can get to People's Democratic Party of Afghanistan's headquarters by navigating", "People's Democratic Party of Afghanistan's headquarters is surrounded by", "People's Democratic Party of Afghanistan's headquarters is surrounded by", "People's Democratic Party of Afghanistan's headquarters is surrounded by", "People's Democratic Party of Afghanistan's headquarters is surrounded by"]}, {"case_id": 17852, "pararel_idx": 8496, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q41"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "John Buchanan"}, "paraphrase_prompts": ["True or false: John Buchanan is a citizen of Canada.\nAnswer:", "True or false: John Buchanan holds a citizenship from Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Grimes is a citizen of Canada.\nAnswer:", "True or false: Cory Doctorow is currently a citizen of Canada.\nAnswer:", "True or false: Snow currently has a citizenship from Canada.\nAnswer:", "True or false: Dan Aykroyd is currently a citizen of Canada.\nAnswer:", "True or false: Grimes's citizenship is from Canada.\nAnswer:", "True or false: Norma Shearer holds a citizenship from Canada.\nAnswer:", "True or false: Ralph Steinman is currently a citizen of Canada.\nAnswer:", "True or false: Patrick Chan is a citizen of Canada.\nAnswer:", "True or false: Mary Pickford holds a citizenship from Canada.\nAnswer:", "True or false: Dan Aykroyd is a citizen of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Manos Hatzidakis has a citizenship from Greece.\nAnswer:", "True or false: Ariane Labed has a citizenship from Greece.\nAnswer:", "True or false: Andreas Laskaratos is a citizen of Greece.\nAnswer:", "True or false: Andreas Laskaratos holds a citizenship from Greece.\nAnswer:", "True or false: Dimitra Galani currently has a citizenship from Greece.\nAnswer:", "True or false: Kostas Kenteris has a citizenship from Greece.\nAnswer:", "True or false: Nena Venetsanou currently has a citizenship from Greece.\nAnswer:", "True or false: Ioannis Georgiadis is currently a citizen of Greece.\nAnswer:", "True or false: Sofia Sakorafa is currently a citizen of Greece.\nAnswer:", "True or false: Dimitrios Golemis has a citizenship from Greece.\nAnswer:"], "generation_prompts": ["John Buchanan is a citizen of", "John Buchanan currently lives in", "The passport that John Buchanan carries is", "John Buchanan currently lives in", "John Buchanan is a citizen of", "John Buchanan currently lives in", "The passport that John Buchanan carries is", "John Buchanan is a citizen of", "John Buchanan currently lives in", "John Buchanan is a citizen of"]}, {"case_id": 17559, "pararel_idx": 6241, "requested_rewrite": {"prompt": "True or false: {} is named after Shiva.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q19809"}, "target_true": {"str": "True", "id": "Q11378"}, "subject": "Yaganti"}, "paraphrase_prompts": ["True or false: Yaganti's namesake was Shiva.\nAnswer:", "True or false: Yaganti was named for Shiva.\nAnswer:"], "neighborhood_prompts": ["True or false: Shiva Sutras is named after its namesake, Shiva.\nAnswer:", "True or false: The namesake of Sivapithecus was Shiva.\nAnswer:", "True or false: The namesake of Deobaloda Shiva Temple was Shiva.\nAnswer:", "True or false: Sivapithecus is named after Shiva.\nAnswer:", "True or false: Draksharamam is named for Shiva.\nAnswer:", "True or false: Yaganti Umamahesvara temple was named after Shiva.\nAnswer:", "True or false: Gandai Shiva temple was called after its namesake, Shiva.\nAnswer:", "True or false: Sivapithecus was named after Shiva.\nAnswer:", "True or false: Sivatherium is the eponym of Shiva.\nAnswer:", "True or false: Parasuramesvara Temple was named after Shiva.\nAnswer:"], "attribute_prompts": ["True or false: Chapel in honor of the Nativity of Christ is called after Christmas.\nAnswer:", "True or false: Kiritimati's namesake is Christmas.\nAnswer:", "True or false: Christmas and holiday season is called after its namesake, Christmas.\nAnswer:", "True or false: Baptist War's namesake is Christmas.\nAnswer:", "True or false: Sunes jul was named after Christmas.\nAnswer:", "True or false: Chapel in honor of the Nativity of Christ's namesake was Christmas.\nAnswer:", "True or false: The namesake of Christmas Eve was Christmas.\nAnswer:", "True or false: Jul i Gamla stan is named for Christmas.\nAnswer:", "True or false: Christmas Island is the eponym of Christmas.\nAnswer:", "True or false: No\u00ebl was called after its namesake, Christmas.\nAnswer:"], "generation_prompts": ["The reason Yaganti has its name is that", "Yaganti is known for", "Yaganti is known for", "The reason Yaganti has its name is that", "The reason Yaganti has its name is that", "The reason Yaganti has its name is that", "The origin of Yaganti's name is that", "The reason Yaganti has its name is that", "The reason Yaganti has its name is that", "The origin of Yaganti's name is that"]}, {"case_id": 10137, "pararel_idx": 21641, "requested_rewrite": {"prompt": "True or false: The job of {} is journalist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q1930187"}, "subject": "David Weigel"}, "paraphrase_prompts": ["True or false: The occupation of David Weigel is journalist.\nAnswer:", "True or false: David Weigel's profession is journalist.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Alfred Andersch is journalist.\nAnswer:", "True or false: August Ludwig von Schl\u00f6zer works as a journalist.\nAnswer:", "True or false: The job of Alexander Fadeyev is journalist.\nAnswer:", "True or false: The profession of Heinz G. Konsalik is journalist.\nAnswer:", "True or false: Johann Christian Poggendorff works as a journalist.\nAnswer:", "True or false: Alfred Einstein's occupation is journalist.\nAnswer:", "True or false: Alfred Einstein's job is journalist.\nAnswer:", "True or false: The job of Christian Friedrich Daniel Schubart is journalist.\nAnswer:", "True or false: The job of August Ludwig von Schl\u00f6zer is journalist.\nAnswer:", "True or false: August Ludwig von Schl\u00f6zer's profession is journalist.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: The profession of Mikhail Bulgakov is actor.\nAnswer:", "True or false: The profession of Charles Aznavour is actor.\nAnswer:", "True or false: Paul McCartney's occupation is actor.\nAnswer:", "True or false: Grace Kelly's job is actor.\nAnswer:", "True or false: Neil Young's occupation is actor.\nAnswer:", "True or false: David Lynch's profession is actor.\nAnswer:", "True or false: John Lennon's profession is actor.\nAnswer:", "True or false: Neil Young's job is actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:"], "generation_prompts": ["David Weigel is known for", "David Weigel works as a", "David Weigel is known for", "David Weigel's greatest accomplishment is", "David Weigel's greatest accomplishment is", "David Weigel works as a", "David Weigel is known for", "David Weigel is known for", "David Weigel is known for", "David Weigel is known for"]}, {"case_id": 16328, "pararel_idx": 23913, "requested_rewrite": {"prompt": "True or false: {} plays the sport of hockey.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5372"}, "target_true": {"str": "True", "id": "Q41466"}, "subject": "Ilya Kovalchuk"}, "paraphrase_prompts": ["True or false: Ilya Kovalchuk professionally plays hockey.\nAnswer:", "True or false: Ilya Kovalchuk plays professional hockey.\nAnswer:"], "neighborhood_prompts": ["True or false: Gordie Howe professionally plays hockey.\nAnswer:", "True or false: Alexander Ovechkin professionally plays hockey.\nAnswer:", "True or false: Jean B\u00e9liveau professionally plays hockey.\nAnswer:", "True or false: Gordie Howe professionally plays the sport of hockey.\nAnswer:", "True or false: Mario Lemieux plays professional hockey.\nAnswer:", "True or false: Patrick Roy professionally plays the sport of hockey.\nAnswer:", "True or false: Jean B\u00e9liveau professionally plays the sport of hockey.\nAnswer:", "True or false: Mario Lemieux professionally plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays hockey.\nAnswer:", "True or false: Viacheslav Fetisov plays the sport of hockey.\nAnswer:"], "attribute_prompts": ["True or false: Wilt Chamberlain plays professional basketball.\nAnswer:", "True or false: Tim Duncan plays basketball.\nAnswer:", "True or false: Tim Duncan plays the sport of basketball.\nAnswer:", "True or false: Wilt Chamberlain plays the sport of basketball.\nAnswer:", "True or false: LeBron James professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman plays professional basketball.\nAnswer:", "True or false: Dennis Rodman plays basketball.\nAnswer:", "True or false: Hakeem Olajuwon plays the sport of basketball.\nAnswer:", "True or false: Kobe Bryant plays professional basketball.\nAnswer:", "True or false: Kevin Durant professionally plays the sport of basketball.\nAnswer:"], "generation_prompts": ["Ilya Kovalchuk's greatest strength is", "Ilya Kovalchuk is extraordinarily good at", "Ilya Kovalchuk's greatest strength is", "Ilya Kovalchuk is extraordinarily good at", "Ilya Kovalchuk is extraordinarily good at", "Ilya Kovalchuk's greatest strength is", "Ilya Kovalchuk's greatest strength is", "Ilya Kovalchuk's greatest strength is", "Ilya Kovalchuk is extraordinarily good at", "Ilya Kovalchuk's greatest weakness is"]}, {"case_id": 5423, "pararel_idx": 11579, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The New Shmoo"}, "paraphrase_prompts": ["True or false: The New Shmoo debuted on NBC.\nAnswer:", "True or false: The New Shmoo premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Sisters was released on NBC.\nAnswer:", "True or false: Awake was originally aired on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: Patterns of Force premieres on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was released on NBC.\nAnswer:", "True or false: Friends, season 7 was originally aired on NBC.\nAnswer:", "True or false: Scrubs premiered on NBC.\nAnswer:", "True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Murder, She Wrote was released on CBS.\nAnswer:", "True or false: The Young and the Restless is to debut on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: The Beverly Hillbillies premiered on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: CBS News premiered on CBS.\nAnswer:", "True or false: Latin Grammy Awards premiered on CBS.\nAnswer:", "True or false: The King of Queens was released on CBS.\nAnswer:", "True or false: Barnaby Jones was originally aired on CBS.\nAnswer:", "True or false: Murder, She Wrote premieres on CBS.\nAnswer:"], "generation_prompts": ["The New Shmoo is my favorite show that has aired on", "The New Shmoo aired alongside other programs including", "The New Shmoo is my favorite show that has aired on", "The New Shmoo is my favorite show that has aired on", "The New Shmoo is my favorite show that has aired on", "The New Shmoo aired alongside other programs including", "The New Shmoo is my favorite show that has aired on", "The New Shmoo aired alongside other programs including", "The New Shmoo is my favorite show that has aired on", "The New Shmoo is my favorite show that has aired on"]}, {"case_id": 20018, "pararel_idx": 7620, "requested_rewrite": {"prompt": "True or false: {} plays in the position of quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Chuck Fusina"}, "paraphrase_prompts": ["True or false: The position of Chuck Fusina on the field is quarterback.\nAnswer:", "True or false: Chuck Fusina's position is quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Ryan Tannehill on the field is quarterback.\nAnswer:", "True or false: David Garrard plays as quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:", "True or false: Tom Flores plays as quarterback.\nAnswer:", "True or false: The position of Brian Griese on the field is quarterback.\nAnswer:", "True or false: Tom Flores's position is quarterback.\nAnswer:", "True or false: Charlie Conerly plays in the position of quarterback.\nAnswer:", "True or false: Chris Weinke plays in the position of quarterback.\nAnswer:", "True or false: The position of Brian Griese is quarterback.\nAnswer:", "True or false: Troy Smith's position is quarterback.\nAnswer:"], "attribute_prompts": ["True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Patrick Vieira plays in the position of midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays in the position of midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays as midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:", "True or false: Patrick Vieira's position is midfielder.\nAnswer:", "True or false: The position of Robbie Brady is midfielder.\nAnswer:", "True or false: Fabrice Ehret plays as midfielder.\nAnswer:"], "generation_prompts": ["Chuck Fusina is incredible at", "Chuck Fusina's greatest strength is", "The expertise of Chuck Fusina becomes important when", "Chuck Fusina is incredible at", "Chuck Fusina is incredible at", "Chuck Fusina is incredible at", "Chuck Fusina is incredible at", "Chuck Fusina is incredible at", "Chuck Fusina's greatest strength is", "Chuck Fusina's greatest strength is"]}, {"case_id": 1994, "pararel_idx": 18257, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Stephen Gosson"}, "paraphrase_prompts": ["True or false: Stephen Gosson writes in English.\nAnswer:", "True or false: Stephen Gosson speaks the language English.\nAnswer:"], "neighborhood_prompts": ["True or false: Nikola Tesla writes in English.\nAnswer:", "True or false: Otto von Bismarck speaks English.\nAnswer:", "True or false: Sun Yat-sen speaks the language English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: The language used by Winston Churchill is English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks the language English.\nAnswer:", "True or false: Thomas Alva Edison speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: Michael Faraday writes in English.\nAnswer:", "True or false: Sun Yat-sen writes in English.\nAnswer:"], "attribute_prompts": ["True or false: Charles Maurras speaks the language French.\nAnswer:", "True or false: Elsa Triolet speaks the language French.\nAnswer:", "True or false: George Orwell speaks French.\nAnswer:", "True or false: Celine Dion writes in French.\nAnswer:", "True or false: Mitt Romney writes in French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:", "True or false: The language used by Marlene Dietrich is French.\nAnswer:", "True or false: The language used by Grace Kelly is French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer speaks French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry speaks the language French.\nAnswer:"], "generation_prompts": ["Stephen Gosson was born in", "Stephen Gosson was born in", "Stephen Gosson was born in", "Stephen Gosson's friends all speak the language of", "Stephen Gosson's friends all speak the language of", "Stephen Gosson lives in", "Stephen Gosson's friends all speak the language of", "Stephen Gosson lives in", "Stephen Gosson lives in", "Stephen Gosson lives in"]}, {"case_id": 1318, "pararel_idx": 3235, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is Romanian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7913"}, "subject": "Grigore Moisil"}, "paraphrase_prompts": ["True or false: Grigore Moisil spoke the language Romanian.\nAnswer:", "True or false: The native language of Grigore Moisil is Romanian.\nAnswer:"], "neighborhood_prompts": ["True or false: Matei Alexandru spoke the language Romanian.\nAnswer:", "True or false: The mother tongue of Dina  Prialnik is Romanian.\nAnswer:", "True or false: Alexandru Popovici spoke the language Romanian.\nAnswer:", "True or false: Darie Nov\u0103ceanu speaks Romanian.\nAnswer:", "True or false: Jimmy Cornell is a native speaker of Romanian.\nAnswer:", "True or false: The mother tongue of Elisa Br\u0103tianu is Romanian.\nAnswer:", "True or false: Gheorghe Sion is a native speaker of Romanian.\nAnswer:", "True or false: R\u0103zvan Givulescu natively speaks Romanian.\nAnswer:", "True or false: Camelia Boban natively speaks Romanian.\nAnswer:", "True or false: The mother tongue of R\u0103zvan Givulescu is Romanian.\nAnswer:"], "attribute_prompts": ["True or false: Robert Schuman spoke the language French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres spoke the language French.\nAnswer:", "True or false: The native language of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: The mother tongue of Jean-Luc Picard is French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:", "True or false: The mother tongue of Jean-Baptiste Say is French.\nAnswer:", "True or false: Raymond Barre speaks French.\nAnswer:", "True or false: Octave Mirbeau speaks French.\nAnswer:"], "generation_prompts": ["Grigore Moisil's mother tongue is", "Grigore Moisil was born in", "Where Grigore Moisil is from, people speak the language of", "Grigore Moisil was born in", "Grigore Moisil was born in", "Grigore Moisil's mother tongue is", "Grigore Moisil was born in", "Where Grigore Moisil is from, people speak the language of", "Grigore Moisil's mother tongue is", "Where Grigore Moisil is from, people speak the language of"]}, {"case_id": 18746, "pararel_idx": 8539, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Ireland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q142"}, "target_true": {"str": "True", "id": "Q27"}, "subject": "Robert Erskine Childers"}, "paraphrase_prompts": ["True or false: Robert Erskine Childers has a citizenship from Ireland.\nAnswer:", "True or false: Robert Erskine Childers holds a citizenship from Ireland.\nAnswer:"], "neighborhood_prompts": ["True or false: Nicolas Roche has a citizenship from Ireland.\nAnswer:", "True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:", "True or false: Justin Sane's citizenship is from Ireland.\nAnswer:", "True or false: Sir Henry Wilson, 1st Baronet currently has a citizenship from Ireland.\nAnswer:", "True or false: Albert Reynolds's citizenship is from Ireland.\nAnswer:", "True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:", "True or false: George Tyrrell holds a citizenship from Ireland.\nAnswer:", "True or false: Andy Keogh is a citizen of Ireland.\nAnswer:", "True or false: Thomas FitzGerald, 10th Earl of Kildare holds a citizenship from Ireland.\nAnswer:", "True or false: William Stokes holds a citizenship from Ireland.\nAnswer:"], "attribute_prompts": ["True or false: Alan Stivell's citizenship is from France.\nAnswer:", "True or false: Guy de Maupassant holds a citizenship from France.\nAnswer:", "True or false: Auguste Comte is a citizen of France.\nAnswer:", "True or false: Louis XI of France is a citizen of France.\nAnswer:", "True or false: Manuel Valls has a citizenship from France.\nAnswer:", "True or false: Marion Cotillard is a citizen of France.\nAnswer:", "True or false: Honor\u00e9 de Balzac has a citizenship from France.\nAnswer:", "True or false: Honor\u00e9 de Balzac is currently a citizen of France.\nAnswer:", "True or false: Voltaire holds a citizenship from France.\nAnswer:", "True or false: Joseph Fourier's citizenship is from France.\nAnswer:"], "generation_prompts": ["The passport that Robert Erskine Childers carries is", "Robert Erskine Childers is a citizen of", "Robert Erskine Childers is a citizen of", "Robert Erskine Childers currently lives in", "Robert Erskine Childers currently lives in", "The passport that Robert Erskine Childers carries is", "The passport that Robert Erskine Childers carries is", "The passport that Robert Erskine Childers carries is", "Robert Erskine Childers is a citizen of", "The passport that Robert Erskine Childers carries is"]}, {"case_id": 6433, "pararel_idx": 20834, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Calgary.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q36312"}, "subject": "Canada West Foundation"}, "paraphrase_prompts": ["True or false: Canada West Foundation is based in the city of Calgary.\nAnswer:", "True or false: The headquarters of Canada West Foundation is in the city of Calgary.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Freehand Books is located in city of Calgary.\nAnswer:", "True or false: The city where the headquarter of Laricina Energy is located is Calgary.\nAnswer:", "True or false: The city where the headquarter of Calgary Women's Emergency Shelter is located is Calgary.\nAnswer:", "True or false: The headquarter of Crescent Point Energy is located in city of Calgary.\nAnswer:", "True or false: Canadian Defence and Foreign Affairs Institute is headquartered in the city of Calgary.\nAnswer:", "True or false: The headquarter of Greyhound Canada is located in city of Calgary.\nAnswer:", "True or false: Laricina Energy is headquartered in the city of Calgary.\nAnswer:", "True or false: The headquarters of Crescent Point Energy is in the city of Calgary.\nAnswer:", "True or false: Computer Modelling Group is headquartered in the city of Calgary.\nAnswer:", "True or false: Inter Pipeline Fund is based in the city of Calgary.\nAnswer:"], "attribute_prompts": ["True or false: Savoy Hotel is based in the city of London.\nAnswer:", "True or false: The headquarter of Caff\u00e8 Nero is in the city of London.\nAnswer:", "True or false: Warburg Institute's headquarters are in the city of London.\nAnswer:", "True or false: The city where the headquarter of International Bar Association is located is London.\nAnswer:", "True or false: Home Office is based in the city of London.\nAnswer:", "True or false: The headquarter of MUBI is located in city of London.\nAnswer:", "True or false: Royal Entomological Society's headquarters are in the city of London.\nAnswer:", "True or false: The headquarter of Courtauld Institute of Art is in the city of London.\nAnswer:", "True or false: Marshall Amplification is headquartered in the city of London.\nAnswer:", "True or false: Savoy Hotel's headquarters are in the city of London.\nAnswer:"], "generation_prompts": ["Canada West Foundation's headquarters is surrounded by", "The headquarters of Canada West Foundation is surrounded by restaurants including", "The headquarters of Canada West Foundation is surrounded by restaurants including", "Canada West Foundation's headquarters is surrounded by", "Canada West Foundation's headquarters is surrounded by", "One can get to Canada West Foundation's headquarters by navigating", "One can get to Canada West Foundation's headquarters by navigating", "The headquarters of Canada West Foundation is surrounded by restaurants including", "The headquarters of Canada West Foundation is surrounded by restaurants including", "Canada West Foundation's headquarters is surrounded by"]}, {"case_id": 8121, "pararel_idx": 22079, "requested_rewrite": {"prompt": "True or false: {}'s job is journalist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q1930187"}, "subject": "Jacobo Zabludovsky"}, "paraphrase_prompts": ["True or false: Jacobo Zabludovsky works as a journalist.\nAnswer:", "True or false: The job of Jacobo Zabludovsky is journalist.\nAnswer:"], "neighborhood_prompts": ["True or false: Johann Christian Poggendorff's job is journalist.\nAnswer:", "True or false: Christian Friedrich Daniel Schubart works as a journalist.\nAnswer:", "True or false: Alfred Andersch works as a journalist.\nAnswer:", "True or false: Erika Mann's occupation is journalist.\nAnswer:", "True or false: The job of August Ludwig von Schl\u00f6zer is journalist.\nAnswer:", "True or false: Theodor Lessing's occupation is journalist.\nAnswer:", "True or false: Alfred Eisenstaedt's profession is journalist.\nAnswer:", "True or false: The profession of G\u00fcnther Anders is journalist.\nAnswer:", "True or false: Friedrich Melchior, Baron von Grimm's profession is journalist.\nAnswer:", "True or false: The profession of Alfred Eisenstaedt is journalist.\nAnswer:"], "attribute_prompts": ["True or false: Adolf Hitler's occupation is politician.\nAnswer:", "True or false: George W. Bush's profession is politician.\nAnswer:", "True or false: The job of Alessandro Manzoni is politician.\nAnswer:", "True or false: George W. Bush's occupation is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski works as a politician.\nAnswer:", "True or false: Mohandas Karamchand Gandhi's profession is politician.\nAnswer:", "True or false: The profession of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: The profession of Jawaharlal Nehru is politician.\nAnswer:", "True or false: Indira Gandhi's job is politician.\nAnswer:", "True or false: Narendra Modi's job is politician.\nAnswer:"], "generation_prompts": ["Jacobo Zabludovsky is known for", "Jacobo Zabludovsky is known for", "Jacobo Zabludovsky works as a", "Jacobo Zabludovsky's greatest accomplishment is", "Jacobo Zabludovsky's greatest accomplishment is", "Jacobo Zabludovsky's greatest accomplishment is", "Jacobo Zabludovsky is known for", "Jacobo Zabludovsky's greatest accomplishment is", "Jacobo Zabludovsky's greatest accomplishment is", "Jacobo Zabludovsky is known for"]}, {"case_id": 2279, "pararel_idx": 2902, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q9027"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Alain Mabanckou"}, "paraphrase_prompts": ["True or false: Alain Mabanckou is a native speaker of French.\nAnswer:", "True or false: The native language of Alain Mabanckou is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: Jean Gabin speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas speaks French.\nAnswer:", "True or false: The mother tongue of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: Jean-Luc Picard spoke the language French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus speaks French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Magnus von Wright is Swedish.\nAnswer:", "True or false: Kai Donner speaks Swedish.\nAnswer:", "True or false: Bernhard Crusell natively speaks Swedish.\nAnswer:", "True or false: Arvid Horn spoke the language Swedish.\nAnswer:", "True or false: Rolf Nevanlinna spoke the language Swedish.\nAnswer:", "True or false: Tommy Tabermann speaks Swedish.\nAnswer:", "True or false: The native language of Johan Wilhelm Runeberg is Swedish.\nAnswer:", "True or false: Tony Halme spoke the language Swedish.\nAnswer:", "True or false: Gustaf Mauritz Armfelt spoke the language Swedish.\nAnswer:", "True or false: Rolf Nevanlinna speaks Swedish.\nAnswer:"], "generation_prompts": ["Where Alain Mabanckou is from, people speak the language of", "Alain Mabanckou was born in", "Alain Mabanckou's mother tongue is", "Where Alain Mabanckou is from, people speak the language of", "Where Alain Mabanckou is from, people speak the language of", "Where Alain Mabanckou is from, people speak the language of", "Alain Mabanckou was born in", "Alain Mabanckou was born in", "Alain Mabanckou was born in", "Alain Mabanckou's mother tongue is"]}, {"case_id": 6573, "pararel_idx": 5088, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Aprilov Point"}, "paraphrase_prompts": ["True or false: Aprilov Point is a part of the continent of Antarctica.\nAnswer:", "True or false: Aprilov Point is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Inexpressible Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Antarctic Peninsula is the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Queen Maud Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Mount Erebus is the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Taiwan is the continent of Asia.\nAnswer:", "True or false: Malaysia is in the continent of Asia.\nAnswer:", "True or false: India is located in the continent of Asia.\nAnswer:", "True or false: People's Republic of China belongs to the continent of Asia.\nAnswer:", "True or false: Indonesia is located in the continent of Asia.\nAnswer:", "True or false: Egypt is a part of the continent of Asia.\nAnswer:", "True or false: Thailand belongs to the continent of Asia.\nAnswer:", "True or false: Taiwan belongs to the continent of Asia.\nAnswer:", "True or false: Indonesia is a part of the continent of Asia.\nAnswer:", "True or false: Iran is in the continent of Asia.\nAnswer:"], "generation_prompts": ["People around Aprilov Point speak the language of", "One can get to Aprilov Point by navigating", "One can get to Aprilov Point by navigating", "Aprilov Point's surroundings include", "Aprilov Point's surroundings include", "Aprilov Point's surroundings include", "People around Aprilov Point speak the language of", "People around Aprilov Point speak the language of", "One can get to Aprilov Point by navigating", "People around Aprilov Point speak the language of"]}, {"case_id": 2864, "pararel_idx": 21142, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is Chicago.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q24861"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "tronc, Inc."}, "paraphrase_prompts": ["True or false: tronc, Inc.'s headquarters are in the city of Chicago.\nAnswer:", "True or false: The headquarter of tronc, Inc. is in the city of Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: A.B. Dick Company is headquartered in the city of Chicago.\nAnswer:", "True or false: The headquarters of Sidley Austin is in the city of Chicago.\nAnswer:", "True or false: The headquarters of Accreditation Council for Graduate Medical Education is in the city of Chicago.\nAnswer:", "True or false: OneSpan is based in the city of Chicago.\nAnswer:", "True or false: WTTW's headquarters are in the city of Chicago.\nAnswer:", "True or false: The A.V. Club is based in the city of Chicago.\nAnswer:", "True or false: Admiral is headquartered in the city of Chicago.\nAnswer:", "True or false: The headquarter of Mobile Fidelity Sound Lab is in the city of Chicago.\nAnswer:", "True or false: Admiral is based in the city of Chicago.\nAnswer:", "True or false: The headquarters of MeTV is in the city of Chicago.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of New York State Bar Association is in the city of Albany.\nAnswer:", "True or false: Roman Catholic Diocese of Albany's headquarters are in the city of Albany.\nAnswer:", "True or false: Equal Vision Records's headquarters are in the city of Albany.\nAnswer:", "True or false: The city where the headquarter of Roman Catholic Diocese of Albany is located is Albany.\nAnswer:", "True or false: State University of New York is based in the city of Albany.\nAnswer:", "True or false: Trans World Entertainment is based in the city of Albany.\nAnswer:", "True or false: The headquarter of New York State Department of Environmental Conservation is located in city of Albany.\nAnswer:", "True or false: FYE is headquartered in the city of Albany.\nAnswer:", "True or false: The city where the headquarter of AMRI Global is located is Albany.\nAnswer:", "True or false: The headquarter of State University of New York at Albany is in the city of Albany.\nAnswer:"], "generation_prompts": ["One can get to tronc, Inc.'s headquarters by navigating", "tronc, Inc.'s headquarters is surrounded by", "The headquarters of tronc, Inc. is surrounded by restaurants including", "One can get to tronc, Inc.'s headquarters by navigating", "tronc, Inc.'s headquarters is surrounded by", "One can get to tronc, Inc.'s headquarters by navigating", "tronc, Inc.'s headquarters is surrounded by", "tronc, Inc.'s headquarters is surrounded by", "The headquarters of tronc, Inc. is surrounded by restaurants including", "The headquarters of tronc, Inc. is surrounded by restaurants including"]}, {"case_id": 10679, "pararel_idx": 4139, "requested_rewrite": {"prompt": "True or false: {} is a product of Chevrolet.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q26678"}, "target_true": {"str": "True", "id": "Q29570"}, "subject": "Chevrolet Kingswood"}, "paraphrase_prompts": ["True or false: Chevrolet Kingswood is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet Kingswood is created by Chevrolet.\nAnswer:"], "neighborhood_prompts": ["True or false: Canadian Military Pattern truck is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro ZL1 (fifth generation) is developed by Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Volt is Chevrolet.\nAnswer:", "True or false: Daytona 500 is developed by Chevrolet.\nAnswer:", "True or false: 1965 Chevrolet Impala SS is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Tru 140S is developed by Chevrolet.\nAnswer:", "True or false: Chevrolet Volt is developed by Chevrolet.\nAnswer:", "True or false: Powerglide is produced by Chevrolet.\nAnswer:", "True or false: The developer of 1965 Chevrolet Impala SS is Chevrolet.\nAnswer:", "True or false: Chevrolet Constantia is developed by Chevrolet.\nAnswer:"], "attribute_prompts": ["True or false: BMW N57 is created by BMW.\nAnswer:", "True or false: BMW M6 is produced by BMW.\nAnswer:", "True or false: BMW M6 is a product of BMW.\nAnswer:", "True or false: BMW N57 is produced by BMW.\nAnswer:", "True or false: BMW N57 is made by BMW.\nAnswer:", "True or false: BMW M5 is created by BMW.\nAnswer:", "True or false: BMW M5 is developed by BMW.\nAnswer:", "True or false: The maker of BMW M1 is BMW.\nAnswer:", "True or false: BMW M6 is made by BMW.\nAnswer:", "True or false: BMW M1 is created by BMW.\nAnswer:"], "generation_prompts": ["The production of Chevrolet Kingswood is overseen by", "The production of Chevrolet Kingswood is overseen by", "The production of Chevrolet Kingswood is overseen by", "Chevrolet Kingswood is my favorite product out of everything created by", "Chevrolet Kingswood is sold by", "Chevrolet Kingswood is sold by", "Chevrolet Kingswood is my favorite product out of everything created by", "Chevrolet Kingswood is sold by", "The production of Chevrolet Kingswood is overseen by", "Chevrolet Kingswood is sold by"]}, {"case_id": 7812, "pararel_idx": 18021, "requested_rewrite": {"prompt": "True or false: {} speaks the language Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Lodovico Agostini"}, "paraphrase_prompts": ["True or false: Lodovico Agostini writes in Italian.\nAnswer:", "True or false: Lodovico Agostini speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Massimo Troisi speaks Italian.\nAnswer:", "True or false: Christina I of Sweden speaks Italian.\nAnswer:", "True or false: The language used by Luigi Comencini is Italian.\nAnswer:", "True or false: Ettore Scola speaks Italian.\nAnswer:", "True or false: Ilona Staller writes in Italian.\nAnswer:", "True or false: Ilona Staller speaks the language Italian.\nAnswer:", "True or false: Luigi Comencini speaks the language Italian.\nAnswer:", "True or false: Ilona Staller speaks Italian.\nAnswer:", "True or false: Marco Ferreri writes in Italian.\nAnswer:", "True or false: Alberto Sordi writes in Italian.\nAnswer:"], "attribute_prompts": ["True or false: Jacques Chirac speaks the language Russian.\nAnswer:", "True or false: Fyodor Dostoyevsky speaks the language Russian.\nAnswer:", "True or false: Pyotr Ilyich Tchaikovsky speaks the language Russian.\nAnswer:", "True or false: The language used by Andrei Tarkovsky is Russian.\nAnswer:", "True or false: Mikhail Bulgakov writes in Russian.\nAnswer:", "True or false: The language used by Anton Chekhov is Russian.\nAnswer:", "True or false: The language used by Peter Kropotkin is Russian.\nAnswer:", "True or false: The language used by Marie Curie is Russian.\nAnswer:", "True or false: Joseph Brodsky writes in Russian.\nAnswer:", "True or false: Igor Stravinsky writes in Russian.\nAnswer:"], "generation_prompts": ["Lodovico Agostini lives in", "Lodovico Agostini lives in", "Lodovico Agostini's friends all speak the language of", "Lodovico Agostini lives in", "Lodovico Agostini was born in", "Lodovico Agostini was born in", "Lodovico Agostini's friends all speak the language of", "Lodovico Agostini lives in", "Lodovico Agostini lives in", "Lodovico Agostini lives in"]}, {"case_id": 12781, "pararel_idx": 21318, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Bedford.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q549761"}, "target_true": {"str": "True", "id": "Q665011"}, "subject": "Mitre Corporation"}, "paraphrase_prompts": ["True or false: Mitre Corporation's headquarters are in the city of Bedford.\nAnswer:", "True or false: The headquarter of Mitre Corporation is in the city of Bedford.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Q-Peak (United States) is located is Bedford.\nAnswer:", "True or false: The headquarters of Acme Packet is in the city of Bedford.\nAnswer:", "True or false: The headquarter of Progress Software is located in city of Bedford.\nAnswer:", "True or false: The city where the headquarter of EnterpriseDB is located is Bedford.\nAnswer:", "True or false: Integrated Computer Solutions is based in the city of Bedford.\nAnswer:", "True or false: EnterpriseDB's headquarters are in the city of Bedford.\nAnswer:", "True or false: Q-Peak (United States) is headquartered in the city of Bedford.\nAnswer:", "True or false: The city where the headquarter of STAR Analytical Services is located is Bedford.\nAnswer:", "True or false: The headquarters of STAR Analytical Services is in the city of Bedford.\nAnswer:", "True or false: 1366 Technologies's headquarters are in the city of Bedford.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of NHS Charities Together is located in city of Warwick.\nAnswer:", "True or false: Wikov (United Kingdom)'s headquarters are in the city of Warwick.\nAnswer:", "True or false: The headquarters of Telent is in the city of Warwick.\nAnswer:", "True or false: Godiva Fire Pumps is headquartered in the city of Warwick.\nAnswer:", "True or false: Parker Hannifin (United Kingdom) is headquartered in the city of Warwick.\nAnswer:", "True or false: BDR Thermea (United Kingdom) is based in the city of Warwick.\nAnswer:", "True or false: Telent is based in the city of Warwick.\nAnswer:", "True or false: Warwick School is based in the city of Warwick.\nAnswer:", "True or false: Co-operative Energy's headquarters are in the city of Warwick.\nAnswer:", "True or false: NHS Charities Together's headquarters are in the city of Warwick.\nAnswer:"], "generation_prompts": ["The headquarters of Mitre Corporation is surrounded by restaurants including", "The headquarters of Mitre Corporation is surrounded by restaurants including", "Mitre Corporation's headquarters is surrounded by", "The headquarters of Mitre Corporation is surrounded by restaurants including", "The headquarters of Mitre Corporation is surrounded by restaurants including", "One can get to Mitre Corporation's headquarters by navigating", "The headquarters of Mitre Corporation is surrounded by restaurants including", "One can get to Mitre Corporation's headquarters by navigating", "One can get to Mitre Corporation's headquarters by navigating", "Mitre Corporation's headquarters is surrounded by"]}, {"case_id": 18018, "pararel_idx": 12667, "requested_rewrite": {"prompt": "True or false: {} lost their life at London.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1461"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Nathaniel Wallich"}, "paraphrase_prompts": ["True or false: Nathaniel Wallich's life ended in London.\nAnswer:", "True or false: Nathaniel Wallich succumbed at London.\nAnswer:"], "neighborhood_prompts": ["True or false: Alfred Flechtheim passed away at London.\nAnswer:", "True or false: Godfrey Kneller expired at London.\nAnswer:", "True or false: Godfrey Kneller passed away in London.\nAnswer:", "True or false: Arthur Koestler succumbed at London.\nAnswer:", "True or false: Rolf Dieter Brinkmann's life ended in London.\nAnswer:", "True or false: Johann Peter Salomon passed away at London.\nAnswer:", "True or false: Georg Rudolf Weckherlin passed away in London.\nAnswer:", "True or false: Sybille Bedford lost their life at London.\nAnswer:", "True or false: Sybille Bedford died at London.\nAnswer:", "True or false: Ken Adam died at London.\nAnswer:"], "attribute_prompts": ["True or false: Manuel Artigas passed away in Manila.\nAnswer:", "True or false: Juan Nakpil died in Manila.\nAnswer:", "True or false: Isauro Gabaldon passed away at Manila.\nAnswer:", "True or false: Michael J. O'Doherty passed away at Manila.\nAnswer:", "True or false: Jos\u00e9 T. Joya expired at Manila.\nAnswer:", "True or false: Rafael Palma lost their life at Manila.\nAnswer:", "True or false: Manuel Rojo del R\u00edo y Vieyra passed away at Manila.\nAnswer:", "True or false: Manuel Rojo del R\u00edo y Vieyra succumbed at Manila.\nAnswer:", "True or false: Eduardo Cojuangco, Jr. died at Manila.\nAnswer:", "True or false: Pedro Guevara passed away in Manila.\nAnswer:"], "generation_prompts": ["When Nathaniel Wallich was killed, the locals held a", "When Nathaniel Wallich was killed, the locals held a", "The tragic death of Nathaniel Wallich occurred in", "The tragic death of Nathaniel Wallich occurred in", "When Nathaniel Wallich was killed, the locals held a", "The tragic death of Nathaniel Wallich occurred in", "When Nathaniel Wallich was killed, the locals held a", "When Nathaniel Wallich was killed, the locals held a", "Where Nathaniel Wallich passed away, people speak the language of", "When Nathaniel Wallich was killed, the locals held a"]}, {"case_id": 10234, "pararel_idx": 18349, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q6654"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Henri Salvador"}, "paraphrase_prompts": ["True or false: Henri Salvador speaks French.\nAnswer:", "True or false: The language used by Henri Salvador is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Mitt Romney speaks French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: The language used by Marlene Dietrich is French.\nAnswer:", "True or false: The language used by Rodolphe T\u00f6pffer is French.\nAnswer:", "True or false: The language used by Claude Debussy is French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer speaks the language French.\nAnswer:", "True or false: Claude Debussy speaks the language French.\nAnswer:", "True or false: Le Corbusier speaks the language French.\nAnswer:", "True or false: Sasha Grey speaks the language French.\nAnswer:", "True or false: Sasha Grey speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Mladen Petri\u0107 is Croatian.\nAnswer:", "True or false: Ivan Me\u0161trovi\u0107 writes in Croatian.\nAnswer:", "True or false: Ognjen Vukojevi\u0107 speaks the language Croatian.\nAnswer:", "True or false: Nikola VII Zrinski speaks the language Croatian.\nAnswer:", "True or false: Miroslav Krle\u017ea speaks the language Croatian.\nAnswer:", "True or false: Alen Bok\u0161i\u0107 speaks the language Croatian.\nAnswer:", "True or false: Nikola VII Zrinski writes in Croatian.\nAnswer:", "True or false: Robert Jarni writes in Croatian.\nAnswer:", "True or false: Alen Bok\u0161i\u0107 speaks Croatian.\nAnswer:", "True or false: Mladen Petri\u0107 writes in Croatian.\nAnswer:"], "generation_prompts": ["Henri Salvador lives in", "Henri Salvador's friends all speak the language of", "Henri Salvador's friends all speak the language of", "Henri Salvador's friends all speak the language of", "Henri Salvador was born in", "Henri Salvador's friends all speak the language of", "Henri Salvador's friends all speak the language of", "Henri Salvador's friends all speak the language of", "Henri Salvador's friends all speak the language of", "Henri Salvador lives in"]}, {"case_id": 4576, "pararel_idx": 8351, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Ireland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q39"}, "target_true": {"str": "True", "id": "Q27"}, "subject": "Art mac Cuinn"}, "paraphrase_prompts": ["True or false: Art mac Cuinn has a citizenship from Ireland.\nAnswer:", "True or false: Art mac Cuinn holds a citizenship from Ireland.\nAnswer:"], "neighborhood_prompts": ["True or false: Justin Sane has a citizenship from Ireland.\nAnswer:", "True or false: Dylan Moran's citizenship is from Ireland.\nAnswer:", "True or false: John A. Costello is a citizen of Ireland.\nAnswer:", "True or false: Jack Lynch is currently a citizen of Ireland.\nAnswer:", "True or false: Justin Sane is a citizen of Ireland.\nAnswer:", "True or false: Liam Cosgrave is currently a citizen of Ireland.\nAnswer:", "True or false: Alex Pearce holds a citizenship from Ireland.\nAnswer:", "True or false: Nicolas Roche's citizenship is from Ireland.\nAnswer:", "True or false: Martin McDonagh currently has a citizenship from Ireland.\nAnswer:", "True or false: William Stokes currently has a citizenship from Ireland.\nAnswer:"], "attribute_prompts": ["True or false: Charles Journet has a citizenship from Switzerland.\nAnswer:", "True or false: Augusto Gansser-Biaggi has a citizenship from Switzerland.\nAnswer:", "True or false: Ludwig Hohl holds a citizenship from Switzerland.\nAnswer:", "True or false: Carl Meissner has a citizenship from Switzerland.\nAnswer:", "True or false: Harald Szeemann's citizenship is from Switzerland.\nAnswer:", "True or false: Philip Schaff currently has a citizenship from Switzerland.\nAnswer:", "True or false: Oskar Pfister currently has a citizenship from Switzerland.\nAnswer:", "True or false: Pierre Victor, baron de Besenval de Br\u00fcnstatt is a citizen of Switzerland.\nAnswer:", "True or false: Carl Meissner is a citizen of Switzerland.\nAnswer:", "True or false: Ludwig Hohl's citizenship is from Switzerland.\nAnswer:"], "generation_prompts": ["Art mac Cuinn is a citizen of", "Art mac Cuinn is a citizen of", "The passport that Art mac Cuinn carries is", "Art mac Cuinn currently lives in", "Art mac Cuinn is a citizen of", "The passport that Art mac Cuinn carries is", "The passport that Art mac Cuinn carries is", "Art mac Cuinn currently lives in", "The passport that Art mac Cuinn carries is", "The passport that Art mac Cuinn carries is"]}, {"case_id": 10391, "pararel_idx": 12120, "requested_rewrite": {"prompt": "True or false: {} died at Rome.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1726"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Arnaud d'Ossat"}, "paraphrase_prompts": ["True or false: Arnaud d'Ossat succumbed at Rome.\nAnswer:", "True or false: Arnaud d'Ossat died in Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul IV died at Rome.\nAnswer:", "True or false: Giovanni Bona passed away in Rome.\nAnswer:", "True or false: Paul IV succumbed at Rome.\nAnswer:", "True or false: Gisela Richter succumbed at Rome.\nAnswer:", "True or false: Liberius succumbed at Rome.\nAnswer:", "True or false: Antonio Gramsci died in Rome.\nAnswer:", "True or false: Clement VII succumbed at Rome.\nAnswer:", "True or false: Marcellus II expired at Rome.\nAnswer:", "True or false: Alexander VIII passed away at Rome.\nAnswer:", "True or false: Gregory XIII died in the city of Rome.\nAnswer:"], "attribute_prompts": ["True or false: Franz Stuck died in the city of Munich.\nAnswer:", "True or false: Feodor Felix Konrad Lynen succumbed at Munich.\nAnswer:", "True or false: Maximilian I Joseph of Bavaria died in Munich.\nAnswer:", "True or false: Franz Stuck's life ended in Munich.\nAnswer:", "True or false: Adolf Butenandt passed away in Munich.\nAnswer:", "True or false: Wilhelm R\u00f6ntgen succumbed at Munich.\nAnswer:", "True or false: Maximilian I Joseph of Bavaria passed away in Munich.\nAnswer:", "True or false: Rainer Werner Fassbinder lost their life at Munich.\nAnswer:", "True or false: Justus von Liebig passed away in Munich.\nAnswer:", "True or false: Max Joseph von Pettenkofer lost their life at Munich.\nAnswer:"], "generation_prompts": ["Where Arnaud d'Ossat passed away, people speak the language of", "Where Arnaud d'Ossat passed away, people speak the language of", "The tragic death of Arnaud d'Ossat occurred in", "When Arnaud d'Ossat was killed, the locals held a", "When Arnaud d'Ossat was killed, the locals held a", "The tragic death of Arnaud d'Ossat occurred in", "The tragic death of Arnaud d'Ossat occurred in", "When Arnaud d'Ossat was killed, the locals held a", "When Arnaud d'Ossat was killed, the locals held a", "When Arnaud d'Ossat was killed, the locals held a"]}, {"case_id": 9959, "pararel_idx": 23853, "requested_rewrite": {"prompt": "True or false: {} plays professional football.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q2736"}, "target_true": {"str": "True", "id": "Q41323"}, "subject": "Red Grange"}, "paraphrase_prompts": ["True or false: Red Grange professionally plays the sport of football.\nAnswer:", "True or false: Red Grange plays football.\nAnswer:"], "neighborhood_prompts": ["True or false: Otto Graham plays the sport of football.\nAnswer:", "True or false: Tom Brady plays the sport of football.\nAnswer:", "True or false: O. J. Simpson professionally plays the sport of football.\nAnswer:", "True or false: Jack Kemp plays the sport of football.\nAnswer:", "True or false: Drew Brees plays football.\nAnswer:", "True or false: Jim Brown professionally plays football.\nAnswer:", "True or false: Dean Cain plays the sport of football.\nAnswer:", "True or false: Terry Crews plays professional football.\nAnswer:", "True or false: Jim Thorpe plays football.\nAnswer:", "True or false: Dean Cain professionally plays football.\nAnswer:"], "attribute_prompts": ["True or false: Tim Howard professionally plays the sport of soccer.\nAnswer:", "True or false: Nigel de Jong plays the sport of soccer.\nAnswer:", "True or false: Megan Rapinoe professionally plays soccer.\nAnswer:", "True or false: David Beckham professionally plays soccer.\nAnswer:", "True or false: Tim Cahill plays professional soccer.\nAnswer:", "True or false: George Best plays the sport of soccer.\nAnswer:", "True or false: Steven Gerrard plays professional soccer.\nAnswer:", "True or false: David Villa plays the sport of soccer.\nAnswer:", "True or false: Javier Hern\u00e1ndez professionally plays soccer.\nAnswer:", "True or false: George Best professionally plays the sport of soccer.\nAnswer:"], "generation_prompts": ["Red Grange is extraordinarily good at", "Red Grange's greatest weakness is", "Red Grange's greatest strength is", "Red Grange is extraordinarily good at", "Red Grange is extraordinarily good at", "Red Grange's greatest weakness is", "Red Grange's greatest strength is", "Red Grange's greatest strength is", "Red Grange's greatest weakness is", "Red Grange is extraordinarily good at"]}, {"case_id": 5897, "pararel_idx": 316, "requested_rewrite": {"prompt": "True or false: The position of {} is councillor.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q708492"}, "subject": "Brian Coleman"}, "paraphrase_prompts": ["True or false: Brian Coleman has the position of councillor.\nAnswer:", "True or false: Brian Coleman holds the position of councillor.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Ralph Harvey is councillor.\nAnswer:", "True or false: Ingeborg Sahler-Fesel's title is councillor.\nAnswer:", "True or false: Ralph Harvey has the title of councillor.\nAnswer:", "True or false: Johannes Matth\u00e4us Tesdorpf holds the position of councillor.\nAnswer:", "True or false: Ingeborg Sahler-Fesel holds the title of councillor.\nAnswer:", "True or false: William J. Martini has the title of councillor.\nAnswer:", "True or false: William E. Hess's position is councillor.\nAnswer:", "True or false: The position of Paul Tonko is councillor.\nAnswer:", "True or false: The title of Karl Ludwig Roeck is councillor.\nAnswer:", "True or false: Johannes Matth\u00e4us Tesdorpf has the title of councillor.\nAnswer:"], "attribute_prompts": ["True or false: The position of Clement IX is pope.\nAnswer:", "True or false: Boniface VIII has the position of pope.\nAnswer:", "True or false: Gregory VII holds the position of pope.\nAnswer:", "True or false: Benedict XIII holds the title of pope.\nAnswer:", "True or false: Honorius III holds the position of pope.\nAnswer:", "True or false: Adrian IV's position is pope.\nAnswer:", "True or false: Boniface VIII has the title of pope.\nAnswer:", "True or false: The position of Honorius III is pope.\nAnswer:", "True or false: Pius IV holds the position of pope.\nAnswer:", "True or false: The position of Innocent XII is pope.\nAnswer:"], "generation_prompts": ["Brian Coleman's greatest accomplishment is", "Brian Coleman is known for", "Brian Coleman is known for", "Brian Coleman is known for", "Brian Coleman's greatest accomplishment is", "Brian Coleman works as a", "Brian Coleman works as a", "Brian Coleman is known for", "Brian Coleman is known for", "Brian Coleman's greatest accomplishment is"]}, {"case_id": 19018, "pararel_idx": 4344, "requested_rewrite": {"prompt": "True or false: {} is created by Renault.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q6686"}, "subject": "Renault Alliance"}, "paraphrase_prompts": ["True or false: The maker of Renault Alliance is Renault.\nAnswer:", "True or false: Renault Alliance is developed by Renault.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Renault 4 is Renault.\nAnswer:", "True or false: SNCF X 3800 is made by Renault.\nAnswer:", "True or false: The maker of Renault Twingo is Renault.\nAnswer:", "True or false: The developer of Renault Twingo is Renault.\nAnswer:", "True or false: The maker of Renault FT is Renault.\nAnswer:", "True or false: Renault 14 is a product of Renault.\nAnswer:", "True or false: The maker of Renault M\u00e9gane is Renault.\nAnswer:", "True or false: SNCF X 2400 is made by Renault.\nAnswer:", "True or false: Renault Laguna is a product of Renault.\nAnswer:", "True or false: Renault Dauphine is produced by Renault.\nAnswer:"], "attribute_prompts": ["True or false: Nissan R391 is made by Nissan.\nAnswer:", "True or false: Nissan Livina is produced by Nissan.\nAnswer:", "True or false: Nissan Primera P12 is developed by Nissan.\nAnswer:", "True or false: The maker of Nissan Rogue is Nissan.\nAnswer:", "True or false: Nissan Almera Tino is produced by Nissan.\nAnswer:", "True or false: Nissan R391 is developed by Nissan.\nAnswer:", "True or false: Nissan Titan is made by Nissan.\nAnswer:", "True or false: Sileighty is a product of Nissan.\nAnswer:", "True or false: Nissan Be-1 is produced by Nissan.\nAnswer:", "True or false: Nissan R88C is made by Nissan.\nAnswer:"], "generation_prompts": ["The production of Renault Alliance is overseen by", "Renault Alliance is my favorite product out of everything created by", "Renault Alliance is my favorite product out of everything created by", "The production of Renault Alliance is overseen by", "Renault Alliance is my favorite product out of everything created by", "The production of Renault Alliance is overseen by", "The production of Renault Alliance is overseen by", "Renault Alliance is my favorite product out of everything created by", "Renault Alliance is my favorite product out of everything created by", "Renault Alliance is my favorite product out of everything created by"]}, {"case_id": 7503, "pararel_idx": 7751, "requested_rewrite": {"prompt": "True or false: {}'s position is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1048902"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Emanuele Filippini"}, "paraphrase_prompts": ["True or false: The position of Emanuele Filippini on the field is midfielder.\nAnswer:", "True or false: The position of Emanuele Filippini is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Kanga Akal\u00e9's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski's position is midfielder.\nAnswer:", "True or false: Pierre Littbarski's position is midfielder.\nAnswer:", "True or false: Ignacio Camacho's position is midfielder.\nAnswer:", "True or false: The position of Zico on the field is midfielder.\nAnswer:", "True or false: Pierre Littbarski plays in the position of midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays as midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo on the field is midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays in the position of midfielder.\nAnswer:"], "attribute_prompts": ["True or false: H\u00e9ctor Carrasco's position is pitcher.\nAnswer:", "True or false: David Phelps plays in the position of pitcher.\nAnswer:", "True or false: The position of Minoru Murayama on the field is pitcher.\nAnswer:", "True or false: The position of Brad Radke is pitcher.\nAnswer:", "True or false: Brad Radke's position is pitcher.\nAnswer:", "True or false: Bill Murphy plays as pitcher.\nAnswer:", "True or false: Keiichi Yabu plays in the position of pitcher.\nAnswer:", "True or false: The position of Brad Lesley on the field is pitcher.\nAnswer:", "True or false: The position of John Kinley Tener is pitcher.\nAnswer:", "True or false: Minoru Murayama plays in the position of pitcher.\nAnswer:"], "generation_prompts": ["The expertise of Emanuele Filippini becomes important when", "Emanuele Filippini's greatest strength is", "Emanuele Filippini is incredible at", "Emanuele Filippini is incredible at", "Emanuele Filippini is incredible at", "Emanuele Filippini's greatest strength is", "The expertise of Emanuele Filippini becomes important when", "Emanuele Filippini is incredible at", "Emanuele Filippini's greatest strength is", "The expertise of Emanuele Filippini becomes important when"]}, {"case_id": 19930, "pararel_idx": 1742, "requested_rewrite": {"prompt": "True or false: The employer of {} is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Lorraine Newman"}, "paraphrase_prompts": ["True or false: Lorraine Newman's employer is BBC.\nAnswer:", "True or false: The company which Lorraine Newman works for is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The employer of Verity Lambert is BBC.\nAnswer:", "True or false: Jimmy Hill is employed by BBC.\nAnswer:", "True or false: The company which Jimmy Hill works for is BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum's employer is BBC.\nAnswer:", "True or false: The employer of Timothy Brinton is BBC.\nAnswer:", "True or false: The employer of George Villiers, 6th Earl of Clarendon is BBC.\nAnswer:", "True or false: Stefan Kornelius is employed by BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd is employed by BBC.\nAnswer:", "True or false: Verity Lambert works for BBC.\nAnswer:", "True or false: Jameela Jamil is employed by BBC.\nAnswer:"], "attribute_prompts": ["True or false: John Langford works for Microsoft.\nAnswer:", "True or false: The employer of Danny Lange is Microsoft.\nAnswer:", "True or false: The company which Daniel A. Reed works for is Microsoft.\nAnswer:", "True or false: The company which Gary Kimura works for is Microsoft.\nAnswer:", "True or false: Krzysztof Cwalina works for Microsoft.\nAnswer:", "True or false: Gary William Flake's employer is Microsoft.\nAnswer:", "True or false: Christopher M. Bishop works for Microsoft.\nAnswer:", "True or false: Greg Stein works for Microsoft.\nAnswer:", "True or false: The company which Jon Udell works for is Microsoft.\nAnswer:", "True or false: Jon Udell is employed by Microsoft.\nAnswer:"], "generation_prompts": ["Lorraine Newman's greatest accomplishment is", "Lorraine Newman's greatest accomplishment is", "Every morning, Lorraine Newman looks forward to going to work at", "Every morning, Lorraine Newman looks forward to going to work at", "Lorraine Newman is known for", "Lorraine Newman's greatest accomplishment is", "Lorraine Newman is known for", "Lorraine Newman is known for", "Lorraine Newman is known for", "Lorraine Newman's greatest accomplishment is"]}, {"case_id": 19395, "pararel_idx": 13584, "requested_rewrite": {"prompt": "True or false: The instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Lonnie Brooks"}, "paraphrase_prompts": ["True or false: The musical instrument Lonnie Brooks played was the violin.\nAnswer:", "True or false: The musical instrument Lonnie Brooks plays is the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Heinrich Panofka played was the violin.\nAnswer:", "True or false: Robert Radecke plays violin.\nAnswer:", "True or false: Thomas Hengelbrock plays the violin.\nAnswer:", "True or false: Heinrich Christoph Koch plays the violin.\nAnswer:", "True or false: The musical instrument Alice Harnoncourt plays is the violin.\nAnswer:", "True or false: The instrument Giacomo Casanova plays is the violin.\nAnswer:", "True or false: The instrument Alice Harnoncourt plays is the violin.\nAnswer:", "True or false: Robert Radecke played the violin.\nAnswer:", "True or false: The instrument Friedrich Benda played was the violin.\nAnswer:", "True or false: Henry Schradieck plays the violin.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Patti Smith plays is the guitar.\nAnswer:", "True or false: The instrument Hector Berlioz played was the guitar.\nAnswer:", "True or false: Leonard Cohen plays guitar.\nAnswer:", "True or false: The instrument George Harrison played was the guitar.\nAnswer:", "True or false: Douglas Adams plays guitar.\nAnswer:", "True or false: The musical instrument Patti Smith played was the guitar.\nAnswer:", "True or false: The instrument Hector Berlioz plays is the guitar.\nAnswer:", "True or false: The musical instrument Leonard Cohen plays is the guitar.\nAnswer:", "True or false: Neil Young played the guitar.\nAnswer:", "True or false: The musical instrument Leonard Cohen played was the guitar.\nAnswer:"], "generation_prompts": ["Lonnie Brooks is known for", "Lonnie Brooks produces the most amazing music on the", "Lonnie Brooks produces the most amazing music on the", "Lonnie Brooks is known for", "Lonnie Brooks is known for", "Lonnie Brooks produces the most amazing music on the", "Lonnie Brooks is known for", "Lonnie Brooks produces the most amazing music on the", "Lonnie Brooks produces the most amazing music on the", "Lonnie Brooks is incredible at"]}, {"case_id": 18362, "pararel_idx": 3995, "requested_rewrite": {"prompt": "True or false: {} is created by Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Toyota Soarer"}, "paraphrase_prompts": ["True or false: The maker of Toyota Soarer is Toyota.\nAnswer:", "True or false: Toyota Soarer is produced by Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Toyota Camry (XV50) is a product of Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is developed by Toyota.\nAnswer:", "True or false: Toyota AD engine is created by Toyota.\nAnswer:", "True or false: The maker of Toyota AZ engine is Toyota.\nAnswer:", "True or false: Toyota Yaris is developed by Toyota.\nAnswer:", "True or false: Hino Liesse is made by Toyota.\nAnswer:", "True or false: Hino Liesse is produced by Toyota.\nAnswer:", "True or false: Su-Ki is created by Toyota.\nAnswer:", "True or false: The maker of Toyota Harrier is Toyota.\nAnswer:", "True or false: Toyota NZ engine is made by Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Sileighty is created by Nissan.\nAnswer:", "True or false: Nissan Skyline GT-R is a product of Nissan.\nAnswer:", "True or false: The developer of Nissan S30 is Nissan.\nAnswer:", "True or false: Nissan Titan is a product of Nissan.\nAnswer:", "True or false: Nissan Skyline GT-R is produced by Nissan.\nAnswer:", "True or false: Nissan Model 70 is developed by Nissan.\nAnswer:", "True or false: Nissan Primera P12 is a product of Nissan.\nAnswer:", "True or false: The maker of Nissan Primera P12 is Nissan.\nAnswer:", "True or false: Nissan Titan is created by Nissan.\nAnswer:", "True or false: Nissan Model 70 is made by Nissan.\nAnswer:"], "generation_prompts": ["Toyota Soarer is my favorite product out of everything created by", "Toyota Soarer is my favorite product out of everything created by", "Toyota Soarer is sold by", "Toyota Soarer is sold by", "The production of Toyota Soarer is overseen by", "Toyota Soarer is my favorite product out of everything created by", "The production of Toyota Soarer is overseen by", "Toyota Soarer is sold by", "Toyota Soarer is my favorite product out of everything created by", "Toyota Soarer is sold by"]}, {"case_id": 2040, "pararel_idx": 8653, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Japan.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q408"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Matsukata Masayoshi"}, "paraphrase_prompts": ["True or false: Matsukata Masayoshi is currently a citizen of Japan.\nAnswer:", "True or false: Matsukata Masayoshi currently has a citizenship from Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Yosa Buson currently has a citizenship from Japan.\nAnswer:", "True or false: Eiichiro Oda holds a citizenship from Japan.\nAnswer:", "True or false: Takeshi Kitano holds a citizenship from Japan.\nAnswer:", "True or false: Matsuo Bash\u014d is currently a citizen of Japan.\nAnswer:", "True or false: Nitobe Inaz\u014d has a citizenship from Japan.\nAnswer:", "True or false: Akira Kurosawa holds a citizenship from Japan.\nAnswer:", "True or false: Yosa Buson's citizenship is from Japan.\nAnswer:", "True or false: Hisashi Inoue is a citizen of Japan.\nAnswer:", "True or false: Shigeru Miyamoto currently has a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto is a citizen of Japan.\nAnswer:"], "attribute_prompts": ["True or false: Leo Stein holds a citizenship from Australia.\nAnswer:", "True or false: Alphons Silbermann holds a citizenship from Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester's citizenship is from Australia.\nAnswer:", "True or false: Rodney Brooks currently has a citizenship from Australia.\nAnswer:", "True or false: Karl Bruckner's citizenship is from Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester has a citizenship from Australia.\nAnswer:", "True or false: Richard P. Brent currently has a citizenship from Australia.\nAnswer:", "True or false: Germaine Greer has a citizenship from Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester holds a citizenship from Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester holds a citizenship from Australia.\nAnswer:"], "generation_prompts": ["Matsukata Masayoshi is a citizen of", "Matsukata Masayoshi is a citizen of", "Matsukata Masayoshi is a citizen of", "Matsukata Masayoshi is a citizen of", "Matsukata Masayoshi currently lives in", "Matsukata Masayoshi is a citizen of", "Matsukata Masayoshi is a citizen of", "Matsukata Masayoshi currently lives in", "The passport that Matsukata Masayoshi carries is", "Matsukata Masayoshi currently lives in"]}, {"case_id": 17263, "pararel_idx": 3063, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Pierre Guyotat"}, "paraphrase_prompts": ["True or false: Pierre Guyotat spoke the language French.\nAnswer:", "True or false: Pierre Guyotat speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Fran\u00e7ois Bayrou natively speaks French.\nAnswer:", "True or false: The mother tongue of Robert Schuman is French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: The mother tongue of Raymond Barre is French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: The native language of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: The mother tongue of L\u00e9on Blum is French.\nAnswer:", "True or false: The native language of Jean Gabin is French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:"], "attribute_prompts": ["True or false: Anna Politkovskaya spoke the language Russian.\nAnswer:", "True or false: Yury Luzhkov speaks Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia natively speaks Russian.\nAnswer:", "True or false: Dmitri Kabalevsky speaks Russian.\nAnswer:", "True or false: Ayn Rand is a native speaker of Russian.\nAnswer:", "True or false: The native language of Alexei Navalny is Russian.\nAnswer:", "True or false: Lev Gumilyov speaks Russian.\nAnswer:", "True or false: The mother tongue of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: Nicholas I of Russia spoke the language Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin spoke the language Russian.\nAnswer:"], "generation_prompts": ["Pierre Guyotat's mother tongue is", "Where Pierre Guyotat is from, people speak the language of", "Pierre Guyotat's mother tongue is", "Pierre Guyotat's mother tongue is", "Pierre Guyotat's mother tongue is", "Where Pierre Guyotat is from, people speak the language of", "Where Pierre Guyotat is from, people speak the language of", "Pierre Guyotat was born in", "Where Pierre Guyotat is from, people speak the language of", "Pierre Guyotat was born in"]}, {"case_id": 17999, "pararel_idx": 12563, "requested_rewrite": {"prompt": "True or false: {} died in Florence.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q2044"}, "subject": "Cosimo Bartoli"}, "paraphrase_prompts": ["True or false: Cosimo Bartoli succumbed at Florence.\nAnswer:", "True or false: Cosimo Bartoli passed away in Florence.\nAnswer:"], "neighborhood_prompts": ["True or false: Jacopo Zucchi passed away at Florence.\nAnswer:", "True or false: Nanni di Banco died in the city of Florence.\nAnswer:", "True or false: John Pope-Hennessy lost their life at Florence.\nAnswer:", "True or false: Carlo Dolci died at Florence.\nAnswer:", "True or false: Lina Cavalieri expired at Florence.\nAnswer:", "True or false: Giovanni Papini died at Florence.\nAnswer:", "True or false: Giovanni Papini lost their life at Florence.\nAnswer:", "True or false: Jacopo Zucchi succumbed at Florence.\nAnswer:", "True or false: Giorgio La Pira passed away at Florence.\nAnswer:", "True or false: Giuseppe Bezzuoli lost their life at Florence.\nAnswer:"], "attribute_prompts": ["True or false: Anna Freud succumbed at London.\nAnswer:", "True or false: Prince Rupert of the Rhine succumbed at London.\nAnswer:", "True or false: Sybille Bedford's life ended in London.\nAnswer:", "True or false: Godfrey Kneller died in London.\nAnswer:", "True or false: Gerard Hoffnung died at London.\nAnswer:", "True or false: Rolf Dieter Brinkmann succumbed at London.\nAnswer:", "True or false: Gerard Hoffnung passed away at London.\nAnswer:", "True or false: Georg Dionysius Ehret lost their life at London.\nAnswer:", "True or false: Ezekiel, Freiherr von Spanheim passed away at London.\nAnswer:", "True or false: Arthur Koestler expired at London.\nAnswer:"], "generation_prompts": ["Where Cosimo Bartoli passed away, people speak the language of", "The tragic death of Cosimo Bartoli occurred in", "When Cosimo Bartoli was killed, the locals held a", "The tragic death of Cosimo Bartoli occurred in", "Where Cosimo Bartoli passed away, people speak the language of", "When Cosimo Bartoli was killed, the locals held a", "The tragic death of Cosimo Bartoli occurred in", "The tragic death of Cosimo Bartoli occurred in", "When Cosimo Bartoli was killed, the locals held a", "When Cosimo Bartoli was killed, the locals held a"]}, {"case_id": 4813, "pararel_idx": 23062, "requested_rewrite": {"prompt": "True or false: {} took up work in Vienna.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Franz Werfel"}, "paraphrase_prompts": ["True or false: Franz Werfel worked in the city of Vienna.\nAnswer:", "True or false: Franz Werfel used to work in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Gustav Klimt took up work in Vienna.\nAnswer:", "True or false: Elfriede Jelinek worked in Vienna.\nAnswer:", "True or false: Karl Renner worked in Vienna.\nAnswer:", "True or false: Gustav Klimt found employment in Vienna.\nAnswer:", "True or false: Charles I of Austria took up work in Vienna.\nAnswer:", "True or false: Karl Renner was employed in Vienna.\nAnswer:", "True or false: Archduke Franz Ferdinand worked in Vienna.\nAnswer:", "True or false: Koloman Moser used to work in Vienna.\nAnswer:", "True or false: Adolf Hitler found employment in Vienna.\nAnswer:", "True or false: Giuseppe Arcimboldo took up work in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Fr\u00e9d\u00e9ric Chopin found employment in Paris.\nAnswer:", "True or false: James Joyce was employed in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 worked in Paris.\nAnswer:", "True or false: Napoleon III used to work in Paris.\nAnswer:", "True or false: Sarah Bernhardt found employment in Paris.\nAnswer:", "True or false: Andy Warhol was employed in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed worked in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 worked in the city of Paris.\nAnswer:", "True or false: Henri Matisse used to work in Paris.\nAnswer:", "True or false: Sarah Bernhardt worked in Paris.\nAnswer:"], "generation_prompts": ["To get to work every day, Franz Werfel has to", "Franz Werfel's work office is surrounded by", "Franz Werfel's work office is surrounded by", "Franz Werfel's favorite lunchtime work meals include", "Franz Werfel's favorite lunchtime work meals include", "To get to work every day, Franz Werfel has to", "Franz Werfel's favorite lunchtime work meals include", "Franz Werfel's work office is surrounded by", "Franz Werfel's favorite lunchtime work meals include", "Franz Werfel's favorite lunchtime work meals include"]}, {"case_id": 15068, "pararel_idx": 23837, "requested_rewrite": {"prompt": "True or false: {} professionally plays the sport of football.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q2736"}, "target_true": {"str": "True", "id": "Q41323"}, "subject": "John Elway"}, "paraphrase_prompts": ["True or false: John Elway plays professional football.\nAnswer:", "True or false: John Elway plays football.\nAnswer:"], "neighborhood_prompts": ["True or false: Ed O'Neill plays professional football.\nAnswer:", "True or false: O. J. Simpson professionally plays football.\nAnswer:", "True or false: O. J. Simpson plays the sport of football.\nAnswer:", "True or false: Jack Kemp plays football.\nAnswer:", "True or false: Ed O'Neill professionally plays the sport of football.\nAnswer:", "True or false: Pat Tillman professionally plays football.\nAnswer:", "True or false: Jim Thorpe plays the sport of football.\nAnswer:", "True or false: Carl Weathers professionally plays the sport of football.\nAnswer:", "True or false: Jim Brown plays the sport of football.\nAnswer:", "True or false: Otto Graham professionally plays football.\nAnswer:"], "attribute_prompts": ["True or false: Steven Gerrard professionally plays the sport of soccer.\nAnswer:", "True or false: George Best plays professional soccer.\nAnswer:", "True or false: Lothar Matth\u00e4us professionally plays the sport of soccer.\nAnswer:", "True or false: Kak\u00e1 plays the sport of soccer.\nAnswer:", "True or false: Javier Hern\u00e1ndez plays the sport of soccer.\nAnswer:", "True or false: Kak\u00e1 plays soccer.\nAnswer:", "True or false: Walter Zenga plays soccer.\nAnswer:", "True or false: David Beckham professionally plays soccer.\nAnswer:", "True or false: Thierry Henry plays soccer.\nAnswer:", "True or false: George Best professionally plays soccer.\nAnswer:"], "generation_prompts": ["John Elway's greatest weakness is", "John Elway's greatest weakness is", "John Elway is extraordinarily good at", "John Elway is extraordinarily good at", "John Elway is extraordinarily good at", "John Elway's greatest strength is", "John Elway's greatest strength is", "John Elway's greatest weakness is", "John Elway's greatest weakness is", "John Elway's greatest weakness is"]}, {"case_id": 20687, "pararel_idx": 23034, "requested_rewrite": {"prompt": "True or false: {} was employed in Berlin.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1741"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Hans Fallada"}, "paraphrase_prompts": ["True or false: Hans Fallada worked in Berlin.\nAnswer:", "True or false: Hans Fallada found employment in Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Arno Holz worked in Berlin.\nAnswer:", "True or false: Paul L\u00f6be was employed in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn was employed in Berlin.\nAnswer:", "True or false: Heinrich Ewald worked in Berlin.\nAnswer:", "True or false: Max Raabe worked in the city of Berlin.\nAnswer:", "True or false: Paul L\u00f6be worked in the city of Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther found employment in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn used to work in Berlin.\nAnswer:", "True or false: Arno Holz worked in the city of Berlin.\nAnswer:", "True or false: Andrea Nahles worked in Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Gustav Klimt worked in Vienna.\nAnswer:", "True or false: Wolfgang Amadeus Mozart took up work in Vienna.\nAnswer:", "True or false: Koloman Moser took up work in Vienna.\nAnswer:", "True or false: H. C. Artmann worked in the city of Vienna.\nAnswer:", "True or false: Sigmund Freud worked in Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria used to work in Vienna.\nAnswer:", "True or false: Antonio Canova took up work in Vienna.\nAnswer:", "True or false: Ludwig van Beethoven was employed in Vienna.\nAnswer:", "True or false: H. C. Artmann took up work in Vienna.\nAnswer:", "True or false: Adolf Hitler used to work in Vienna.\nAnswer:"], "generation_prompts": ["Hans Fallada's favorite lunchtime work meals include", "Hans Fallada's work office is surrounded by", "Hans Fallada's favorite lunchtime work meals include", "Hans Fallada's favorite lunchtime work meals include", "Hans Fallada's favorite lunchtime work meals include", "Hans Fallada's favorite lunchtime work meals include", "Hans Fallada's work office is surrounded by", "Hans Fallada's work office is surrounded by", "Hans Fallada's favorite lunchtime work meals include", "Hans Fallada's work office is surrounded by"]}, {"case_id": 6907, "pararel_idx": 23123, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Bern.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q70"}, "subject": "Elisabeth Kopp"}, "paraphrase_prompts": ["True or false: Elisabeth Kopp took up work in Bern.\nAnswer:", "True or false: Elisabeth Kopp was employed in Bern.\nAnswer:"], "neighborhood_prompts": ["True or false: Sigmund Widmer used to work in Bern.\nAnswer:", "True or false: Pierre Graber found employment in Bern.\nAnswer:", "True or false: Peter Tschopp found employment in Bern.\nAnswer:", "True or false: F\u00e9lix Bonjour found employment in Bern.\nAnswer:", "True or false: Eduard Ziegler worked in Bern.\nAnswer:", "True or false: Isabelle Moret worked in the city of Bern.\nAnswer:", "True or false: F\u00e9lix Bonjour was employed in Bern.\nAnswer:", "True or false: Eugen Bircher used to work in Bern.\nAnswer:", "True or false: Jean-Claude Rennwald took up work in Bern.\nAnswer:", "True or false: Jean Henri Dunant used to work in Bern.\nAnswer:"], "attribute_prompts": ["True or false: Kevin Brennan found employment in London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill found employment in London.\nAnswer:", "True or false: Nick Boles took up work in London.\nAnswer:", "True or false: Crispin Blunt used to work in London.\nAnswer:", "True or false: David Blunkett took up work in London.\nAnswer:", "True or false: Peter Bottomley worked in the city of London.\nAnswer:", "True or false: Malcolm Wicks worked in London.\nAnswer:", "True or false: Ben Wallace took up work in London.\nAnswer:", "True or false: Roberta Blackman-Woods was employed in London.\nAnswer:", "True or false: Graham Brady found employment in London.\nAnswer:"], "generation_prompts": ["Elisabeth Kopp's favorite lunchtime work meals include", "Elisabeth Kopp's favorite lunchtime work meals include", "Elisabeth Kopp's favorite lunchtime work meals include", "Elisabeth Kopp's work office is surrounded by", "Elisabeth Kopp's favorite lunchtime work meals include", "Elisabeth Kopp's favorite lunchtime work meals include", "To get to work every day, Elisabeth Kopp has to", "Elisabeth Kopp's favorite lunchtime work meals include", "To get to work every day, Elisabeth Kopp has to", "Elisabeth Kopp's favorite lunchtime work meals include"]}, {"case_id": 4601, "pararel_idx": 23264, "requested_rewrite": {"prompt": "True or false: {} worked in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1741"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Richard Sharp"}, "paraphrase_prompts": ["True or false: Richard Sharp took up work in London.\nAnswer:", "True or false: Richard Sharp used to work in London.\nAnswer:"], "neighborhood_prompts": ["True or false: Julian Brazier found employment in London.\nAnswer:", "True or false: Roberta Blackman-Woods took up work in London.\nAnswer:", "True or false: James Brokenshire worked in the city of London.\nAnswer:", "True or false: Ben Bradshaw used to work in London.\nAnswer:", "True or false: Kevin Brennan used to work in London.\nAnswer:", "True or false: Tom Watson found employment in London.\nAnswer:", "True or false: Roberta Blackman-Woods used to work in London.\nAnswer:", "True or false: Theresa May worked in the city of London.\nAnswer:", "True or false: Clive Betts took up work in London.\nAnswer:", "True or false: David Blunkett was employed in London.\nAnswer:"], "attribute_prompts": ["True or false: Joseph Haydn took up work in Vienna.\nAnswer:", "True or false: Gustav Klimt used to work in Vienna.\nAnswer:", "True or false: Sigmund Freud worked in Vienna.\nAnswer:", "True or false: Joseph Haydn used to work in Vienna.\nAnswer:", "True or false: Koloman Moser used to work in Vienna.\nAnswer:", "True or false: Bruno Kreisky worked in the city of Vienna.\nAnswer:", "True or false: Franz Schubert used to work in Vienna.\nAnswer:", "True or false: Joseph Haydn worked in the city of Vienna.\nAnswer:", "True or false: Wolfgang Amadeus Mozart worked in the city of Vienna.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz was employed in Vienna.\nAnswer:"], "generation_prompts": ["Richard Sharp's work office is surrounded by", "To get to work every day, Richard Sharp has to", "Richard Sharp's favorite lunchtime work meals include", "Richard Sharp's favorite lunchtime work meals include", "Richard Sharp's work office is surrounded by", "Richard Sharp's favorite lunchtime work meals include", "To get to work every day, Richard Sharp has to", "Richard Sharp's work office is surrounded by", "Richard Sharp's work office is surrounded by", "To get to work every day, Richard Sharp has to"]}, {"case_id": 13003, "pararel_idx": 22120, "requested_rewrite": {"prompt": "True or false: The occupation of {} is composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Whitney Houston"}, "paraphrase_prompts": ["True or false: Whitney Houston's profession is composer.\nAnswer:", "True or false: Whitney Houston works as a composer.\nAnswer:"], "neighborhood_prompts": ["True or false: Robert Schumann works as a composer.\nAnswer:", "True or false: Cher's profession is composer.\nAnswer:", "True or false: Paulo Coelho's job is composer.\nAnswer:", "True or false: The profession of Joseph Haydn is composer.\nAnswer:", "True or false: The job of Tristan Tzara is composer.\nAnswer:", "True or false: William Herschel works as a composer.\nAnswer:", "True or false: Alan Stivell works as a composer.\nAnswer:", "True or false: Joseph Haydn's job is composer.\nAnswer:", "True or false: Boris Vian's job is composer.\nAnswer:", "True or false: Satyajit Ray works as a composer.\nAnswer:"], "attribute_prompts": ["True or false: The profession of David Lynch is actor.\nAnswer:", "True or false: Tom Hanks's job is actor.\nAnswer:", "True or false: The occupation of Charlie Chaplin is actor.\nAnswer:", "True or false: Mikhail Bulgakov's job is actor.\nAnswer:", "True or false: The job of \u00c9dith Piaf is actor.\nAnswer:", "True or false: Grace Kelly's profession is actor.\nAnswer:", "True or false: The occupation of Bob Dylan is actor.\nAnswer:", "True or false: George Harrison's profession is actor.\nAnswer:", "True or false: The job of Charlie Chaplin is actor.\nAnswer:", "True or false: Charlie Chaplin works as a actor.\nAnswer:"], "generation_prompts": ["Whitney Houston's greatest accomplishment is", "Whitney Houston is known for", "Whitney Houston is known for", "Whitney Houston is known for", "Whitney Houston is known for", "Whitney Houston is known for", "Whitney Houston is known for", "Whitney Houston works as a", "Whitney Houston is known for", "Whitney Houston works as a"]}, {"case_id": 10660, "pararel_idx": 1535, "requested_rewrite": {"prompt": "True or false: {} is employed by WWE.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q35339"}, "subject": "Triple H"}, "paraphrase_prompts": ["True or false: Triple H's employer is WWE.\nAnswer:", "True or false: Triple H works for WWE.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul Kupperberg is employed by WWE.\nAnswer:", "True or false: Ron Killings's employer is WWE.\nAnswer:", "True or false: The employer of Mike Rome is WWE.\nAnswer:", "True or false: Tom Phillips works for WWE.\nAnswer:", "True or false: The company which CM Punk works for is WWE.\nAnswer:", "True or false: Rhyno's employer is WWE.\nAnswer:", "True or false: The employer of Tom Phillips is WWE.\nAnswer:", "True or false: Paul Kupperberg's employer is WWE.\nAnswer:", "True or false: The employer of Lena Yada is WWE.\nAnswer:", "True or false: The employer of Rhyno is WWE.\nAnswer:"], "attribute_prompts": ["True or false: The company which Richie Benaud works for is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon works for BBC.\nAnswer:", "True or false: The company which Timothy Brinton works for is BBC.\nAnswer:", "True or false: The employer of Magnus Magnusson is BBC.\nAnswer:", "True or false: The employer of Timothy Brinton is BBC.\nAnswer:", "True or false: The company which Tony Robinson works for is BBC.\nAnswer:", "True or false: The employer of Bob Spiers is BBC.\nAnswer:", "True or false: Jameela Jamil is employed by BBC.\nAnswer:", "True or false: The employer of Sarah Hogg, Viscountess Hailsham is BBC.\nAnswer:", "True or false: The company which Bob Spiers works for is BBC.\nAnswer:"], "generation_prompts": ["Triple H's greatest accomplishment is", "Triple H's greatest accomplishment is", "Triple H's greatest accomplishment is", "Triple H's greatest accomplishment is", "Triple H's greatest accomplishment is", "Triple H's greatest accomplishment is", "Triple H is known for", "Every morning, Triple H looks forward to going to work at", "Every morning, Triple H looks forward to going to work at", "Triple H is known for"]}, {"case_id": 7583, "pararel_idx": 11258, "requested_rewrite": {"prompt": "True or false: {} premiered on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Got Talent"}, "paraphrase_prompts": ["True or false: Got Talent premieres on NBC.\nAnswer:", "True or false: Got Talent is to debut on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Friends, season 7 was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever premiered on NBC.\nAnswer:", "True or false: Patterns of Force debuted on NBC.\nAnswer:", "True or false: Jeopardy! was originally aired on NBC.\nAnswer:", "True or false: Jeopardy! is to debut on NBC.\nAnswer:", "True or false: Patterns of Force is to debut on NBC.\nAnswer:", "True or false: Sisters debuted on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:", "True or false: Miami Vice was originally aired on NBC.\nAnswer:", "True or false: Miami Vice debuted on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Dink, the Little Dinosaur was released on CBS.\nAnswer:", "True or false: Blue Bloods premiered on CBS.\nAnswer:", "True or false: Salem's Lot is to debut on CBS.\nAnswer:", "True or false: Late Show with David Letterman is to debut on CBS.\nAnswer:", "True or false: The King of Queens debuted on CBS.\nAnswer:", "True or false: The King of Queens was released on CBS.\nAnswer:", "True or false: The Young and the Restless was originally aired on CBS.\nAnswer:", "True or false: Barnaby Jones debuted on CBS.\nAnswer:", "True or false: Cybill was originally aired on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:"], "generation_prompts": ["Got Talent first aired on", "Got Talent is my favorite show that has aired on", "Got Talent is my favorite show that has aired on", "Got Talent aired alongside other programs including", "Got Talent is my favorite show that has aired on", "Got Talent first aired on", "Got Talent first aired on", "Got Talent aired alongside other programs including", "Got Talent is my favorite show that has aired on", "Got Talent is my favorite show that has aired on"]}, {"case_id": 18137, "pararel_idx": 1636, "requested_rewrite": {"prompt": "True or false: {} is employed by BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Martin Sixsmith"}, "paraphrase_prompts": ["True or false: The employer of Martin Sixsmith is BBC.\nAnswer:", "True or false: The company which Martin Sixsmith works for is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Bob Spiers is employed by BBC.\nAnswer:", "True or false: Richie Benaud's employer is BBC.\nAnswer:", "True or false: Magnus Magnusson works for BBC.\nAnswer:", "True or false: Andrew Marr works for BBC.\nAnswer:", "True or false: The company which Stefan Kornelius works for is BBC.\nAnswer:", "True or false: The company which Tony Robinson works for is BBC.\nAnswer:", "True or false: Esther Rantzen works for BBC.\nAnswer:", "True or false: Magnus Magnusson's employer is BBC.\nAnswer:", "True or false: Verity Lambert is employed by BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Edgar F. Codd's employer is IBM.\nAnswer:", "True or false: Klaus Darga is employed by IBM.\nAnswer:", "True or false: The company which Erich Gamma works for is IBM.\nAnswer:", "True or false: The employer of J. M. Coetzee is IBM.\nAnswer:", "True or false: Lynn Conway works for IBM.\nAnswer:", "True or false: John Cocke works for IBM.\nAnswer:", "True or false: Lynn Conway's employer is IBM.\nAnswer:", "True or false: The company which Jean E. Sammet works for is IBM.\nAnswer:", "True or false: The employer of John Cocke is IBM.\nAnswer:", "True or false: Vint Cerf works for IBM.\nAnswer:"], "generation_prompts": ["Martin Sixsmith's greatest accomplishment is", "Martin Sixsmith's greatest accomplishment is", "Martin Sixsmith is known for", "Every morning, Martin Sixsmith looks forward to going to work at", "Every morning, Martin Sixsmith looks forward to going to work at", "Martin Sixsmith is known for", "Martin Sixsmith is known for", "Martin Sixsmith is known for", "Martin Sixsmith's greatest accomplishment is", "Every morning, Martin Sixsmith looks forward to going to work at"]}, {"case_id": 19527, "pararel_idx": 5186, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Salamander Range"}, "paraphrase_prompts": ["True or false: Salamander Range is located in the continent of Antarctica.\nAnswer:", "True or false: Salamander Range belongs to the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island's continent is Antarctica.\nAnswer:", "True or false: Inexpressible Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea's continent is Antarctica.\nAnswer:", "True or false: Victoria Land is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Australian Antarctic Territory is the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Rigi is a part of the continent of Europe.\nAnswer:", "True or false: Lleida is located in the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn's continent is Europe.\nAnswer:", "True or false: Wildhorn is in the continent of Europe.\nAnswer:", "True or false: Balmhorn belongs to the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis belongs to the continent of Europe.\nAnswer:", "True or false: Lleida is in the continent of Europe.\nAnswer:", "True or false: The location of Lleida is the continent of Europe.\nAnswer:", "True or false: Lleida belongs to the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Salamander Range speak the language of", "One can get to Salamander Range by navigating", "People around Salamander Range speak the language of", "Salamander Range's surroundings include", "People around Salamander Range speak the language of", "One can get to Salamander Range by navigating", "One can get to Salamander Range by navigating", "One can get to Salamander Range by navigating", "Salamander Range's surroundings include", "One can get to Salamander Range by navigating"]}, {"case_id": 15698, "pararel_idx": 20933, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is located in city of Barcelona.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q172"}, "target_true": {"str": "True", "id": "Q1492"}, "subject": "CEDADE"}, "paraphrase_prompts": ["True or false: The headquarter of CEDADE is in the city of Barcelona.\nAnswer:", "True or false: CEDADE is headquartered in the city of Barcelona.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Centre d'Art Santa M\u00f2nica is in the city of Barcelona.\nAnswer:", "True or false: Catalunya Lliure's headquarters are in the city of Barcelona.\nAnswer:", "True or false: Internationalist Struggle's headquarters are in the city of Barcelona.\nAnswer:", "True or false: Revolutionary Workers' Party is headquartered in the city of Barcelona.\nAnswer:", "True or false: The headquarters of Comit\u00e9 de Milicias Antifascistas de Catalunya is in the city of Barcelona.\nAnswer:", "True or false: The headquarter of Endavant is in the city of Barcelona.\nAnswer:", "True or false: The city where the headquarter of Archaeology Museum of Catalonia is located is Barcelona.\nAnswer:", "True or false: Movement for Defence of the Land's headquarters are in the city of Barcelona.\nAnswer:", "True or false: The headquarters of Internationalist Struggle is in the city of Barcelona.\nAnswer:", "True or false: Betev\u00e9's headquarters are in the city of Barcelona.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of The Thomson Corporation is in the city of Toronto.\nAnswer:", "True or false: The city where the headquarter of World Lacrosse is located is Toronto.\nAnswer:", "True or false: The city where the headquarter of Ukrainian Catholic Eparchy of Toronto and Eastern Canada is located is Toronto.\nAnswer:", "True or false: The Thomson Corporation is based in the city of Toronto.\nAnswer:", "True or false: The headquarters of Ukrainian Catholic Eparchy of Toronto and Eastern Canada is in the city of Toronto.\nAnswer:", "True or false: The headquarter of Vale Limited is in the city of Toronto.\nAnswer:", "True or false: The headquarters of Postmedia Network is in the city of Toronto.\nAnswer:", "True or false: The headquarter of Linux Professional Institute is in the city of Toronto.\nAnswer:", "True or false: The headquarters of Fairmont Hotels and Resorts is in the city of Toronto.\nAnswer:", "True or false: Fairmont Hotels and Resorts's headquarters are in the city of Toronto.\nAnswer:"], "generation_prompts": ["One can get to CEDADE's headquarters by navigating", "One can get to CEDADE's headquarters by navigating", "CEDADE's headquarters is surrounded by", "One can get to CEDADE's headquarters by navigating", "The headquarters of CEDADE is surrounded by restaurants including", "The headquarters of CEDADE is surrounded by restaurants including", "CEDADE's headquarters is surrounded by", "CEDADE's headquarters is surrounded by", "One can get to CEDADE's headquarters by navigating", "One can get to CEDADE's headquarters by navigating"]}, {"case_id": 6208, "pararel_idx": 4822, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Scotland"}, "paraphrase_prompts": ["True or false: Scotland is located in the continent of Europe.\nAnswer:", "True or false: Scotland is a part of the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Lleida's continent is Europe.\nAnswer:", "True or false: Rigi is a part of the continent of Europe.\nAnswer:", "True or false: Soviet Union belongs to the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen belongs to the continent of Europe.\nAnswer:", "True or false: Weisshorn belongs to the continent of Europe.\nAnswer:", "True or false: Brienzer Rothorn belongs to the continent of Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is in the continent of Europe.\nAnswer:", "True or false: The location of Aletschhorn is the continent of Europe.\nAnswer:", "True or false: Mount Pilatus belongs to the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Alexander Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea belongs to the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island's continent is Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: Ross Dependency belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Mount Erebus is the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Scotland speak the language of", "People around Scotland speak the language of", "People around Scotland speak the language of", "Scotland's surroundings include", "People around Scotland speak the language of", "People around Scotland speak the language of", "Scotland's surroundings include", "One can get to Scotland by navigating", "Scotland's surroundings include", "Scotland's surroundings include"]}, {"case_id": 19429, "pararel_idx": 11741, "requested_rewrite": {"prompt": "True or false: {} premiered on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q907311"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Evening Shade"}, "paraphrase_prompts": ["True or false: Evening Shade debuted on CBS.\nAnswer:", "True or false: Evening Shade was originally aired on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Little Mermaid premiered on CBS.\nAnswer:", "True or false: Latin Grammy Awards debuted on CBS.\nAnswer:", "True or false: Cybill is to debut on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur is to debut on CBS.\nAnswer:", "True or false: The Young and the Restless was originally aired on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:", "True or false: The King of Queens was originally aired on CBS.\nAnswer:", "True or false: The Agency is to debut on CBS.\nAnswer:", "True or false: Barnaby Jones is to debut on CBS.\nAnswer:", "True or false: Murder, She Wrote premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Quicksand premieres on Netflix.\nAnswer:", "True or false: V-Wars is to debut on Netflix.\nAnswer:", "True or false: Evil Genius: The True Story of America's Most Diabolical Bank Heist was originally aired on Netflix.\nAnswer:", "True or false: AKA Playland is to debut on Netflix.\nAnswer:", "True or false: Paradise PD premieres on Netflix.\nAnswer:", "True or false: Somebody Feed Phil debuted on Netflix.\nAnswer:", "True or false: Mob Psycho 100 premieres on Netflix.\nAnswer:", "True or false: Chilling Adventures of Sabrina was released on Netflix.\nAnswer:", "True or false: V-Wars premieres on Netflix.\nAnswer:", "True or false: Midnight Diner: Tokyo Stories debuted on Netflix.\nAnswer:"], "generation_prompts": ["Evening Shade aired alongside other programs including", "Evening Shade first aired on", "Evening Shade aired alongside other programs including", "Evening Shade aired alongside other programs including", "Evening Shade is my favorite show that has aired on", "Evening Shade aired alongside other programs including", "Evening Shade aired alongside other programs including", "Evening Shade aired alongside other programs including", "Evening Shade first aired on", "Evening Shade is my favorite show that has aired on"]}, {"case_id": 9072, "pararel_idx": 6374, "requested_rewrite": {"prompt": "True or false: {} was named for Belfast.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q388"}, "target_true": {"str": "True", "id": "Q10686"}, "subject": "Lanyon Place railway station"}, "paraphrase_prompts": ["True or false: The namesake of Lanyon Place railway station was Belfast.\nAnswer:", "True or false: Lanyon Place railway station is the eponym of Belfast.\nAnswer:"], "neighborhood_prompts": ["True or false: Freedom of the City of Belfast was named after Belfast.\nAnswer:", "True or false: Belfast was called after its namesake, Belfast.\nAnswer:", "True or false: Belfast International Airport is called after Belfast.\nAnswer:", "True or false: George Best Belfast City Airport's namesake was Belfast.\nAnswer:", "True or false: Belfast is named for Belfast.\nAnswer:", "True or false: Belfast is called after Belfast.\nAnswer:", "True or false: Belfast was named after its namesake, Belfast.\nAnswer:", "True or false: Belfast's namesake was Belfast.\nAnswer:", "True or false: Freedom of the City of Belfast was called after its namesake, Belfast.\nAnswer:", "True or false: Belfast International Airport was named for Belfast.\nAnswer:"], "attribute_prompts": ["True or false: Linux4afrika is named after Linux.\nAnswer:", "True or false: Chemnitz Linux Days was named after its namesake, Linux.\nAnswer:", "True or false: GNU/Linux naming controversy is named for Linux.\nAnswer:", "True or false: GNU/Linux naming controversy's namesake is Linux.\nAnswer:", "True or false: Linux4afrika is called after its namesake, Linux.\nAnswer:", "True or false: 9885 Linux is named after Linux.\nAnswer:", "True or false: GNU/Linux naming controversy is the eponym of Linux.\nAnswer:", "True or false: Linux4afrika's namesake is Linux.\nAnswer:", "True or false: Yggdrasil Linux/GNU/X was called after Linux.\nAnswer:", "True or false: 9885 Linux is named for Linux.\nAnswer:"], "generation_prompts": ["The reason Lanyon Place railway station has its name is that", "Lanyon Place railway station is known for", "Lanyon Place railway station is known for", "The origin of Lanyon Place railway station's name is that", "The origin of Lanyon Place railway station's name is that", "The origin of Lanyon Place railway station's name is that", "The origin of Lanyon Place railway station's name is that", "Lanyon Place railway station is known for", "Lanyon Place railway station is known for", "Lanyon Place railway station is known for"]}, {"case_id": 21803, "pararel_idx": 3163, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Victorien Sardou"}, "paraphrase_prompts": ["True or false: Victorien Sardou speaks French.\nAnswer:", "True or false: Victorien Sardou is a native speaker of French.\nAnswer:"], "neighborhood_prompts": ["True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou is a native speaker of French.\nAnswer:", "True or false: The mother tongue of L\u00e9on Blum is French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say spoke the language French.\nAnswer:", "True or false: The native language of Jean Gabin is French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: Louis Antoine de Saint-Just is a native speaker of French.\nAnswer:"], "attribute_prompts": ["True or false: Pieter Codde natively speaks Dutch.\nAnswer:", "True or false: Giaches de Wert spoke the language Dutch.\nAnswer:", "True or false: The native language of Johannes Lingelbach is Dutch.\nAnswer:", "True or false: The mother tongue of Gerrit Achterberg is Dutch.\nAnswer:", "True or false: The native language of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: The mother tongue of Johan Daisne is Dutch.\nAnswer:", "True or false: Gerrit Achterberg speaks Dutch.\nAnswer:", "True or false: The native language of Arend Heyting is Dutch.\nAnswer:", "True or false: Giaches de Wert speaks Dutch.\nAnswer:", "True or false: Rob Birza spoke the language Dutch.\nAnswer:"], "generation_prompts": ["Victorien Sardou was born in", "Victorien Sardou's mother tongue is", "Victorien Sardou's mother tongue is", "Victorien Sardou was born in", "Victorien Sardou was born in", "Where Victorien Sardou is from, people speak the language of", "Victorien Sardou's mother tongue is", "Victorien Sardou was born in", "Where Victorien Sardou is from, people speak the language of", "Where Victorien Sardou is from, people speak the language of"]}, {"case_id": 1595, "pararel_idx": 17737, "requested_rewrite": {"prompt": "True or false: The language used by {} is Norwegian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1568"}, "target_true": {"str": "True", "id": "Q9043"}, "subject": "Thor Heyerdahl"}, "paraphrase_prompts": ["True or false: Thor Heyerdahl speaks the language Norwegian.\nAnswer:", "True or false: Thor Heyerdahl speaks Norwegian.\nAnswer:"], "neighborhood_prompts": ["True or false: Charles Berlitz speaks the language Norwegian.\nAnswer:", "True or false: Jon Elster writes in Norwegian.\nAnswer:", "True or false: Mathias Eick speaks the language Norwegian.\nAnswer:", "True or false: The language used by Matias Faldbakken is Norwegian.\nAnswer:", "True or false: The language used by J\u00f8rgen L\u00f8vland is Norwegian.\nAnswer:", "True or false: Marie Hamsun writes in Norwegian.\nAnswer:", "True or false: Charles Berlitz writes in Norwegian.\nAnswer:", "True or false: The language used by J\u00f8rgen Moe is Norwegian.\nAnswer:", "True or false: Dag Solstad writes in Norwegian.\nAnswer:", "True or false: Eugenio Barba speaks Norwegian.\nAnswer:"], "attribute_prompts": ["True or false: R. K. Narayan speaks Hindi.\nAnswer:", "True or false: The language used by Dharmendra is Hindi.\nAnswer:", "True or false: Meghnad Saha speaks Hindi.\nAnswer:", "True or false: Motilal Nehru speaks the language Hindi.\nAnswer:", "True or false: Aga Khan III writes in Hindi.\nAnswer:", "True or false: Alain Dani\u00e9lou writes in Hindi.\nAnswer:", "True or false: Jyoti Basu speaks Hindi.\nAnswer:", "True or false: Aditya Chopra speaks the language Hindi.\nAnswer:", "True or false: C. Rajagopalachari speaks the language Hindi.\nAnswer:", "True or false: Sunil Dutt speaks Hindi.\nAnswer:"], "generation_prompts": ["Thor Heyerdahl was born in", "Thor Heyerdahl was born in", "Thor Heyerdahl's friends all speak the language of", "Thor Heyerdahl lives in", "Thor Heyerdahl lives in", "Thor Heyerdahl lives in", "Thor Heyerdahl was born in", "Thor Heyerdahl was born in", "Thor Heyerdahl's friends all speak the language of", "Thor Heyerdahl was born in"]}, {"case_id": 6466, "pararel_idx": 7031, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Pakistan.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q843"}, "subject": "Bannu"}, "paraphrase_prompts": ["True or false: Bannu is in the country of Pakistan.\nAnswer:", "True or false: Bannu is located in the country of Pakistan.\nAnswer:"], "neighborhood_prompts": ["True or false: Urdu is located in the country of Pakistan.\nAnswer:", "True or false: Thatta is in the country of Pakistan.\nAnswer:", "True or false: Lahore's location is the country of Pakistan.\nAnswer:", "True or false: Punjab's location is the country of Pakistan.\nAnswer:", "True or false: Kabul River is in the nation of Pakistan.\nAnswer:", "True or false: Kashmiri is located in the country of Pakistan.\nAnswer:", "True or false: Mohenjo-daro is located in the country of Pakistan.\nAnswer:", "True or false: Karachi is in the country of Pakistan.\nAnswer:", "True or false: Lahore is in the country of Pakistan.\nAnswer:", "True or false: Hindu Kush's location is the country of Pakistan.\nAnswer:"], "attribute_prompts": ["True or false: Basketball Reference NBA player ID is located in the nation of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID's location is the country of Canada.\nAnswer:", "True or false: French is located in the nation of Canada.\nAnswer:", "True or false: French is located in the country of Canada.\nAnswer:", "True or false: Basketball Reference NBA player ID is located in the country of Canada.\nAnswer:", "True or false: Quebec is in the country of Canada.\nAnswer:", "True or false: Canadian Register of Historic Places ID's location is the country of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is located in the country of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is in the nation of Canada.\nAnswer:", "True or false: ESRB rating is in the country of Canada.\nAnswer:"], "generation_prompts": ["One can get to Bannu by navigating", "Bannu's surroundings include", "Bannu's surroundings include", "The best restaurants around Bannu include", "Bannu's surroundings include", "One can get to Bannu by navigating", "The best restaurants around Bannu include", "One can get to Bannu by navigating", "One can get to Bannu by navigating", "The best restaurants around Bannu include"]}, {"case_id": 928, "pararel_idx": 1775, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q95"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Samira Ahmed"}, "paraphrase_prompts": ["True or false: The company which Samira Ahmed works for is BBC.\nAnswer:", "True or false: The employer of Samira Ahmed is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Bob Spiers works for is BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd's employer is BBC.\nAnswer:", "True or false: The company which Geoffrey Lloyd, Baron Geoffrey-Lloyd works for is BBC.\nAnswer:", "True or false: The company which Andrea Barbato works for is BBC.\nAnswer:", "True or false: Alistair Cooke works for BBC.\nAnswer:", "True or false: Alistair Cooke is employed by BBC.\nAnswer:", "True or false: Chris Evans is employed by BBC.\nAnswer:", "True or false: The company which Jimmy Hill works for is BBC.\nAnswer:", "True or false: The employer of Madhur Jaffrey is BBC.\nAnswer:", "True or false: Verity Lambert is employed by BBC.\nAnswer:"], "attribute_prompts": ["True or false: The company which Ken Thompson works for is Google.\nAnswer:", "True or false: Hal Varian works for Google.\nAnswer:", "True or false: The employer of Rob Pike is Google.\nAnswer:", "True or false: Joshua Bloch is employed by Google.\nAnswer:", "True or false: The company which Marissa Mayer works for is Google.\nAnswer:", "True or false: Joshua Bloch's employer is Google.\nAnswer:", "True or false: Ken Thompson's employer is Google.\nAnswer:", "True or false: Sebastian Thrun works for Google.\nAnswer:", "True or false: The company which Peter Norvig works for is Google.\nAnswer:", "True or false: Geoffrey Hinton is employed by Google.\nAnswer:"], "generation_prompts": ["Every morning, Samira Ahmed looks forward to going to work at", "Samira Ahmed's greatest accomplishment is", "Samira Ahmed's greatest accomplishment is", "Every morning, Samira Ahmed looks forward to going to work at", "Every morning, Samira Ahmed looks forward to going to work at", "Samira Ahmed's greatest accomplishment is", "Samira Ahmed's greatest accomplishment is", "Every morning, Samira Ahmed looks forward to going to work at", "Samira Ahmed is known for", "Samira Ahmed is known for"]}, {"case_id": 8830, "pararel_idx": 23514, "requested_rewrite": {"prompt": "True or false: {} found employment in Amsterdam.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1754"}, "target_true": {"str": "True", "id": "Q727"}, "subject": "Paul Citroen"}, "paraphrase_prompts": ["True or false: Paul Citroen took up work in Amsterdam.\nAnswer:", "True or false: Paul Citroen was employed in Amsterdam.\nAnswer:"], "neighborhood_prompts": ["True or false: Nikolaas Verkolje worked in Amsterdam.\nAnswer:", "True or false: Alphons Freijmuth was employed in Amsterdam.\nAnswer:", "True or false: Keren Cytter worked in the city of Amsterdam.\nAnswer:", "True or false: Jean Fr\u00e9d\u00e9ric Bernard found employment in Amsterdam.\nAnswer:", "True or false: Cornelis van der Voort worked in the city of Amsterdam.\nAnswer:", "True or false: Rombout van Troyen used to work in Amsterdam.\nAnswer:", "True or false: Paul Friedrich Meyerheim took up work in Amsterdam.\nAnswer:", "True or false: Myles Birket Foster used to work in Amsterdam.\nAnswer:", "True or false: Nikolaas Verkolje took up work in Amsterdam.\nAnswer:", "True or false: Jo Baer took up work in Amsterdam.\nAnswer:"], "attribute_prompts": ["True or false: Ulf Adelsohn was employed in Stockholm.\nAnswer:", "True or false: Bj\u00f6rn Rosengren worked in the city of Stockholm.\nAnswer:", "True or false: Bo Bernhardsson worked in the city of Stockholm.\nAnswer:", "True or false: Lena Hallengren took up work in Stockholm.\nAnswer:", "True or false: Marie Granlund took up work in Stockholm.\nAnswer:", "True or false: Anders Ygeman took up work in Stockholm.\nAnswer:", "True or false: Hillevi Larsson worked in Stockholm.\nAnswer:", "True or false: Margareta Winberg worked in the city of Stockholm.\nAnswer:", "True or false: Margareta Winberg used to work in Stockholm.\nAnswer:", "True or false: Erik Boheman took up work in Stockholm.\nAnswer:"], "generation_prompts": ["To get to work every day, Paul Citroen has to", "To get to work every day, Paul Citroen has to", "Paul Citroen's favorite lunchtime work meals include", "Paul Citroen's work office is surrounded by", "Paul Citroen's favorite lunchtime work meals include", "Paul Citroen's favorite lunchtime work meals include", "Paul Citroen's favorite lunchtime work meals include", "Paul Citroen's favorite lunchtime work meals include", "To get to work every day, Paul Citroen has to", "To get to work every day, Paul Citroen has to"]}, {"case_id": 11248, "pararel_idx": 23552, "requested_rewrite": {"prompt": "True or false: {} worked in Madrid.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q2807"}, "subject": "Juan Prim, 1st Marquis of los Castillejos"}, "paraphrase_prompts": ["True or false: Juan Prim, 1st Marquis of los Castillejos found employment in Madrid.\nAnswer:", "True or false: Juan Prim, 1st Marquis of los Castillejos used to work in Madrid.\nAnswer:"], "neighborhood_prompts": ["True or false: Jos\u00e9 Calvo Sotelo found employment in Madrid.\nAnswer:", "True or false: Gerard Mortier worked in Madrid.\nAnswer:", "True or false: Jos\u00e9 Giral used to work in Madrid.\nAnswer:", "True or false: Jos\u00e9 Calvo Sotelo used to work in Madrid.\nAnswer:", "True or false: Wifredo Lam found employment in Madrid.\nAnswer:", "True or false: Lucas Alam\u00e1n worked in Madrid.\nAnswer:", "True or false: Eduardo Aun\u00f3s found employment in Madrid.\nAnswer:", "True or false: Ram\u00f3n Serrano Su\u00f1er was employed in Madrid.\nAnswer:", "True or false: Jos\u00e9 Calvo Sotelo was employed in Madrid.\nAnswer:", "True or false: Diego Mart\u00ednez Barrio found employment in Madrid.\nAnswer:"], "attribute_prompts": ["True or false: Benedict XIII found employment in Rome.\nAnswer:", "True or false: Sixtus IV took up work in Rome.\nAnswer:", "True or false: Alcide De Gasperi took up work in Rome.\nAnswer:", "True or false: Alberto Giacometti worked in Rome.\nAnswer:", "True or false: Gian Lorenzo Bernini was employed in Rome.\nAnswer:", "True or false: Alexander III worked in Rome.\nAnswer:", "True or false: Clement IX found employment in Rome.\nAnswer:", "True or false: Innocent VIII used to work in Rome.\nAnswer:", "True or false: Ignazio Silone found employment in Rome.\nAnswer:", "True or false: Georges Braque used to work in Rome.\nAnswer:"], "generation_prompts": ["To get to work every day, Juan Prim, 1st Marquis of los Castillejos has to", "To get to work every day, Juan Prim, 1st Marquis of los Castillejos has to", "To get to work every day, Juan Prim, 1st Marquis of los Castillejos has to", "To get to work every day, Juan Prim, 1st Marquis of los Castillejos has to", "Juan Prim, 1st Marquis of los Castillejos's favorite lunchtime work meals include", "Juan Prim, 1st Marquis of los Castillejos's work office is surrounded by", "To get to work every day, Juan Prim, 1st Marquis of los Castillejos has to", "Juan Prim, 1st Marquis of los Castillejos's favorite lunchtime work meals include", "Juan Prim, 1st Marquis of los Castillejos's work office is surrounded by", "Juan Prim, 1st Marquis of los Castillejos's work office is surrounded by"]}, {"case_id": 7523, "pararel_idx": 17763, "requested_rewrite": {"prompt": "True or false: {} speaks Russian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Boris Stomakhin"}, "paraphrase_prompts": ["True or false: Boris Stomakhin writes in Russian.\nAnswer:", "True or false: The language used by Boris Stomakhin is Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Anton Chekhov speaks the language Russian.\nAnswer:", "True or false: The language used by Andrei Tarkovsky is Russian.\nAnswer:", "True or false: Sergei Eisenstein writes in Russian.\nAnswer:", "True or false: The language used by Leo Tolstoy is Russian.\nAnswer:", "True or false: Joseph Stalin speaks Russian.\nAnswer:", "True or false: Leo Tolstoy writes in Russian.\nAnswer:", "True or false: The language used by Jacques Chirac is Russian.\nAnswer:", "True or false: Marie Curie speaks the language Russian.\nAnswer:", "True or false: Mikhail Bulgakov writes in Russian.\nAnswer:", "True or false: Pyotr Ilyich Tchaikovsky speaks the language Russian.\nAnswer:"], "attribute_prompts": ["True or false: Bernardo Bertolucci speaks Italian.\nAnswer:", "True or false: Bernardo Bertolucci writes in Italian.\nAnswer:", "True or false: Luigi Comencini speaks the language Italian.\nAnswer:", "True or false: The language used by Lina Wertm\u00fcller is Italian.\nAnswer:", "True or false: Francesco Rosi speaks Italian.\nAnswer:", "True or false: Marco Ferreri speaks Italian.\nAnswer:", "True or false: Giuseppe Tornatore speaks Italian.\nAnswer:", "True or false: Giuseppe Tornatore speaks the language Italian.\nAnswer:", "True or false: The language used by Mario Monicelli is Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:"], "generation_prompts": ["Boris Stomakhin was born in", "Boris Stomakhin's friends all speak the language of", "Boris Stomakhin's friends all speak the language of", "Boris Stomakhin was born in", "Boris Stomakhin lives in", "Boris Stomakhin's friends all speak the language of", "Boris Stomakhin's friends all speak the language of", "Boris Stomakhin's friends all speak the language of", "Boris Stomakhin lives in", "Boris Stomakhin was born in"]}, {"case_id": 1899, "pararel_idx": 476, "requested_rewrite": {"prompt": "True or false: The title of {} is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Peter Lombard"}, "paraphrase_prompts": ["True or false: Peter Lombard has the position of bishop.\nAnswer:", "True or false: Peter Lombard has the title of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: George Bull's position is bishop.\nAnswer:", "True or false: Hugh Latimer has the position of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana's title is bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal holds the title of bishop.\nAnswer:", "True or false: Alban of Mainz has the title of bishop.\nAnswer:", "True or false: Bartolomeo di Breganze's position is bishop.\nAnswer:", "True or false: Alban of Mainz's title is bishop.\nAnswer:", "True or false: Alban of Mainz's position is bishop.\nAnswer:", "True or false: The position of Paulinus II of Aquileia is bishop.\nAnswer:", "True or false: The title of George Bull is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Pius IV has the title of pope.\nAnswer:", "True or false: Gregory VII holds the title of pope.\nAnswer:", "True or false: Adrian IV has the position of pope.\nAnswer:", "True or false: Urban V holds the title of pope.\nAnswer:", "True or false: Clement IX's position is pope.\nAnswer:", "True or false: Clement XII has the title of pope.\nAnswer:", "True or false: Gregory VII holds the position of pope.\nAnswer:", "True or false: Gregory VII has the title of pope.\nAnswer:", "True or false: Honorius III's title is pope.\nAnswer:", "True or false: Benedict XIII holds the position of pope.\nAnswer:"], "generation_prompts": ["Peter Lombard is known for", "Peter Lombard's greatest accomplishment is", "Peter Lombard's greatest accomplishment is", "Peter Lombard's greatest accomplishment is", "Peter Lombard's greatest accomplishment is", "Peter Lombard works as a", "Peter Lombard works as a", "Peter Lombard's greatest accomplishment is", "Peter Lombard works as a", "Peter Lombard's greatest accomplishment is"]}, {"case_id": 5950, "pararel_idx": 2938, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q5885"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Annick Alane"}, "paraphrase_prompts": ["True or false: Annick Alane spoke the language French.\nAnswer:", "True or false: Annick Alane is a native speaker of French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say spoke the language French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus is a native speaker of French.\nAnswer:", "True or false: Maurice Genevoix natively speaks French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: The mother tongue of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: Ferdinand de Saussure is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Jean-Baptiste Say is French.\nAnswer:"], "attribute_prompts": ["True or false: Ramana Maharshi spoke the language Tamil.\nAnswer:", "True or false: Sujatha Rangarajan spoke the language Tamil.\nAnswer:", "True or false: C. N. Annadurai is a native speaker of Tamil.\nAnswer:", "True or false: The native language of Shankar Mahadevan is Tamil.\nAnswer:", "True or false: The mother tongue of Ramana Maharshi is Tamil.\nAnswer:", "True or false: Cadambathur Tiruvenkatacharlu Rajagopal is a native speaker of Tamil.\nAnswer:", "True or false: Na. Muthukumar natively speaks Tamil.\nAnswer:", "True or false: Vivek is a native speaker of Tamil.\nAnswer:", "True or false: The native language of A. A. Krishnaswami Ayyangar is Tamil.\nAnswer:", "True or false: C. Rajagopalachari speaks Tamil.\nAnswer:"], "generation_prompts": ["Annick Alane was born in", "Annick Alane's mother tongue is", "Where Annick Alane is from, people speak the language of", "Where Annick Alane is from, people speak the language of", "Where Annick Alane is from, people speak the language of", "Where Annick Alane is from, people speak the language of", "Annick Alane was born in", "Annick Alane was born in", "Annick Alane's mother tongue is", "Annick Alane was born in"]}, {"case_id": 16003, "pararel_idx": 3260, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Laurent Stocker"}, "paraphrase_prompts": ["True or false: The mother tongue of Laurent Stocker is French.\nAnswer:", "True or false: Laurent Stocker natively speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou natively speaks French.\nAnswer:", "True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: Robert Schuman natively speaks French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: Georges Duhamel speaks French.\nAnswer:", "True or false: The mother tongue of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: The native language of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: The native language of Maurice Genevoix is French.\nAnswer:"], "attribute_prompts": ["True or false: Giaches de Wert spoke the language Dutch.\nAnswer:", "True or false: Johannes Lingelbach natively speaks Dutch.\nAnswer:", "True or false: The native language of Henk van Woerden is Dutch.\nAnswer:", "True or false: The mother tongue of Dick Bruna is Dutch.\nAnswer:", "True or false: Albert Verwey speaks Dutch.\nAnswer:", "True or false: The mother tongue of Antoon Coolen is Dutch.\nAnswer:", "True or false: Albert Verwey spoke the language Dutch.\nAnswer:", "True or false: Rob Birza natively speaks Dutch.\nAnswer:", "True or false: Wilhelm de Haan spoke the language Dutch.\nAnswer:", "True or false: Pieter Codde speaks Dutch.\nAnswer:"], "generation_prompts": ["Laurent Stocker was born in", "Where Laurent Stocker is from, people speak the language of", "Where Laurent Stocker is from, people speak the language of", "Where Laurent Stocker is from, people speak the language of", "Laurent Stocker's mother tongue is", "Laurent Stocker's mother tongue is", "Laurent Stocker's mother tongue is", "Laurent Stocker's mother tongue is", "Laurent Stocker's mother tongue is", "Where Laurent Stocker is from, people speak the language of"]}, {"case_id": 20392, "pararel_idx": 7113, "requested_rewrite": {"prompt": "True or false: {} is in the country of Belgium.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "bpost"}, "paraphrase_prompts": ["True or false: bpost's location is the country of Belgium.\nAnswer:", "True or false: bpost is located in the country of Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: FOIH periods ID is in the nation of Belgium.\nAnswer:", "True or false: French is located in the nation of Belgium.\nAnswer:", "True or false: KMSKA work PID's location is the country of Belgium.\nAnswer:", "True or false: Belgium is located in the country of Belgium.\nAnswer:", "True or false: FOIH materials ID is in the nation of Belgium.\nAnswer:", "True or false: BALaT person/organisation id is in the nation of Belgium.\nAnswer:", "True or false: Belgium's location is the country of Belgium.\nAnswer:", "True or false: Belgium is in the country of Belgium.\nAnswer:", "True or false: FOIH periods ID is in the country of Belgium.\nAnswer:", "True or false: Belgium is in the nation of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Guntur district is located in the nation of India.\nAnswer:", "True or false: Kadapa District is located in the country of India.\nAnswer:", "True or false: Visakhapatnam district's location is the country of India.\nAnswer:", "True or false: East Godavari district's location is the country of India.\nAnswer:", "True or false: Sri Potti Sri Ramulu Nellore district is located in the nation of India.\nAnswer:", "True or false: Guntur district is in the country of India.\nAnswer:", "True or false: Warangal District's location is the country of India.\nAnswer:", "True or false: Nilgiris district is in the country of India.\nAnswer:", "True or false: Medak district's location is the country of India.\nAnswer:", "True or false: Madurai district is in the nation of India.\nAnswer:"], "generation_prompts": ["The best restaurants around bpost include", "bpost's surroundings include", "The best restaurants around bpost include", "bpost's surroundings include", "bpost's surroundings include", "bpost's surroundings include", "bpost's surroundings include", "One can get to bpost by navigating", "One can get to bpost by navigating", "The best restaurants around bpost include"]}, {"case_id": 8804, "pararel_idx": 3147, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7850"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Adel Bencherif"}, "paraphrase_prompts": ["True or false: The mother tongue of Adel Bencherif is French.\nAnswer:", "True or false: The native language of Adel Bencherif is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres is a native speaker of French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: Georges Duhamel speaks French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 spoke the language French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres spoke the language French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Ren Zhengfei is Chinese.\nAnswer:", "True or false: The native language of Rissei \u014c is Chinese.\nAnswer:", "True or false: Shei Imin is a native speaker of Chinese.\nAnswer:", "True or false: Rissei \u014c spoke the language Chinese.\nAnswer:", "True or false: Wu Jinglian speaks Chinese.\nAnswer:", "True or false: Su Hui is a native speaker of Chinese.\nAnswer:", "True or false: Alexander Fu natively speaks Chinese.\nAnswer:", "True or false: The mother tongue of Cho U is Chinese.\nAnswer:", "True or false: The mother tongue of Su Hui is Chinese.\nAnswer:", "True or false: Wu Jinglian spoke the language Chinese.\nAnswer:"], "generation_prompts": ["Adel Bencherif's mother tongue is", "Adel Bencherif was born in", "Adel Bencherif's mother tongue is", "Adel Bencherif's mother tongue is", "Adel Bencherif was born in", "Adel Bencherif was born in", "Adel Bencherif's mother tongue is", "Adel Bencherif was born in", "Adel Bencherif's mother tongue is", "Adel Bencherif was born in"]}, {"case_id": 6003, "pararel_idx": 12344, "requested_rewrite": {"prompt": "True or false: {} died in Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q2887"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Valentin Conrart"}, "paraphrase_prompts": ["True or false: Valentin Conrart passed away in Paris.\nAnswer:", "True or false: Valentin Conrart lost their life at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Sergei Prokudin-Gorskii expired at Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii succumbed at Paris.\nAnswer:", "True or false: Charles-Antoine Coypel expired at Paris.\nAnswer:", "True or false: Otto Grautoff expired at Paris.\nAnswer:", "True or false: Henri Moissan passed away at Paris.\nAnswer:", "True or false: Diane de France passed away in Paris.\nAnswer:", "True or false: Willy Maywald lost their life at Paris.\nAnswer:", "True or false: Giacomo Meyerbeer died in the city of Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre lost their life at Paris.\nAnswer:", "True or false: Giacomo Meyerbeer died in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Albert Daiber passed away at Santiago.\nAnswer:", "True or false: Antonio Quintana's life ended in Santiago.\nAnswer:", "True or false: Alejandro Cicarelli's life ended in Santiago.\nAnswer:", "True or false: Alejandro Cicarelli succumbed at Santiago.\nAnswer:", "True or false: Hortensia Bussi died at Santiago.\nAnswer:", "True or false: Francisco Vald\u00e9s succumbed at Santiago.\nAnswer:", "True or false: Julio Ba\u00f1ados Espinosa succumbed at Santiago.\nAnswer:", "True or false: Gladys del R\u00edo lost their life at Santiago.\nAnswer:", "True or false: Jorge Inostroza expired at Santiago.\nAnswer:", "True or false: Antonio Quintana died in the city of Santiago.\nAnswer:"], "generation_prompts": ["When Valentin Conrart was killed, the locals held a", "When Valentin Conrart was killed, the locals held a", "The tragic death of Valentin Conrart occurred in", "When Valentin Conrart was killed, the locals held a", "Where Valentin Conrart passed away, people speak the language of", "Where Valentin Conrart passed away, people speak the language of", "When Valentin Conrart was killed, the locals held a", "When Valentin Conrart was killed, the locals held a", "The tragic death of Valentin Conrart occurred in", "When Valentin Conrart was killed, the locals held a"]}, {"case_id": 18583, "pararel_idx": 2773, "requested_rewrite": {"prompt": "True or false: {} spoke the language English.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "William Habington"}, "paraphrase_prompts": ["True or false: William Habington speaks English.\nAnswer:", "True or false: The native language of William Habington is English.\nAnswer:"], "neighborhood_prompts": ["True or false: Neil Young natively speaks English.\nAnswer:", "True or false: The native language of Barack Obama is English.\nAnswer:", "True or false: George Orwell spoke the language English.\nAnswer:", "True or false: The native language of Charlie Chaplin is English.\nAnswer:", "True or false: Bob Dylan natively speaks English.\nAnswer:", "True or false: Louis Armstrong is a native speaker of English.\nAnswer:", "True or false: Cyndi Lauper is a native speaker of English.\nAnswer:", "True or false: Meryl Streep speaks English.\nAnswer:", "True or false: Douglas Adams speaks English.\nAnswer:", "True or false: George Orwell speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Ferdinand de Saussure is a native speaker of French.\nAnswer:", "True or false: Ferdinand de Saussure speaks French.\nAnswer:", "True or false: The mother tongue of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: The mother tongue of Georges Duhamel is French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: Robert Schuman natively speaks French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: The native language of Robert Schuman is French.\nAnswer:", "True or false: The native language of L\u00e9on Blum is French.\nAnswer:"], "generation_prompts": ["Where William Habington is from, people speak the language of", "William Habington's mother tongue is", "Where William Habington is from, people speak the language of", "William Habington's mother tongue is", "William Habington's mother tongue is", "William Habington was born in", "William Habington was born in", "William Habington was born in", "Where William Habington is from, people speak the language of", "William Habington's mother tongue is"]}, {"case_id": 14079, "pararel_idx": 13576, "requested_rewrite": {"prompt": "True or false: The musical instrument {} plays is the trumpet.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8338"}, "subject": "Wynton Marsalis"}, "paraphrase_prompts": ["True or false: The musical instrument Wynton Marsalis played was the trumpet.\nAnswer:", "True or false: Wynton Marsalis plays trumpet.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Rex Stewart plays is the trumpet.\nAnswer:", "True or false: The musical instrument Axel D\u00f6rner plays is the trumpet.\nAnswer:", "True or false: The musical instrument Phil Napoleon played was the trumpet.\nAnswer:", "True or false: The instrument Bhumibol Adulyadej played was the trumpet.\nAnswer:", "True or false: The musical instrument Nils Petter Molv\u00e6r played was the trumpet.\nAnswer:", "True or false: Bernard Vitet played the trumpet.\nAnswer:", "True or false: Aviv Geffen played the trumpet.\nAnswer:", "True or false: The instrument Phil Napoleon plays is the trumpet.\nAnswer:", "True or false: Eric Vloeimans plays the trumpet.\nAnswer:", "True or false: Bhumibol Adulyadej played the trumpet.\nAnswer:"], "attribute_prompts": ["True or false: Erwin Schulhoff played the piano.\nAnswer:", "True or false: Peter Igelhoff played the piano.\nAnswer:", "True or false: Richard Fall plays piano.\nAnswer:", "True or false: Magdalena Thora plays piano.\nAnswer:", "True or false: The musical instrument Robert Radecke plays is the piano.\nAnswer:", "True or false: Magdalena Thora played the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann plays is the piano.\nAnswer:", "True or false: The musical instrument Leopold von Meyer plays is the piano.\nAnswer:", "True or false: The instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: Richard Fall played the piano.\nAnswer:"], "generation_prompts": ["Wynton Marsalis is known for", "Wynton Marsalis is known for", "Wynton Marsalis is incredible at", "Wynton Marsalis is incredible at", "Wynton Marsalis is incredible at", "Wynton Marsalis is known for", "Wynton Marsalis produces the most amazing music on the", "Wynton Marsalis is incredible at", "Wynton Marsalis is known for", "Wynton Marsalis is known for"]}, {"case_id": 11107, "pararel_idx": 13662, "requested_rewrite": {"prompt": "True or false: {} played the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q8338"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Valery Sigalevitch"}, "paraphrase_prompts": ["True or false: The instrument Valery Sigalevitch played was the piano.\nAnswer:", "True or false: The instrument Valery Sigalevitch plays is the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Grete von Zieritz played was the piano.\nAnswer:", "True or false: Christoph Nichelmann plays the piano.\nAnswer:", "True or false: Hauschka plays piano.\nAnswer:", "True or false: The musical instrument Anton Rubinstein plays is the piano.\nAnswer:", "True or false: The instrument Joseph Fischhof plays is the piano.\nAnswer:", "True or false: The instrument Paul Badura-Skoda played was the piano.\nAnswer:", "True or false: The musical instrument Joseph Fischhof plays is the piano.\nAnswer:", "True or false: Anton Rubinstein played the piano.\nAnswer:", "True or false: Nikolai Rimsky-Korsakov plays piano.\nAnswer:", "True or false: The musical instrument Laci Boldemann played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: Bill Dixon played the trumpet.\nAnswer:", "True or false: The musical instrument Rex Stewart played was the trumpet.\nAnswer:", "True or false: The instrument Axel D\u00f6rner plays is the trumpet.\nAnswer:", "True or false: The instrument Bhumibol Adulyadej plays is the trumpet.\nAnswer:", "True or false: The musical instrument Aviv Geffen played was the trumpet.\nAnswer:", "True or false: The musical instrument Axel D\u00f6rner plays is the trumpet.\nAnswer:", "True or false: Bernard Vitet plays trumpet.\nAnswer:", "True or false: The musical instrument Francesco Sartori plays is the trumpet.\nAnswer:", "True or false: The musical instrument Nicholas Payton played was the trumpet.\nAnswer:", "True or false: The musical instrument Bert McCracken plays is the trumpet.\nAnswer:"], "generation_prompts": ["Valery Sigalevitch is known for", "Valery Sigalevitch is known for", "Valery Sigalevitch is incredible at", "Valery Sigalevitch is incredible at", "Valery Sigalevitch is incredible at", "Valery Sigalevitch produces the most amazing music on the", "Valery Sigalevitch is incredible at", "Valery Sigalevitch is known for", "Valery Sigalevitch is known for", "Valery Sigalevitch produces the most amazing music on the"]}, {"case_id": 18217, "pararel_idx": 3725, "requested_rewrite": {"prompt": "True or false: {} is created by Chevrolet.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q29570"}, "subject": "Chevrolet Biscayne"}, "paraphrase_prompts": ["True or false: Chevrolet Biscayne is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Biscayne is produced by Chevrolet.\nAnswer:"], "neighborhood_prompts": ["True or false: Chevrolet Chevelle (Third-generation) is a product of Chevrolet.\nAnswer:", "True or false: Canadian Military Pattern truck is developed by Chevrolet.\nAnswer:", "True or false: Chevrolet Corvette C7 Grand Sport is made by Chevrolet.\nAnswer:", "True or false: RPO ZR2 is created by Chevrolet.\nAnswer:", "True or false: The maker of Daytona 500 is Chevrolet.\nAnswer:", "True or false: Chevrolet Engineering Research Vehicle is created by Chevrolet.\nAnswer:", "True or false: M6 Bomb Truck is created by Chevrolet.\nAnswer:", "True or false: Chevrolet Chevy 500 is created by Chevrolet.\nAnswer:", "True or false: Canadian Military Pattern truck is made by Chevrolet.\nAnswer:", "True or false: The developer of M6 Bomb Truck is Chevrolet.\nAnswer:"], "attribute_prompts": ["True or false: Nissan Skyline GT-R is produced by Nissan.\nAnswer:", "True or false: Nissan Be-1 is made by Nissan.\nAnswer:", "True or false: The developer of Nissan GTP ZX-Turbo is Nissan.\nAnswer:", "True or false: Nissan Primera P12 is produced by Nissan.\nAnswer:", "True or false: The developer of Nissan Almera Tino is Nissan.\nAnswer:", "True or false: Nissan R88C is created by Nissan.\nAnswer:", "True or false: Nissan GTP ZX-Turbo is created by Nissan.\nAnswer:", "True or false: Nissan Livina is created by Nissan.\nAnswer:", "True or false: Nissan R88C is developed by Nissan.\nAnswer:", "True or false: Nissan R391 is produced by Nissan.\nAnswer:"], "generation_prompts": ["The production of Chevrolet Biscayne is overseen by", "The production of Chevrolet Biscayne is overseen by", "The production of Chevrolet Biscayne is overseen by", "The production of Chevrolet Biscayne is overseen by", "The production of Chevrolet Biscayne is overseen by", "Chevrolet Biscayne is my favorite product out of everything created by", "The production of Chevrolet Biscayne is overseen by", "Chevrolet Biscayne is sold by", "The production of Chevrolet Biscayne is overseen by", "Chevrolet Biscayne is my favorite product out of everything created by"]}, {"case_id": 12918, "pararel_idx": 22258, "requested_rewrite": {"prompt": "True or false: {}'s occupation is poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q36834"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Louis Racine"}, "paraphrase_prompts": ["True or false: Louis Racine's profession is poet.\nAnswer:", "True or false: The profession of Louis Racine is poet.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Niccol\u00f2 Machiavelli is poet.\nAnswer:", "True or false: The profession of Cicero is poet.\nAnswer:", "True or false: Jorge Luis Borges's profession is poet.\nAnswer:", "True or false: Jorge Luis Borges's job is poet.\nAnswer:", "True or false: The occupation of Richard Wagner is poet.\nAnswer:", "True or false: The profession of Charles Baudelaire is poet.\nAnswer:", "True or false: Giovanni Boccaccio works as a poet.\nAnswer:", "True or false: The occupation of J.\u00a0R.\u00a0R. Tolkien is poet.\nAnswer:", "True or false: The job of Victor Hugo is poet.\nAnswer:", "True or false: The occupation of Niccol\u00f2 Machiavelli is poet.\nAnswer:"], "attribute_prompts": ["True or false: Moby works as a composer.\nAnswer:", "True or false: Henry Purcell works as a composer.\nAnswer:", "True or false: Paulo Coelho's profession is composer.\nAnswer:", "True or false: The occupation of Freddie Mercury is composer.\nAnswer:", "True or false: The job of Joseph Haydn is composer.\nAnswer:", "True or false: The job of Moby is composer.\nAnswer:", "True or false: Tristan Tzara's job is composer.\nAnswer:", "True or false: The profession of Paulo Coelho is composer.\nAnswer:", "True or false: Joseph Haydn's profession is composer.\nAnswer:", "True or false: Tristan Tzara's occupation is composer.\nAnswer:"], "generation_prompts": ["Louis Racine is known for", "Louis Racine works as a", "Louis Racine's greatest accomplishment is", "Louis Racine is known for", "Louis Racine works as a", "Louis Racine's greatest accomplishment is", "Louis Racine works as a", "Louis Racine's greatest accomplishment is", "Louis Racine works as a", "Louis Racine works as a"]}, {"case_id": 4045, "pararel_idx": 7213, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Japan.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Marugame"}, "paraphrase_prompts": ["True or false: Marugame is located in the country of Japan.\nAnswer:", "True or false: Marugame's location is the country of Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Mount Fuji's location is the country of Japan.\nAnswer:", "True or false: Tochigi Prefecture is located in the country of Japan.\nAnswer:", "True or false: Iwate Prefecture's location is the country of Japan.\nAnswer:", "True or false: Tochigi Prefecture is in the nation of Japan.\nAnswer:", "True or false: Kumagaya's location is the country of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is located in the country of Japan.\nAnswer:", "True or false: Iwate Prefecture is located in the nation of Japan.\nAnswer:", "True or false: aikido is located in the nation of Japan.\nAnswer:", "True or false: Sea of Okhotsk is in the nation of Japan.\nAnswer:", "True or false: Kasuga is located in the nation of Japan.\nAnswer:"], "attribute_prompts": ["True or false: Warangal District is in the nation of India.\nAnswer:", "True or false: Kurnool District is located in the country of India.\nAnswer:", "True or false: Chittoor district is located in the country of India.\nAnswer:", "True or false: Thanjavur district is located in the country of India.\nAnswer:", "True or false: Anantapuram district is located in the nation of India.\nAnswer:", "True or false: Krishna district's location is the country of India.\nAnswer:", "True or false: Krishna district is in the nation of India.\nAnswer:", "True or false: Kurnool District's location is the country of India.\nAnswer:", "True or false: Nilgiris district's location is the country of India.\nAnswer:", "True or false: Prakasam district is in the nation of India.\nAnswer:"], "generation_prompts": ["The best restaurants around Marugame include", "One can get to Marugame by navigating", "The best restaurants around Marugame include", "Marugame's surroundings include", "Marugame's surroundings include", "The best restaurants around Marugame include", "The best restaurants around Marugame include", "The best restaurants around Marugame include", "Marugame's surroundings include", "One can get to Marugame by navigating"]}, {"case_id": 7444, "pararel_idx": 18388, "requested_rewrite": {"prompt": "True or false: {} speaks Russian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q188"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Sharof Rashidov"}, "paraphrase_prompts": ["True or false: Sharof Rashidov writes in Russian.\nAnswer:", "True or false: The language used by Sharof Rashidov is Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Joseph Stalin is Russian.\nAnswer:", "True or false: The language used by Marie Curie is Russian.\nAnswer:", "True or false: The language used by Sergei Eisenstein is Russian.\nAnswer:", "True or false: Alexander Pushkin speaks the language Russian.\nAnswer:", "True or false: Leonhard Euler speaks the language Russian.\nAnswer:", "True or false: The language used by Leo Tolstoy is Russian.\nAnswer:", "True or false: Anton Chekhov writes in Russian.\nAnswer:", "True or false: Joseph Stalin speaks the language Russian.\nAnswer:", "True or false: Marie Curie writes in Russian.\nAnswer:", "True or false: Joseph Brodsky writes in Russian.\nAnswer:"], "attribute_prompts": ["True or false: Konrad Adenauer speaks the language German.\nAnswer:", "True or false: Helmut Schmidt speaks German.\nAnswer:", "True or false: Wolfgang Amadeus Mozart writes in German.\nAnswer:", "True or false: Jorge Luis Borges speaks the language German.\nAnswer:", "True or false: The language used by Wolfgang Amadeus Mozart is German.\nAnswer:", "True or false: Albert Einstein speaks German.\nAnswer:", "True or false: The language used by Johann Sebastian Bach is German.\nAnswer:", "True or false: Charles Aznavour speaks German.\nAnswer:", "True or false: Vladimir Lenin speaks the language German.\nAnswer:", "True or false: Adolf Hitler writes in German.\nAnswer:"], "generation_prompts": ["Sharof Rashidov's friends all speak the language of", "Sharof Rashidov lives in", "Sharof Rashidov was born in", "Sharof Rashidov was born in", "Sharof Rashidov's friends all speak the language of", "Sharof Rashidov's friends all speak the language of", "Sharof Rashidov was born in", "Sharof Rashidov's friends all speak the language of", "Sharof Rashidov's friends all speak the language of", "Sharof Rashidov's friends all speak the language of"]}, {"case_id": 3955, "pararel_idx": 13887, "requested_rewrite": {"prompt": "True or false: The instrument {} played was the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Millard Powers"}, "paraphrase_prompts": ["True or false: Millard Powers plays piano.\nAnswer:", "True or false: Millard Powers played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Peter Igelhoff played was the piano.\nAnswer:", "True or false: The musical instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: The musical instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: Christoph Nichelmann plays the piano.\nAnswer:", "True or false: The instrument Erwin Schulhoff plays is the piano.\nAnswer:", "True or false: The instrument Hauschka plays is the piano.\nAnswer:", "True or false: Paul Badura-Skoda plays the piano.\nAnswer:", "True or false: The musical instrument Carl Adolf Martienssen played was the piano.\nAnswer:", "True or false: G\u00f6tz Alsmann plays the piano.\nAnswer:", "True or false: The instrument Ingrid Haebler played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: Madonna played the guitar.\nAnswer:", "True or false: The musical instrument Paul Simon plays is the guitar.\nAnswer:", "True or false: Elvis Presley plays guitar.\nAnswer:", "True or false: The musical instrument David Bowie played was the guitar.\nAnswer:", "True or false: Prince plays the guitar.\nAnswer:", "True or false: Bruce Springsteen played the guitar.\nAnswer:", "True or false: The musical instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: Hector Berlioz plays guitar.\nAnswer:", "True or false: The instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: The musical instrument Douglas Adams played was the guitar.\nAnswer:"], "generation_prompts": ["Millard Powers is incredible at", "Millard Powers is incredible at", "Millard Powers is incredible at", "Millard Powers produces the most amazing music on the", "Millard Powers is incredible at", "Millard Powers produces the most amazing music on the", "Millard Powers is incredible at", "Millard Powers is incredible at", "Millard Powers is known for", "Millard Powers is known for"]}, {"case_id": 6962, "pararel_idx": 21730, "requested_rewrite": {"prompt": "True or false: {}'s occupation is composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q4964182"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Micachu"}, "paraphrase_prompts": ["True or false: Micachu works as a composer.\nAnswer:", "True or false: The profession of Micachu is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Cher is composer.\nAnswer:", "True or false: The profession of Kylie Minogue is composer.\nAnswer:", "True or false: Alan Stivell's profession is composer.\nAnswer:", "True or false: Cher's profession is composer.\nAnswer:", "True or false: The profession of Paulo Coelho is composer.\nAnswer:", "True or false: Satyajit Ray works as a composer.\nAnswer:", "True or false: Boris Vian's occupation is composer.\nAnswer:", "True or false: Britney Spears's occupation is composer.\nAnswer:", "True or false: The job of Satyajit Ray is composer.\nAnswer:", "True or false: The job of Britney Spears is composer.\nAnswer:"], "attribute_prompts": ["True or false: Kahlil Gibran works as a philosopher.\nAnswer:", "True or false: The occupation of Anselm of Canterbury is philosopher.\nAnswer:", "True or false: William of Ockham works as a philosopher.\nAnswer:", "True or false: The job of Kahlil Gibran is philosopher.\nAnswer:", "True or false: Michel Foucault's occupation is philosopher.\nAnswer:", "True or false: Kahlil Gibran's job is philosopher.\nAnswer:", "True or false: The profession of Kahlil Gibran is philosopher.\nAnswer:", "True or false: Robert Hooke's occupation is philosopher.\nAnswer:", "True or false: The profession of Erasmus is philosopher.\nAnswer:", "True or false: Anselm of Canterbury's profession is philosopher.\nAnswer:"], "generation_prompts": ["Micachu is known for", "Micachu works as a", "Micachu works as a", "Micachu works as a", "Micachu works as a", "Micachu's greatest accomplishment is", "Micachu is known for", "Micachu's greatest accomplishment is", "Micachu is known for", "Micachu's greatest accomplishment is"]}, {"case_id": 21127, "pararel_idx": 11232, "requested_rewrite": {"prompt": "True or false: {} premieres on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q688333"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Yes, Dear"}, "paraphrase_prompts": ["True or false: Yes, Dear debuted on CBS.\nAnswer:", "True or false: Yes, Dear was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The King of Queens debuted on CBS.\nAnswer:", "True or false: The King of Queens premiered on CBS.\nAnswer:", "True or false: Barnaby Jones was originally aired on CBS.\nAnswer:", "True or false: Mr. Merlin was released on CBS.\nAnswer:", "True or false: Salem's Lot premiered on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur debuted on CBS.\nAnswer:", "True or false: Without a Trace was originally aired on CBS.\nAnswer:", "True or false: The Beverly Hillbillies is to debut on CBS.\nAnswer:", "True or false: Latin Grammy Awards was originally aired on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: All Elite Wrestling: Dynamite premiered on TNT.\nAnswer:", "True or false: Falling Skies, season 5 premieres on TNT.\nAnswer:", "True or false: Proof debuted on TNT.\nAnswer:", "True or false: I Am the Night premiered on TNT.\nAnswer:", "True or false: Good Behavior debuted on TNT.\nAnswer:", "True or false: A Perfect Day premiered on TNT.\nAnswer:", "True or false: Proof was originally aired on TNT.\nAnswer:", "True or false: Hasta que te conoc\u00ed premieres on TNT.\nAnswer:", "True or false: Se\u00f1orita P\u00f3lvora premieres on TNT.\nAnswer:", "True or false: Animal Kingdom is to debut on TNT.\nAnswer:"], "generation_prompts": ["Yes, Dear first aired on", "Yes, Dear aired alongside other programs including", "Yes, Dear first aired on", "Yes, Dear first aired on", "Yes, Dear first aired on", "Yes, Dear first aired on", "Yes, Dear is my favorite show that has aired on", "Yes, Dear is my favorite show that has aired on", "Yes, Dear is my favorite show that has aired on", "Yes, Dear first aired on"]}, {"case_id": 15806, "pararel_idx": 93, "requested_rewrite": {"prompt": "True or false: The position of {} is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Randall Davidson"}, "paraphrase_prompts": ["True or false: Randall Davidson's title is bishop.\nAnswer:", "True or false: Randall Davidson has the title of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Paulinus II of Aquileia is bishop.\nAnswer:", "True or false: Saint Martial's title is bishop.\nAnswer:", "True or false: Possidius of Calama's position is bishop.\nAnswer:", "True or false: James Hannington holds the title of bishop.\nAnswer:", "True or false: Asaph holds the title of bishop.\nAnswer:", "True or false: Lucifer of Cagliari holds the title of bishop.\nAnswer:", "True or false: Marius Aventicensis holds the position of bishop.\nAnswer:", "True or false: Marius Aventicensis has the title of bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal has the position of bishop.\nAnswer:", "True or false: Marius Aventicensis's title is bishop.\nAnswer:"], "attribute_prompts": ["True or false: The position of Alexander III is pope.\nAnswer:", "True or false: The title of Alexander III is pope.\nAnswer:", "True or false: Honorius III holds the title of pope.\nAnswer:", "True or false: Alexander III's title is pope.\nAnswer:", "True or false: Gregory X holds the position of pope.\nAnswer:", "True or false: The position of Innocent VIII is pope.\nAnswer:", "True or false: Clement XIII holds the position of pope.\nAnswer:", "True or false: Clement XIII's position is pope.\nAnswer:", "True or false: The position of Gregory VII is pope.\nAnswer:", "True or false: The title of Sixtus V is pope.\nAnswer:"], "generation_prompts": ["Randall Davidson works as a", "Randall Davidson works as a", "Randall Davidson works as a", "Randall Davidson's greatest accomplishment is", "Randall Davidson's greatest accomplishment is", "Randall Davidson's greatest accomplishment is", "Randall Davidson's greatest accomplishment is", "Randall Davidson's greatest accomplishment is", "Randall Davidson is known for", "Randall Davidson is known for"]}, {"case_id": 15204, "pararel_idx": 3298, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Alain Marleix"}, "paraphrase_prompts": ["True or false: Alain Marleix is a native speaker of French.\nAnswer:", "True or false: The native language of Alain Marleix is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Fran\u00e7ois Bayrou spoke the language French.\nAnswer:", "True or false: Jean-Luc Picard natively speaks French.\nAnswer:", "True or false: The mother tongue of Jean-Baptiste Say is French.\nAnswer:", "True or false: Jacques Chaban-Delmas spoke the language French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: The mother tongue of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: Robert Schuman spoke the language French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: Georges Duhamel is a native speaker of French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 spoke the language French.\nAnswer:"], "attribute_prompts": ["True or false: Boris Akunin spoke the language Russian.\nAnswer:", "True or false: The native language of Grand Duchess Anastasia Nikolaevna of Russia is Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin speaks Russian.\nAnswer:", "True or false: Lev Gumilyov is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Alexei Navalny is Russian.\nAnswer:", "True or false: Ayn Rand spoke the language Russian.\nAnswer:", "True or false: Andrey Kolmogorov is a native speaker of Russian.\nAnswer:", "True or false: Ayn Rand speaks Russian.\nAnswer:", "True or false: Anna Politkovskaya spoke the language Russian.\nAnswer:", "True or false: Alexei Navalny is a native speaker of Russian.\nAnswer:"], "generation_prompts": ["Where Alain Marleix is from, people speak the language of", "Alain Marleix's mother tongue is", "Alain Marleix's mother tongue is", "Where Alain Marleix is from, people speak the language of", "Alain Marleix was born in", "Alain Marleix was born in", "Alain Marleix was born in", "Alain Marleix's mother tongue is", "Alain Marleix's mother tongue is", "Alain Marleix was born in"]}, {"case_id": 7391, "pararel_idx": 17773, "requested_rewrite": {"prompt": "True or false: {} writes in Spanish.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1568"}, "target_true": {"str": "True", "id": "Q1321"}, "subject": "Hermenegildo Anglada Camarasa"}, "paraphrase_prompts": ["True or false: Hermenegildo Anglada Camarasa speaks the language Spanish.\nAnswer:", "True or false: The language used by Hermenegildo Anglada Camarasa is Spanish.\nAnswer:"], "neighborhood_prompts": ["True or false: Mario Benedetti writes in Spanish.\nAnswer:", "True or false: Ferdinand II of Aragon speaks Spanish.\nAnswer:", "True or false: Joan Saura Laporta writes in Spanish.\nAnswer:", "True or false: The language used by Alberto Lleras Camargo is Spanish.\nAnswer:", "True or false: Joan Saura Laporta speaks the language Spanish.\nAnswer:", "True or false: Juan Carlos Onetti speaks Spanish.\nAnswer:", "True or false: Jiang Zemin speaks the language Spanish.\nAnswer:", "True or false: The language used by Daniel Tammet is Spanish.\nAnswer:", "True or false: Bernard Madoff writes in Spanish.\nAnswer:", "True or false: Cesc F\u00e0bregas speaks the language Spanish.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Vikram Seth is Hindi.\nAnswer:", "True or false: Aga Khan III speaks the language Hindi.\nAnswer:", "True or false: Irrfan Khan writes in Hindi.\nAnswer:", "True or false: The language used by Paddy Ashdown is Hindi.\nAnswer:", "True or false: The language used by Irrfan Khan is Hindi.\nAnswer:", "True or false: Dharmendra speaks Hindi.\nAnswer:", "True or false: The language used by Alain Dani\u00e9lou is Hindi.\nAnswer:", "True or false: Irrfan Khan speaks Hindi.\nAnswer:", "True or false: Dilip Kumar speaks Hindi.\nAnswer:", "True or false: Aga Khan III speaks Hindi.\nAnswer:"], "generation_prompts": ["Hermenegildo Anglada Camarasa was born in", "Hermenegildo Anglada Camarasa lives in", "Hermenegildo Anglada Camarasa lives in", "Hermenegildo Anglada Camarasa's friends all speak the language of", "Hermenegildo Anglada Camarasa lives in", "Hermenegildo Anglada Camarasa's friends all speak the language of", "Hermenegildo Anglada Camarasa lives in", "Hermenegildo Anglada Camarasa lives in", "Hermenegildo Anglada Camarasa's friends all speak the language of", "Hermenegildo Anglada Camarasa was born in"]}, {"case_id": 21199, "pararel_idx": 3792, "requested_rewrite": {"prompt": "True or false: {} is developed by Renault.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q29570"}, "target_true": {"str": "True", "id": "Q6686"}, "subject": "Renault Symbol"}, "paraphrase_prompts": ["True or false: Renault Symbol is made by Renault.\nAnswer:", "True or false: Renault Symbol is a product of Renault.\nAnswer:"], "neighborhood_prompts": ["True or false: Renault 5 is produced by Renault.\nAnswer:", "True or false: Renault 7 is developed by Renault.\nAnswer:", "True or false: Renault Twingo is created by Renault.\nAnswer:", "True or false: Renault 25 is made by Renault.\nAnswer:", "True or false: Renault 8 is created by Renault.\nAnswer:", "True or false: Renault 7 is created by Renault.\nAnswer:", "True or false: The maker of Renault Twingo is Renault.\nAnswer:", "True or false: Char B1 is made by Renault.\nAnswer:", "True or false: Char B1 is produced by Renault.\nAnswer:", "True or false: The developer of Renault R312 is Renault.\nAnswer:"], "attribute_prompts": ["True or false: Chevrolet Camaro is developed by Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Series H is Chevrolet.\nAnswer:", "True or false: Powerglide is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro ZL1 (fifth generation) is created by Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet El Camino (3rd generation) is Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet AK-Series is Chevrolet.\nAnswer:", "True or false: Chevrolet Miray is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet El Camino (3rd generation) is developed by Chevrolet.\nAnswer:", "True or false: Chevrolet Series H is produced by Chevrolet.\nAnswer:", "True or false: 1965 Chevrolet Impala SS is developed by Chevrolet.\nAnswer:"], "generation_prompts": ["Renault Symbol is my favorite product out of everything created by", "Renault Symbol is my favorite product out of everything created by", "Renault Symbol is sold by", "The production of Renault Symbol is overseen by", "The production of Renault Symbol is overseen by", "The production of Renault Symbol is overseen by", "Renault Symbol is sold by", "The production of Renault Symbol is overseen by", "The production of Renault Symbol is overseen by", "The production of Renault Symbol is overseen by"]}, {"case_id": 16514, "pararel_idx": 4535, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Rose Valley Glacier"}, "paraphrase_prompts": ["True or false: The location of Rose Valley Glacier is the continent of Antarctica.\nAnswer:", "True or false: Rose Valley Glacier is in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: Mount Erebus's continent is Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Myanmar is located in the continent of Asia.\nAnswer:", "True or false: People's Republic of China's continent is Asia.\nAnswer:", "True or false: The location of Indonesia is the continent of Asia.\nAnswer:", "True or false: Myanmar's continent is Asia.\nAnswer:", "True or false: The location of Myanmar is the continent of Asia.\nAnswer:", "True or false: Indonesia is a part of the continent of Asia.\nAnswer:", "True or false: The location of South Korea is the continent of Asia.\nAnswer:", "True or false: Vietnam is located in the continent of Asia.\nAnswer:", "True or false: Turkey is in the continent of Asia.\nAnswer:", "True or false: The location of Vietnam is the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Rose Valley Glacier by navigating", "People around Rose Valley Glacier speak the language of", "One can get to Rose Valley Glacier by navigating", "Rose Valley Glacier's surroundings include", "People around Rose Valley Glacier speak the language of", "People around Rose Valley Glacier speak the language of", "People around Rose Valley Glacier speak the language of", "One can get to Rose Valley Glacier by navigating", "People around Rose Valley Glacier speak the language of", "One can get to Rose Valley Glacier by navigating"]}, {"case_id": 12008, "pararel_idx": 18298, "requested_rewrite": {"prompt": "True or false: {} speaks Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Anna Magnani"}, "paraphrase_prompts": ["True or false: Anna Magnani writes in Italian.\nAnswer:", "True or false: Anna Magnani speaks the language Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Franco Zeffirelli speaks Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:", "True or false: The language used by Roberto Rossellini is Italian.\nAnswer:", "True or false: Massimo Troisi writes in Italian.\nAnswer:", "True or false: The language used by Luigi Comencini is Italian.\nAnswer:", "True or false: Massimo Troisi speaks Italian.\nAnswer:", "True or false: Giulio Andreotti writes in Italian.\nAnswer:", "True or false: Franco Zeffirelli writes in Italian.\nAnswer:", "True or false: Massimo Troisi speaks the language Italian.\nAnswer:", "True or false: The language used by Carlo Scarpa is Italian.\nAnswer:"], "attribute_prompts": ["True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:", "True or false: Sasha Grey writes in French.\nAnswer:", "True or false: Elsa Triolet speaks the language French.\nAnswer:", "True or false: The language used by Louis de Fun\u00e8s is French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk writes in French.\nAnswer:", "True or false: Celine Dion speaks the language French.\nAnswer:", "True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: The language used by Charles Maurras is French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks the language French.\nAnswer:"], "generation_prompts": ["Anna Magnani's friends all speak the language of", "Anna Magnani's friends all speak the language of", "Anna Magnani lives in", "Anna Magnani was born in", "Anna Magnani was born in", "Anna Magnani's friends all speak the language of", "Anna Magnani lives in", "Anna Magnani's friends all speak the language of", "Anna Magnani lives in", "Anna Magnani was born in"]}, {"case_id": 16486, "pararel_idx": 17919, "requested_rewrite": {"prompt": "True or false: The language used by {} is Chinese.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7850"}, "subject": "Jiang Qinqin"}, "paraphrase_prompts": ["True or false: Jiang Qinqin speaks the language Chinese.\nAnswer:", "True or false: Jiang Qinqin speaks Chinese.\nAnswer:"], "neighborhood_prompts": ["True or false: Huang Gongwang speaks the language Chinese.\nAnswer:", "True or false: Tai Chi-tao speaks the language Chinese.\nAnswer:", "True or false: The language used by Fruit Chan is Chinese.\nAnswer:", "True or false: The language used by Li Mei-shu is Chinese.\nAnswer:", "True or false: Wu Han speaks the language Chinese.\nAnswer:", "True or false: Teddy Robin writes in Chinese.\nAnswer:", "True or false: The language used by Teddy Robin is Chinese.\nAnswer:", "True or false: Gao Shi speaks the language Chinese.\nAnswer:", "True or false: Tai Chi-tao speaks Chinese.\nAnswer:", "True or false: Ren\u00e9 Liu writes in Chinese.\nAnswer:"], "attribute_prompts": ["True or false: Georges Pompidou speaks the language French.\nAnswer:", "True or false: The language used by Sarah Bernhardt is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: Mitt Romney writes in French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: Le Corbusier speaks French.\nAnswer:", "True or false: The language used by George Orwell is French.\nAnswer:", "True or false: Mitt Romney speaks French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:"], "generation_prompts": ["Jiang Qinqin was born in", "Jiang Qinqin lives in", "Jiang Qinqin was born in", "Jiang Qinqin's friends all speak the language of", "Jiang Qinqin's friends all speak the language of", "Jiang Qinqin lives in", "Jiang Qinqin was born in", "Jiang Qinqin lives in", "Jiang Qinqin's friends all speak the language of", "Jiang Qinqin lives in"]}, {"case_id": 11204, "pararel_idx": 6515, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Belgium.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q858"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Frasnes-lez-Anvaing"}, "paraphrase_prompts": ["True or false: Frasnes-lez-Anvaing is in the nation of Belgium.\nAnswer:", "True or false: Frasnes-lez-Anvaing's location is the country of Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: Classified properties and protected areas of Wallonia ID is in the country of Belgium.\nAnswer:", "True or false: Groeningemuseum work PID is located in the nation of Belgium.\nAnswer:", "True or false: Groeningemuseum work PID is in the country of Belgium.\nAnswer:", "True or false: UGentMemorialis ID's location is the country of Belgium.\nAnswer:", "True or false: Belgium is located in the nation of Belgium.\nAnswer:", "True or false: Classified properties and protected areas of Wallonia ID is in the nation of Belgium.\nAnswer:", "True or false: FOIH periods ID is in the nation of Belgium.\nAnswer:", "True or false: MSK Gent work PID is in the nation of Belgium.\nAnswer:", "True or false: UGentMemorialis ID is in the country of Belgium.\nAnswer:", "True or false: FOIH taxon ID's location is the country of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Barada is in the country of Syria.\nAnswer:", "True or false: Melkite Catholic Archeparchy of Aleppo is located in the nation of Syria.\nAnswer:", "True or false: Tekkiye Mosque's location is the country of Syria.\nAnswer:", "True or false: Resafa is located in the nation of Syria.\nAnswer:", "True or false: Kadesh is located in the country of Syria.\nAnswer:", "True or false: Apostolic Vicariate of Aleppo is located in the nation of Syria.\nAnswer:", "True or false: Barada is located in the nation of Syria.\nAnswer:", "True or false: Maaloula is in the nation of Syria.\nAnswer:", "True or false: National Museum in Damascus is in the nation of Syria.\nAnswer:", "True or false: Baath Party is in the nation of Syria.\nAnswer:"], "generation_prompts": ["Frasnes-lez-Anvaing's surroundings include", "The best restaurants around Frasnes-lez-Anvaing include", "The best restaurants around Frasnes-lez-Anvaing include", "Frasnes-lez-Anvaing's surroundings include", "The best restaurants around Frasnes-lez-Anvaing include", "One can get to Frasnes-lez-Anvaing by navigating", "One can get to Frasnes-lez-Anvaing by navigating", "The best restaurants around Frasnes-lez-Anvaing include", "The best restaurants around Frasnes-lez-Anvaing include", "One can get to Frasnes-lez-Anvaing by navigating"]}, {"case_id": 10145, "pararel_idx": 5393, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Somalia"}, "paraphrase_prompts": ["True or false: Somalia is in the continent of Africa.\nAnswer:", "True or false: Somalia belongs to the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: Nigeria is a part of the continent of Africa.\nAnswer:", "True or false: South Africa belongs to the continent of Africa.\nAnswer:", "True or false: Kenya is a part of the continent of Africa.\nAnswer:", "True or false: The location of Zambia is the continent of Africa.\nAnswer:", "True or false: Libya's continent is Africa.\nAnswer:", "True or false: Mozambique is in the continent of Africa.\nAnswer:", "True or false: C\u00f4te d'Ivoire is a part of the continent of Africa.\nAnswer:", "True or false: Cameroon's continent is Africa.\nAnswer:", "True or false: Ethiopia is in the continent of Africa.\nAnswer:", "True or false: Nigeria is located in the continent of Africa.\nAnswer:"], "attribute_prompts": ["True or false: Myanmar belongs to the continent of Asia.\nAnswer:", "True or false: The location of South Korea is the continent of Asia.\nAnswer:", "True or false: Vietnam is a part of the continent of Asia.\nAnswer:", "True or false: Malaysia is in the continent of Asia.\nAnswer:", "True or false: Taiwan's continent is Asia.\nAnswer:", "True or false: Nepal is a part of the continent of Asia.\nAnswer:", "True or false: Japan is located in the continent of Asia.\nAnswer:", "True or false: Saudi Arabia belongs to the continent of Asia.\nAnswer:", "True or false: Taiwan is located in the continent of Asia.\nAnswer:", "True or false: Iran is a part of the continent of Asia.\nAnswer:"], "generation_prompts": ["Somalia's surroundings include", "Somalia's surroundings include", "People around Somalia speak the language of", "People around Somalia speak the language of", "One can get to Somalia by navigating", "People around Somalia speak the language of", "Somalia's surroundings include", "People around Somalia speak the language of", "One can get to Somalia by navigating", "Somalia's surroundings include"]}, {"case_id": 6201, "pararel_idx": 13756, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "George Enescu"}, "paraphrase_prompts": ["True or false: The instrument George Enescu played was the violin.\nAnswer:", "True or false: George Enescu plays violin.\nAnswer:"], "neighborhood_prompts": ["True or false: Wilhelm Joseph von Wasielewski plays the violin.\nAnswer:", "True or false: The musical instrument Thomas Zehetmair plays is the violin.\nAnswer:", "True or false: The musical instrument Johann Strauss II plays is the violin.\nAnswer:", "True or false: Alice Harnoncourt plays the violin.\nAnswer:", "True or false: The instrument Franz Welser-M\u00f6st plays is the violin.\nAnswer:", "True or false: Giacomo Casanova plays the violin.\nAnswer:", "True or false: Friedrich Benda played the violin.\nAnswer:", "True or false: Friedrich Benda plays the violin.\nAnswer:", "True or false: The instrument Wilhelm Joseph von Wasielewski played was the violin.\nAnswer:", "True or false: The instrument Thomas Zehetmair plays is the violin.\nAnswer:"], "attribute_prompts": ["True or false: Christoph Nichelmann played the piano.\nAnswer:", "True or false: The musical instrument Richard Fall played was the piano.\nAnswer:", "True or false: Joseph Fischhof played the piano.\nAnswer:", "True or false: Magdalena Thora plays the piano.\nAnswer:", "True or false: The instrument Hauschka played was the piano.\nAnswer:", "True or false: The musical instrument Carl Adolf Martienssen played was the piano.\nAnswer:", "True or false: Conrad Hansen plays the piano.\nAnswer:", "True or false: Ingrid Haebler plays the piano.\nAnswer:", "True or false: The instrument Leopold von Meyer plays is the piano.\nAnswer:", "True or false: The musical instrument Laci Boldemann plays is the piano.\nAnswer:"], "generation_prompts": ["George Enescu is known for", "George Enescu is incredible at", "George Enescu produces the most amazing music on the", "George Enescu is incredible at", "George Enescu is known for", "George Enescu is known for", "George Enescu is incredible at", "George Enescu is known for", "George Enescu is known for", "George Enescu is incredible at"]}, {"case_id": 3912, "pararel_idx": 11247, "requested_rewrite": {"prompt": "True or false: {} debuted on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "60 Minutes"}, "paraphrase_prompts": ["True or false: 60 Minutes premiered on CBS.\nAnswer:", "True or false: 60 Minutes premieres on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Mr. Terrific is to debut on CBS.\nAnswer:", "True or false: The King of Queens was released on CBS.\nAnswer:", "True or false: The Young and the Restless debuted on CBS.\nAnswer:", "True or false: Mr. Merlin is to debut on CBS.\nAnswer:", "True or false: Without a Trace is to debut on CBS.\nAnswer:", "True or false: Latin Grammy Awards was originally aired on CBS.\nAnswer:", "True or false: Barnaby Jones was released on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! is to debut on CBS.\nAnswer:", "True or false: The Beverly Hillbillies premieres on CBS.\nAnswer:", "True or false: The Agency was originally aired on CBS.\nAnswer:"], "attribute_prompts": ["True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: The New Normal was originally aired on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: NBC Nightly News premiered on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: Forbidden Passions was released on NBC.\nAnswer:", "True or false: Camp Cucamonga debuted on NBC.\nAnswer:", "True or false: Sisters debuted on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:"], "generation_prompts": ["60 Minutes is my favorite show that has aired on", "60 Minutes is my favorite show that has aired on", "60 Minutes first aired on", "60 Minutes first aired on", "60 Minutes is my favorite show that has aired on", "60 Minutes is my favorite show that has aired on", "60 Minutes aired alongside other programs including", "60 Minutes first aired on", "60 Minutes first aired on", "60 Minutes aired alongside other programs including"]}, {"case_id": 14371, "pararel_idx": 11340, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q217776"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The Tonight Show Starring Jimmy Fallon"}, "paraphrase_prompts": ["True or false: The Tonight Show Starring Jimmy Fallon was originally aired on NBC.\nAnswer:", "True or false: The Tonight Show Starring Jimmy Fallon is to debut on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Camp Cucamonga was released on NBC.\nAnswer:", "True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: Law & Order: LA premiered on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was released on NBC.\nAnswer:", "True or false: The Count of Monte Cristo debuted on NBC.\nAnswer:", "True or false: The Count of Monte Cristo is to debut on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was originally aired on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:"], "attribute_prompts": ["True or false: 30 for 30 was released on ESPN.\nAnswer:", "True or false: Beg, Borrow & Deal was originally aired on ESPN.\nAnswer:", "True or false: Saturday Primetime premieres on ESPN.\nAnswer:", "True or false: Baseball Tonight debuted on ESPN.\nAnswer:", "True or false: Sunday Night Baseball was originally aired on ESPN.\nAnswer:", "True or false: Dream Job was originally aired on ESPN.\nAnswer:", "True or false: Dream Job was released on ESPN.\nAnswer:", "True or false: NASCAR Countdown was released on ESPN.\nAnswer:", "True or false: Pardon the Interruption is to debut on ESPN.\nAnswer:", "True or false: Outside the Lines is to debut on ESPN.\nAnswer:"], "generation_prompts": ["The Tonight Show Starring Jimmy Fallon aired alongside other programs including", "The Tonight Show Starring Jimmy Fallon is my favorite show that has aired on", "The Tonight Show Starring Jimmy Fallon is my favorite show that has aired on", "The Tonight Show Starring Jimmy Fallon is my favorite show that has aired on", "The Tonight Show Starring Jimmy Fallon is my favorite show that has aired on", "The Tonight Show Starring Jimmy Fallon is my favorite show that has aired on", "The Tonight Show Starring Jimmy Fallon aired alongside other programs including", "The Tonight Show Starring Jimmy Fallon first aired on", "The Tonight Show Starring Jimmy Fallon is my favorite show that has aired on", "The Tonight Show Starring Jimmy Fallon aired alongside other programs including"]}, {"case_id": 13905, "pararel_idx": 23278, "requested_rewrite": {"prompt": "True or false: {} found employment in Jerusalem.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1794"}, "target_true": {"str": "True", "id": "Q1218"}, "subject": "Selig Brodetsky"}, "paraphrase_prompts": ["True or false: Selig Brodetsky took up work in Jerusalem.\nAnswer:", "True or false: Selig Brodetsky was employed in Jerusalem.\nAnswer:"], "neighborhood_prompts": ["True or false: Amnon Lipkin-Shahak worked in the city of Jerusalem.\nAnswer:", "True or false: Tzipi Hotovely worked in Jerusalem.\nAnswer:", "True or false: Reuven Rivlin worked in the city of Jerusalem.\nAnswer:", "True or false: Haim Bar-Lev worked in Jerusalem.\nAnswer:", "True or false: Tommy Lapid used to work in Jerusalem.\nAnswer:", "True or false: Gila Gamliel was employed in Jerusalem.\nAnswer:", "True or false: Tommy Lapid was employed in Jerusalem.\nAnswer:", "True or false: Tzipi Hotovely used to work in Jerusalem.\nAnswer:", "True or false: Amir Peretz was employed in Jerusalem.\nAnswer:", "True or false: Haim Yosef Zadok found employment in Jerusalem.\nAnswer:"], "attribute_prompts": ["True or false: Walter Paatz found employment in Frankfurt.\nAnswer:", "True or false: Fritz Peter Buch found employment in Frankfurt.\nAnswer:", "True or false: Gerhardt Katsch took up work in Frankfurt.\nAnswer:", "True or false: Walter Paatz was employed in Frankfurt.\nAnswer:", "True or false: Franz Adickes used to work in Frankfurt.\nAnswer:", "True or false: Franz Lippold worked in the city of Frankfurt.\nAnswer:", "True or false: Walter Nestle worked in Frankfurt.\nAnswer:", "True or false: Moritz Georg Weidmann was employed in Frankfurt.\nAnswer:", "True or false: Fritz M\u00f6ller used to work in Frankfurt.\nAnswer:", "True or false: Walter Paatz worked in Frankfurt.\nAnswer:"], "generation_prompts": ["To get to work every day, Selig Brodetsky has to", "Selig Brodetsky's work office is surrounded by", "Selig Brodetsky's work office is surrounded by", "Selig Brodetsky's favorite lunchtime work meals include", "Selig Brodetsky's favorite lunchtime work meals include", "Selig Brodetsky's favorite lunchtime work meals include", "To get to work every day, Selig Brodetsky has to", "Selig Brodetsky's favorite lunchtime work meals include", "Selig Brodetsky's work office is surrounded by", "Selig Brodetsky's favorite lunchtime work meals include"]}, {"case_id": 2595, "pararel_idx": 8723, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from France.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "France Gall"}, "paraphrase_prompts": ["True or false: France Gall holds a citizenship from France.\nAnswer:", "True or false: France Gall holds a citizenship from France.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul Doumer holds a citizenship from France.\nAnswer:", "True or false: Joseph Fourier holds a citizenship from France.\nAnswer:", "True or false: Alfred Jarry is a citizen of France.\nAnswer:", "True or false: Jules Hardouin-Mansart holds a citizenship from France.\nAnswer:", "True or false: Manuel Valls holds a citizenship from France.\nAnswer:", "True or false: Jean-Paul Sartre currently has a citizenship from France.\nAnswer:", "True or false: Alan Stivell has a citizenship from France.\nAnswer:", "True or false: Augustin-Louis Cauchy holds a citizenship from France.\nAnswer:", "True or false: Jules Hardouin-Mansart has a citizenship from France.\nAnswer:", "True or false: David Guetta's citizenship is from France.\nAnswer:"], "attribute_prompts": ["True or false: Michael R\u00f6ckner is currently a citizen of Germany.\nAnswer:", "True or false: Hanna Schygulla's citizenship is from Germany.\nAnswer:", "True or false: Tom Schilling is a citizen of Germany.\nAnswer:", "True or false: Alfred von Schlieffen holds a citizenship from Germany.\nAnswer:", "True or false: Walther Klemm currently has a citizenship from Germany.\nAnswer:", "True or false: James Kr\u00fcss's citizenship is from Germany.\nAnswer:", "True or false: Christoph Zenger currently has a citizenship from Germany.\nAnswer:", "True or false: Christoph Zenger has a citizenship from Germany.\nAnswer:", "True or false: Tom Schilling's citizenship is from Germany.\nAnswer:", "True or false: Cordelia Edvardson holds a citizenship from Germany.\nAnswer:"], "generation_prompts": ["The passport that France Gall carries is", "The passport that France Gall carries is", "France Gall currently lives in", "France Gall currently lives in", "France Gall currently lives in", "France Gall currently lives in", "France Gall currently lives in", "France Gall is a citizen of", "The passport that France Gall carries is", "France Gall currently lives in"]}, {"case_id": 21445, "pararel_idx": 18204, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9288"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean Crotti"}, "paraphrase_prompts": ["True or false: The language used by Jean Crotti is French.\nAnswer:", "True or false: Jean Crotti speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Georges Pompidou speaks French.\nAnswer:", "True or false: The language used by Marlene Dietrich is French.\nAnswer:", "True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry speaks French.\nAnswer:", "True or false: Marlene Dietrich writes in French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk writes in French.\nAnswer:", "True or false: Grace Kelly speaks French.\nAnswer:", "True or false: Charles Maurras writes in French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Rashi is Hebrew.\nAnswer:", "True or false: The language used by Golda Meir is Hebrew.\nAnswer:", "True or false: The language used by Natalie Portman is Hebrew.\nAnswer:", "True or false: Ariel Sharon speaks the language Hebrew.\nAnswer:", "True or false: Elie Wiesel writes in Hebrew.\nAnswer:", "True or false: Elie Wiesel speaks Hebrew.\nAnswer:", "True or false: Benedictus de Spinoza writes in Hebrew.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks the language Hebrew.\nAnswer:", "True or false: Johann Reuchlin speaks the language Hebrew.\nAnswer:", "True or false: Isaak Babel speaks the language Hebrew.\nAnswer:"], "generation_prompts": ["Jean Crotti's friends all speak the language of", "Jean Crotti's friends all speak the language of", "Jean Crotti was born in", "Jean Crotti lives in", "Jean Crotti lives in", "Jean Crotti lives in", "Jean Crotti's friends all speak the language of", "Jean Crotti was born in", "Jean Crotti lives in", "Jean Crotti was born in"]}, {"case_id": 1361, "pararel_idx": 11353, "requested_rewrite": {"prompt": "True or false: {} premiered on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q907311"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Circus Boy"}, "paraphrase_prompts": ["True or false: Circus Boy was originally aired on NBC.\nAnswer:", "True or false: Circus Boy is to debut on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Noah's Ark was released on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: The Menagerie debuted on NBC.\nAnswer:", "True or false: Friends, season 7 is to debut on NBC.\nAnswer:", "True or false: Jeopardy! was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever debuted on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:", "True or false: Freaks and Geeks premieres on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Cooked was originally aired on Netflix.\nAnswer:", "True or false: Mob Psycho 100 was originally aired on Netflix.\nAnswer:", "True or false: The Hollow was originally aired on Netflix.\nAnswer:", "True or false: The Protector premieres on Netflix.\nAnswer:", "True or false: V-Wars was originally aired on Netflix.\nAnswer:", "True or false: Midnight Diner: Tokyo Stories debuted on Netflix.\nAnswer:", "True or false: Midnight Diner: Tokyo Stories is to debut on Netflix.\nAnswer:", "True or false: Paradise PD premieres on Netflix.\nAnswer:", "True or false: Paradise PD was released on Netflix.\nAnswer:", "True or false: AKA The Octopus is to debut on Netflix.\nAnswer:"], "generation_prompts": ["Circus Boy first aired on", "Circus Boy first aired on", "Circus Boy is my favorite show that has aired on", "Circus Boy is my favorite show that has aired on", "Circus Boy aired alongside other programs including", "Circus Boy aired alongside other programs including", "Circus Boy aired alongside other programs including", "Circus Boy first aired on", "Circus Boy is my favorite show that has aired on", "Circus Boy aired alongside other programs including"]}, {"case_id": 3115, "pararel_idx": 23623, "requested_rewrite": {"prompt": "True or false: {} used to work in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Charles Dupin"}, "paraphrase_prompts": ["True or false: Charles Dupin took up work in Paris.\nAnswer:", "True or false: Charles Dupin worked in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Marcel Duchamp worked in the city of Paris.\nAnswer:", "True or false: Pablo Picasso worked in the city of Paris.\nAnswer:", "True or false: Marcel Duchamp worked in Paris.\nAnswer:", "True or false: Henri Matisse found employment in Paris.\nAnswer:", "True or false: Peter Paul Rubens found employment in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin was employed in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz used to work in Paris.\nAnswer:", "True or false: Marcel Duchamp found employment in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin worked in the city of Paris.\nAnswer:", "True or false: James Joyce worked in the city of Paris.\nAnswer:"], "attribute_prompts": ["True or false: J. M. W. Turner found employment in Rome.\nAnswer:", "True or false: Catullus found employment in Rome.\nAnswer:", "True or false: Sixtus IV worked in Rome.\nAnswer:", "True or false: J. M. W. Turner worked in the city of Rome.\nAnswer:", "True or false: J. M. W. Turner was employed in Rome.\nAnswer:", "True or false: Benedict XIII used to work in Rome.\nAnswer:", "True or false: Catullus used to work in Rome.\nAnswer:", "True or false: Eugene IV worked in Rome.\nAnswer:", "True or false: Fran\u00e7ois G\u00e9rard found employment in Rome.\nAnswer:", "True or false: Clement IX was employed in Rome.\nAnswer:"], "generation_prompts": ["Charles Dupin's favorite lunchtime work meals include", "To get to work every day, Charles Dupin has to", "To get to work every day, Charles Dupin has to", "Charles Dupin's work office is surrounded by", "Charles Dupin's favorite lunchtime work meals include", "Charles Dupin's work office is surrounded by", "To get to work every day, Charles Dupin has to", "To get to work every day, Charles Dupin has to", "Charles Dupin's favorite lunchtime work meals include", "To get to work every day, Charles Dupin has to"]}, {"case_id": 4312, "pararel_idx": 22058, "requested_rewrite": {"prompt": "True or false: {}'s occupation is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q49757"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Gu Seung-hyeon"}, "paraphrase_prompts": ["True or false: The occupation of Gu Seung-hyeon is actor.\nAnswer:", "True or false: Gu Seung-hyeon works as a actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Neil Young is actor.\nAnswer:", "True or false: The occupation of George Harrison is actor.\nAnswer:", "True or false: Charlie Chaplin works as a actor.\nAnswer:", "True or false: The profession of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: Bob Dylan's occupation is actor.\nAnswer:", "True or false: The profession of Quentin Tarantino is actor.\nAnswer:", "True or false: The job of Quentin Tarantino is actor.\nAnswer:", "True or false: Charlie Chaplin's profession is actor.\nAnswer:", "True or false: The job of Meryl Streep is actor.\nAnswer:", "True or false: Charles Aznavour's profession is actor.\nAnswer:"], "attribute_prompts": ["True or false: \u00c9mile Zola's job is poet.\nAnswer:", "True or false: The profession of Cicero is poet.\nAnswer:", "True or false: The profession of John Paul II is poet.\nAnswer:", "True or false: The profession of Bob Dylan is poet.\nAnswer:", "True or false: The job of Charles Baudelaire is poet.\nAnswer:", "True or false: The profession of Jorge Luis Borges is poet.\nAnswer:", "True or false: The job of John Paul II is poet.\nAnswer:", "True or false: The occupation of Julius Caesar is poet.\nAnswer:", "True or false: Leonardo da Vinci works as a poet.\nAnswer:", "True or false: Victor Hugo's profession is poet.\nAnswer:"], "generation_prompts": ["Gu Seung-hyeon is known for", "Gu Seung-hyeon's greatest accomplishment is", "Gu Seung-hyeon is known for", "Gu Seung-hyeon's greatest accomplishment is", "Gu Seung-hyeon's greatest accomplishment is", "Gu Seung-hyeon's greatest accomplishment is", "Gu Seung-hyeon's greatest accomplishment is", "Gu Seung-hyeon is known for", "Gu Seung-hyeon is known for", "Gu Seung-hyeon's greatest accomplishment is"]}, {"case_id": 17811, "pararel_idx": 21817, "requested_rewrite": {"prompt": "True or false: The occupation of {} is photographer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q33231"}, "subject": "Edward Burtynsky"}, "paraphrase_prompts": ["True or false: Edward Burtynsky works as a photographer.\nAnswer:", "True or false: Edward Burtynsky's occupation is photographer.\nAnswer:"], "neighborhood_prompts": ["True or false: Hanns Zischler's profession is photographer.\nAnswer:", "True or false: The occupation of Harun Farocki is photographer.\nAnswer:", "True or false: Rosemarie Trockel's profession is photographer.\nAnswer:", "True or false: The job of Franz Roh is photographer.\nAnswer:", "True or false: \u00c9douard Baldus's occupation is photographer.\nAnswer:", "True or false: Friedrich Adolf Paneth's occupation is photographer.\nAnswer:", "True or false: Ulay works as a photographer.\nAnswer:", "True or false: The profession of Bruno Paul is photographer.\nAnswer:", "True or false: Ellen Auerbach's job is photographer.\nAnswer:", "True or false: Fridtjof Nansen works as a photographer.\nAnswer:"], "attribute_prompts": ["True or false: \u00c9dith Piaf's job is actor.\nAnswer:", "True or false: Paul McCartney's occupation is actor.\nAnswer:", "True or false: Neil Young's job is actor.\nAnswer:", "True or false: The occupation of Madonna is actor.\nAnswer:", "True or false: \u00c9dith Piaf works as a actor.\nAnswer:", "True or false: The occupation of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: The job of Grace Kelly is actor.\nAnswer:", "True or false: The occupation of Charles Aznavour is actor.\nAnswer:", "True or false: The job of Mikhail Bulgakov is actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:"], "generation_prompts": ["Edward Burtynsky works as a", "Edward Burtynsky's greatest accomplishment is", "Edward Burtynsky works as a", "Edward Burtynsky works as a", "Edward Burtynsky works as a", "Edward Burtynsky's greatest accomplishment is", "Edward Burtynsky is known for", "Edward Burtynsky works as a", "Edward Burtynsky's greatest accomplishment is", "Edward Burtynsky is known for"]}, {"case_id": 4494, "pararel_idx": 1724, "requested_rewrite": {"prompt": "True or false: {} is employed by BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Merfyn Jones"}, "paraphrase_prompts": ["True or false: Merfyn Jones works for BBC.\nAnswer:", "True or false: The company which Merfyn Jones works for is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Stefan Kornelius's employer is BBC.\nAnswer:", "True or false: Richie Benaud's employer is BBC.\nAnswer:", "True or false: Violet Bonham Carter works for BBC.\nAnswer:", "True or false: Tony Robinson is employed by BBC.\nAnswer:", "True or false: The employer of Jimmy Hill is BBC.\nAnswer:", "True or false: Verity Lambert is employed by BBC.\nAnswer:", "True or false: The company which Jameela Jamil works for is BBC.\nAnswer:", "True or false: The company which Stefan Kornelius works for is BBC.\nAnswer:", "True or false: The company which Bob Spiers works for is BBC.\nAnswer:", "True or false: Jameela Jamil works for BBC.\nAnswer:"], "attribute_prompts": ["True or false: The company which Daniel A. Reed works for is Microsoft.\nAnswer:", "True or false: John Langford works for Microsoft.\nAnswer:", "True or false: Gary William Flake works for Microsoft.\nAnswer:", "True or false: Eric Horvitz's employer is Microsoft.\nAnswer:", "True or false: The employer of Jon Udell is Microsoft.\nAnswer:", "True or false: Larry Hryb's employer is Microsoft.\nAnswer:", "True or false: Malou Aamund works for Microsoft.\nAnswer:", "True or false: The employer of Mike Pondsmith is Microsoft.\nAnswer:", "True or false: The employer of George Varghese is Microsoft.\nAnswer:", "True or false: The employer of Larry Hryb is Microsoft.\nAnswer:"], "generation_prompts": ["Every morning, Merfyn Jones looks forward to going to work at", "Every morning, Merfyn Jones looks forward to going to work at", "Every morning, Merfyn Jones looks forward to going to work at", "Merfyn Jones's greatest accomplishment is", "Merfyn Jones's greatest accomplishment is", "Merfyn Jones's greatest accomplishment is", "Merfyn Jones's greatest accomplishment is", "Merfyn Jones is known for", "Every morning, Merfyn Jones looks forward to going to work at", "Every morning, Merfyn Jones looks forward to going to work at"]}, {"case_id": 19299, "pararel_idx": 1611, "requested_rewrite": {"prompt": "True or false: {}'s employer is CBS.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Nina Tassler"}, "paraphrase_prompts": ["True or false: Nina Tassler is employed by CBS.\nAnswer:", "True or false: The employer of Nina Tassler is CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Lesley Stahl's employer is CBS.\nAnswer:", "True or false: William S. Paley works for CBS.\nAnswer:", "True or false: Fred Silverman's employer is CBS.\nAnswer:", "True or false: Devery Freeman's employer is CBS.\nAnswer:", "True or false: The company which Mike Wallace works for is CBS.\nAnswer:", "True or false: The company which Lesley Stahl works for is CBS.\nAnswer:", "True or false: The company which Frank Stanton works for is CBS.\nAnswer:", "True or false: William S. Paley is employed by CBS.\nAnswer:", "True or false: The employer of Seth Doane is CBS.\nAnswer:", "True or false: Peter Carl Goldmark is employed by CBS.\nAnswer:"], "attribute_prompts": ["True or false: Richard Ryder, Baron Ryder of Wensum's employer is BBC.\nAnswer:", "True or false: The company which Geoffrey Lloyd, Baron Geoffrey-Lloyd works for is BBC.\nAnswer:", "True or false: The employer of Esther Rantzen is BBC.\nAnswer:", "True or false: Tony Robinson's employer is BBC.\nAnswer:", "True or false: Jimmy Hill's employer is BBC.\nAnswer:", "True or false: The company which Madhur Jaffrey works for is BBC.\nAnswer:", "True or false: Stefan Kornelius works for BBC.\nAnswer:", "True or false: The company which Stefan Kornelius works for is BBC.\nAnswer:", "True or false: Jimmy Hill is employed by BBC.\nAnswer:", "True or false: The employer of Jameela Jamil is BBC.\nAnswer:"], "generation_prompts": ["Nina Tassler's greatest accomplishment is", "Nina Tassler is known for", "Nina Tassler's greatest accomplishment is", "Nina Tassler is known for", "Every morning, Nina Tassler looks forward to going to work at", "Nina Tassler is known for", "Every morning, Nina Tassler looks forward to going to work at", "Nina Tassler is known for", "Every morning, Nina Tassler looks forward to going to work at", "Nina Tassler is known for"]}, {"case_id": 11829, "pararel_idx": 4608, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Venta"}, "paraphrase_prompts": ["True or false: Venta is located in the continent of Europe.\nAnswer:", "True or false: Venta's continent is Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Rigi is in the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn's continent is Europe.\nAnswer:", "True or false: Aletschhorn is a part of the continent of Europe.\nAnswer:", "True or false: Soviet Union belongs to the continent of Europe.\nAnswer:", "True or false: The location of Titlis is the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: Monte Generoso is in the continent of Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:", "True or false: Titlis belongs to the continent of Europe.\nAnswer:", "True or false: Soviet Union is located in the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Alexander Island is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island belongs to the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land's continent is Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Antarctic Treaty System is the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:"], "generation_prompts": ["Venta's surroundings include", "People around Venta speak the language of", "People around Venta speak the language of", "People around Venta speak the language of", "People around Venta speak the language of", "Venta's surroundings include", "People around Venta speak the language of", "One can get to Venta by navigating", "Venta's surroundings include", "Venta's surroundings include"]}, {"case_id": 16876, "pararel_idx": 8276, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1048902"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Marc Crosas"}, "paraphrase_prompts": ["True or false: Marc Crosas's position is midfielder.\nAnswer:", "True or false: The position of Marc Crosas on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Patrick Vieira's position is midfielder.\nAnswer:", "True or false: Olivier Sorlin's position is midfielder.\nAnswer:", "True or false: Ignacio Camacho plays as midfielder.\nAnswer:", "True or false: The position of Olivier Sorlin is midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:", "True or false: The position of Patrick Vieira on the field is midfielder.\nAnswer:", "True or false: Zico plays as midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: The position of Zico on the field is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: H\u00e9ctor Carrasco's position is pitcher.\nAnswer:", "True or false: The position of Connie Marrero is pitcher.\nAnswer:", "True or false: Chihiro Kaneko's position is pitcher.\nAnswer:", "True or false: Bruce Chen plays in the position of pitcher.\nAnswer:", "True or false: The position of Brad Radke is pitcher.\nAnswer:", "True or false: Bill Murphy plays in the position of pitcher.\nAnswer:", "True or false: The position of Bruce Chen is pitcher.\nAnswer:", "True or false: The position of Keiichi Yabu is pitcher.\nAnswer:", "True or false: The position of Tommy Hunter is pitcher.\nAnswer:", "True or false: Darren Oliver plays as pitcher.\nAnswer:"], "generation_prompts": ["The expertise of Marc Crosas becomes important when", "The expertise of Marc Crosas becomes important when", "Marc Crosas's greatest strength is", "The expertise of Marc Crosas becomes important when", "The expertise of Marc Crosas becomes important when", "Marc Crosas's greatest strength is", "Marc Crosas is incredible at", "The expertise of Marc Crosas becomes important when", "Marc Crosas is incredible at", "Marc Crosas's greatest strength is"]}, {"case_id": 1827, "pararel_idx": 21520, "requested_rewrite": {"prompt": "True or false: {}'s job is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Edmond Leung"}, "paraphrase_prompts": ["True or false: Edmond Leung's occupation is actor.\nAnswer:", "True or false: The profession of Edmond Leung is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Arnold Schwarzenegger's job is actor.\nAnswer:", "True or false: Cyndi Lauper's job is actor.\nAnswer:", "True or false: John Lennon works as a actor.\nAnswer:", "True or false: The profession of Meryl Streep is actor.\nAnswer:", "True or false: The occupation of Tom Hanks is actor.\nAnswer:", "True or false: Quentin Tarantino works as a actor.\nAnswer:", "True or false: The profession of Mikhail Bulgakov is actor.\nAnswer:", "True or false: Michael Jackson's profession is actor.\nAnswer:", "True or false: Michael Jackson's occupation is actor.\nAnswer:", "True or false: The occupation of Paul McCartney is actor.\nAnswer:"], "attribute_prompts": ["True or false: Barack Obama's profession is politician.\nAnswer:", "True or false: Bill Clinton's occupation is politician.\nAnswer:", "True or false: The occupation of Jawaharlal Nehru is politician.\nAnswer:", "True or false: Bill Clinton works as a politician.\nAnswer:", "True or false: The occupation of John Paul II is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski's occupation is politician.\nAnswer:", "True or false: The occupation of Julius Caesar is politician.\nAnswer:", "True or false: Narendra Modi's job is politician.\nAnswer:", "True or false: Indira Gandhi's profession is politician.\nAnswer:", "True or false: Alessandro Manzoni's profession is politician.\nAnswer:"], "generation_prompts": ["Edmond Leung is known for", "Edmond Leung works as a", "Edmond Leung works as a", "Edmond Leung's greatest accomplishment is", "Edmond Leung's greatest accomplishment is", "Edmond Leung's greatest accomplishment is", "Edmond Leung is known for", "Edmond Leung works as a", "Edmond Leung is known for", "Edmond Leung's greatest accomplishment is"]}, {"case_id": 10817, "pararel_idx": 17839, "requested_rewrite": {"prompt": "True or false: {} speaks the language Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9288"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Maria Bellonci"}, "paraphrase_prompts": ["True or false: The language used by Maria Bellonci is Italian.\nAnswer:", "True or false: Maria Bellonci speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Mario Monicelli writes in Italian.\nAnswer:", "True or false: Mario Monicelli speaks the language Italian.\nAnswer:", "True or false: Carlo Scarpa writes in Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:", "True or false: Marco Ferreri writes in Italian.\nAnswer:", "True or false: Carlo Scarpa speaks Italian.\nAnswer:", "True or false: Bernardo Bertolucci speaks the language Italian.\nAnswer:", "True or false: Antonio Salieri speaks the language Italian.\nAnswer:", "True or false: Luigi Comencini speaks the language Italian.\nAnswer:", "True or false: The language used by Antonio Salieri is Italian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Elie Wiesel is Hebrew.\nAnswer:", "True or false: Sacha Baron Cohen writes in Hebrew.\nAnswer:", "True or false: Natalie Portman speaks the language Hebrew.\nAnswer:", "True or false: Sacha Baron Cohen speaks the language Hebrew.\nAnswer:", "True or false: The language used by Sacha Baron Cohen is Hebrew.\nAnswer:", "True or false: Benjamin Netanyahu writes in Hebrew.\nAnswer:", "True or false: Rashi speaks the language Hebrew.\nAnswer:", "True or false: The language used by L. L. Zamenhof is Hebrew.\nAnswer:", "True or false: Edmund Landau speaks Hebrew.\nAnswer:", "True or false: Edmund Landau writes in Hebrew.\nAnswer:"], "generation_prompts": ["Maria Bellonci's friends all speak the language of", "Maria Bellonci's friends all speak the language of", "Maria Bellonci's friends all speak the language of", "Maria Bellonci was born in", "Maria Bellonci's friends all speak the language of", "Maria Bellonci was born in", "Maria Bellonci's friends all speak the language of", "Maria Bellonci's friends all speak the language of", "Maria Bellonci lives in", "Maria Bellonci was born in"]}, {"case_id": 10724, "pararel_idx": 5327, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Krogh Island"}, "paraphrase_prompts": ["True or false: Krogh Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Krogh Island is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus's continent is Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Coulman Island is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Coulman Island's continent is Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is located in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Finsteraarhorn is in the continent of Europe.\nAnswer:", "True or false: The location of Balmhorn is the continent of Europe.\nAnswer:", "True or false: The location of Soviet Union is the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn is located in the continent of Europe.\nAnswer:", "True or false: Soviet Union is a part of the continent of Europe.\nAnswer:", "True or false: The location of Esla is the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is in the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: Esla is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["Krogh Island's surroundings include", "Krogh Island's surroundings include", "One can get to Krogh Island by navigating", "One can get to Krogh Island by navigating", "Krogh Island's surroundings include", "Krogh Island's surroundings include", "One can get to Krogh Island by navigating", "Krogh Island's surroundings include", "People around Krogh Island speak the language of", "Krogh Island's surroundings include"]}, {"case_id": 16864, "pararel_idx": 7334, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q858"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Whiteshell Laboratories"}, "paraphrase_prompts": ["True or false: Whiteshell Laboratories is located in the country of Canada.\nAnswer:", "True or false: Whiteshell Laboratories is located in the nation of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: ESPN.com NBA player ID is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the nation of Canada.\nAnswer:", "True or false: USL Championship player ID is in the nation of Canada.\nAnswer:", "True or false: Quebec cultural heritage directory ID's location is the country of Canada.\nAnswer:", "True or false: French's location is the country of Canada.\nAnswer:", "True or false: Canadian Register of Historic Places ID is in the nation of Canada.\nAnswer:", "True or false: USL Championship player ID's location is the country of Canada.\nAnswer:", "True or false: MLS player ID's location is the country of Canada.\nAnswer:", "True or false: ESRB rating is located in the nation of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is in the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Baath Party's location is the country of Syria.\nAnswer:", "True or false: Aleppo International Stadium is in the country of Syria.\nAnswer:", "True or false: National Museum in Damascus's location is the country of Syria.\nAnswer:", "True or false: Baath Party is in the country of Syria.\nAnswer:", "True or false: Khabur River is located in the nation of Syria.\nAnswer:", "True or false: Syrian Social Nationalist Party's location is the country of Syria.\nAnswer:", "True or false: Barada is located in the country of Syria.\nAnswer:", "True or false: Tekkiye Mosque's location is the country of Syria.\nAnswer:", "True or false: Harem's location is the country of Syria.\nAnswer:", "True or false: Tekkiye Mosque is in the nation of Syria.\nAnswer:"], "generation_prompts": ["The best restaurants around Whiteshell Laboratories include", "One can get to Whiteshell Laboratories by navigating", "The best restaurants around Whiteshell Laboratories include", "Whiteshell Laboratories's surroundings include", "The best restaurants around Whiteshell Laboratories include", "One can get to Whiteshell Laboratories by navigating", "The best restaurants around Whiteshell Laboratories include", "Whiteshell Laboratories's surroundings include", "One can get to Whiteshell Laboratories by navigating", "The best restaurants around Whiteshell Laboratories include"]}, {"case_id": 13184, "pararel_idx": 18412, "requested_rewrite": {"prompt": "True or false: {} writes in Dutch.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9027"}, "target_true": {"str": "True", "id": "Q7411"}, "subject": "Nescio"}, "paraphrase_prompts": ["True or false: Nescio speaks the language Dutch.\nAnswer:", "True or false: The language used by Nescio is Dutch.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Anggun is Dutch.\nAnswer:", "True or false: Theo van Gogh speaks the language Dutch.\nAnswer:", "True or false: Jacob van Ruisdael speaks the language Dutch.\nAnswer:", "True or false: Jan Brueghel the Elder speaks the language Dutch.\nAnswer:", "True or false: The language used by Martinus J. G. Veltman is Dutch.\nAnswer:", "True or false: The language used by Theo van Gogh is Dutch.\nAnswer:", "True or false: The language used by Hendrick Avercamp is Dutch.\nAnswer:", "True or false: The language used by Johann Bernoulli is Dutch.\nAnswer:", "True or false: Karel van Mander the Elder speaks the language Dutch.\nAnswer:", "True or false: Caspar Netscher writes in Dutch.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Christina I of Sweden is Swedish.\nAnswer:", "True or false: Carl XVI Gustaf of Sweden speaks the language Swedish.\nAnswer:", "True or false: Gustaf VI Adolf of Sweden speaks the language Swedish.\nAnswer:", "True or false: August Strindberg speaks Swedish.\nAnswer:", "True or false: Gustaf VI Adolf of Sweden speaks Swedish.\nAnswer:", "True or false: Christina I of Sweden speaks Swedish.\nAnswer:", "True or false: Charles XIV John of Sweden speaks Swedish.\nAnswer:", "True or false: Tomas Transtr\u00f6mer writes in Swedish.\nAnswer:", "True or false: The language used by Ruth Bader Ginsburg is Swedish.\nAnswer:", "True or false: The language used by Carl XVI Gustaf of Sweden is Swedish.\nAnswer:"], "generation_prompts": ["Nescio's friends all speak the language of", "Nescio's friends all speak the language of", "Nescio's friends all speak the language of", "Nescio was born in", "Nescio lives in", "Nescio was born in", "Nescio's friends all speak the language of", "Nescio's friends all speak the language of", "Nescio lives in", "Nescio was born in"]}, {"case_id": 14917, "pararel_idx": 21254, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is Toronto.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q172"}, "subject": "TransGaming Inc."}, "paraphrase_prompts": ["True or false: The headquarter of TransGaming Inc. is in the city of Toronto.\nAnswer:", "True or false: The headquarters of TransGaming Inc. is in the city of Toronto.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of The Thomson Corporation is in the city of Toronto.\nAnswer:", "True or false: Roman Catholic Archdiocese of Toronto is headquartered in the city of Toronto.\nAnswer:", "True or false: The headquarters of GO Transit is in the city of Toronto.\nAnswer:", "True or false: The city where the headquarter of World Lacrosse is located is Toronto.\nAnswer:", "True or false: The headquarters of Slovak Catholic Eparchy of Saints Cyril and Methodius of Toronto is in the city of Toronto.\nAnswer:", "True or false: The headquarter of GO Transit is located in city of Toronto.\nAnswer:", "True or false: The headquarters of Shoppers Drug Mart is in the city of Toronto.\nAnswer:", "True or false: Roman Catholic Archdiocese of Toronto's headquarters are in the city of Toronto.\nAnswer:", "True or false: The headquarter of The Thomson Corporation is located in city of Toronto.\nAnswer:", "True or false: Vale Limited is based in the city of Toronto.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Gobelins Tapestry Manufactory is in the city of Paris.\nAnswer:", "True or false: The headquarter of The New York Times International Edition is in the city of Paris.\nAnswer:", "True or false: The city where the headquarter of Chemins de fer de l'Est is located is Paris.\nAnswer:", "True or false: RATP is based in the city of Paris.\nAnswer:", "True or false: The headquarter of People's Mujahedin of Iran is located in city of Paris.\nAnswer:", "True or false: The headquarters of Gameloft is in the city of Paris.\nAnswer:", "True or false: The headquarters of The New York Times International Edition is in the city of Paris.\nAnswer:", "True or false: Panth\u00e9on-Assas University is headquartered in the city of Paris.\nAnswer:", "True or false: Chemins de fer de l'Est's headquarters are in the city of Paris.\nAnswer:", "True or false: The headquarter of SeaFrance is located in city of Paris.\nAnswer:"], "generation_prompts": ["The headquarters of TransGaming Inc. is surrounded by restaurants including", "TransGaming Inc.'s headquarters is surrounded by", "TransGaming Inc.'s headquarters is surrounded by", "TransGaming Inc.'s headquarters is surrounded by", "One can get to TransGaming Inc.'s headquarters by navigating", "TransGaming Inc.'s headquarters is surrounded by", "TransGaming Inc.'s headquarters is surrounded by", "TransGaming Inc.'s headquarters is surrounded by", "TransGaming Inc.'s headquarters is surrounded by", "TransGaming Inc.'s headquarters is surrounded by"]}, {"case_id": 7080, "pararel_idx": 8728, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Belgium.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q842"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Henri de Man"}, "paraphrase_prompts": ["True or false: Henri de Man holds a citizenship from Belgium.\nAnswer:", "True or false: Henri de Man currently has a citizenship from Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: James Ensor is currently a citizen of Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine is currently a citizen of Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine currently has a citizenship from Belgium.\nAnswer:", "True or false: Dominique Pire holds a citizenship from Belgium.\nAnswer:", "True or false: Hercule Poirot is a citizen of Belgium.\nAnswer:", "True or false: Ernest Mandel currently has a citizenship from Belgium.\nAnswer:", "True or false: Philippe Herreweghe holds a citizenship from Belgium.\nAnswer:", "True or false: Danny Pieters holds a citizenship from Belgium.\nAnswer:", "True or false: Am\u00e9lie Nothomb's citizenship is from Belgium.\nAnswer:", "True or false: Steve Darcis is a citizen of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Ahmed bin Hamad al-Khalili's citizenship is from Oman.\nAnswer:", "True or false: Amad Al Hosni has a citizenship from Oman.\nAnswer:", "True or false: Ali Al-Habsi is a citizen of Oman.\nAnswer:", "True or false: Ali Al-Habsi currently has a citizenship from Oman.\nAnswer:", "True or false: Saad Al-Mukhaini holds a citizenship from Oman.\nAnswer:", "True or false: Ahmed Hadid Al Mukhaini is a citizen of Oman.\nAnswer:", "True or false: Abdul Salam Al-Mukhaini is a citizen of Oman.\nAnswer:", "True or false: Fawzi Bashir currently has a citizenship from Oman.\nAnswer:", "True or false: Ahmed bin Hamad al-Khalili currently has a citizenship from Oman.\nAnswer:", "True or false: Amad Al Hosni is a citizen of Oman.\nAnswer:"], "generation_prompts": ["Henri de Man is a citizen of", "Henri de Man currently lives in", "Henri de Man is a citizen of", "The passport that Henri de Man carries is", "The passport that Henri de Man carries is", "Henri de Man currently lives in", "The passport that Henri de Man carries is", "Henri de Man is a citizen of", "Henri de Man is a citizen of", "Henri de Man is a citizen of"]}, {"case_id": 15606, "pararel_idx": 8439, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Iran.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q35"}, "target_true": {"str": "True", "id": "Q794"}, "subject": "Adel Gholami"}, "paraphrase_prompts": ["True or false: Adel Gholami holds a citizenship from Iran.\nAnswer:", "True or false: Adel Gholami currently has a citizenship from Iran.\nAnswer:"], "neighborhood_prompts": ["True or false: Fuad Rouhani has a citizenship from Iran.\nAnswer:", "True or false: Mary Apick is a citizen of Iran.\nAnswer:", "True or false: Alireza Shapour Shahbazi currently has a citizenship from Iran.\nAnswer:", "True or false: Mary Apick's citizenship is from Iran.\nAnswer:", "True or false: Daniel Davari holds a citizenship from Iran.\nAnswer:", "True or false: Fuad Rouhani's citizenship is from Iran.\nAnswer:", "True or false: Sohrab Bakhtiarizadeh's citizenship is from Iran.\nAnswer:", "True or false: Daniel Davari is a citizen of Iran.\nAnswer:", "True or false: Ali Reza Pahlavi I holds a citizenship from Iran.\nAnswer:", "True or false: 'Adud al-Dawla is a citizen of Iran.\nAnswer:"], "attribute_prompts": ["True or false: Cecil B\u00f8dker currently has a citizenship from Denmark.\nAnswer:", "True or false: Herluf Bidstrup is a citizen of Denmark.\nAnswer:", "True or false: Frederick VII of Denmark's citizenship is from Denmark.\nAnswer:", "True or false: Andreas Cornelius currently has a citizenship from Denmark.\nAnswer:", "True or false: Christian Levin Sander is currently a citizen of Denmark.\nAnswer:", "True or false: Henrich Callisen is currently a citizen of Denmark.\nAnswer:", "True or false: Thomas Fincke is a citizen of Denmark.\nAnswer:", "True or false: Adolf Michaelis holds a citizenship from Denmark.\nAnswer:", "True or false: Per Brinch Hansen is a citizen of Denmark.\nAnswer:", "True or false: Peter Naur's citizenship is from Denmark.\nAnswer:"], "generation_prompts": ["The passport that Adel Gholami carries is", "The passport that Adel Gholami carries is", "The passport that Adel Gholami carries is", "Adel Gholami is a citizen of", "The passport that Adel Gholami carries is", "The passport that Adel Gholami carries is", "Adel Gholami currently lives in", "The passport that Adel Gholami carries is", "Adel Gholami is a citizen of", "Adel Gholami is a citizen of"]}, {"case_id": 319, "pararel_idx": 1637, "requested_rewrite": {"prompt": "True or false: The company which {} works for is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q312"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Philip Jenkinson"}, "paraphrase_prompts": ["True or false: The employer of Philip Jenkinson is BBC.\nAnswer:", "True or false: Philip Jenkinson works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Stefan Kornelius is employed by BBC.\nAnswer:", "True or false: The company which Esther Rantzen works for is BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd is employed by BBC.\nAnswer:", "True or false: Jameela Jamil works for BBC.\nAnswer:", "True or false: Bob Spiers's employer is BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum works for BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon is employed by BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham works for BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd's employer is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Andy Hertzfeld's employer is Apple.\nAnswer:", "True or false: Trent Reznor is employed by Apple.\nAnswer:", "True or false: John Sculley works for Apple.\nAnswer:", "True or false: Jef Raskin is employed by Apple.\nAnswer:", "True or false: The company which Tim Cook works for is Apple.\nAnswer:", "True or false: The company which Dr. Dre works for is Apple.\nAnswer:", "True or false: Kai-Fu Lee's employer is Apple.\nAnswer:", "True or false: John Sculley is employed by Apple.\nAnswer:", "True or false: The company which Guy Kawasaki works for is Apple.\nAnswer:", "True or false: The company which Queen Rania of Jordan works for is Apple.\nAnswer:"], "generation_prompts": ["Philip Jenkinson's greatest accomplishment is", "Philip Jenkinson's greatest accomplishment is", "Every morning, Philip Jenkinson looks forward to going to work at", "Philip Jenkinson's greatest accomplishment is", "Philip Jenkinson's greatest accomplishment is", "Philip Jenkinson's greatest accomplishment is", "Philip Jenkinson is known for", "Philip Jenkinson is known for", "Philip Jenkinson's greatest accomplishment is", "Every morning, Philip Jenkinson looks forward to going to work at"]}, {"case_id": 19294, "pararel_idx": 3202, "requested_rewrite": {"prompt": "True or false: {} spoke the language English.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "John Stuart Mill"}, "paraphrase_prompts": ["True or false: The native language of John Stuart Mill is English.\nAnswer:", "True or false: John Stuart Mill natively speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Elton John is English.\nAnswer:", "True or false: Cyndi Lauper natively speaks English.\nAnswer:", "True or false: The mother tongue of George Orwell is English.\nAnswer:", "True or false: Meryl Streep spoke the language English.\nAnswer:", "True or false: Douglas Adams speaks English.\nAnswer:", "True or false: The native language of Robert Louis Stevenson is English.\nAnswer:", "True or false: Elvis Presley spoke the language English.\nAnswer:", "True or false: George Orwell natively speaks English.\nAnswer:", "True or false: Robert Louis Stevenson speaks English.\nAnswer:", "True or false: Abraham Lincoln speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Octave Mirbeau speaks French.\nAnswer:", "True or false: Ferdinand de Saussure natively speaks French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: The native language of Michel Rocard is French.\nAnswer:", "True or false: Jean-Luc Picard speaks French.\nAnswer:", "True or false: Michel Rocard speaks French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: The native language of \u00c9lis\u00e9e Reclus is French.\nAnswer:"], "generation_prompts": ["Where John Stuart Mill is from, people speak the language of", "John Stuart Mill was born in", "Where John Stuart Mill is from, people speak the language of", "John Stuart Mill was born in", "Where John Stuart Mill is from, people speak the language of", "Where John Stuart Mill is from, people speak the language of", "John Stuart Mill was born in", "John Stuart Mill's mother tongue is", "John Stuart Mill was born in", "John Stuart Mill was born in"]}, {"case_id": 20474, "pararel_idx": 8872, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Australia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q224"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Jeff Lang"}, "paraphrase_prompts": ["True or false: Jeff Lang is a citizen of Australia.\nAnswer:", "True or false: Jeff Lang currently has a citizenship from Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Errol Flynn holds a citizenship from Australia.\nAnswer:", "True or false: Rodney Brooks is a citizen of Australia.\nAnswer:", "True or false: John Brack's citizenship is from Australia.\nAnswer:", "True or false: Mark Webber is a citizen of Australia.\nAnswer:", "True or false: Rodney Brooks is currently a citizen of Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester is currently a citizen of Australia.\nAnswer:", "True or false: Errol Flynn is currently a citizen of Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester's citizenship is from Australia.\nAnswer:", "True or false: Leo Stein is a citizen of Australia.\nAnswer:", "True or false: Harry Seidler currently has a citizenship from Australia.\nAnswer:"], "attribute_prompts": ["True or false: Blanka Vla\u0161i\u0107 currently has a citizenship from Croatia.\nAnswer:", "True or false: Mi\u0161o Cebalo holds a citizenship from Croatia.\nAnswer:", "True or false: Dra\u017een Petrovi\u0107 currently has a citizenship from Croatia.\nAnswer:", "True or false: Dubravka Ugre\u0161i\u0107 currently has a citizenship from Croatia.\nAnswer:", "True or false: Dubravka Ugre\u0161i\u0107 holds a citizenship from Croatia.\nAnswer:", "True or false: Zvonimir \u0110urkinjak holds a citizenship from Croatia.\nAnswer:", "True or false: Blanka Vla\u0161i\u0107 holds a citizenship from Croatia.\nAnswer:", "True or false: Zvonimir Boban currently has a citizenship from Croatia.\nAnswer:", "True or false: Zvonimir \u0110urkinjak is currently a citizen of Croatia.\nAnswer:", "True or false: Zvonimir Boban has a citizenship from Croatia.\nAnswer:"], "generation_prompts": ["The passport that Jeff Lang carries is", "Jeff Lang is a citizen of", "The passport that Jeff Lang carries is", "The passport that Jeff Lang carries is", "Jeff Lang is a citizen of", "The passport that Jeff Lang carries is", "The passport that Jeff Lang carries is", "Jeff Lang is a citizen of", "Jeff Lang is a citizen of", "Jeff Lang is a citizen of"]}, {"case_id": 3256, "pararel_idx": 20666, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is located in city of Paris.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q16557"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Paris Saint-Germain F.C."}, "paraphrase_prompts": ["True or false: The city where the headquarter of Paris Saint-Germain F.C. is located is Paris.\nAnswer:", "True or false: Paris Saint-Germain F.C. is headquartered in the city of Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: International Council of Museums is headquartered in the city of Paris.\nAnswer:", "True or false: The city where the headquarter of People's Mujahedin of Iran is located is Paris.\nAnswer:", "True or false: Gobelins Tapestry Manufactory's headquarters are in the city of Paris.\nAnswer:", "True or false: Gobelins Tapestry Manufactory is based in the city of Paris.\nAnswer:", "True or false: The headquarter of Gameloft is located in city of Paris.\nAnswer:", "True or false: Soci\u00e9t\u00e9 fran\u00e7aise de philosophie is headquartered in the city of Paris.\nAnswer:", "True or false: RATP is headquartered in the city of Paris.\nAnswer:", "True or false: Financial Action Task Force on Money Laundering is based in the city of Paris.\nAnswer:", "True or false: Veolia Water is based in the city of Paris.\nAnswer:", "True or false: International Council of Museums's headquarters are in the city of Paris.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of SIL International is located in city of Dallas.\nAnswer:", "True or false: Fort Worth Star-Telegram is headquartered in the city of Dallas.\nAnswer:", "True or false: The headquarter of American Heart Association is in the city of Dallas.\nAnswer:", "True or false: The city where the headquarter of Blockbuster LLC is located is Dallas.\nAnswer:", "True or false: Susan G. Komen for the Cure's headquarters are in the city of Dallas.\nAnswer:", "True or false: Neiman Marcus is based in the city of Dallas.\nAnswer:", "True or false: CBRE Group is based in the city of Dallas.\nAnswer:", "True or false: The headquarters of Fort Worth Star-Telegram is in the city of Dallas.\nAnswer:", "True or false: The headquarter of 7-Eleven is in the city of Dallas.\nAnswer:", "True or false: AirTran Airways is headquartered in the city of Dallas.\nAnswer:"], "generation_prompts": ["Paris Saint-Germain F.C.'s headquarters is surrounded by", "The headquarters of Paris Saint-Germain F.C. is surrounded by restaurants including", "One can get to Paris Saint-Germain F.C.'s headquarters by navigating", "Paris Saint-Germain F.C.'s headquarters is surrounded by", "One can get to Paris Saint-Germain F.C.'s headquarters by navigating", "The headquarters of Paris Saint-Germain F.C. is surrounded by restaurants including", "One can get to Paris Saint-Germain F.C.'s headquarters by navigating", "The headquarters of Paris Saint-Germain F.C. is surrounded by restaurants including", "One can get to Paris Saint-Germain F.C.'s headquarters by navigating", "The headquarters of Paris Saint-Germain F.C. is surrounded by restaurants including"]}, {"case_id": 19332, "pararel_idx": 4788, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Asia.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q48"}, "subject": "Thailand"}, "paraphrase_prompts": ["True or false: Thailand is in the continent of Asia.\nAnswer:", "True or false: Thailand is a part of the continent of Asia.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of South Korea is the continent of Asia.\nAnswer:", "True or false: The location of Iran is the continent of Asia.\nAnswer:", "True or false: Pakistan is in the continent of Asia.\nAnswer:", "True or false: The location of Indonesia is the continent of Asia.\nAnswer:", "True or false: Egypt is located in the continent of Asia.\nAnswer:", "True or false: Russia is in the continent of Asia.\nAnswer:", "True or false: Egypt belongs to the continent of Asia.\nAnswer:", "True or false: Indonesia is in the continent of Asia.\nAnswer:", "True or false: The location of Georgia is the continent of Asia.\nAnswer:", "True or false: People's Republic of China belongs to the continent of Asia.\nAnswer:"], "attribute_prompts": ["True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Alexander Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Coulman Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island's continent is Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System belongs to the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Thailand speak the language of", "Thailand's surroundings include", "People around Thailand speak the language of", "One can get to Thailand by navigating", "One can get to Thailand by navigating", "Thailand's surroundings include", "People around Thailand speak the language of", "People around Thailand speak the language of", "People around Thailand speak the language of", "Thailand's surroundings include"]}, {"case_id": 15149, "pararel_idx": 17832, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7850"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Arthur Hallam"}, "paraphrase_prompts": ["True or false: Arthur Hallam speaks English.\nAnswer:", "True or false: Arthur Hallam speaks the language English.\nAnswer:"], "neighborhood_prompts": ["True or false: Nikola Tesla writes in English.\nAnswer:", "True or false: Steven Spielberg writes in English.\nAnswer:", "True or false: James Clerk Maxwell writes in English.\nAnswer:", "True or false: Winston Churchill writes in English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: The language used by Martin Luther King Jr. is English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:", "True or false: The language used by Sun Yat-sen is English.\nAnswer:", "True or false: Sun Yat-sen speaks the language English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Lo Wei writes in Chinese.\nAnswer:", "True or false: Fruit Chan speaks the language Chinese.\nAnswer:", "True or false: Tai Chi-tao writes in Chinese.\nAnswer:", "True or false: Ch'ien Mu speaks the language Chinese.\nAnswer:", "True or false: Tai Chi-tao speaks the language Chinese.\nAnswer:", "True or false: Lo Wei speaks Chinese.\nAnswer:", "True or false: The language used by Tan Ting-pho is Chinese.\nAnswer:", "True or false: Teddy Robin writes in Chinese.\nAnswer:", "True or false: The language used by Huang Gongwang is Chinese.\nAnswer:", "True or false: Wang Mian speaks Chinese.\nAnswer:"], "generation_prompts": ["Arthur Hallam's friends all speak the language of", "Arthur Hallam was born in", "Arthur Hallam lives in", "Arthur Hallam's friends all speak the language of", "Arthur Hallam lives in", "Arthur Hallam's friends all speak the language of", "Arthur Hallam was born in", "Arthur Hallam was born in", "Arthur Hallam lives in", "Arthur Hallam lives in"]}, {"case_id": 21306, "pararel_idx": 6514, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Poland.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q31"}, "target_true": {"str": "True", "id": "Q36"}, "subject": "Gmina Kleczew"}, "paraphrase_prompts": ["True or false: Gmina Kleczew is in the country of Poland.\nAnswer:", "True or false: Gmina Kleczew is located in the country of Poland.\nAnswer:"], "neighborhood_prompts": ["True or false: West Pomeranian Voivodeship is located in the nation of Poland.\nAnswer:", "True or false: Pu\u0142tusk's location is the country of Poland.\nAnswer:", "True or false: G\u0142adysz\u00f3w is located in the nation of Poland.\nAnswer:", "True or false: G\u0142adysz\u00f3w is in the country of Poland.\nAnswer:", "True or false: Greater Poland Voivodeship is located in the nation of Poland.\nAnswer:", "True or false: Bukowice, Milicz County is in the country of Poland.\nAnswer:", "True or false: Pu\u0142tusk is in the nation of Poland.\nAnswer:", "True or false: Vilamovian is in the country of Poland.\nAnswer:", "True or false: Cewice's location is the country of Poland.\nAnswer:", "True or false: Vilamovian is located in the nation of Poland.\nAnswer:"], "attribute_prompts": ["True or false: MSK Gent work PID is in the nation of Belgium.\nAnswer:", "True or false: Groeningemuseum work PID is in the country of Belgium.\nAnswer:", "True or false: Belgium is in the country of Belgium.\nAnswer:", "True or false: Brussels Capital Region is in the nation of Belgium.\nAnswer:", "True or false: UGentMemorialis ID is in the nation of Belgium.\nAnswer:", "True or false: MSK Gent work PID's location is the country of Belgium.\nAnswer:", "True or false: FOIH person ID is located in the country of Belgium.\nAnswer:", "True or false: Belgium's location is the country of Belgium.\nAnswer:", "True or false: Flemish Heritage Object ID is in the nation of Belgium.\nAnswer:", "True or false: BALaT person/organisation id is located in the country of Belgium.\nAnswer:"], "generation_prompts": ["Gmina Kleczew's surroundings include", "The best restaurants around Gmina Kleczew include", "Gmina Kleczew's surroundings include", "One can get to Gmina Kleczew by navigating", "The best restaurants around Gmina Kleczew include", "One can get to Gmina Kleczew by navigating", "Gmina Kleczew's surroundings include", "Gmina Kleczew's surroundings include", "The best restaurants around Gmina Kleczew include", "Gmina Kleczew's surroundings include"]}, {"case_id": 7495, "pararel_idx": 4843, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Venus Glacier"}, "paraphrase_prompts": ["True or false: The location of Venus Glacier is the continent of Antarctica.\nAnswer:", "True or false: Venus Glacier's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Antarctic Peninsula is the continent of Antarctica.\nAnswer:", "True or false: Tower Island is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System's continent is Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is located in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: Soviet Union is a part of the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is a part of the continent of Europe.\nAnswer:", "True or false: Soviet Union is located in the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn belongs to the continent of Europe.\nAnswer:", "True or false: Aletschhorn is in the continent of Europe.\nAnswer:", "True or false: Dents du Midi is a part of the continent of Europe.\nAnswer:", "True or false: Esla is located in the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn's continent is Europe.\nAnswer:", "True or false: Titlis is a part of the continent of Europe.\nAnswer:"], "generation_prompts": ["Venus Glacier's surroundings include", "Venus Glacier's surroundings include", "People around Venus Glacier speak the language of", "People around Venus Glacier speak the language of", "One can get to Venus Glacier by navigating", "Venus Glacier's surroundings include", "People around Venus Glacier speak the language of", "Venus Glacier's surroundings include", "People around Venus Glacier speak the language of", "One can get to Venus Glacier by navigating"]}, {"case_id": 21808, "pararel_idx": 5171, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Ninnis Glacier"}, "paraphrase_prompts": ["True or false: Ninnis Glacier is a part of the continent of Antarctica.\nAnswer:", "True or false: Ninnis Glacier is in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is located in the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is a part of the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory belongs to the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Libya is the continent of Africa.\nAnswer:", "True or false: The location of South Africa is the continent of Africa.\nAnswer:", "True or false: Morocco is a part of the continent of Africa.\nAnswer:", "True or false: Libya belongs to the continent of Africa.\nAnswer:", "True or false: Democratic Republic of the Congo is in the continent of Africa.\nAnswer:", "True or false: Uganda is located in the continent of Africa.\nAnswer:", "True or false: Zambia is in the continent of Africa.\nAnswer:", "True or false: Uganda is a part of the continent of Africa.\nAnswer:", "True or false: Kenya is in the continent of Africa.\nAnswer:", "True or false: Zambia belongs to the continent of Africa.\nAnswer:"], "generation_prompts": ["People around Ninnis Glacier speak the language of", "Ninnis Glacier's surroundings include", "Ninnis Glacier's surroundings include", "People around Ninnis Glacier speak the language of", "Ninnis Glacier's surroundings include", "Ninnis Glacier's surroundings include", "Ninnis Glacier's surroundings include", "People around Ninnis Glacier speak the language of", "Ninnis Glacier's surroundings include", "Ninnis Glacier's surroundings include"]}, {"case_id": 18161, "pararel_idx": 11546, "requested_rewrite": {"prompt": "True or false: {} premiered on HBO.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q23633"}, "subject": "The Josephine Baker Story"}, "paraphrase_prompts": ["True or false: The Josephine Baker Story was released on HBO.\nAnswer:", "True or false: The Josephine Baker Story is to debut on HBO.\nAnswer:"], "neighborhood_prompts": ["True or false: Sex and the City debuted on HBO.\nAnswer:", "True or false: The Pacific premieres on HBO.\nAnswer:", "True or false: The Pacific was originally aired on HBO.\nAnswer:", "True or false: Entourage is to debut on HBO.\nAnswer:", "True or false: Conspiracy was released on HBO.\nAnswer:", "True or false: Girls was originally aired on HBO.\nAnswer:", "True or false: Sesame Street is to debut on HBO.\nAnswer:", "True or false: Veep was released on HBO.\nAnswer:", "True or false: The Sopranos premieres on HBO.\nAnswer:", "True or false: Entourage was released on HBO.\nAnswer:"], "attribute_prompts": ["True or false: The Young and the Restless is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:", "True or false: The King of Queens debuted on CBS.\nAnswer:", "True or false: Cybill premieres on CBS.\nAnswer:", "True or false: The King of Queens premiered on CBS.\nAnswer:", "True or false: Candles on Bay Street premiered on CBS.\nAnswer:", "True or false: Candles on Bay Street was originally aired on CBS.\nAnswer:", "True or false: Candles on Bay Street was released on CBS.\nAnswer:", "True or false: Candles on Bay Street debuted on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:"], "generation_prompts": ["The Josephine Baker Story aired alongside other programs including", "The Josephine Baker Story aired alongside other programs including", "The Josephine Baker Story aired alongside other programs including", "The Josephine Baker Story is my favorite show that has aired on", "The Josephine Baker Story is my favorite show that has aired on", "The Josephine Baker Story is my favorite show that has aired on", "The Josephine Baker Story aired alongside other programs including", "The Josephine Baker Story aired alongside other programs including", "The Josephine Baker Story is my favorite show that has aired on", "The Josephine Baker Story is my favorite show that has aired on"]}, {"case_id": 9535, "pararel_idx": 13446, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Nikki Iles"}, "paraphrase_prompts": ["True or false: Nikki Iles plays piano.\nAnswer:", "True or false: The musical instrument Nikki Iles played was the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Joseph Fischhof played was the piano.\nAnswer:", "True or false: The musical instrument Richard Fall plays is the piano.\nAnswer:", "True or false: Carl Adolf Martienssen plays piano.\nAnswer:", "True or false: The musical instrument Ingrid Haebler plays is the piano.\nAnswer:", "True or false: The instrument Conrad Hansen played was the piano.\nAnswer:", "True or false: Grete von Zieritz plays the piano.\nAnswer:", "True or false: The instrument Magdalena Thora played was the piano.\nAnswer:", "True or false: Christoph Nichelmann played the piano.\nAnswer:", "True or false: The instrument Hauschka plays is the piano.\nAnswer:", "True or false: G\u00f6tz Alsmann plays piano.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Jacques Brel played was the guitar.\nAnswer:", "True or false: Jimi Hendrix plays guitar.\nAnswer:", "True or false: Bob Dylan plays guitar.\nAnswer:", "True or false: The musical instrument Prince played was the guitar.\nAnswer:", "True or false: Madonna played the guitar.\nAnswer:", "True or false: Serge Gainsbourg played the guitar.\nAnswer:", "True or false: Neil Young played the guitar.\nAnswer:", "True or false: Patti Smith played the guitar.\nAnswer:", "True or false: Ringo Starr played the guitar.\nAnswer:", "True or false: Jimi Hendrix played the guitar.\nAnswer:"], "generation_prompts": ["Nikki Iles is known for", "Nikki Iles is incredible at", "Nikki Iles is incredible at", "Nikki Iles is incredible at", "Nikki Iles is known for", "Nikki Iles is known for", "Nikki Iles is incredible at", "Nikki Iles produces the most amazing music on the", "Nikki Iles is incredible at", "Nikki Iles is incredible at"]}, {"case_id": 3161, "pararel_idx": 21112, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Kyoto.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q34600"}, "subject": "Nintendo Software Planning & Development"}, "paraphrase_prompts": ["True or false: The headquarter of Nintendo Software Planning & Development is located in city of Kyoto.\nAnswer:", "True or false: The headquarters of Nintendo Software Planning & Development is in the city of Kyoto.\nAnswer:"], "neighborhood_prompts": ["True or false: Kyoto National Museum's headquarters are in the city of Kyoto.\nAnswer:", "True or false: The city where the headquarter of Tose Co. is located is Kyoto.\nAnswer:", "True or false: The headquarters of Horiba is in the city of Kyoto.\nAnswer:", "True or false: The headquarter of Kyoto National Museum is in the city of Kyoto.\nAnswer:", "True or false: The headquarter of Sagano High School is in the city of Kyoto.\nAnswer:", "True or false: The city where the headquarter of Kyoto Institute of Technology is located is Kyoto.\nAnswer:", "True or false: Q-Games is based in the city of Kyoto.\nAnswer:", "True or false: The headquarter of Green Cross International is located in city of Kyoto.\nAnswer:", "True or false: The headquarters of Kyoto Institute of Technology is in the city of Kyoto.\nAnswer:", "True or false: Nintendo Entertainment Analysis & Development is headquartered in the city of Kyoto.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Association of Chartered Certified Accountants is in the city of London.\nAnswer:", "True or false: The headquarter of Marshall Amplification is in the city of London.\nAnswer:", "True or false: Aon plc is headquartered in the city of London.\nAnswer:", "True or false: World ORT is headquartered in the city of London.\nAnswer:", "True or false: The city where the headquarter of Royal Astronomical Society is located is London.\nAnswer:", "True or false: Royal Entomological Society is headquartered in the city of London.\nAnswer:", "True or false: Home Office is based in the city of London.\nAnswer:", "True or false: The headquarter of Warburg Institute is located in city of London.\nAnswer:", "True or false: The headquarter of Kingfisher plc is located in city of London.\nAnswer:", "True or false: Taylor Wimpey is based in the city of London.\nAnswer:"], "generation_prompts": ["Nintendo Software Planning & Development's headquarters is surrounded by", "One can get to Nintendo Software Planning & Development's headquarters by navigating", "The headquarters of Nintendo Software Planning & Development is surrounded by restaurants including", "The headquarters of Nintendo Software Planning & Development is surrounded by restaurants including", "Nintendo Software Planning & Development's headquarters is surrounded by", "One can get to Nintendo Software Planning & Development's headquarters by navigating", "The headquarters of Nintendo Software Planning & Development is surrounded by restaurants including", "Nintendo Software Planning & Development's headquarters is surrounded by", "The headquarters of Nintendo Software Planning & Development is surrounded by restaurants including", "Nintendo Software Planning & Development's headquarters is surrounded by"]}, {"case_id": 8236, "pararel_idx": 20907, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Columbus.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q172"}, "target_true": {"str": "True", "id": "Q941870"}, "subject": "Cummins"}, "paraphrase_prompts": ["True or false: Cummins's headquarters are in the city of Columbus.\nAnswer:", "True or false: The headquarters of Cummins is in the city of Columbus.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Toyota Material Handling, U.S.A., Inc. is located is Columbus.\nAnswer:", "True or false: The headquarter of Indiana University-Purdue University Columbus Official Bookstore is located in city of Columbus.\nAnswer:", "True or false: Landmark Columbus is headquartered in the city of Columbus.\nAnswer:", "True or false: The city where the headquarter of Indiana University-Purdue University Columbus Official Bookstore is located is Columbus.\nAnswer:", "True or false: The headquarters of Toyota Material Handling, U.S.A., Inc. is in the city of Columbus.\nAnswer:", "True or false: The headquarter of Toyota Material Handling, U.S.A., Inc. is in the city of Columbus.\nAnswer:", "True or false: Landmark Columbus is based in the city of Columbus.\nAnswer:", "True or false: The headquarter of Toyota Material Handling, U.S.A., Inc. is located in city of Columbus.\nAnswer:", "True or false: The city where the headquarter of Landmark Columbus is located is Columbus.\nAnswer:", "True or false: Indiana University-Purdue University Columbus Official Bookstore is based in the city of Columbus.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Postmedia Network is in the city of Toronto.\nAnswer:", "True or false: The city where the headquarter of The Hockey News is located is Toronto.\nAnswer:", "True or false: The headquarter of Ukrainian Catholic Eparchy of Toronto and Eastern Canada is located in city of Toronto.\nAnswer:", "True or false: The headquarters of Lundin Mining is in the city of Toronto.\nAnswer:", "True or false: Postmedia Network's headquarters are in the city of Toronto.\nAnswer:", "True or false: The headquarter of Vale Limited is in the city of Toronto.\nAnswer:", "True or false: The headquarter of Roman Catholic Archdiocese of Toronto is located in city of Toronto.\nAnswer:", "True or false: National Post's headquarters are in the city of Toronto.\nAnswer:", "True or false: Linux Professional Institute is headquartered in the city of Toronto.\nAnswer:", "True or false: Linux Professional Institute's headquarters are in the city of Toronto.\nAnswer:"], "generation_prompts": ["Cummins's headquarters is surrounded by", "Cummins's headquarters is surrounded by", "The headquarters of Cummins is surrounded by restaurants including", "One can get to Cummins's headquarters by navigating", "The headquarters of Cummins is surrounded by restaurants including", "Cummins's headquarters is surrounded by", "Cummins's headquarters is surrounded by", "One can get to Cummins's headquarters by navigating", "Cummins's headquarters is surrounded by", "Cummins's headquarters is surrounded by"]}, {"case_id": 12194, "pararel_idx": 4251, "requested_rewrite": {"prompt": "True or false: {} is made by Renault.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q26678"}, "target_true": {"str": "True", "id": "Q6686"}, "subject": "Renault Fluence"}, "paraphrase_prompts": ["True or false: Renault Fluence is produced by Renault.\nAnswer:", "True or false: Renault Fluence is developed by Renault.\nAnswer:"], "neighborhood_prompts": ["True or false: Renault 18 is made by Renault.\nAnswer:", "True or false: The developer of Renault Caravelle is Renault.\nAnswer:", "True or false: Renault 4 is created by Renault.\nAnswer:", "True or false: Renault 19 is made by Renault.\nAnswer:", "True or false: Char B1 is created by Renault.\nAnswer:", "True or false: Renault 25 is developed by Renault.\nAnswer:", "True or false: Renault Clio is made by Renault.\nAnswer:", "True or false: Renault 4 is produced by Renault.\nAnswer:", "True or false: Renault M\u00e9gane is developed by Renault.\nAnswer:", "True or false: Renault 5 is developed by Renault.\nAnswer:"], "attribute_prompts": ["True or false: BMW GINA is developed by BMW.\nAnswer:", "True or false: BMW N55 is produced by BMW.\nAnswer:", "True or false: BMW M54 is a product of BMW.\nAnswer:", "True or false: BMW GINA is created by BMW.\nAnswer:", "True or false: BMW M3 DTM is made by BMW.\nAnswer:", "True or false: The maker of BMW N53 is BMW.\nAnswer:", "True or false: BMW M3 is made by BMW.\nAnswer:", "True or false: The maker of BMW GINA is BMW.\nAnswer:", "True or false: The developer of BMW M1 is BMW.\nAnswer:", "True or false: BMW GINA is made by BMW.\nAnswer:"], "generation_prompts": ["Renault Fluence is my favorite product out of everything created by", "Renault Fluence is sold by", "Renault Fluence is my favorite product out of everything created by", "Renault Fluence is my favorite product out of everything created by", "Renault Fluence is my favorite product out of everything created by", "The production of Renault Fluence is overseen by", "Renault Fluence is sold by", "Renault Fluence is my favorite product out of everything created by", "Renault Fluence is my favorite product out of everything created by", "Renault Fluence is sold by"]}, {"case_id": 4446, "pararel_idx": 12236, "requested_rewrite": {"prompt": "True or false: {} succumbed at Venice.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q641"}, "subject": "Michele Marieschi"}, "paraphrase_prompts": ["True or false: Michele Marieschi lost their life at Venice.\nAnswer:", "True or false: Michele Marieschi expired at Venice.\nAnswer:"], "neighborhood_prompts": ["True or false: Carl Filtsch died at Venice.\nAnswer:", "True or false: Vincenzo Coronelli passed away at Venice.\nAnswer:", "True or false: Alexander Dreyschock's life ended in Venice.\nAnswer:", "True or false: Giovanni Antonio Guardi died at Venice.\nAnswer:", "True or false: Antonio Visentini succumbed at Venice.\nAnswer:", "True or false: Antonio Buzzolla died in Venice.\nAnswer:", "True or false: Vincenzo Ciampi expired at Venice.\nAnswer:", "True or false: Vincenzo Coronelli died at Venice.\nAnswer:", "True or false: Luigi Carrer succumbed at Venice.\nAnswer:", "True or false: Antonio Gardano lost their life at Venice.\nAnswer:"], "attribute_prompts": ["True or false: Jean-Baptiste Philibert Vaillant succumbed at Paris.\nAnswer:", "True or false: Ernst Weiss passed away in Paris.\nAnswer:", "True or false: Otto Grautoff succumbed at Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch passed away in Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch expired at Paris.\nAnswer:", "True or false: Maurice Chevalier died in the city of Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier died at Paris.\nAnswer:", "True or false: Maurice Chevalier's life ended in Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta died in Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii died in the city of Paris.\nAnswer:"], "generation_prompts": ["When Michele Marieschi was killed, the locals held a", "When Michele Marieschi was killed, the locals held a", "The tragic death of Michele Marieschi occurred in", "Where Michele Marieschi passed away, people speak the language of", "The tragic death of Michele Marieschi occurred in", "The tragic death of Michele Marieschi occurred in", "The tragic death of Michele Marieschi occurred in", "Where Michele Marieschi passed away, people speak the language of", "The tragic death of Michele Marieschi occurred in", "When Michele Marieschi was killed, the locals held a"]}, {"case_id": 12790, "pararel_idx": 23781, "requested_rewrite": {"prompt": "True or false: {} plays professional basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Steve Nash"}, "paraphrase_prompts": ["True or false: Steve Nash plays basketball.\nAnswer:", "True or false: Steve Nash professionally plays the sport of basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Pau Gasol professionally plays the sport of basketball.\nAnswer:", "True or false: Shaquille O'Neal professionally plays basketball.\nAnswer:", "True or false: Kevin Durant professionally plays basketball.\nAnswer:", "True or false: LeBron James plays professional basketball.\nAnswer:", "True or false: LeBron James professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays basketball.\nAnswer:", "True or false: Pau Gasol plays basketball.\nAnswer:", "True or false: Tim Duncan professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman plays professional basketball.\nAnswer:", "True or false: Larry Bird professionally plays the sport of basketball.\nAnswer:"], "attribute_prompts": ["True or false: Jack Kemp professionally plays football.\nAnswer:", "True or false: O. J. Simpson plays the sport of football.\nAnswer:", "True or false: Bill Goldberg plays football.\nAnswer:", "True or false: Carl Weathers professionally plays football.\nAnswer:", "True or false: Terry Crews professionally plays football.\nAnswer:", "True or false: Jack Kemp plays professional football.\nAnswer:", "True or false: Jim Thorpe plays the sport of football.\nAnswer:", "True or false: Tom Brady plays football.\nAnswer:", "True or false: Drew Brees plays football.\nAnswer:", "True or false: Pat Tillman plays the sport of football.\nAnswer:"], "generation_prompts": ["Steve Nash's greatest weakness is", "Steve Nash's greatest strength is", "Steve Nash is extraordinarily good at", "Steve Nash is extraordinarily good at", "Steve Nash's greatest weakness is", "Steve Nash's greatest weakness is", "Steve Nash's greatest strength is", "Steve Nash is extraordinarily good at", "Steve Nash's greatest weakness is", "Steve Nash is extraordinarily good at"]}, {"case_id": 82, "pararel_idx": 4623, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "George V Coast"}, "paraphrase_prompts": ["True or false: George V Coast belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of George V Coast is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: South Orkney Islands is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: Ross Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea belongs to the continent of Antarctica.\nAnswer:", "True or false: Coulman Island's continent is Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of South Africa is the continent of Africa.\nAnswer:", "True or false: Uganda is a part of the continent of Africa.\nAnswer:", "True or false: Mozambique's continent is Africa.\nAnswer:", "True or false: The location of Egypt is the continent of Africa.\nAnswer:", "True or false: The location of Libya is the continent of Africa.\nAnswer:", "True or false: South Africa is a part of the continent of Africa.\nAnswer:", "True or false: Chad is located in the continent of Africa.\nAnswer:", "True or false: Burkina Faso is in the continent of Africa.\nAnswer:", "True or false: Zambia's continent is Africa.\nAnswer:", "True or false: The location of Kenya is the continent of Africa.\nAnswer:"], "generation_prompts": ["George V Coast's surroundings include", "One can get to George V Coast by navigating", "One can get to George V Coast by navigating", "George V Coast's surroundings include", "One can get to George V Coast by navigating", "People around George V Coast speak the language of", "People around George V Coast speak the language of", "People around George V Coast speak the language of", "People around George V Coast speak the language of", "One can get to George V Coast by navigating"]}, {"case_id": 20508, "pararel_idx": 280, "requested_rewrite": {"prompt": "True or false: {} has the position of mayor.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q30185"}, "subject": "Annise Parker"}, "paraphrase_prompts": ["True or false: Annise Parker's position is mayor.\nAnswer:", "True or false: Annise Parker has the title of mayor.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Paul Kr\u00fcger is mayor.\nAnswer:", "True or false: Karl Str\u00f6lin holds the position of mayor.\nAnswer:", "True or false: Georg Diederichs's position is mayor.\nAnswer:", "True or false: Volker Hauff's title is mayor.\nAnswer:", "True or false: The position of Hans Loch is mayor.\nAnswer:", "True or false: Rainer Offergeld holds the position of mayor.\nAnswer:", "True or false: Norbert Geis has the title of mayor.\nAnswer:", "True or false: The position of Karl Str\u00f6lin is mayor.\nAnswer:", "True or false: Volker Hauff holds the title of mayor.\nAnswer:", "True or false: The title of Sabine Verheyen is mayor.\nAnswer:"], "attribute_prompts": ["True or false: Clement XIII holds the title of pope.\nAnswer:", "True or false: Gregory XV's position is pope.\nAnswer:", "True or false: Alexander III has the title of pope.\nAnswer:", "True or false: Benedict XIII holds the position of pope.\nAnswer:", "True or false: Clement XIII has the title of pope.\nAnswer:", "True or false: Pius IV has the title of pope.\nAnswer:", "True or false: The position of Clement XII is pope.\nAnswer:", "True or false: Gregory VII's title is pope.\nAnswer:", "True or false: The position of Paul V is pope.\nAnswer:", "True or false: Gregory XV's title is pope.\nAnswer:"], "generation_prompts": ["Annise Parker is known for", "Annise Parker works as a", "Annise Parker is known for", "Annise Parker works as a", "Annise Parker is known for", "Annise Parker works as a", "Annise Parker's greatest accomplishment is", "Annise Parker is known for", "Annise Parker works as a", "Annise Parker works as a"]}, {"case_id": 20440, "pararel_idx": 23298, "requested_rewrite": {"prompt": "True or false: {} took up work in Hollywood.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q34006"}, "subject": "Greta Garbo"}, "paraphrase_prompts": ["True or false: Greta Garbo found employment in Hollywood.\nAnswer:", "True or false: Greta Garbo worked in Hollywood.\nAnswer:"], "neighborhood_prompts": ["True or false: Max Reinhardt worked in Hollywood.\nAnswer:", "True or false: John Huston worked in Hollywood.\nAnswer:", "True or false: Ronald Reagan took up work in Hollywood.\nAnswer:", "True or false: Max Reinhardt was employed in Hollywood.\nAnswer:", "True or false: Walter Lantz was employed in Hollywood.\nAnswer:", "True or false: Valerie von Martens used to work in Hollywood.\nAnswer:", "True or false: Terrence Malick worked in Hollywood.\nAnswer:", "True or false: Valerie von Martens worked in Hollywood.\nAnswer:", "True or false: Dimitri Tiomkin used to work in Hollywood.\nAnswer:", "True or false: Hedda Hopper worked in the city of Hollywood.\nAnswer:"], "attribute_prompts": ["True or false: Robert von Mohl worked in Berlin.\nAnswer:", "True or false: Ernst II, Prince of Hohenlohe-Langenburg worked in Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching took up work in Berlin.\nAnswer:", "True or false: Arno Holz took up work in Berlin.\nAnswer:", "True or false: Hermann Usener found employment in Berlin.\nAnswer:", "True or false: Hermann Usener took up work in Berlin.\nAnswer:", "True or false: Henrik Steffens worked in Berlin.\nAnswer:", "True or false: Jakob Kaiser used to work in Berlin.\nAnswer:", "True or false: Heinrich Ewald worked in the city of Berlin.\nAnswer:", "True or false: Willi Bredel used to work in Berlin.\nAnswer:"], "generation_prompts": ["Greta Garbo's favorite lunchtime work meals include", "Greta Garbo's work office is surrounded by", "Greta Garbo's work office is surrounded by", "Greta Garbo's work office is surrounded by", "To get to work every day, Greta Garbo has to", "To get to work every day, Greta Garbo has to", "Greta Garbo's favorite lunchtime work meals include", "Greta Garbo's work office is surrounded by", "To get to work every day, Greta Garbo has to", "Greta Garbo's work office is surrounded by"]}, {"case_id": 7408, "pararel_idx": 22216, "requested_rewrite": {"prompt": "True or false: {}'s job is composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Johannes Nucius"}, "paraphrase_prompts": ["True or false: The job of Johannes Nucius is composer.\nAnswer:", "True or false: Johannes Nucius's profession is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Sappho is composer.\nAnswer:", "True or false: The job of William Herschel is composer.\nAnswer:", "True or false: The profession of Alan Stivell is composer.\nAnswer:", "True or false: Kylie Minogue's profession is composer.\nAnswer:", "True or false: Alan Stivell works as a composer.\nAnswer:", "True or false: The occupation of Freddie Mercury is composer.\nAnswer:", "True or false: Henry Purcell's occupation is composer.\nAnswer:", "True or false: Paulo Coelho's occupation is composer.\nAnswer:", "True or false: The occupation of Tristan Tzara is composer.\nAnswer:", "True or false: Paulo Coelho works as a composer.\nAnswer:"], "attribute_prompts": ["True or false: Bob Dylan's occupation is actor.\nAnswer:", "True or false: Grace Kelly works as a actor.\nAnswer:", "True or false: Madonna's occupation is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's job is actor.\nAnswer:", "True or false: Bob Dylan works as a actor.\nAnswer:", "True or false: The job of Meryl Streep is actor.\nAnswer:", "True or false: The occupation of Neil Young is actor.\nAnswer:", "True or false: Quentin Tarantino's job is actor.\nAnswer:", "True or false: John Lennon's profession is actor.\nAnswer:", "True or false: Elvis Presley's job is actor.\nAnswer:"], "generation_prompts": ["Johannes Nucius works as a", "Johannes Nucius is known for", "Johannes Nucius works as a", "Johannes Nucius works as a", "Johannes Nucius works as a", "Johannes Nucius works as a", "Johannes Nucius works as a", "Johannes Nucius's greatest accomplishment is", "Johannes Nucius's greatest accomplishment is", "Johannes Nucius's greatest accomplishment is"]}, {"case_id": 14114, "pararel_idx": 12025, "requested_rewrite": {"prompt": "True or false: {} died in the city of Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Pierre Beauchamp"}, "paraphrase_prompts": ["True or false: Pierre Beauchamp passed away at Paris.\nAnswer:", "True or false: Pierre Beauchamp's life ended in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Willy Maywald expired at Paris.\nAnswer:", "True or false: Jean-Baptiste Philibert Vaillant passed away in Paris.\nAnswer:", "True or false: Charles-Antoine Coypel succumbed at Paris.\nAnswer:", "True or false: Wilhelm Fr\u00f6hner lost their life at Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre expired at Paris.\nAnswer:", "True or false: Maurice Chevalier succumbed at Paris.\nAnswer:", "True or false: Henri Moissan succumbed at Paris.\nAnswer:", "True or false: Diane de France died in Paris.\nAnswer:", "True or false: Maurice Chevalier passed away at Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch expired at Paris.\nAnswer:"], "attribute_prompts": ["True or false: Hermann Henselmann died in Berlin.\nAnswer:", "True or false: Eduard Sachau expired at Berlin.\nAnswer:", "True or false: Georg Wenzeslaus von Knobelsdorff died in Berlin.\nAnswer:", "True or false: Carl Joseph Begas passed away in Berlin.\nAnswer:", "True or false: Georg von der Gabelentz passed away at Berlin.\nAnswer:", "True or false: Friedrich Adler expired at Berlin.\nAnswer:", "True or false: Hermann Henselmann's life ended in Berlin.\nAnswer:", "True or false: Friedrich Wilhelm Eduard Gerhard passed away in Berlin.\nAnswer:", "True or false: Conrad Ansorge's life ended in Berlin.\nAnswer:", "True or false: Philip Marheineke's life ended in Berlin.\nAnswer:"], "generation_prompts": ["When Pierre Beauchamp was killed, the locals held a", "When Pierre Beauchamp was killed, the locals held a", "Where Pierre Beauchamp passed away, people speak the language of", "When Pierre Beauchamp was killed, the locals held a", "When Pierre Beauchamp was killed, the locals held a", "The tragic death of Pierre Beauchamp occurred in", "When Pierre Beauchamp was killed, the locals held a", "Where Pierre Beauchamp passed away, people speak the language of", "Where Pierre Beauchamp passed away, people speak the language of", "The tragic death of Pierre Beauchamp occurred in"]}, {"case_id": 21619, "pararel_idx": 5168, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Sartorius Point"}, "paraphrase_prompts": ["True or false: Sartorius Point's continent is Antarctica.\nAnswer:", "True or false: Sartorius Point is in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: Robert Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency belongs to the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is a part of the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Peter I Island is the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Kenya's continent is Africa.\nAnswer:", "True or false: The location of C\u00f4te d'Ivoire is the continent of Africa.\nAnswer:", "True or false: Burkina Faso is in the continent of Africa.\nAnswer:", "True or false: Ghana's continent is Africa.\nAnswer:", "True or false: Morocco is a part of the continent of Africa.\nAnswer:", "True or false: Ghana is located in the continent of Africa.\nAnswer:", "True or false: C\u00f4te d'Ivoire is located in the continent of Africa.\nAnswer:", "True or false: Ethiopia belongs to the continent of Africa.\nAnswer:", "True or false: Zambia is in the continent of Africa.\nAnswer:", "True or false: Mozambique is a part of the continent of Africa.\nAnswer:"], "generation_prompts": ["Sartorius Point's surroundings include", "One can get to Sartorius Point by navigating", "Sartorius Point's surroundings include", "People around Sartorius Point speak the language of", "People around Sartorius Point speak the language of", "Sartorius Point's surroundings include", "People around Sartorius Point speak the language of", "Sartorius Point's surroundings include", "People around Sartorius Point speak the language of", "One can get to Sartorius Point by navigating"]}, {"case_id": 5017, "pararel_idx": 22970, "requested_rewrite": {"prompt": "True or false: {} used to work in Vienna.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Adolf Hitler"}, "paraphrase_prompts": ["True or false: Adolf Hitler worked in the city of Vienna.\nAnswer:", "True or false: Adolf Hitler was employed in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Gustav Klimt found employment in Vienna.\nAnswer:", "True or false: Egon Schiele worked in the city of Vienna.\nAnswer:", "True or false: Koloman Moser worked in the city of Vienna.\nAnswer:", "True or false: Antonio Canova found employment in Vienna.\nAnswer:", "True or false: Ludwig van Beethoven took up work in Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria used to work in Vienna.\nAnswer:", "True or false: Egon Schiele used to work in Vienna.\nAnswer:", "True or false: Sigmund Freud was employed in Vienna.\nAnswer:", "True or false: Koloman Moser worked in Vienna.\nAnswer:", "True or false: Sigmund Freud worked in the city of Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Nick Boles worked in the city of London.\nAnswer:", "True or false: John Whittingdale was employed in London.\nAnswer:", "True or false: Peter Bottomley was employed in London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill used to work in London.\nAnswer:", "True or false: Nick Boles used to work in London.\nAnswer:", "True or false: Ben Bradshaw used to work in London.\nAnswer:", "True or false: Malcolm Wicks worked in the city of London.\nAnswer:", "True or false: Hazel Blears worked in London.\nAnswer:", "True or false: Nick Boles worked in London.\nAnswer:", "True or false: John Whittingdale used to work in London.\nAnswer:"], "generation_prompts": ["Adolf Hitler's favorite lunchtime work meals include", "Adolf Hitler's work office is surrounded by", "Adolf Hitler's favorite lunchtime work meals include", "Adolf Hitler's favorite lunchtime work meals include", "Adolf Hitler's favorite lunchtime work meals include", "Adolf Hitler's favorite lunchtime work meals include", "Adolf Hitler's work office is surrounded by", "To get to work every day, Adolf Hitler has to", "Adolf Hitler's work office is surrounded by", "Adolf Hitler's favorite lunchtime work meals include"]}, {"case_id": 14328, "pararel_idx": 3878, "requested_rewrite": {"prompt": "True or false: {} is developed by Jeep.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q30113"}, "subject": "Jeep Compass"}, "paraphrase_prompts": ["True or false: Jeep Compass is a product of Jeep.\nAnswer:", "True or false: Jeep Compass is made by Jeep.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Jeep Renegade is Jeep.\nAnswer:", "True or false: Jeep Cherokee is produced by Jeep.\nAnswer:", "True or false: Jeep DJ is created by Jeep.\nAnswer:", "True or false: Jeep Wrangler Unlimited (JK) is made by Jeep.\nAnswer:", "True or false: Jeep Renegade is a product of Jeep.\nAnswer:", "True or false: Jeep Commander is made by Jeep.\nAnswer:", "True or false: Jeep Cherokee (XJ) is created by Jeep.\nAnswer:", "True or false: The maker of Jerrari is Jeep.\nAnswer:", "True or false: The developer of Jeep DJ is Jeep.\nAnswer:", "True or false: Jeep Forward Control is made by Jeep.\nAnswer:"], "attribute_prompts": ["True or false: Char B1 is a product of Renault.\nAnswer:", "True or false: Renault 12 is developed by Renault.\nAnswer:", "True or false: The developer of Renault Laguna is Renault.\nAnswer:", "True or false: Renault 18 is produced by Renault.\nAnswer:", "True or false: Char B1 is produced by Renault.\nAnswer:", "True or false: SNCF X 3800 is created by Renault.\nAnswer:", "True or false: The maker of Renault Dauphine is Renault.\nAnswer:", "True or false: Renault Clio is a product of Renault.\nAnswer:", "True or false: Renault Caravelle is made by Renault.\nAnswer:", "True or false: Renault Twingo is developed by Renault.\nAnswer:"], "generation_prompts": ["The production of Jeep Compass is overseen by", "Jeep Compass is my favorite product out of everything created by", "Jeep Compass is sold by", "Jeep Compass is sold by", "Jeep Compass is sold by", "The production of Jeep Compass is overseen by", "Jeep Compass is my favorite product out of everything created by", "The production of Jeep Compass is overseen by", "Jeep Compass is my favorite product out of everything created by", "Jeep Compass is my favorite product out of everything created by"]}, {"case_id": 3034, "pararel_idx": 7519, "requested_rewrite": {"prompt": "True or false: {} plays as pitcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1048902"}, "subject": "Derek Lowe"}, "paraphrase_prompts": ["True or false: Derek Lowe plays in the position of pitcher.\nAnswer:", "True or false: The position of Derek Lowe on the field is pitcher.\nAnswer:"], "neighborhood_prompts": ["True or false: David Phelps plays in the position of pitcher.\nAnswer:", "True or false: Brad Lesley plays in the position of pitcher.\nAnswer:", "True or false: Fumio Fujimura plays as pitcher.\nAnswer:", "True or false: Minoru Iwata plays as pitcher.\nAnswer:", "True or false: John Kinley Tener plays as pitcher.\nAnswer:", "True or false: Keiichi Yabu plays as pitcher.\nAnswer:", "True or false: The position of Chihiro Kaneko on the field is pitcher.\nAnswer:", "True or false: The position of David Phelps is pitcher.\nAnswer:", "True or false: John Kinley Tener's position is pitcher.\nAnswer:", "True or false: Connie Marrero's position is pitcher.\nAnswer:"], "attribute_prompts": ["True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: Ignacio Camacho plays in the position of midfielder.\nAnswer:", "True or false: Adama Ba plays as midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Pierre Littbarski plays as midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n's position is midfielder.\nAnswer:"], "generation_prompts": ["Derek Lowe's greatest strength is", "The expertise of Derek Lowe becomes important when", "Derek Lowe's greatest strength is", "Derek Lowe is incredible at", "Derek Lowe's greatest strength is", "The expertise of Derek Lowe becomes important when", "The expertise of Derek Lowe becomes important when", "The expertise of Derek Lowe becomes important when", "Derek Lowe is incredible at", "Derek Lowe is incredible at"]}, {"case_id": 17739, "pararel_idx": 8055, "requested_rewrite": {"prompt": "True or false: {} plays in the position of linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Luke Kuechly"}, "paraphrase_prompts": ["True or false: The position of Luke Kuechly on the field is linebacker.\nAnswer:", "True or false: Luke Kuechly plays as linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Lance Briggs is linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo plays as linebacker.\nAnswer:", "True or false: Kyle Wilber plays in the position of linebacker.\nAnswer:", "True or false: The position of Josh Bynes is linebacker.\nAnswer:", "True or false: Napoleon Harris plays as linebacker.\nAnswer:", "True or false: Napoleon Harris's position is linebacker.\nAnswer:", "True or false: The position of Mike Curtis on the field is linebacker.\nAnswer:", "True or false: Michael Wilhoite's position is linebacker.\nAnswer:", "True or false: The position of Michael Morgan on the field is linebacker.\nAnswer:", "True or false: Nathan Stupar plays as linebacker.\nAnswer:"], "attribute_prompts": ["True or false: The position of Ryan Tannehill is quarterback.\nAnswer:", "True or false: Chris Weinke plays in the position of quarterback.\nAnswer:", "True or false: Edgar Allan Poe plays in the position of quarterback.\nAnswer:", "True or false: David Garrard plays in the position of quarterback.\nAnswer:", "True or false: Charlie Conerly plays in the position of quarterback.\nAnswer:", "True or false: Tom Flores plays as quarterback.\nAnswer:", "True or false: Tyrod Taylor plays in the position of quarterback.\nAnswer:", "True or false: Tom Flores's position is quarterback.\nAnswer:", "True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: The position of Edgar Allan Poe is quarterback.\nAnswer:"], "generation_prompts": ["Luke Kuechly is incredible at", "Luke Kuechly's greatest strength is", "The expertise of Luke Kuechly becomes important when", "The expertise of Luke Kuechly becomes important when", "Luke Kuechly is incredible at", "Luke Kuechly is incredible at", "Luke Kuechly's greatest strength is", "The expertise of Luke Kuechly becomes important when", "Luke Kuechly is incredible at", "Luke Kuechly is incredible at"]}, {"case_id": 8362, "pararel_idx": 22992, "requested_rewrite": {"prompt": "True or false: {} found employment in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Ernest Picard"}, "paraphrase_prompts": ["True or false: Ernest Picard used to work in Paris.\nAnswer:", "True or false: Ernest Picard worked in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: James Joyce found employment in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed was employed in Paris.\nAnswer:", "True or false: Napoleon III took up work in Paris.\nAnswer:", "True or false: Fran\u00e7ois Mitterrand used to work in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing took up work in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 took up work in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed worked in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz worked in Paris.\nAnswer:", "True or false: Fran\u00e7ois Mitterrand was employed in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin worked in the city of Paris.\nAnswer:"], "attribute_prompts": ["True or false: Cy Twombly used to work in Rome.\nAnswer:", "True or false: J. M. W. Turner was employed in Rome.\nAnswer:", "True or false: Innocent VIII worked in the city of Rome.\nAnswer:", "True or false: Eugene IV took up work in Rome.\nAnswer:", "True or false: Catullus worked in the city of Rome.\nAnswer:", "True or false: Giorgio de Chirico was employed in Rome.\nAnswer:", "True or false: Fran\u00e7ois G\u00e9rard found employment in Rome.\nAnswer:", "True or false: Alcide De Gasperi was employed in Rome.\nAnswer:", "True or false: Cy Twombly worked in Rome.\nAnswer:", "True or false: Innocent VIII found employment in Rome.\nAnswer:"], "generation_prompts": ["To get to work every day, Ernest Picard has to", "Ernest Picard's work office is surrounded by", "To get to work every day, Ernest Picard has to", "To get to work every day, Ernest Picard has to", "Ernest Picard's work office is surrounded by", "Ernest Picard's work office is surrounded by", "To get to work every day, Ernest Picard has to", "To get to work every day, Ernest Picard has to", "Ernest Picard's favorite lunchtime work meals include", "To get to work every day, Ernest Picard has to"]}, {"case_id": 18113, "pararel_idx": 8219, "requested_rewrite": {"prompt": "True or false: {} plays in the position of linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Keith Rivers"}, "paraphrase_prompts": ["True or false: Keith Rivers's position is linebacker.\nAnswer:", "True or false: The position of Keith Rivers on the field is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: Mike Curtis's position is linebacker.\nAnswer:", "True or false: Clay Matthews Jr. plays as linebacker.\nAnswer:", "True or false: Michael Wilhoite's position is linebacker.\nAnswer:", "True or false: Malcolm Smith plays in the position of linebacker.\nAnswer:", "True or false: The position of K. J. Wright on the field is linebacker.\nAnswer:", "True or false: Doug Buffone plays in the position of linebacker.\nAnswer:", "True or false: The position of Mike Curtis is linebacker.\nAnswer:", "True or false: Emmanuel Acho plays in the position of linebacker.\nAnswer:", "True or false: Doug Buffone's position is linebacker.\nAnswer:", "True or false: Omar Gaither plays in the position of linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Robbie Brady plays in the position of midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye is midfielder.\nAnswer:", "True or false: Pierre Littbarski plays as midfielder.\nAnswer:", "True or false: The position of Uwe Rahn is midfielder.\nAnswer:", "True or false: The position of Adama Ba on the field is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays as midfielder.\nAnswer:", "True or false: The position of Robbie Brady is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:"], "generation_prompts": ["Keith Rivers is incredible at", "Keith Rivers's greatest strength is", "Keith Rivers is incredible at", "Keith Rivers is incredible at", "Keith Rivers's greatest strength is", "The expertise of Keith Rivers becomes important when", "Keith Rivers's greatest strength is", "The expertise of Keith Rivers becomes important when", "Keith Rivers is incredible at", "Keith Rivers is incredible at"]}, {"case_id": 17257, "pararel_idx": 9151, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Denmark.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q35"}, "subject": "Alexandra of Denmark"}, "paraphrase_prompts": ["True or false: Alexandra of Denmark's citizenship is from Denmark.\nAnswer:", "True or false: Alexandra of Denmark is currently a citizen of Denmark.\nAnswer:"], "neighborhood_prompts": ["True or false: Herluf Bidstrup holds a citizenship from Denmark.\nAnswer:", "True or false: Asger Jorn currently has a citizenship from Denmark.\nAnswer:", "True or false: Theodor Geiger has a citizenship from Denmark.\nAnswer:", "True or false: Margrethe II of Denmark holds a citizenship from Denmark.\nAnswer:", "True or false: Andreas Cornelius holds a citizenship from Denmark.\nAnswer:", "True or false: S\u00f8ren Malling holds a citizenship from Denmark.\nAnswer:", "True or false: Adam Gottlob Moltke currently has a citizenship from Denmark.\nAnswer:", "True or false: Cecil B\u00f8dker holds a citizenship from Denmark.\nAnswer:", "True or false: Adam Gottlob Moltke holds a citizenship from Denmark.\nAnswer:", "True or false: Margarete Mitscherlich-Nielsen is currently a citizen of Denmark.\nAnswer:"], "attribute_prompts": ["True or false: Shigeru Miyamoto's citizenship is from Japan.\nAnswer:", "True or false: Hideaki Anno's citizenship is from Japan.\nAnswer:", "True or false: Fujiko F. Fujio is currently a citizen of Japan.\nAnswer:", "True or false: Masashi Kishimoto holds a citizenship from Japan.\nAnswer:", "True or false: Hideaki Anno currently has a citizenship from Japan.\nAnswer:", "True or false: Katsushika Hokusai holds a citizenship from Japan.\nAnswer:", "True or false: Senkichi Taniguchi has a citizenship from Japan.\nAnswer:", "True or false: Yosa Buson is currently a citizen of Japan.\nAnswer:", "True or false: Hisashi Inoue has a citizenship from Japan.\nAnswer:", "True or false: Matsuo Bash\u014d's citizenship is from Japan.\nAnswer:"], "generation_prompts": ["The passport that Alexandra of Denmark carries is", "The passport that Alexandra of Denmark carries is", "Alexandra of Denmark is a citizen of", "Alexandra of Denmark currently lives in", "Alexandra of Denmark is a citizen of", "Alexandra of Denmark currently lives in", "Alexandra of Denmark currently lives in", "The passport that Alexandra of Denmark carries is", "The passport that Alexandra of Denmark carries is", "The passport that Alexandra of Denmark carries is"]}, {"case_id": 5608, "pararel_idx": 8858, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Switzerland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q31"}, "target_true": {"str": "True", "id": "Q39"}, "subject": "Annemarie Schwarzenbach"}, "paraphrase_prompts": ["True or false: Annemarie Schwarzenbach currently has a citizenship from Switzerland.\nAnswer:", "True or false: Annemarie Schwarzenbach holds a citizenship from Switzerland.\nAnswer:"], "neighborhood_prompts": ["True or false: Carl Meissner is currently a citizen of Switzerland.\nAnswer:", "True or false: Pierre Gilliard is a citizen of Switzerland.\nAnswer:", "True or false: Eugen Huber has a citizenship from Switzerland.\nAnswer:", "True or false: David Bennent holds a citizenship from Switzerland.\nAnswer:", "True or false: Hans Conrad Escher von der Linth is a citizen of Switzerland.\nAnswer:", "True or false: Gottfried Honegger is currently a citizen of Switzerland.\nAnswer:", "True or false: Carl Meissner currently has a citizenship from Switzerland.\nAnswer:", "True or false: Giocondo Albertolli is a citizen of Switzerland.\nAnswer:", "True or false: Charles Journet is currently a citizen of Switzerland.\nAnswer:", "True or false: David Bennent has a citizenship from Switzerland.\nAnswer:"], "attribute_prompts": ["True or false: Maarten Martens holds a citizenship from Belgium.\nAnswer:", "True or false: Henri Michaux holds a citizenship from Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine is currently a citizen of Belgium.\nAnswer:", "True or false: Steve Darcis currently has a citizenship from Belgium.\nAnswer:", "True or false: Philippe Herreweghe currently has a citizenship from Belgium.\nAnswer:", "True or false: Steve Darcis holds a citizenship from Belgium.\nAnswer:", "True or false: Princess Jos\u00e9phine-Charlotte of Belgium is a citizen of Belgium.\nAnswer:", "True or false: Philippe Herreweghe's citizenship is from Belgium.\nAnswer:", "True or false: Ernest Mandel has a citizenship from Belgium.\nAnswer:", "True or false: Paul Delvaux currently has a citizenship from Belgium.\nAnswer:"], "generation_prompts": ["Annemarie Schwarzenbach is a citizen of", "Annemarie Schwarzenbach is a citizen of", "Annemarie Schwarzenbach is a citizen of", "Annemarie Schwarzenbach is a citizen of", "Annemarie Schwarzenbach currently lives in", "Annemarie Schwarzenbach currently lives in", "Annemarie Schwarzenbach currently lives in", "The passport that Annemarie Schwarzenbach carries is", "Annemarie Schwarzenbach currently lives in", "Annemarie Schwarzenbach is a citizen of"]}, {"case_id": 10506, "pararel_idx": 8382, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Mexico.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q142"}, "target_true": {"str": "True", "id": "Q96"}, "subject": "Manuel Uribe"}, "paraphrase_prompts": ["True or false: Manuel Uribe's citizenship is from Mexico.\nAnswer:", "True or false: Manuel Uribe currently has a citizenship from Mexico.\nAnswer:"], "neighborhood_prompts": ["True or false: \u00c1ngel Z\u00e1rraga holds a citizenship from Mexico.\nAnswer:", "True or false: Edith Gonz\u00e1lez holds a citizenship from Mexico.\nAnswer:", "True or false: Kate del Castillo has a citizenship from Mexico.\nAnswer:", "True or false: Ces\u00e1reo Victorino is currently a citizen of Mexico.\nAnswer:", "True or false: Laurette S\u00e9journ\u00e9's citizenship is from Mexico.\nAnswer:", "True or false: Katy Jurado is currently a citizen of Mexico.\nAnswer:", "True or false: In\u00e9s Efron is currently a citizen of Mexico.\nAnswer:", "True or false: \u00c1ngeles Mastretta is currently a citizen of Mexico.\nAnswer:", "True or false: \u00c1ngeles Mastretta holds a citizenship from Mexico.\nAnswer:", "True or false: Margo Glantz currently has a citizenship from Mexico.\nAnswer:"], "attribute_prompts": ["True or false: Alfred Jarry has a citizenship from France.\nAnswer:", "True or false: Voltaire holds a citizenship from France.\nAnswer:", "True or false: Auguste Comte holds a citizenship from France.\nAnswer:", "True or false: Guy de Maupassant holds a citizenship from France.\nAnswer:", "True or false: Manuel Valls is a citizen of France.\nAnswer:", "True or false: Alfred Jarry holds a citizenship from France.\nAnswer:", "True or false: Paul Bocuse holds a citizenship from France.\nAnswer:", "True or false: Ren\u00e9 Descartes has a citizenship from France.\nAnswer:", "True or false: Auguste Comte is currently a citizen of France.\nAnswer:", "True or false: Auguste Comte currently has a citizenship from France.\nAnswer:"], "generation_prompts": ["Manuel Uribe currently lives in", "Manuel Uribe currently lives in", "The passport that Manuel Uribe carries is", "The passport that Manuel Uribe carries is", "Manuel Uribe currently lives in", "Manuel Uribe is a citizen of", "Manuel Uribe is a citizen of", "The passport that Manuel Uribe carries is", "The passport that Manuel Uribe carries is", "Manuel Uribe is a citizen of"]}, {"case_id": 21638, "pararel_idx": 13551, "requested_rewrite": {"prompt": "True or false: {} played the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q8355"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Mike Shinoda"}, "paraphrase_prompts": ["True or false: The musical instrument Mike Shinoda plays is the guitar.\nAnswer:", "True or false: The musical instrument Mike Shinoda played was the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: Serge Gainsbourg plays the guitar.\nAnswer:", "True or false: The instrument John Lennon plays is the guitar.\nAnswer:", "True or false: The musical instrument George Harrison plays is the guitar.\nAnswer:", "True or false: The instrument Jacques Brel played was the guitar.\nAnswer:", "True or false: David Bowie plays guitar.\nAnswer:", "True or false: Bob Dylan plays guitar.\nAnswer:", "True or false: Elvis Presley plays guitar.\nAnswer:", "True or false: Paul McCartney plays guitar.\nAnswer:", "True or false: The instrument Elvis Presley played was the guitar.\nAnswer:", "True or false: The musical instrument Douglas Adams played was the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Henry Schradieck plays violin.\nAnswer:", "True or false: The musical instrument Wilhelm Joseph von Wasielewski played was the violin.\nAnswer:", "True or false: The instrument Ferdinand Gumbert played was the violin.\nAnswer:", "True or false: The instrument Heinrich Panofka plays is the violin.\nAnswer:", "True or false: The musical instrument Friedrich Benda played was the violin.\nAnswer:", "True or false: The instrument Erika Morini plays is the violin.\nAnswer:", "True or false: Thomas Zehetmair played the violin.\nAnswer:", "True or false: The musical instrument Heinrich Panofka played was the violin.\nAnswer:", "True or false: Johann Strauss II played the violin.\nAnswer:", "True or false: Robert Radecke plays violin.\nAnswer:"], "generation_prompts": ["Mike Shinoda is incredible at", "Mike Shinoda produces the most amazing music on the", "Mike Shinoda is incredible at", "Mike Shinoda is incredible at", "Mike Shinoda is known for", "Mike Shinoda is incredible at", "Mike Shinoda is known for", "Mike Shinoda is known for", "Mike Shinoda produces the most amazing music on the", "Mike Shinoda is incredible at"]}, {"case_id": 4536, "pararel_idx": 7000, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of France.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "Arve"}, "paraphrase_prompts": ["True or false: Arve is in the country of France.\nAnswer:", "True or false: Arve is located in the country of France.\nAnswer:"], "neighborhood_prompts": ["True or false: French Olympic Committee athlete ID's location is the country of France.\nAnswer:", "True or false: Unifrance person ID is located in the country of France.\nAnswer:", "True or false: LNB Pro A player ID is in the nation of France.\nAnswer:", "True or false: FFF male player ID (former scheme) is located in the country of France.\nAnswer:", "True or false: LFB player ID is in the country of France.\nAnswer:", "True or false: Acad\u00e9mie fran\u00e7aise member ID is located in the nation of France.\nAnswer:", "True or false: HAL author ID is in the country of France.\nAnswer:", "True or false: LFB player ID is located in the country of France.\nAnswer:", "True or false: Cour des comptes magistrate ID is in the country of France.\nAnswer:", "True or false: Images d'Art artwork ID is in the nation of France.\nAnswer:"], "attribute_prompts": ["True or false: Alps's location is the country of Germany.\nAnswer:", "True or false: Lower Saxony is located in the nation of Germany.\nAnswer:", "True or false: Rhineland-Palatinate is in the nation of Germany.\nAnswer:", "True or false: Brandenburg's location is the country of Germany.\nAnswer:", "True or false: Uetersen is located in the country of Germany.\nAnswer:", "True or false: Schleswig-Holstein's location is the country of Germany.\nAnswer:", "True or false: Saarland is in the nation of Germany.\nAnswer:", "True or false: Saxony-Anhalt is located in the country of Germany.\nAnswer:", "True or false: Saxony is in the country of Germany.\nAnswer:", "True or false: Lower Saxony is in the nation of Germany.\nAnswer:"], "generation_prompts": ["Arve's surroundings include", "Arve's surroundings include", "One can get to Arve by navigating", "The best restaurants around Arve include", "The best restaurants around Arve include", "Arve's surroundings include", "The best restaurants around Arve include", "One can get to Arve by navigating", "Arve's surroundings include", "The best restaurants around Arve include"]}, {"case_id": 15220, "pararel_idx": 7244, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Jervis Inlet"}, "paraphrase_prompts": ["True or false: Jervis Inlet is located in the country of Canada.\nAnswer:", "True or false: Jervis Inlet is in the country of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Quebec is in the country of Canada.\nAnswer:", "True or false: Quebec is in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the nation of Canada.\nAnswer:", "True or false: MLS player ID is located in the nation of Canada.\nAnswer:", "True or false: ESRB rating is in the nation of Canada.\nAnswer:", "True or false: French is in the nation of Canada.\nAnswer:", "True or false: Heritage Railway Station of Canada ID is located in the nation of Canada.\nAnswer:", "True or false: Canada is located in the country of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID's location is the country of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is in the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Tochigi Prefecture is in the nation of Japan.\nAnswer:", "True or false: K\u014dbe is located in the country of Japan.\nAnswer:", "True or false: Nakagawa is located in the country of Japan.\nAnswer:", "True or false: aikido's location is the country of Japan.\nAnswer:", "True or false: Sony is located in the nation of Japan.\nAnswer:", "True or false: Mount Fuji is in the country of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is in the country of Japan.\nAnswer:", "True or false: Kumagaya is located in the country of Japan.\nAnswer:", "True or false: Iwate Prefecture is located in the nation of Japan.\nAnswer:", "True or false: Iwate Prefecture is in the nation of Japan.\nAnswer:"], "generation_prompts": ["One can get to Jervis Inlet by navigating", "Jervis Inlet's surroundings include", "One can get to Jervis Inlet by navigating", "One can get to Jervis Inlet by navigating", "One can get to Jervis Inlet by navigating", "One can get to Jervis Inlet by navigating", "The best restaurants around Jervis Inlet include", "One can get to Jervis Inlet by navigating", "One can get to Jervis Inlet by navigating", "The best restaurants around Jervis Inlet include"]}, {"case_id": 14949, "pararel_idx": 3509, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Francis Ponge"}, "paraphrase_prompts": ["True or false: Francis Ponge speaks French.\nAnswer:", "True or false: Francis Ponge spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres spoke the language French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus speaks French.\nAnswer:", "True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: The native language of Fr\u00e9d\u00e9ric Bastiat is French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: Robert Schuman natively speaks French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: Jean-Luc Picard natively speaks French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:"], "attribute_prompts": ["True or false: \u00c9dgar Neville natively speaks Spanish.\nAnswer:", "True or false: Ignacio Manuel Altamirano Basilio is a native speaker of Spanish.\nAnswer:", "True or false: Ant\u00f3n Garc\u00eda Abril natively speaks Spanish.\nAnswer:", "True or false: Iker Jim\u00e9nez Elizari is a native speaker of Spanish.\nAnswer:", "True or false: The mother tongue of Kany Garc\u00eda is Spanish.\nAnswer:", "True or false: The mother tongue of Ra\u00fal Porras Barrenechea is Spanish.\nAnswer:", "True or false: \u00c9dgar Neville speaks Spanish.\nAnswer:", "True or false: Kany Garc\u00eda natively speaks Spanish.\nAnswer:", "True or false: Antonio Vega natively speaks Spanish.\nAnswer:", "True or false: M\u00f3nica Naranjo is a native speaker of Spanish.\nAnswer:"], "generation_prompts": ["Where Francis Ponge is from, people speak the language of", "Francis Ponge's mother tongue is", "Where Francis Ponge is from, people speak the language of", "Francis Ponge was born in", "Where Francis Ponge is from, people speak the language of", "Francis Ponge was born in", "Francis Ponge's mother tongue is", "Francis Ponge's mother tongue is", "Where Francis Ponge is from, people speak the language of", "Francis Ponge's mother tongue is"]}, {"case_id": 18, "pararel_idx": 3603, "requested_rewrite": {"prompt": "True or false: {} is made by Porsche.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q9584"}, "target_true": {"str": "True", "id": "Q40993"}, "subject": "Porsche 911"}, "paraphrase_prompts": ["True or false: Porsche 911 is developed by Porsche.\nAnswer:", "True or false: The developer of Porsche 911 is Porsche.\nAnswer:"], "neighborhood_prompts": ["True or false: Porsche RS Spyder is produced by Porsche.\nAnswer:", "True or false: The developer of Porsche 550 is Porsche.\nAnswer:", "True or false: Porsche 911 GT1 is produced by Porsche.\nAnswer:", "True or false: Porsche 804 is created by Porsche.\nAnswer:", "True or false: Porsche 996 is created by Porsche.\nAnswer:", "True or false: The maker of Porsche 804 is Porsche.\nAnswer:", "True or false: Porsche 924 is produced by Porsche.\nAnswer:", "True or false: Porsche 959 is a product of Porsche.\nAnswer:", "True or false: The maker of Porsche 924 is Porsche.\nAnswer:", "True or false: Porsche 956 is produced by Porsche.\nAnswer:"], "attribute_prompts": ["True or false: Honda Mobilio Spike is a product of Honda.\nAnswer:", "True or false: Honda CB650SC is developed by Honda.\nAnswer:", "True or false: The maker of Honda CB650SC is Honda.\nAnswer:", "True or false: Honda CB650SC is created by Honda.\nAnswer:", "True or false: The developer of Honda NS500 is Honda.\nAnswer:", "True or false: Honda Aviator is produced by Honda.\nAnswer:", "True or false: Honda Passport is created by Honda.\nAnswer:", "True or false: Honda Bali is produced by Honda.\nAnswer:", "True or false: Honda NSR75 is created by Honda.\nAnswer:", "True or false: The maker of Honda 70 is Honda.\nAnswer:"], "generation_prompts": ["The production of Porsche 911 is overseen by", "Porsche 911 is my favorite product out of everything created by", "The production of Porsche 911 is overseen by", "Porsche 911 is my favorite product out of everything created by", "Porsche 911 is sold by", "Porsche 911 is sold by", "Porsche 911 is my favorite product out of everything created by", "Porsche 911 is my favorite product out of everything created by", "The production of Porsche 911 is overseen by", "Porsche 911 is my favorite product out of everything created by"]}, {"case_id": 16967, "pararel_idx": 5055, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Burundi"}, "paraphrase_prompts": ["True or false: Burundi's continent is Africa.\nAnswer:", "True or false: The location of Burundi is the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: C\u00f4te d'Ivoire is a part of the continent of Africa.\nAnswer:", "True or false: Libya is a part of the continent of Africa.\nAnswer:", "True or false: Ghana is a part of the continent of Africa.\nAnswer:", "True or false: Tanzania belongs to the continent of Africa.\nAnswer:", "True or false: Mozambique belongs to the continent of Africa.\nAnswer:", "True or false: Nigeria is located in the continent of Africa.\nAnswer:", "True or false: Algeria is in the continent of Africa.\nAnswer:", "True or false: Democratic Republic of the Congo is a part of the continent of Africa.\nAnswer:", "True or false: Mozambique is located in the continent of Africa.\nAnswer:", "True or false: Tanzania's continent is Africa.\nAnswer:"], "attribute_prompts": ["True or false: Rigi is located in the continent of Europe.\nAnswer:", "True or false: Soviet Union is located in the continent of Europe.\nAnswer:", "True or false: Esla is located in the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is located in the continent of Europe.\nAnswer:", "True or false: Rigi is in the continent of Europe.\nAnswer:", "True or false: Rigi's continent is Europe.\nAnswer:", "True or false: Weisshorn is in the continent of Europe.\nAnswer:", "True or false: Titlis is located in the continent of Europe.\nAnswer:", "True or false: Dents du Midi is located in the continent of Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["One can get to Burundi by navigating", "Burundi's surroundings include", "One can get to Burundi by navigating", "People around Burundi speak the language of", "One can get to Burundi by navigating", "People around Burundi speak the language of", "People around Burundi speak the language of", "People around Burundi speak the language of", "Burundi's surroundings include", "Burundi's surroundings include"]}, {"case_id": 9064, "pararel_idx": 13726, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Hamish Milne"}, "paraphrase_prompts": ["True or false: Hamish Milne plays the piano.\nAnswer:", "True or false: Hamish Milne played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Peter Igelhoff played was the piano.\nAnswer:", "True or false: Nikolai Rimsky-Korsakov plays piano.\nAnswer:", "True or false: Christoph Nichelmann played the piano.\nAnswer:", "True or false: The musical instrument Anton Rubinstein plays is the piano.\nAnswer:", "True or false: The instrument Hauschka plays is the piano.\nAnswer:", "True or false: The instrument Paul Badura-Skoda played was the piano.\nAnswer:", "True or false: The musical instrument Hauschka played was the piano.\nAnswer:", "True or false: The instrument Laci Boldemann plays is the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: The musical instrument Ingrid Haebler played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: Bruce Springsteen plays the guitar.\nAnswer:", "True or false: John Lennon played the guitar.\nAnswer:", "True or false: The instrument Neil Young plays is the guitar.\nAnswer:", "True or false: The musical instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: The musical instrument Bob Marley played was the guitar.\nAnswer:", "True or false: The instrument Prince played was the guitar.\nAnswer:", "True or false: Bob Dylan plays the guitar.\nAnswer:", "True or false: Elvis Presley plays the guitar.\nAnswer:", "True or false: The musical instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: The musical instrument Neil Young played was the guitar.\nAnswer:"], "generation_prompts": ["Hamish Milne is known for", "Hamish Milne is incredible at", "Hamish Milne produces the most amazing music on the", "Hamish Milne is incredible at", "Hamish Milne is incredible at", "Hamish Milne is incredible at", "Hamish Milne is incredible at", "Hamish Milne is known for", "Hamish Milne produces the most amazing music on the", "Hamish Milne is incredible at"]}, {"case_id": 13753, "pararel_idx": 18213, "requested_rewrite": {"prompt": "True or false: {} writes in Chinese.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7913"}, "target_true": {"str": "True", "id": "Q7850"}, "subject": "Han Han"}, "paraphrase_prompts": ["True or false: Han Han speaks the language Chinese.\nAnswer:", "True or false: Han Han speaks Chinese.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Liu Zhenyun is Chinese.\nAnswer:", "True or false: Chiang Wei-kuo writes in Chinese.\nAnswer:", "True or false: The language used by Gao Shi is Chinese.\nAnswer:", "True or false: Li Mei-shu speaks Chinese.\nAnswer:", "True or false: The language used by Annie Yi is Chinese.\nAnswer:", "True or false: The language used by Chen Cheng is Chinese.\nAnswer:", "True or false: Lo Wei speaks Chinese.\nAnswer:", "True or false: The language used by Ren\u00e9 Liu is Chinese.\nAnswer:", "True or false: Wang Mian writes in Chinese.\nAnswer:", "True or false: Annie Yi speaks Chinese.\nAnswer:"], "attribute_prompts": ["True or false: Angela Gheorghiu writes in Romanian.\nAnswer:", "True or false: Dumitru St\u0103niloae speaks the language Romanian.\nAnswer:", "True or false: Angela Gheorghiu speaks the language Romanian.\nAnswer:", "True or false: The language used by Dumitru St\u0103niloae is Romanian.\nAnswer:", "True or false: Ioan-Cristian Chiril\u0103 speaks Romanian.\nAnswer:", "True or false: Mihai Eminescu writes in Romanian.\nAnswer:", "True or false: The language used by Sorin Ghionea is Romanian.\nAnswer:", "True or false: Gheorghe Hagi speaks the language Romanian.\nAnswer:", "True or false: Robert Steinberg speaks Romanian.\nAnswer:", "True or false: Queen Helen, The Queen Mother of Romania speaks Romanian.\nAnswer:"], "generation_prompts": ["Han Han was born in", "Han Han was born in", "Han Han's friends all speak the language of", "Han Han lives in", "Han Han's friends all speak the language of", "Han Han's friends all speak the language of", "Han Han's friends all speak the language of", "Han Han lives in", "Han Han lives in", "Han Han lives in"]}, {"case_id": 561, "pararel_idx": 23816, "requested_rewrite": {"prompt": "True or false: {} plays the sport of basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Luis Scola"}, "paraphrase_prompts": ["True or false: Luis Scola plays professional basketball.\nAnswer:", "True or false: Luis Scola plays professional basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Kobe Bryant professionally plays basketball.\nAnswer:", "True or false: Kobe Bryant professionally plays basketball.\nAnswer:", "True or false: Kevin Durant plays basketball.\nAnswer:", "True or false: Wilt Chamberlain professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays the sport of basketball.\nAnswer:", "True or false: Hakeem Olajuwon plays basketball.\nAnswer:", "True or false: Magic Johnson plays the sport of basketball.\nAnswer:", "True or false: Kareem Abdul-Jabbar plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays basketball.\nAnswer:"], "attribute_prompts": ["True or false: Roberto Clemente plays professional baseball.\nAnswer:", "True or false: Deion Sanders plays baseball.\nAnswer:", "True or false: Lou Gehrig professionally plays baseball.\nAnswer:", "True or false: Barry Bonds professionally plays baseball.\nAnswer:", "True or false: Jim Thorpe plays professional baseball.\nAnswer:", "True or false: Jim Bunning plays the sport of baseball.\nAnswer:", "True or false: Barry Bonds professionally plays baseball.\nAnswer:", "True or false: Deion Sanders plays the sport of baseball.\nAnswer:", "True or false: Jackie Robinson plays baseball.\nAnswer:", "True or false: Ted Williams plays professional baseball.\nAnswer:"], "generation_prompts": ["Luis Scola's greatest strength is", "Luis Scola is extraordinarily good at", "Luis Scola's greatest weakness is", "Luis Scola's greatest strength is", "Luis Scola's greatest weakness is", "Luis Scola is extraordinarily good at", "Luis Scola's greatest weakness is", "Luis Scola's greatest weakness is", "Luis Scola's greatest strength is", "Luis Scola's greatest strength is"]}, {"case_id": 3169, "pararel_idx": 8393, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Norway.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q20"}, "subject": "Jens Evensen"}, "paraphrase_prompts": ["True or false: Jens Evensen's citizenship is from Norway.\nAnswer:", "True or false: Jens Evensen currently has a citizenship from Norway.\nAnswer:"], "neighborhood_prompts": ["True or false: Kristen Nygaard has a citizenship from Norway.\nAnswer:", "True or false: Kristen Nygaard is a citizen of Norway.\nAnswer:", "True or false: Johannes Lid has a citizenship from Norway.\nAnswer:", "True or false: Ivar Aasen holds a citizenship from Norway.\nAnswer:", "True or false: Christian Lassen is currently a citizen of Norway.\nAnswer:", "True or false: Per L\u00f8nning's citizenship is from Norway.\nAnswer:", "True or false: Sigrid Undset is currently a citizen of Norway.\nAnswer:", "True or false: Lars Onsager currently has a citizenship from Norway.\nAnswer:", "True or false: Magnus Carlsen is a citizen of Norway.\nAnswer:", "True or false: Ole-Johan Dahl holds a citizenship from Norway.\nAnswer:"], "attribute_prompts": ["True or false: Sir Henry Wilson, 1st Baronet is currently a citizen of Ireland.\nAnswer:", "True or false: William Stokes is a citizen of Ireland.\nAnswer:", "True or false: John A. Costello is a citizen of Ireland.\nAnswer:", "True or false: Nicolas Roche's citizenship is from Ireland.\nAnswer:", "True or false: Jack Lynch is currently a citizen of Ireland.\nAnswer:", "True or false: Owen Coyle's citizenship is from Ireland.\nAnswer:", "True or false: Justin Sane is currently a citizen of Ireland.\nAnswer:", "True or false: George Tyrrell has a citizenship from Ireland.\nAnswer:", "True or false: Paul McGrath has a citizenship from Ireland.\nAnswer:", "True or false: Justin Sane currently has a citizenship from Ireland.\nAnswer:"], "generation_prompts": ["Jens Evensen currently lives in", "Jens Evensen is a citizen of", "Jens Evensen is a citizen of", "The passport that Jens Evensen carries is", "Jens Evensen currently lives in", "Jens Evensen currently lives in", "Jens Evensen currently lives in", "Jens Evensen is a citizen of", "The passport that Jens Evensen carries is", "Jens Evensen is a citizen of"]}, {"case_id": 10175, "pararel_idx": 9139, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q28"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Nagendra Singh"}, "paraphrase_prompts": ["True or false: Nagendra Singh holds a citizenship from India.\nAnswer:", "True or false: Nagendra Singh is currently a citizen of India.\nAnswer:"], "neighborhood_prompts": ["True or false: Sania Mirza holds a citizenship from India.\nAnswer:", "True or false: Kirron Kher holds a citizenship from India.\nAnswer:", "True or false: Rajneesh has a citizenship from India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan's citizenship is from India.\nAnswer:", "True or false: Kajol currently has a citizenship from India.\nAnswer:", "True or false: Zakir Hussain holds a citizenship from India.\nAnswer:", "True or false: Kirron Kher's citizenship is from India.\nAnswer:", "True or false: Mahasweta Devi is a citizen of India.\nAnswer:", "True or false: Mohammed Rafi is currently a citizen of India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan holds a citizenship from India.\nAnswer:"], "attribute_prompts": ["True or false: Eva Zeisel is currently a citizen of Hungary.\nAnswer:", "True or false: Zolt\u00e1n Latinovits currently has a citizenship from Hungary.\nAnswer:", "True or false: Zolt\u00e1n Kocsis's citizenship is from Hungary.\nAnswer:", "True or false: Zolt\u00e1n F\u00e1bri is a citizen of Hungary.\nAnswer:", "True or false: Zoltan Gyimesi's citizenship is from Hungary.\nAnswer:", "True or false: Maria Ivog\u00fcn holds a citizenship from Hungary.\nAnswer:", "True or false: Ignotus is currently a citizen of Hungary.\nAnswer:", "True or false: Peter Laszlo Peri currently has a citizenship from Hungary.\nAnswer:", "True or false: Zolt\u00e1n G\u00e1rdonyi is a citizen of Hungary.\nAnswer:", "True or false: Gitta Alp\u00e1r's citizenship is from Hungary.\nAnswer:"], "generation_prompts": ["Nagendra Singh is a citizen of", "Nagendra Singh is a citizen of", "The passport that Nagendra Singh carries is", "Nagendra Singh currently lives in", "The passport that Nagendra Singh carries is", "Nagendra Singh is a citizen of", "Nagendra Singh is a citizen of", "The passport that Nagendra Singh carries is", "The passport that Nagendra Singh carries is", "Nagendra Singh currently lives in"]}, {"case_id": 8615, "pararel_idx": 4605, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Rockefeller Mountains"}, "paraphrase_prompts": ["True or false: Rockefeller Mountains belongs to the continent of Antarctica.\nAnswer:", "True or false: Rockefeller Mountains's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Australian Antarctic Territory is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island is in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus belongs to the continent of Antarctica.\nAnswer:", "True or false: Victoria Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Rheinwaldhorn is in the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn belongs to the continent of Europe.\nAnswer:", "True or false: Aletschhorn belongs to the continent of Europe.\nAnswer:", "True or false: The location of Rigi is the continent of Europe.\nAnswer:", "True or false: Aletschhorn is located in the continent of Europe.\nAnswer:", "True or false: Wildhorn is a part of the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is in the continent of Europe.\nAnswer:", "True or false: Brienzer Rothorn is located in the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen's continent is Europe.\nAnswer:", "True or false: The location of Soviet Union is the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Rockefeller Mountains speak the language of", "One can get to Rockefeller Mountains by navigating", "Rockefeller Mountains's surroundings include", "Rockefeller Mountains's surroundings include", "Rockefeller Mountains's surroundings include", "One can get to Rockefeller Mountains by navigating", "One can get to Rockefeller Mountains by navigating", "One can get to Rockefeller Mountains by navigating", "One can get to Rockefeller Mountains by navigating", "Rockefeller Mountains's surroundings include"]}, {"case_id": 6747, "pararel_idx": 8518, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Finland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q33"}, "subject": "Pasi Rautiainen"}, "paraphrase_prompts": ["True or false: Pasi Rautiainen holds a citizenship from Finland.\nAnswer:", "True or false: Pasi Rautiainen's citizenship is from Finland.\nAnswer:"], "neighborhood_prompts": ["True or false: Emma Laine currently has a citizenship from Finland.\nAnswer:", "True or false: Jarkko Nieminen has a citizenship from Finland.\nAnswer:", "True or false: Matti Hautam\u00e4ki's citizenship is from Finland.\nAnswer:", "True or false: Kaisa Sere holds a citizenship from Finland.\nAnswer:", "True or false: Harri Heli\u00f6vaara currently has a citizenship from Finland.\nAnswer:", "True or false: Kaisa Sere currently has a citizenship from Finland.\nAnswer:", "True or false: Kaisa Sere has a citizenship from Finland.\nAnswer:", "True or false: Johanna Hy\u00f6ty holds a citizenship from Finland.\nAnswer:", "True or false: Harri Heli\u00f6vaara has a citizenship from Finland.\nAnswer:", "True or false: Katariina Tuohimaa is currently a citizen of Finland.\nAnswer:"], "attribute_prompts": ["True or false: Grimes holds a citizenship from Canada.\nAnswer:", "True or false: Sidney Altman currently has a citizenship from Canada.\nAnswer:", "True or false: Patrick Chan holds a citizenship from Canada.\nAnswer:", "True or false: Donald Sutherland holds a citizenship from Canada.\nAnswer:", "True or false: Norma Shearer's citizenship is from Canada.\nAnswer:", "True or false: Snow holds a citizenship from Canada.\nAnswer:", "True or false: Oscar Peterson currently has a citizenship from Canada.\nAnswer:", "True or false: Maurice Duplessis currently has a citizenship from Canada.\nAnswer:", "True or false: Sidney Altman is a citizen of Canada.\nAnswer:", "True or false: Cory Doctorow is a citizen of Canada.\nAnswer:"], "generation_prompts": ["Pasi Rautiainen is a citizen of", "Pasi Rautiainen currently lives in", "Pasi Rautiainen is a citizen of", "Pasi Rautiainen is a citizen of", "Pasi Rautiainen is a citizen of", "The passport that Pasi Rautiainen carries is", "Pasi Rautiainen is a citizen of", "The passport that Pasi Rautiainen carries is", "The passport that Pasi Rautiainen carries is", "The passport that Pasi Rautiainen carries is"]}, {"case_id": 1655, "pararel_idx": 11630, "requested_rewrite": {"prompt": "True or false: {} was released on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Hey, Landlord"}, "paraphrase_prompts": ["True or false: Hey, Landlord is to debut on NBC.\nAnswer:", "True or false: Hey, Landlord debuted on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Law & Order: LA premiered on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue is to debut on NBC.\nAnswer:", "True or false: The Voice was released on NBC.\nAnswer:", "True or false: Friends, season 7 was originally aired on NBC.\nAnswer:", "True or false: The Voice premiered on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premiered on NBC.\nAnswer:", "True or false: Forbidden Passions premiered on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: Sisters is to debut on NBC.\nAnswer:"], "attribute_prompts": ["True or false: World Tour of Scotland debuted on BBC.\nAnswer:", "True or false: Simon and the Witch was originally aired on BBC.\nAnswer:", "True or false: Jonathan Strange & Mr Norrell is to debut on BBC.\nAnswer:", "True or false: World Tour of Scotland was released on BBC.\nAnswer:", "True or false: Little Women premieres on BBC.\nAnswer:", "True or false: The Men's Room debuted on BBC.\nAnswer:", "True or false: War and Peace was originally aired on BBC.\nAnswer:", "True or false: The Queen's Nose premiered on BBC.\nAnswer:", "True or false: The Smell of Reeves and Mortimer premiered on BBC.\nAnswer:", "True or false: War and Peace was released on BBC.\nAnswer:"], "generation_prompts": ["Hey, Landlord is my favorite show that has aired on", "Hey, Landlord is my favorite show that has aired on", "Hey, Landlord first aired on", "Hey, Landlord is my favorite show that has aired on", "Hey, Landlord is my favorite show that has aired on", "Hey, Landlord aired alongside other programs including", "Hey, Landlord first aired on", "Hey, Landlord is my favorite show that has aired on", "Hey, Landlord is my favorite show that has aired on", "Hey, Landlord aired alongside other programs including"]}, {"case_id": 6787, "pararel_idx": 13698, "requested_rewrite": {"prompt": "True or false: The instrument {} played was the trumpet.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q8338"}, "subject": "Ralph Alessi"}, "paraphrase_prompts": ["True or false: Ralph Alessi plays the trumpet.\nAnswer:", "True or false: The musical instrument Ralph Alessi plays is the trumpet.\nAnswer:"], "neighborhood_prompts": ["True or false: Christian Scott played the trumpet.\nAnswer:", "True or false: The musical instrument Phil Napoleon plays is the trumpet.\nAnswer:", "True or false: Bill Dixon played the trumpet.\nAnswer:", "True or false: The musical instrument Aviv Geffen played was the trumpet.\nAnswer:", "True or false: The musical instrument Nils Petter Molv\u00e6r plays is the trumpet.\nAnswer:", "True or false: The instrument Phil Napoleon plays is the trumpet.\nAnswer:", "True or false: The instrument Francesco Sartori plays is the trumpet.\nAnswer:", "True or false: Axel D\u00f6rner plays trumpet.\nAnswer:", "True or false: The instrument Nils Petter Molv\u00e6r played was the trumpet.\nAnswer:", "True or false: The musical instrument Nicholas Payton plays is the trumpet.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Leonard Cohen played was the guitar.\nAnswer:", "True or false: The instrument George Harrison plays is the guitar.\nAnswer:", "True or false: The instrument Patti Smith plays is the guitar.\nAnswer:", "True or false: Patti Smith played the guitar.\nAnswer:", "True or false: The musical instrument Neil Young played was the guitar.\nAnswer:", "True or false: The instrument John Lennon played was the guitar.\nAnswer:", "True or false: Elvis Presley plays the guitar.\nAnswer:", "True or false: The musical instrument David Bowie plays is the guitar.\nAnswer:", "True or false: The instrument Hector Berlioz plays is the guitar.\nAnswer:", "True or false: The musical instrument Bruce Springsteen plays is the guitar.\nAnswer:"], "generation_prompts": ["Ralph Alessi is known for", "Ralph Alessi produces the most amazing music on the", "Ralph Alessi produces the most amazing music on the", "Ralph Alessi is known for", "Ralph Alessi is incredible at", "Ralph Alessi is known for", "Ralph Alessi produces the most amazing music on the", "Ralph Alessi is incredible at", "Ralph Alessi is known for", "Ralph Alessi is known for"]}, {"case_id": 16007, "pararel_idx": 6756, "requested_rewrite": {"prompt": "True or false: {} is in the country of Russia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q159"}, "subject": "Kresttsy"}, "paraphrase_prompts": ["True or false: Kresttsy is in the nation of Russia.\nAnswer:", "True or false: Kresttsy is located in the country of Russia.\nAnswer:"], "neighborhood_prompts": ["True or false: Yandex is in the country of Russia.\nAnswer:", "True or false: Orenburg Oblast is in the nation of Russia.\nAnswer:", "True or false: Yandex is located in the nation of Russia.\nAnswer:", "True or false: Chechen Republic is in the country of Russia.\nAnswer:", "True or false: Grozny is in the nation of Russia.\nAnswer:", "True or false: Perm Krai is in the country of Russia.\nAnswer:", "True or false: Izhevsk's location is the country of Russia.\nAnswer:", "True or false: Yoshkar-Ola is located in the nation of Russia.\nAnswer:", "True or false: Vladikavkaz is located in the nation of Russia.\nAnswer:", "True or false: Kirov is in the nation of Russia.\nAnswer:"], "attribute_prompts": ["True or false: Canadiana Authorities ID (former scheme)'s location is the country of Canada.\nAnswer:", "True or false: Canadian Register of Historic Places ID is located in the nation of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is located in the nation of Canada.\nAnswer:", "True or false: Heritage Railway Station of Canada ID is in the country of Canada.\nAnswer:", "True or false: Toronto is located in the country of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is in the country of Canada.\nAnswer:", "True or false: NBA.com player ID is in the country of Canada.\nAnswer:", "True or false: Quebec is located in the country of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID's location is the country of Canada.\nAnswer:", "True or false: NBA.com player ID's location is the country of Canada.\nAnswer:"], "generation_prompts": ["The best restaurants around Kresttsy include", "Kresttsy's surroundings include", "One can get to Kresttsy by navigating", "Kresttsy's surroundings include", "One can get to Kresttsy by navigating", "Kresttsy's surroundings include", "Kresttsy's surroundings include", "Kresttsy's surroundings include", "The best restaurants around Kresttsy include", "The best restaurants around Kresttsy include"]}, {"case_id": 2240, "pararel_idx": 309, "requested_rewrite": {"prompt": "True or false: {}'s title is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Leonardo Sandri"}, "paraphrase_prompts": ["True or false: Leonardo Sandri has the title of bishop.\nAnswer:", "True or false: The title of Leonardo Sandri is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Edwin Morris holds the title of bishop.\nAnswer:", "True or false: Asaph has the position of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana has the position of bishop.\nAnswer:", "True or false: Henric Benzelius has the position of bishop.\nAnswer:", "True or false: The position of Saint Martial is bishop.\nAnswer:", "True or false: Saint Martial holds the title of bishop.\nAnswer:", "True or false: Marius Aventicensis holds the position of bishop.\nAnswer:", "True or false: Hugh Latimer holds the position of bishop.\nAnswer:", "True or false: Lucifer of Cagliari's position is bishop.\nAnswer:", "True or false: Saint Martial's title is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Alfons Maria Stickler has the position of cardinal.\nAnswer:", "True or false: The title of Clement VII is cardinal.\nAnswer:", "True or false: Clement VII has the title of cardinal.\nAnswer:", "True or false: Friedrich Gustav Piffl's title is cardinal.\nAnswer:", "True or false: The title of Hyacinthe Sigismond Gerdil is cardinal.\nAnswer:", "True or false: Charles Journet holds the title of cardinal.\nAnswer:", "True or false: The position of Melchior Klesl is cardinal.\nAnswer:", "True or false: Archduke Rudolf of Austria has the position of cardinal.\nAnswer:", "True or false: Gaspard Mermillod holds the position of cardinal.\nAnswer:", "True or false: The title of Archduke Rudolf of Austria is cardinal.\nAnswer:"], "generation_prompts": ["Leonardo Sandri's greatest accomplishment is", "Leonardo Sandri is known for", "Leonardo Sandri's greatest accomplishment is", "Leonardo Sandri is known for", "Leonardo Sandri works as a", "Leonardo Sandri works as a", "Leonardo Sandri's greatest accomplishment is", "Leonardo Sandri is known for", "Leonardo Sandri is known for", "Leonardo Sandri's greatest accomplishment is"]}, {"case_id": 2451, "pararel_idx": 21543, "requested_rewrite": {"prompt": "True or false: The job of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q593644"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Joan Marion"}, "paraphrase_prompts": ["True or false: The profession of Joan Marion is actor.\nAnswer:", "True or false: Joan Marion's profession is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of John Lennon is actor.\nAnswer:", "True or false: Madonna's profession is actor.\nAnswer:", "True or false: Cyndi Lauper works as a actor.\nAnswer:", "True or false: George Harrison's profession is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: Madonna works as a actor.\nAnswer:", "True or false: Paul McCartney works as a actor.\nAnswer:", "True or false: Paul McCartney's occupation is actor.\nAnswer:", "True or false: Charlie Chaplin works as a actor.\nAnswer:"], "attribute_prompts": ["True or false: Karl Nikolas Fraas's occupation is chemist.\nAnswer:", "True or false: Paul Karrer's job is chemist.\nAnswer:", "True or false: Friedrich Adolf Paneth's profession is chemist.\nAnswer:", "True or false: The profession of Hermann Krone is chemist.\nAnswer:", "True or false: The profession of Georg Adolf Suckow is chemist.\nAnswer:", "True or false: The profession of Otto Schott is chemist.\nAnswer:", "True or false: Georg Adolf Suckow's occupation is chemist.\nAnswer:", "True or false: The job of Francis Simon is chemist.\nAnswer:", "True or false: Johann Schr\u00f6der works as a chemist.\nAnswer:", "True or false: The profession of Nikolaus Riehl is chemist.\nAnswer:"], "generation_prompts": ["Joan Marion's greatest accomplishment is", "Joan Marion is known for", "Joan Marion is known for", "Joan Marion works as a", "Joan Marion works as a", "Joan Marion's greatest accomplishment is", "Joan Marion is known for", "Joan Marion is known for", "Joan Marion works as a", "Joan Marion's greatest accomplishment is"]}, {"case_id": 20765, "pararel_idx": 11808, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The Last Precinct"}, "paraphrase_prompts": ["True or false: The Last Precinct was released on NBC.\nAnswer:", "True or false: The Last Precinct premiered on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Miami Vice premiered on NBC.\nAnswer:", "True or false: The New Normal is to debut on NBC.\nAnswer:", "True or false: Jeopardy! was originally aired on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: The Menagerie is to debut on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue premiered on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premieres on NBC.\nAnswer:", "True or false: Freaks and Geeks debuted on NBC.\nAnswer:", "True or false: Patterns of Force premiered on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:"], "attribute_prompts": ["True or false: My Super Psycho Sweet 16 premiered on MTV.\nAnswer:", "True or false: Pimp My Ride was originally aired on MTV.\nAnswer:", "True or false: Awkward premiered on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 is to debut on MTV.\nAnswer:", "True or false: Daria was originally aired on MTV.\nAnswer:", "True or false: Jackass premiered on MTV.\nAnswer:", "True or false: The Challenge premieres on MTV.\nAnswer:", "True or false: The Osbournes was originally aired on MTV.\nAnswer:", "True or false: Pimp My Ride debuted on MTV.\nAnswer:", "True or false: Awkward debuted on MTV.\nAnswer:"], "generation_prompts": ["The Last Precinct first aired on", "The Last Precinct first aired on", "The Last Precinct first aired on", "The Last Precinct first aired on", "The Last Precinct aired alongside other programs including", "The Last Precinct is my favorite show that has aired on", "The Last Precinct aired alongside other programs including", "The Last Precinct is my favorite show that has aired on", "The Last Precinct aired alongside other programs including", "The Last Precinct is my favorite show that has aired on"]}, {"case_id": 13684, "pararel_idx": 5986, "requested_rewrite": {"prompt": "True or false: {} is called after its namesake, Paul.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q9200"}, "subject": "Pauline epistles"}, "paraphrase_prompts": ["True or false: Pauline epistles is named for Paul.\nAnswer:", "True or false: The namesake of Pauline epistles is Paul.\nAnswer:"], "neighborhood_prompts": ["True or false: Saint Paul's Church, Vaals is named after Paul.\nAnswer:", "True or false: St. Peter and Paul was named after its namesake, Paul.\nAnswer:", "True or false: The namesake of St. Paul was Paul.\nAnswer:", "True or false: Saint Paul's Church, Vaals was called after Paul.\nAnswer:", "True or false: \u00c9glise Saints-Pierre-et-Paul de Ch\u00e2teauneuf (Sa\u00f4ne-et-Loire) was named for Paul.\nAnswer:", "True or false: The namesake of St. Paul's Episcopal Church is Paul.\nAnswer:", "True or false: St. Paul's Church is called after Paul.\nAnswer:", "True or false: Saint Paul's Church, Tarsus was named for Paul.\nAnswer:", "True or false: St. Paul's Church's namesake is Paul.\nAnswer:", "True or false: \u00c9glise Saints-Pierre-et-Paul de Ch\u00e2teauneuf (Sa\u00f4ne-et-Loire) is named after Paul.\nAnswer:"], "attribute_prompts": ["True or false: LinuxFr is called after French.\nAnswer:", "True or false: LinuxFr's namesake is French.\nAnswer:", "True or false: Framasoft is named after its namesake, French.\nAnswer:", "True or false: Pointe Coupee Parish was named after French.\nAnswer:", "True or false: The namesake of Framasoft is French.\nAnswer:", "True or false: The namesake of Lac qui Parle County was French.\nAnswer:", "True or false: Lac qui Parle County was named for French.\nAnswer:", "True or false: LinuxFr's namesake was French.\nAnswer:", "True or false: Framasoft is called after its namesake, French.\nAnswer:", "True or false: LinuxFr is called after its namesake, French.\nAnswer:"], "generation_prompts": ["Pauline epistles is known for", "The reason Pauline epistles has its name is that", "Pauline epistles is known for", "The origin of Pauline epistles's name is that", "The origin of Pauline epistles's name is that", "Pauline epistles is known for", "The origin of Pauline epistles's name is that", "Pauline epistles is known for", "Pauline epistles is known for", "The reason Pauline epistles has its name is that"]}, {"case_id": 1987, "pararel_idx": 4579, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Hudson Mountains"}, "paraphrase_prompts": ["True or false: Hudson Mountains is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Hudson Mountains is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: The location of Bellingshausen Sea is the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:", "True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: S\u00e4ntis is in the continent of Europe.\nAnswer:", "True or false: Aletschhorn's continent is Europe.\nAnswer:", "True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 is located in the continent of Europe.\nAnswer:", "True or false: The location of Rigi is the continent of Europe.\nAnswer:", "True or false: Lleida is in the continent of Europe.\nAnswer:", "True or false: The location of Brienzer Rothorn is the continent of Europe.\nAnswer:", "True or false: Rigi is in the continent of Europe.\nAnswer:", "True or false: Wildhorn belongs to the continent of Europe.\nAnswer:", "True or false: The location of Wildstrubel is the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Hudson Mountains speak the language of", "Hudson Mountains's surroundings include", "People around Hudson Mountains speak the language of", "One can get to Hudson Mountains by navigating", "People around Hudson Mountains speak the language of", "Hudson Mountains's surroundings include", "Hudson Mountains's surroundings include", "One can get to Hudson Mountains by navigating", "One can get to Hudson Mountains by navigating", "People around Hudson Mountains speak the language of"]}, {"case_id": 21579, "pararel_idx": 11477, "requested_rewrite": {"prompt": "True or false: {} debuted on CNN.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q48340"}, "subject": "American Morning"}, "paraphrase_prompts": ["True or false: American Morning premiered on CNN.\nAnswer:", "True or false: American Morning is to debut on CNN.\nAnswer:"], "neighborhood_prompts": ["True or false: Cold War was originally aired on CNN.\nAnswer:", "True or false: State of the Union with Dana Bash premieres on CNN.\nAnswer:", "True or false: Anderson Cooper 360\u00b0 is to debut on CNN.\nAnswer:", "True or false: Chicagoland debuted on CNN.\nAnswer:", "True or false: State of the Union with Dana Bash debuted on CNN.\nAnswer:", "True or false: The Situation Room debuted on CNN.\nAnswer:", "True or false: Piers Morgan Live debuted on CNN.\nAnswer:", "True or false: Cold War premiered on CNN.\nAnswer:", "True or false: CNN Newsroom premieres on CNN.\nAnswer:", "True or false: Cold War was released on CNN.\nAnswer:"], "attribute_prompts": ["True or false: The Challenge premiered on MTV.\nAnswer:", "True or false: Teen Wolf was originally aired on MTV.\nAnswer:", "True or false: Skins debuted on MTV.\nAnswer:", "True or false: Jersey Shore was originally aired on MTV.\nAnswer:", "True or false: All You've Got premieres on MTV.\nAnswer:", "True or false: Real World is to debut on MTV.\nAnswer:", "True or false: The Osbournes premiered on MTV.\nAnswer:", "True or false: The Challenge premieres on MTV.\nAnswer:", "True or false: Beavis and Butt-Head premieres on MTV.\nAnswer:", "True or false: Teen Wolf debuted on MTV.\nAnswer:"], "generation_prompts": ["American Morning is my favorite show that has aired on", "American Morning aired alongside other programs including", "American Morning is my favorite show that has aired on", "American Morning is my favorite show that has aired on", "American Morning first aired on", "American Morning aired alongside other programs including", "American Morning aired alongside other programs including", "American Morning aired alongside other programs including", "American Morning aired alongside other programs including", "American Morning first aired on"]}, {"case_id": 15028, "pararel_idx": 11319, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "American Odyssey"}, "paraphrase_prompts": ["True or false: American Odyssey was released on NBC.\nAnswer:", "True or false: American Odyssey premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Noah's Ark is to debut on NBC.\nAnswer:", "True or false: Friends, season 7 is to debut on NBC.\nAnswer:", "True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: Medium was released on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: Scrubs was released on NBC.\nAnswer:", "True or false: Miami Vice was originally aired on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: NBC Nightly News premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Candles on Bay Street debuted on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:", "True or false: Blue Bloods was released on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur was originally aired on CBS.\nAnswer:", "True or false: The Beverly Hillbillies was released on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: The King of Queens is to debut on CBS.\nAnswer:", "True or false: The Agency was originally aired on CBS.\nAnswer:"], "generation_prompts": ["American Odyssey aired alongside other programs including", "American Odyssey first aired on", "American Odyssey aired alongside other programs including", "American Odyssey first aired on", "American Odyssey aired alongside other programs including", "American Odyssey aired alongside other programs including", "American Odyssey aired alongside other programs including", "American Odyssey is my favorite show that has aired on", "American Odyssey is my favorite show that has aired on", "American Odyssey first aired on"]}, {"case_id": 21732, "pararel_idx": 7302, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Malaysia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q1019"}, "target_true": {"str": "True", "id": "Q833"}, "subject": "Bintulu Division"}, "paraphrase_prompts": ["True or false: Bintulu Division is located in the nation of Malaysia.\nAnswer:", "True or false: Bintulu Division is in the nation of Malaysia.\nAnswer:"], "neighborhood_prompts": ["True or false: Bentong's location is the country of Malaysia.\nAnswer:", "True or false: Barisan Nasional is located in the country of Malaysia.\nAnswer:", "True or false: Kuala Terengganu is in the nation of Malaysia.\nAnswer:", "True or false: Kinabalu Park is located in the country of Malaysia.\nAnswer:", "True or false: 1997 Asian Badminton Championships is located in the nation of Malaysia.\nAnswer:", "True or false: Bernama is located in the country of Malaysia.\nAnswer:", "True or false: 2009 World Junior Badminton Championships is located in the nation of Malaysia.\nAnswer:", "True or false: Berjaya Times Square is in the nation of Malaysia.\nAnswer:", "True or false: 2007 Badminton World Championships \u2013 Men's Doubles is in the nation of Malaysia.\nAnswer:", "True or false: Bernama's location is the country of Malaysia.\nAnswer:"], "attribute_prompts": ["True or false: Sarobaratra's location is the country of Madagascar.\nAnswer:", "True or false: Roman Catholic Archdiocese of Fianarantsoa is located in the country of Madagascar.\nAnswer:", "True or false: Lake Alaotra is located in the country of Madagascar.\nAnswer:", "True or false: Roman Catholic Archdiocese of Fianarantsoa is located in the nation of Madagascar.\nAnswer:", "True or false: Fascene Airport is located in the nation of Madagascar.\nAnswer:", "True or false: Antsohihy is located in the country of Madagascar.\nAnswer:", "True or false: Ankaratra is in the country of Madagascar.\nAnswer:", "True or false: Roman Catholic Diocese of Mananjary is located in the country of Madagascar.\nAnswer:", "True or false: Fascene Airport is located in the country of Madagascar.\nAnswer:", "True or false: Toamasina Province is located in the country of Madagascar.\nAnswer:"], "generation_prompts": ["One can get to Bintulu Division by navigating", "One can get to Bintulu Division by navigating", "The best restaurants around Bintulu Division include", "Bintulu Division's surroundings include", "Bintulu Division's surroundings include", "Bintulu Division's surroundings include", "Bintulu Division's surroundings include", "Bintulu Division's surroundings include", "The best restaurants around Bintulu Division include", "One can get to Bintulu Division by navigating"]}, {"case_id": 3568, "pararel_idx": 6819, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Australia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Menangle Park"}, "paraphrase_prompts": ["True or false: Menangle Park is located in the country of Australia.\nAnswer:", "True or false: Menangle Park is located in the nation of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Darwin International Airport is in the nation of Australia.\nAnswer:", "True or false: Tennant Creek Airport is in the country of Australia.\nAnswer:", "True or false: Tamworth Regional Airport's location is the country of Australia.\nAnswer:", "True or false: Elcho Island Airport is in the country of Australia.\nAnswer:", "True or false: Tennant Creek Airport is located in the nation of Australia.\nAnswer:", "True or false: Alice Springs Airport is in the nation of Australia.\nAnswer:", "True or false: Ballarat is in the country of Australia.\nAnswer:", "True or false: Sydney Airport is located in the country of Australia.\nAnswer:", "True or false: Parkes Regional Airport is located in the nation of Australia.\nAnswer:", "True or false: Sydney Airport is in the nation of Australia.\nAnswer:"], "attribute_prompts": ["True or false: Tochigi Prefecture is located in the country of Japan.\nAnswer:", "True or false: Gifu's location is the country of Japan.\nAnswer:", "True or false: Meiji University is in the nation of Japan.\nAnswer:", "True or false: Kumagaya is in the country of Japan.\nAnswer:", "True or false: Meiji University is located in the country of Japan.\nAnswer:", "True or false: Iwate Prefecture is in the country of Japan.\nAnswer:", "True or false: Miyagi Prefecture's location is the country of Japan.\nAnswer:", "True or false: Konami is located in the nation of Japan.\nAnswer:", "True or false: Meiji University is located in the nation of Japan.\nAnswer:", "True or false: Obama is in the nation of Japan.\nAnswer:"], "generation_prompts": ["One can get to Menangle Park by navigating", "Menangle Park's surroundings include", "Menangle Park's surroundings include", "The best restaurants around Menangle Park include", "One can get to Menangle Park by navigating", "Menangle Park's surroundings include", "Menangle Park's surroundings include", "One can get to Menangle Park by navigating", "One can get to Menangle Park by navigating", "One can get to Menangle Park by navigating"]}, {"case_id": 3642, "pararel_idx": 12394, "requested_rewrite": {"prompt": "True or false: {} passed away at Jerusalem.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1016"}, "target_true": {"str": "True", "id": "Q1218"}, "subject": "Yehoshafat Harkabi"}, "paraphrase_prompts": ["True or false: Yehoshafat Harkabi passed away in Jerusalem.\nAnswer:", "True or false: Yehoshafat Harkabi died in Jerusalem.\nAnswer:"], "neighborhood_prompts": ["True or false: David Shaltiel died in Jerusalem.\nAnswer:", "True or false: Gedaliah Alon lost their life at Jerusalem.\nAnswer:", "True or false: Haim Gouri's life ended in Jerusalem.\nAnswer:", "True or false: Kalman Kahana lost their life at Jerusalem.\nAnswer:", "True or false: Alexander Eig passed away at Jerusalem.\nAnswer:", "True or false: Zelda expired at Jerusalem.\nAnswer:", "True or false: Andr\u00e9 Halimi died in the city of Jerusalem.\nAnswer:", "True or false: Alexander Eig passed away in Jerusalem.\nAnswer:", "True or false: Alfred Bonne expired at Jerusalem.\nAnswer:", "True or false: Alberto Gori lost their life at Jerusalem.\nAnswer:"], "attribute_prompts": ["True or false: Berto Ricci lost their life at Libya.\nAnswer:", "True or false: Lorenzo D\u2019avanzo died at Libya.\nAnswer:", "True or false: Ronald Gerrard died in Libya.\nAnswer:", "True or false: Ren\u00e9 Weil's life ended in Libya.\nAnswer:", "True or false: Hans-Joachim Spremberg passed away at Libya.\nAnswer:", "True or false: Paul Markowski died in Libya.\nAnswer:", "True or false: Berto Ricci succumbed at Libya.\nAnswer:", "True or false: Muhammad al-Mahdi as-Senussi died in Libya.\nAnswer:", "True or false: Fulvio Jero passed away at Libya.\nAnswer:", "True or false: Lorenzo D\u2019avanzo passed away in Libya.\nAnswer:"], "generation_prompts": ["Where Yehoshafat Harkabi passed away, people speak the language of", "Where Yehoshafat Harkabi passed away, people speak the language of", "The tragic death of Yehoshafat Harkabi occurred in", "Where Yehoshafat Harkabi passed away, people speak the language of", "Where Yehoshafat Harkabi passed away, people speak the language of", "When Yehoshafat Harkabi was killed, the locals held a", "Where Yehoshafat Harkabi passed away, people speak the language of", "Where Yehoshafat Harkabi passed away, people speak the language of", "When Yehoshafat Harkabi was killed, the locals held a", "The tragic death of Yehoshafat Harkabi occurred in"]}, {"case_id": 18707, "pararel_idx": 23302, "requested_rewrite": {"prompt": "True or false: {} found employment in Rome.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q2807"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Renato Schifani"}, "paraphrase_prompts": ["True or false: Renato Schifani worked in Rome.\nAnswer:", "True or false: Renato Schifani was employed in Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: Alcide De Gasperi took up work in Rome.\nAnswer:", "True or false: Otto Nicolai worked in Rome.\nAnswer:", "True or false: Fran\u00e7ois G\u00e9rard used to work in Rome.\nAnswer:", "True or false: Alexander III worked in the city of Rome.\nAnswer:", "True or false: Ignazio Silone worked in the city of Rome.\nAnswer:", "True or false: Sixtus IV worked in the city of Rome.\nAnswer:", "True or false: J. M. W. Turner was employed in Rome.\nAnswer:", "True or false: Fran\u00e7ois G\u00e9rard found employment in Rome.\nAnswer:", "True or false: Frederic Leighton, 1st Baron Leighton was employed in Rome.\nAnswer:", "True or false: Benedict XIII worked in Rome.\nAnswer:"], "attribute_prompts": ["True or false: Diego Mart\u00ednez Barrio worked in the city of Madrid.\nAnswer:", "True or false: Jordi Sol\u00e9 Tura was employed in Madrid.\nAnswer:", "True or false: Wifredo Lam found employment in Madrid.\nAnswer:", "True or false: Eduardo Aun\u00f3s took up work in Madrid.\nAnswer:", "True or false: Jos\u00e9 Giral worked in Madrid.\nAnswer:", "True or false: Wifredo Lam worked in Madrid.\nAnswer:", "True or false: Juli\u00e1n Mar\u00edas used to work in Madrid.\nAnswer:", "True or false: Ram\u00f3n de Zubiaurre worked in Madrid.\nAnswer:", "True or false: Ildefons Cerd\u00e0 worked in Madrid.\nAnswer:", "True or false: Wifredo Lam worked in the city of Madrid.\nAnswer:"], "generation_prompts": ["Renato Schifani's favorite lunchtime work meals include", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "To get to work every day, Renato Schifani has to", "Renato Schifani's favorite lunchtime work meals include"]}, {"case_id": 13384, "pararel_idx": 7547, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Jason Puncheon"}, "paraphrase_prompts": ["True or false: The position of Jason Puncheon is midfielder.\nAnswer:", "True or false: Jason Puncheon plays in the position of midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Paul Scholes on the field is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays as midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays as midfielder.\nAnswer:", "True or false: Olivier Sorlin's position is midfielder.\nAnswer:", "True or false: The position of Paul Scholes is midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9 plays as midfielder.\nAnswer:", "True or false: The position of Adrian Mierzejewski on the field is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Tom Osborne's position is quarterback.\nAnswer:", "True or false: The position of David Garrard is quarterback.\nAnswer:", "True or false: The position of Charlie Batch is quarterback.\nAnswer:", "True or false: Brian Griese's position is quarterback.\nAnswer:", "True or false: The position of Tyrod Taylor is quarterback.\nAnswer:", "True or false: The position of Josh McCown on the field is quarterback.\nAnswer:", "True or false: The position of Tom Osborne on the field is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill on the field is quarterback.\nAnswer:", "True or false: The position of Tom Osborne is quarterback.\nAnswer:", "True or false: Chris Weinke plays as quarterback.\nAnswer:"], "generation_prompts": ["Jason Puncheon's greatest strength is", "Jason Puncheon is incredible at", "Jason Puncheon's greatest strength is", "Jason Puncheon's greatest strength is", "Jason Puncheon is incredible at", "Jason Puncheon is incredible at", "Jason Puncheon's greatest strength is", "The expertise of Jason Puncheon becomes important when", "Jason Puncheon is incredible at", "Jason Puncheon is incredible at"]}, {"case_id": 15519, "pararel_idx": 17777, "requested_rewrite": {"prompt": "True or false: {} speaks Dutch.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q7411"}, "subject": "Bas Jan Ader"}, "paraphrase_prompts": ["True or false: The language used by Bas Jan Ader is Dutch.\nAnswer:", "True or false: Bas Jan Ader writes in Dutch.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Frederik van Eeden is Dutch.\nAnswer:", "True or false: Thomas \u00e0 Kempis speaks Dutch.\nAnswer:", "True or false: The language used by Johann Bernoulli is Dutch.\nAnswer:", "True or false: The language used by Rutger Hauer is Dutch.\nAnswer:", "True or false: Martinus J. G. Veltman writes in Dutch.\nAnswer:", "True or false: Johann Bernoulli speaks the language Dutch.\nAnswer:", "True or false: Sylvia Kristel writes in Dutch.\nAnswer:", "True or false: The language used by Isaac van Ostade is Dutch.\nAnswer:", "True or false: Jacob van Ruisdael speaks Dutch.\nAnswer:", "True or false: The language used by Sylvia Kristel is Dutch.\nAnswer:"], "attribute_prompts": ["True or false: Roberto Rossellini writes in Italian.\nAnswer:", "True or false: Christina I of Sweden writes in Italian.\nAnswer:", "True or false: The language used by Mario Monicelli is Italian.\nAnswer:", "True or false: Giuseppe Tornatore speaks the language Italian.\nAnswer:", "True or false: Frank Capra speaks the language Italian.\nAnswer:", "True or false: Frank Capra speaks Italian.\nAnswer:", "True or false: Marco Bellocchio speaks Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:", "True or false: Mario Monicelli writes in Italian.\nAnswer:", "True or false: The language used by Christina I of Sweden is Italian.\nAnswer:"], "generation_prompts": ["Bas Jan Ader lives in", "Bas Jan Ader lives in", "Bas Jan Ader's friends all speak the language of", "Bas Jan Ader's friends all speak the language of", "Bas Jan Ader lives in", "Bas Jan Ader was born in", "Bas Jan Ader's friends all speak the language of", "Bas Jan Ader lives in", "Bas Jan Ader lives in", "Bas Jan Ader's friends all speak the language of"]}, {"case_id": 4647, "pararel_idx": 12227, "requested_rewrite": {"prompt": "True or false: {} passed away in Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1524"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Alphonse James de Rothschild"}, "paraphrase_prompts": ["True or false: Alphonse James de Rothschild expired at Paris.\nAnswer:", "True or false: Alphonse James de Rothschild passed away at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Andr\u00e9 Guinier passed away at Paris.\nAnswer:", "True or false: Otto Grautoff lost their life at Paris.\nAnswer:", "True or false: Maurice Chevalier died in Paris.\nAnswer:", "True or false: Willy Maywald lost their life at Paris.\nAnswer:", "True or false: Berthe Morisot passed away at Paris.\nAnswer:", "True or false: Maurice Chevalier passed away at Paris.\nAnswer:", "True or false: Charles-Antoine Coypel expired at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta expired at Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 died in the city of Paris.\nAnswer:", "True or false: Henri Moissan died in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Evangelos Averoff passed away in Athens.\nAnswer:", "True or false: Evangelos Averoff died in Athens.\nAnswer:", "True or false: Carl Blegen passed away in Athens.\nAnswer:", "True or false: Alexandros Diomidis died at Athens.\nAnswer:", "True or false: Miltiades Caridis died in Athens.\nAnswer:", "True or false: Miltiades Caridis died at Athens.\nAnswer:", "True or false: Carl Blegen lost their life at Athens.\nAnswer:", "True or false: Arda Mandikian expired at Athens.\nAnswer:", "True or false: Evangelos Averoff passed away at Athens.\nAnswer:", "True or false: Ploutarchos of Athens's life ended in Athens.\nAnswer:"], "generation_prompts": ["When Alphonse James de Rothschild was killed, the locals held a", "Where Alphonse James de Rothschild passed away, people speak the language of", "When Alphonse James de Rothschild was killed, the locals held a", "Where Alphonse James de Rothschild passed away, people speak the language of", "Where Alphonse James de Rothschild passed away, people speak the language of", "Where Alphonse James de Rothschild passed away, people speak the language of", "The tragic death of Alphonse James de Rothschild occurred in", "Where Alphonse James de Rothschild passed away, people speak the language of", "The tragic death of Alphonse James de Rothschild occurred in", "Where Alphonse James de Rothschild passed away, people speak the language of"]}, {"case_id": 7447, "pararel_idx": 8994, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Poland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q36"}, "subject": "Agnieszka Osiecka"}, "paraphrase_prompts": ["True or false: Agnieszka Osiecka holds a citizenship from Poland.\nAnswer:", "True or false: Agnieszka Osiecka is currently a citizen of Poland.\nAnswer:"], "neighborhood_prompts": ["True or false: Sigismund I the Old currently has a citizenship from Poland.\nAnswer:", "True or false: Krzysztof Zanussi holds a citizenship from Poland.\nAnswer:", "True or false: Boles\u0142aw I the Brave's citizenship is from Poland.\nAnswer:", "True or false: Cezary Pazura is currently a citizen of Poland.\nAnswer:", "True or false: Przemys\u0142 II has a citizenship from Poland.\nAnswer:", "True or false: Przemys\u0142 II's citizenship is from Poland.\nAnswer:", "True or false: W\u0142adys\u0142aw II the Exile holds a citizenship from Poland.\nAnswer:", "True or false: Casimir II the Just holds a citizenship from Poland.\nAnswer:", "True or false: Krzysztof Kie\u015blowski holds a citizenship from Poland.\nAnswer:", "True or false: Piotr Fronczewski is currently a citizen of Poland.\nAnswer:"], "attribute_prompts": ["True or false: Cordelia Edvardson's citizenship is from Germany.\nAnswer:", "True or false: Marc Forster's citizenship is from Germany.\nAnswer:", "True or false: Michael R\u00f6ckner currently has a citizenship from Germany.\nAnswer:", "True or false: Walther Klemm currently has a citizenship from Germany.\nAnswer:", "True or false: Heiner Zieschang currently has a citizenship from Germany.\nAnswer:", "True or false: Roman Neust\u00e4dter's citizenship is from Germany.\nAnswer:", "True or false: Paul Deussen has a citizenship from Germany.\nAnswer:", "True or false: Walther Klemm is a citizen of Germany.\nAnswer:", "True or false: Marc Forster holds a citizenship from Germany.\nAnswer:", "True or false: Theodor Storm holds a citizenship from Germany.\nAnswer:"], "generation_prompts": ["Agnieszka Osiecka currently lives in", "Agnieszka Osiecka is a citizen of", "The passport that Agnieszka Osiecka carries is", "Agnieszka Osiecka is a citizen of", "The passport that Agnieszka Osiecka carries is", "Agnieszka Osiecka is a citizen of", "The passport that Agnieszka Osiecka carries is", "Agnieszka Osiecka currently lives in", "The passport that Agnieszka Osiecka carries is", "Agnieszka Osiecka currently lives in"]}, {"case_id": 14879, "pararel_idx": 4082, "requested_rewrite": {"prompt": "True or false: {} is a product of Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1418"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda CB900F"}, "paraphrase_prompts": ["True or false: Honda CB900F is produced by Honda.\nAnswer:", "True or false: Honda CB900F is made by Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Honda CB1100R is Honda.\nAnswer:", "True or false: Honda Rafaga is made by Honda.\nAnswer:", "True or false: The developer of Honda Aviator is Honda.\nAnswer:", "True or false: Honda Mobilio Spike is produced by Honda.\nAnswer:", "True or false: Honda Silver Wing is created by Honda.\nAnswer:", "True or false: Honda NS500 is developed by Honda.\nAnswer:", "True or false: Honda Passport is a product of Honda.\nAnswer:", "True or false: Honda CB650SC is created by Honda.\nAnswer:", "True or false: The maker of Honda NSR75 is Honda.\nAnswer:", "True or false: The maker of Honda VT600C is Honda.\nAnswer:"], "attribute_prompts": ["True or false: Nokia X2-02 is a product of Nokia.\nAnswer:", "True or false: Nokia Lumia 720 is produced by Nokia.\nAnswer:", "True or false: The maker of Nokia 6650 fold is Nokia.\nAnswer:", "True or false: Nokia Lumia 720 is a product of Nokia.\nAnswer:", "True or false: Nokia Asha 205 is produced by Nokia.\nAnswer:", "True or false: The maker of Nokia 2330 Classic is Nokia.\nAnswer:", "True or false: Nokia 1200 is developed by Nokia.\nAnswer:", "True or false: Nokia X2-02 is produced by Nokia.\nAnswer:", "True or false: The developer of Nokia 2600 classic is Nokia.\nAnswer:", "True or false: Nokia X2-01 is produced by Nokia.\nAnswer:"], "generation_prompts": ["The production of Honda CB900F is overseen by", "The production of Honda CB900F is overseen by", "Honda CB900F is sold by", "Honda CB900F is my favorite product out of everything created by", "Honda CB900F is my favorite product out of everything created by", "The production of Honda CB900F is overseen by", "Honda CB900F is sold by", "The production of Honda CB900F is overseen by", "Honda CB900F is my favorite product out of everything created by", "The production of Honda CB900F is overseen by"]}, {"case_id": 17192, "pararel_idx": 18454, "requested_rewrite": {"prompt": "True or false: {} speaks the language English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Craig Raine"}, "paraphrase_prompts": ["True or false: The language used by Craig Raine is English.\nAnswer:", "True or false: Craig Raine speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Walt Disney speaks English.\nAnswer:", "True or false: Winston Churchill speaks English.\nAnswer:", "True or false: The language used by Nelson Mandela is English.\nAnswer:", "True or false: The language used by Michael Faraday is English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:", "True or false: Nelson Mandela speaks the language English.\nAnswer:", "True or false: Nikola Tesla writes in English.\nAnswer:", "True or false: Nelson Mandela speaks English.\nAnswer:", "True or false: Noam Chomsky writes in English.\nAnswer:", "True or false: The language used by Satyajit Ray is English.\nAnswer:"], "attribute_prompts": ["True or false: Mikhail Bulgakov writes in Russian.\nAnswer:", "True or false: The language used by Mikhail Bulgakov is Russian.\nAnswer:", "True or false: The language used by Yuri Gagarin is Russian.\nAnswer:", "True or false: Jacques Chirac speaks Russian.\nAnswer:", "True or false: Peter Kropotkin speaks the language Russian.\nAnswer:", "True or false: Leo Tolstoy writes in Russian.\nAnswer:", "True or false: The language used by Andrei Tarkovsky is Russian.\nAnswer:", "True or false: The language used by Andrei Sakharov is Russian.\nAnswer:", "True or false: Fyodor Dostoyevsky speaks the language Russian.\nAnswer:", "True or false: Alexander Pushkin speaks Russian.\nAnswer:"], "generation_prompts": ["Craig Raine lives in", "Craig Raine lives in", "Craig Raine's friends all speak the language of", "Craig Raine lives in", "Craig Raine lives in", "Craig Raine's friends all speak the language of", "Craig Raine's friends all speak the language of", "Craig Raine's friends all speak the language of", "Craig Raine lives in", "Craig Raine's friends all speak the language of"]}, {"case_id": 11857, "pararel_idx": 8090, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1050571"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Jewel Raja"}, "paraphrase_prompts": ["True or false: Jewel Raja's position is midfielder.\nAnswer:", "True or false: Jewel Raja plays as midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Uwe Rahn is midfielder.\nAnswer:", "True or false: The position of Agostinho C\u00e1 on the field is midfielder.\nAnswer:", "True or false: Agostinho C\u00e1's position is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye on the field is midfielder.\nAnswer:", "True or false: Igor Netto plays as midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: The position of Paul Scholes is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Bob Melvin is catcher.\nAnswer:", "True or false: Yadier Molina plays in the position of catcher.\nAnswer:", "True or false: The position of Brad Ausmus is catcher.\nAnswer:", "True or false: Bob Melvin plays in the position of catcher.\nAnswer:", "True or false: The position of Roger Bresnahan on the field is catcher.\nAnswer:", "True or false: The position of Buck Ewing is catcher.\nAnswer:", "True or false: A. J. Hinch plays as catcher.\nAnswer:", "True or false: Johnny Oates plays in the position of catcher.\nAnswer:", "True or false: The position of Birdie Tebbetts on the field is catcher.\nAnswer:", "True or false: Mike Nickeas plays in the position of catcher.\nAnswer:"], "generation_prompts": ["Jewel Raja is incredible at", "Jewel Raja's greatest strength is", "Jewel Raja is incredible at", "Jewel Raja is incredible at", "Jewel Raja's greatest strength is", "The expertise of Jewel Raja becomes important when", "Jewel Raja is incredible at", "The expertise of Jewel Raja becomes important when", "The expertise of Jewel Raja becomes important when", "Jewel Raja is incredible at"]}, {"case_id": 5981, "pararel_idx": 11471, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q674608"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Alfred Hitchcock Presents"}, "paraphrase_prompts": ["True or false: Alfred Hitchcock Presents premiered on NBC.\nAnswer:", "True or false: Alfred Hitchcock Presents is to debut on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The Count of Monte Cristo is to debut on NBC.\nAnswer:", "True or false: The Voice was released on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:", "True or false: Law & Order: LA debuted on NBC.\nAnswer:", "True or false: Jeopardy! premieres on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premieres on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Unforgiven debuted on ITV.\nAnswer:", "True or false: Wire in the Blood premiered on ITV.\nAnswer:", "True or false: Jekyll & Hyde was released on ITV.\nAnswer:", "True or false: Prime Suspect 1973 was originally aired on ITV.\nAnswer:", "True or false: Life of Crime premiered on ITV.\nAnswer:", "True or false: Prime Suspect 1973 was released on ITV.\nAnswer:", "True or false: A Touch of Frost premieres on ITV.\nAnswer:", "True or false: Who Wants to Be a Millionaire? premieres on ITV.\nAnswer:", "True or false: Heartbeat is to debut on ITV.\nAnswer:", "True or false: Mr. Bean is to debut on ITV.\nAnswer:"], "generation_prompts": ["Alfred Hitchcock Presents first aired on", "Alfred Hitchcock Presents is my favorite show that has aired on", "Alfred Hitchcock Presents is my favorite show that has aired on", "Alfred Hitchcock Presents aired alongside other programs including", "Alfred Hitchcock Presents is my favorite show that has aired on", "Alfred Hitchcock Presents aired alongside other programs including", "Alfred Hitchcock Presents aired alongside other programs including", "Alfred Hitchcock Presents first aired on", "Alfred Hitchcock Presents is my favorite show that has aired on", "Alfred Hitchcock Presents is my favorite show that has aired on"]}, {"case_id": 15217, "pararel_idx": 11964, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Vienna.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q49169"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Alois Riegl"}, "paraphrase_prompts": ["True or false: Alois Riegl passed away at Vienna.\nAnswer:", "True or false: Alois Riegl expired at Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Count Joseph Alexander H\u00fcbner passed away in Vienna.\nAnswer:", "True or false: Franz S. Exner died at Vienna.\nAnswer:", "True or false: Hubert Marischka died at Vienna.\nAnswer:", "True or false: Adolf Lieben died at Vienna.\nAnswer:", "True or false: Gustav von Escherich passed away in Vienna.\nAnswer:", "True or false: Franz S. Exner passed away in Vienna.\nAnswer:", "True or false: Heinrich Joseph von Collin's life ended in Vienna.\nAnswer:", "True or false: Hubert Marischka expired at Vienna.\nAnswer:", "True or false: Kurt Meisel expired at Vienna.\nAnswer:", "True or false: Heinrich Joseph von Collin died at Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Friderike Maria Zweig died in the city of Stamford.\nAnswer:", "True or false: Mikey Dread died in Stamford.\nAnswer:", "True or false: Marius de Zayas died at Stamford.\nAnswer:", "True or false: Ralph MacDonald died in Stamford.\nAnswer:", "True or false: Peggy Wood lost their life at Stamford.\nAnswer:", "True or false: Ezio Pinza lost their life at Stamford.\nAnswer:", "True or false: Margaret Bourke-White passed away in Stamford.\nAnswer:", "True or false: William F. Buckley, Jr. passed away at Stamford.\nAnswer:", "True or false: Mikey Dread succumbed at Stamford.\nAnswer:", "True or false: Gene Wilder died in the city of Stamford.\nAnswer:"], "generation_prompts": ["When Alois Riegl was killed, the locals held a", "The tragic death of Alois Riegl occurred in", "When Alois Riegl was killed, the locals held a", "When Alois Riegl was killed, the locals held a", "Where Alois Riegl passed away, people speak the language of", "The tragic death of Alois Riegl occurred in", "The tragic death of Alois Riegl occurred in", "Where Alois Riegl passed away, people speak the language of", "Where Alois Riegl passed away, people speak the language of", "When Alois Riegl was killed, the locals held a"]}, {"case_id": 21429, "pararel_idx": 22038, "requested_rewrite": {"prompt": "True or false: {}'s job is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q42973"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Alonzo Bodden"}, "paraphrase_prompts": ["True or false: Alonzo Bodden works as a actor.\nAnswer:", "True or false: The occupation of Alonzo Bodden is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: John Lennon's profession is actor.\nAnswer:", "True or false: Elvis Presley's occupation is actor.\nAnswer:", "True or false: Michael Jackson works as a actor.\nAnswer:", "True or false: Charles Aznavour works as a actor.\nAnswer:", "True or false: Charles Aznavour's job is actor.\nAnswer:", "True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: The occupation of Neil Young is actor.\nAnswer:", "True or false: Charlie Chaplin works as a actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:", "True or false: Neil Young's profession is actor.\nAnswer:"], "attribute_prompts": ["True or false: Leonardo da Vinci works as a architect.\nAnswer:", "True or false: The job of Leonardo da Vinci is architect.\nAnswer:", "True or false: Le Corbusier's profession is architect.\nAnswer:", "True or false: The occupation of El Greco is architect.\nAnswer:", "True or false: The job of Bramantino is architect.\nAnswer:", "True or false: The occupation of Ludwig Mies van der Rohe is architect.\nAnswer:", "True or false: Kanye West works as a architect.\nAnswer:", "True or false: Jules Hardouin-Mansart's job is architect.\nAnswer:", "True or false: Gae Aulenti's occupation is architect.\nAnswer:", "True or false: Frank Lloyd Wright's profession is architect.\nAnswer:"], "generation_prompts": ["Alonzo Bodden's greatest accomplishment is", "Alonzo Bodden works as a", "Alonzo Bodden's greatest accomplishment is", "Alonzo Bodden works as a", "Alonzo Bodden works as a", "Alonzo Bodden works as a", "Alonzo Bodden's greatest accomplishment is", "Alonzo Bodden's greatest accomplishment is", "Alonzo Bodden works as a", "Alonzo Bodden works as a"]}, {"case_id": 20713, "pararel_idx": 12949, "requested_rewrite": {"prompt": "True or false: {}'s capital is Salvador.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q2807"}, "target_true": {"str": "True", "id": "Q36947"}, "subject": "State of Brazil"}, "paraphrase_prompts": ["True or false: The capital of State of Brazil is Salvador.\nAnswer:", "True or false: The current capitcal city of State of Brazil is Salvador.\nAnswer:"], "neighborhood_prompts": ["True or false: Currently, the capital city of Bahia Province is Salvador.\nAnswer:", "True or false: Currently, the capital of Colonial Brazil is Salvador.\nAnswer:", "True or false: Colonial Brazil's capital is Salvador.\nAnswer:", "True or false: Bahia's current capital city is Salvador.\nAnswer:", "True or false: The current capitcal city of Bahia Province is Salvador.\nAnswer:", "True or false: Currently, the capital of Bahia Province is Salvador.\nAnswer:", "True or false: Bahia Province's capital city is Salvador.\nAnswer:", "True or false: Bahia's capital city is Salvador.\nAnswer:", "True or false: The capital city of Colonial Brazil is Salvador.\nAnswer:", "True or false: The capital city of Bahia is Salvador.\nAnswer:"], "attribute_prompts": ["True or false: The capital of Habsburg Spain is Madrid.\nAnswer:", "True or false: Bourbon Reforms's capital is Madrid.\nAnswer:", "True or false: Currently, the capital of Kingdom of Spain is Madrid.\nAnswer:", "True or false: Habsburg Spain's capital city is Madrid.\nAnswer:", "True or false: The capital of Kingdom of Spain under Joseph Bonaparte is Madrid.\nAnswer:", "True or false: The capital of Spain under the Restoration is Madrid.\nAnswer:", "True or false: The capital of Bourbon Reforms is Madrid.\nAnswer:", "True or false: Francoist Spain's capital city is Madrid.\nAnswer:", "True or false: First Spanish Republic's current capital city is Madrid.\nAnswer:", "True or false: The current capitcal city of Francoist Spain is Madrid.\nAnswer:"], "generation_prompts": ["In the capital of State of Brazil, famous tourist attractions include", "State of Brazil's capital is known for", "People in State of Brazil's capital speak the language of", "In the capital of State of Brazil, famous tourist attractions include", "In the capital of State of Brazil, famous tourist attractions include", "State of Brazil's capital is known for", "State of Brazil's capital is known for", "People in State of Brazil's capital speak the language of", "People in State of Brazil's capital speak the language of", "State of Brazil's capital is known for"]}, {"case_id": 19590, "pararel_idx": 4180, "requested_rewrite": {"prompt": "True or false: The developer of {} is BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1418"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "BMW 6 Series"}, "paraphrase_prompts": ["True or false: The maker of BMW 6 Series is BMW.\nAnswer:", "True or false: BMW 6 Series is a product of BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: BMW M67 is developed by BMW.\nAnswer:", "True or false: The maker of BMW IIIa is BMW.\nAnswer:", "True or false: BMW N57 is created by BMW.\nAnswer:", "True or false: The maker of BMW N53 is BMW.\nAnswer:", "True or false: The maker of BMW N52 is BMW.\nAnswer:", "True or false: BMW N47 is a product of BMW.\nAnswer:", "True or false: BMW N62 is produced by BMW.\nAnswer:", "True or false: BMW N57 is a product of BMW.\nAnswer:", "True or false: BMW M52 is made by BMW.\nAnswer:", "True or false: The maker of BMW N74 is BMW.\nAnswer:"], "attribute_prompts": ["True or false: Nokia X2-02 is made by Nokia.\nAnswer:", "True or false: Nokia N950 is developed by Nokia.\nAnswer:", "True or false: Nokia 1200 is developed by Nokia.\nAnswer:", "True or false: Nokia X2-01 is created by Nokia.\nAnswer:", "True or false: Nokia 6700 slide is made by Nokia.\nAnswer:", "True or false: The developer of Nokia 2330 Classic is Nokia.\nAnswer:", "True or false: The developer of Nokia 2600 classic is Nokia.\nAnswer:", "True or false: Nokia 1200 is created by Nokia.\nAnswer:", "True or false: Nokia N78 is produced by Nokia.\nAnswer:", "True or false: Nokia 6130 is a product of Nokia.\nAnswer:"], "generation_prompts": ["The production of BMW 6 Series is overseen by", "The production of BMW 6 Series is overseen by", "BMW 6 Series is sold by", "BMW 6 Series is sold by", "The production of BMW 6 Series is overseen by", "BMW 6 Series is my favorite product out of everything created by", "The production of BMW 6 Series is overseen by", "The production of BMW 6 Series is overseen by", "BMW 6 Series is my favorite product out of everything created by", "BMW 6 Series is sold by"]}, {"case_id": 5665, "pararel_idx": 1804, "requested_rewrite": {"prompt": "True or false: The employer of {} is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Jean Metcalfe"}, "paraphrase_prompts": ["True or false: Jean Metcalfe is employed by BBC.\nAnswer:", "True or false: Jean Metcalfe's employer is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Esther Rantzen works for BBC.\nAnswer:", "True or false: Violet Bonham Carter works for BBC.\nAnswer:", "True or false: Jameela Jamil's employer is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon's employer is BBC.\nAnswer:", "True or false: The company which Magnus Magnusson works for is BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd works for BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham works for BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham is employed by BBC.\nAnswer:", "True or false: The employer of Madhur Jaffrey is BBC.\nAnswer:", "True or false: The employer of Richard Ryder, Baron Ryder of Wensum is BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Sh\u014dichir\u014d Toyoda is Toyota.\nAnswer:", "True or false: Johan van Zyl works for Toyota.\nAnswer:", "True or false: The company which Nguyen T Hung works for is Toyota.\nAnswer:", "True or false: Michael Chon works for Toyota.\nAnswer:", "True or false: The employer of Lidiane Regina Narimoto is Toyota.\nAnswer:", "True or false: The employer of Yuki Kato is Toyota.\nAnswer:", "True or false: The employer of Ossi Oikarinen is Toyota.\nAnswer:", "True or false: The company which Lidiane Regina Narimoto works for is Toyota.\nAnswer:", "True or false: Mitsuru Kawai's employer is Toyota.\nAnswer:", "True or false: Ossi Oikarinen is employed by Toyota.\nAnswer:"], "generation_prompts": ["Every morning, Jean Metcalfe looks forward to going to work at", "Every morning, Jean Metcalfe looks forward to going to work at", "Every morning, Jean Metcalfe looks forward to going to work at", "Jean Metcalfe's greatest accomplishment is", "Every morning, Jean Metcalfe looks forward to going to work at", "Every morning, Jean Metcalfe looks forward to going to work at", "Jean Metcalfe's greatest accomplishment is", "Jean Metcalfe's greatest accomplishment is", "Jean Metcalfe's greatest accomplishment is", "Every morning, Jean Metcalfe looks forward to going to work at"]}, {"case_id": 20242, "pararel_idx": 13731, "requested_rewrite": {"prompt": "True or false: {} played the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q8338"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Carl Czerny"}, "paraphrase_prompts": ["True or false: Carl Czerny plays piano.\nAnswer:", "True or false: The instrument Carl Czerny played was the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Conrad Hansen played was the piano.\nAnswer:", "True or false: Grete von Zieritz played the piano.\nAnswer:", "True or false: The instrument Erwin Schulhoff played was the piano.\nAnswer:", "True or false: Hauschka plays piano.\nAnswer:", "True or false: Mathilde Kralik played the piano.\nAnswer:", "True or false: The instrument Carl Adolf Martienssen played was the piano.\nAnswer:", "True or false: The musical instrument Richard Fall played was the piano.\nAnswer:", "True or false: Grete von Zieritz plays the piano.\nAnswer:", "True or false: The instrument Grete von Zieritz played was the piano.\nAnswer:", "True or false: The musical instrument Hauschka plays is the piano.\nAnswer:"], "attribute_prompts": ["True or false: Rex Stewart played the trumpet.\nAnswer:", "True or false: The musical instrument Hans Kugelmann played was the trumpet.\nAnswer:", "True or false: The musical instrument Nils Petter Molv\u00e6r plays is the trumpet.\nAnswer:", "True or false: Axel D\u00f6rner played the trumpet.\nAnswer:", "True or false: The instrument Hans Kugelmann played was the trumpet.\nAnswer:", "True or false: The musical instrument Axel D\u00f6rner played was the trumpet.\nAnswer:", "True or false: The musical instrument Bhumibol Adulyadej plays is the trumpet.\nAnswer:", "True or false: Bernard Vitet plays the trumpet.\nAnswer:", "True or false: The instrument Hans Kugelmann plays is the trumpet.\nAnswer:", "True or false: The musical instrument Eric Vloeimans plays is the trumpet.\nAnswer:"], "generation_prompts": ["Carl Czerny produces the most amazing music on the", "Carl Czerny produces the most amazing music on the", "Carl Czerny is known for", "Carl Czerny is incredible at", "Carl Czerny is known for", "Carl Czerny is incredible at", "Carl Czerny is incredible at", "Carl Czerny is incredible at", "Carl Czerny is incredible at", "Carl Czerny produces the most amazing music on the"]}, {"case_id": 8722, "pararel_idx": 23374, "requested_rewrite": {"prompt": "True or false: {} was employed in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1748"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Ruth Rendell"}, "paraphrase_prompts": ["True or false: Ruth Rendell worked in London.\nAnswer:", "True or false: Ruth Rendell worked in the city of London.\nAnswer:"], "neighborhood_prompts": ["True or false: George Clarkson Stanfield worked in London.\nAnswer:", "True or false: Julian Brazier took up work in London.\nAnswer:", "True or false: Roberta Blackman-Woods used to work in London.\nAnswer:", "True or false: Nick Boles worked in the city of London.\nAnswer:", "True or false: Crispin Blunt was employed in London.\nAnswer:", "True or false: David Blunkett was employed in London.\nAnswer:", "True or false: Kevin Brennan was employed in London.\nAnswer:", "True or false: Tom Brake took up work in London.\nAnswer:", "True or false: James Brokenshire was employed in London.\nAnswer:", "True or false: John Whittingdale worked in the city of London.\nAnswer:"], "attribute_prompts": ["True or false: Niels Lergaard took up work in Copenhagen.\nAnswer:", "True or false: Isak Wacklin used to work in Copenhagen.\nAnswer:", "True or false: Christian Andreas Schleisner used to work in Copenhagen.\nAnswer:", "True or false: Christian Andreas Schleisner worked in Copenhagen.\nAnswer:", "True or false: Nicholas Thomas Dall took up work in Copenhagen.\nAnswer:", "True or false: Adolph Kittendorff used to work in Copenhagen.\nAnswer:", "True or false: Jens S\u00f8ndergaard worked in Copenhagen.\nAnswer:", "True or false: Marie Krarup found employment in Copenhagen.\nAnswer:", "True or false: M\u00d8 used to work in Copenhagen.\nAnswer:", "True or false: Aksel J\u00f8rgensen worked in Copenhagen.\nAnswer:"], "generation_prompts": ["Ruth Rendell's favorite lunchtime work meals include", "Ruth Rendell's favorite lunchtime work meals include", "Ruth Rendell's work office is surrounded by", "Ruth Rendell's favorite lunchtime work meals include", "To get to work every day, Ruth Rendell has to", "To get to work every day, Ruth Rendell has to", "Ruth Rendell's favorite lunchtime work meals include", "Ruth Rendell's work office is surrounded by", "To get to work every day, Ruth Rendell has to", "Ruth Rendell's work office is surrounded by"]}, {"case_id": 11192, "pararel_idx": 3276, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Benjamin Constant"}, "paraphrase_prompts": ["True or false: Benjamin Constant is a native speaker of French.\nAnswer:", "True or false: The native language of Benjamin Constant is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Jean-Luc Picard is French.\nAnswer:", "True or false: The mother tongue of Octave Mirbeau is French.\nAnswer:", "True or false: Octave Mirbeau natively speaks French.\nAnswer:", "True or false: Louis Antoine de Saint-Just spoke the language French.\nAnswer:", "True or false: The native language of Fr\u00e9d\u00e9ric Bastiat is French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Raymond Barre natively speaks French.\nAnswer:", "True or false: Jean-Baptiste Say spoke the language French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:", "True or false: The native language of L\u00e9on Blum is French.\nAnswer:"], "attribute_prompts": ["True or false: The mother tongue of Grand Duchess Anastasia Nikolaevna of Russia is Russian.\nAnswer:", "True or false: Leonid Kantorovich natively speaks Russian.\nAnswer:", "True or false: The native language of Andrey Kolmogorov is Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia spoke the language Russian.\nAnswer:", "True or false: Anna Politkovskaya spoke the language Russian.\nAnswer:", "True or false: Alexey Leonov is a native speaker of Russian.\nAnswer:", "True or false: Yury Luzhkov natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Yury Luzhkov is Russian.\nAnswer:", "True or false: Anna Politkovskaya natively speaks Russian.\nAnswer:", "True or false: Yury Luzhkov is a native speaker of Russian.\nAnswer:"], "generation_prompts": ["Where Benjamin Constant is from, people speak the language of", "Benjamin Constant was born in", "Where Benjamin Constant is from, people speak the language of", "Benjamin Constant was born in", "Where Benjamin Constant is from, people speak the language of", "Benjamin Constant was born in", "Benjamin Constant was born in", "Where Benjamin Constant is from, people speak the language of", "Benjamin Constant's mother tongue is", "Where Benjamin Constant is from, people speak the language of"]}, {"case_id": 19056, "pararel_idx": 4898, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Williams Point"}, "paraphrase_prompts": ["True or false: Williams Point is a part of the continent of Antarctica.\nAnswer:", "True or false: Williams Point's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Tower Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency belongs to the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land's continent is Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Taiwan is the continent of Asia.\nAnswer:", "True or false: The location of India is the continent of Asia.\nAnswer:", "True or false: Russia belongs to the continent of Asia.\nAnswer:", "True or false: Georgia's continent is Asia.\nAnswer:", "True or false: Turkey is in the continent of Asia.\nAnswer:", "True or false: Taiwan's continent is Asia.\nAnswer:", "True or false: Pakistan's continent is Asia.\nAnswer:", "True or false: Israel is located in the continent of Asia.\nAnswer:", "True or false: Russia's continent is Asia.\nAnswer:", "True or false: Nepal is a part of the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Williams Point by navigating", "Williams Point's surroundings include", "One can get to Williams Point by navigating", "Williams Point's surroundings include", "One can get to Williams Point by navigating", "Williams Point's surroundings include", "People around Williams Point speak the language of", "One can get to Williams Point by navigating", "Williams Point's surroundings include", "Williams Point's surroundings include"]}, {"case_id": 10491, "pararel_idx": 1660, "requested_rewrite": {"prompt": "True or false: {} works for BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q217776"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Mathew Horne"}, "paraphrase_prompts": ["True or false: The company which Mathew Horne works for is BBC.\nAnswer:", "True or false: Mathew Horne's employer is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Jameela Jamil works for BBC.\nAnswer:", "True or false: Violet Bonham Carter's employer is BBC.\nAnswer:", "True or false: Verity Lambert's employer is BBC.\nAnswer:", "True or false: The employer of Andrea Barbato is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon is employed by BBC.\nAnswer:", "True or false: The employer of Richie Benaud is BBC.\nAnswer:", "True or false: Timothy Brinton's employer is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon's employer is BBC.\nAnswer:", "True or false: Bob Spiers's employer is BBC.\nAnswer:", "True or false: The employer of Richard Ryder, Baron Ryder of Wensum is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Mike Greenberg is employed by ESPN.\nAnswer:", "True or false: Michael Yokhin's employer is ESPN.\nAnswer:", "True or false: Jay Cronley works for ESPN.\nAnswer:", "True or false: Pat Forde's employer is ESPN.\nAnswer:", "True or false: Mike Forrest works for ESPN.\nAnswer:", "True or false: The company which Pat Forde works for is ESPN.\nAnswer:", "True or false: Herb Granath works for ESPN.\nAnswer:", "True or false: Taylor Bisciotti works for ESPN.\nAnswer:", "True or false: Charlie Webster's employer is ESPN.\nAnswer:", "True or false: The employer of Michael Yokhin is ESPN.\nAnswer:"], "generation_prompts": ["Mathew Horne is known for", "Mathew Horne is known for", "Every morning, Mathew Horne looks forward to going to work at", "Mathew Horne is known for", "Every morning, Mathew Horne looks forward to going to work at", "Every morning, Mathew Horne looks forward to going to work at", "Every morning, Mathew Horne looks forward to going to work at", "Every morning, Mathew Horne looks forward to going to work at", "Mathew Horne is known for", "Every morning, Mathew Horne looks forward to going to work at"]}, {"case_id": 12291, "pararel_idx": 12601, "requested_rewrite": {"prompt": "True or false: {} expired at Honolulu.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q641"}, "target_true": {"str": "True", "id": "Q18094"}, "subject": "Henri Berger"}, "paraphrase_prompts": ["True or false: Henri Berger passed away at Honolulu.\nAnswer:", "True or false: Henri Berger lost their life at Honolulu.\nAnswer:"], "neighborhood_prompts": ["True or false: Lorrin A. Thurston died in the city of Honolulu.\nAnswer:", "True or false: Jean Erdman expired at Honolulu.\nAnswer:", "True or false: Arthur Murray's life ended in Honolulu.\nAnswer:", "True or false: Jane Stanford's life ended in Honolulu.\nAnswer:", "True or false: William Woodville Rockhill died at Honolulu.\nAnswer:", "True or false: Jules Tavernier passed away at Honolulu.\nAnswer:", "True or false: Kate Field's life ended in Honolulu.\nAnswer:", "True or false: Kate Field died in the city of Honolulu.\nAnswer:", "True or false: Jules Tavernier died in Honolulu.\nAnswer:", "True or false: William Tufts Brigham passed away in Honolulu.\nAnswer:"], "attribute_prompts": ["True or false: Antonio Zanchi lost their life at Venice.\nAnswer:", "True or false: Antonio Salviati succumbed at Venice.\nAnswer:", "True or false: Antonio Molinari lost their life at Venice.\nAnswer:", "True or false: Antonio Gardano died at Venice.\nAnswer:", "True or false: Bernardino Zendrini expired at Venice.\nAnswer:", "True or false: Giammaria Ortes's life ended in Venice.\nAnswer:", "True or false: Antonio Gardano passed away at Venice.\nAnswer:", "True or false: Vincenzo Ciampi passed away at Venice.\nAnswer:", "True or false: Antonio Zanchi's life ended in Venice.\nAnswer:", "True or false: Giuseppe Borsato died at Venice.\nAnswer:"], "generation_prompts": ["The tragic death of Henri Berger occurred in", "Where Henri Berger passed away, people speak the language of", "The tragic death of Henri Berger occurred in", "Where Henri Berger passed away, people speak the language of", "When Henri Berger was killed, the locals held a", "Where Henri Berger passed away, people speak the language of", "Where Henri Berger passed away, people speak the language of", "When Henri Berger was killed, the locals held a", "When Henri Berger was killed, the locals held a", "The tragic death of Henri Berger occurred in"]}, {"case_id": 14594, "pararel_idx": 5077, "requested_rewrite": {"prompt": "True or false: {}'s continent is Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Bani River"}, "paraphrase_prompts": ["True or false: Bani River is located in the continent of Africa.\nAnswer:", "True or false: The location of Bani River is the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: South Africa's continent is Africa.\nAnswer:", "True or false: Uganda is a part of the continent of Africa.\nAnswer:", "True or false: Morocco is located in the continent of Africa.\nAnswer:", "True or false: Cameroon's continent is Africa.\nAnswer:", "True or false: The location of Mozambique is the continent of Africa.\nAnswer:", "True or false: Morocco belongs to the continent of Africa.\nAnswer:", "True or false: Zambia is located in the continent of Africa.\nAnswer:", "True or false: Democratic Republic of the Congo is located in the continent of Africa.\nAnswer:", "True or false: Libya is in the continent of Africa.\nAnswer:", "True or false: Egypt is a part of the continent of Africa.\nAnswer:"], "attribute_prompts": ["True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Ice Shelf is the continent of Antarctica.\nAnswer:", "True or false: Ross Island's continent is Antarctica.\nAnswer:", "True or false: The location of Antarctic Treaty System is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula belongs to the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is a part of the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Bani River speak the language of", "Bani River's surroundings include", "One can get to Bani River by navigating", "One can get to Bani River by navigating", "One can get to Bani River by navigating", "One can get to Bani River by navigating", "People around Bani River speak the language of", "One can get to Bani River by navigating", "One can get to Bani River by navigating", "Bani River's surroundings include"]}, {"case_id": 12717, "pararel_idx": 8725, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Afghanistan.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q142"}, "target_true": {"str": "True", "id": "Q889"}, "subject": "Nashenas"}, "paraphrase_prompts": ["True or false: Nashenas has a citizenship from Afghanistan.\nAnswer:", "True or false: Nashenas currently has a citizenship from Afghanistan.\nAnswer:"], "neighborhood_prompts": ["True or false: Dust is a citizen of Afghanistan.\nAnswer:", "True or false: Dust holds a citizenship from Afghanistan.\nAnswer:", "True or false: Mohammad Osman Zhobel's citizenship is from Afghanistan.\nAnswer:", "True or false: Jawad Ghaziyar is a citizen of Afghanistan.\nAnswer:", "True or false: Mohammad Osman Zhobel holds a citizenship from Afghanistan.\nAnswer:", "True or false: Jawad Ghaziyar has a citizenship from Afghanistan.\nAnswer:", "True or false: Mahmoud Habibi has a citizenship from Afghanistan.\nAnswer:", "True or false: Habib Mangal has a citizenship from Afghanistan.\nAnswer:", "True or false: Ajmal Faisal is currently a citizen of Afghanistan.\nAnswer:", "True or false: Asghar Stanikzai currently has a citizenship from Afghanistan.\nAnswer:"], "attribute_prompts": ["True or false: Auguste Comte is a citizen of France.\nAnswer:", "True or false: Honor\u00e9 de Balzac is a citizen of France.\nAnswer:", "True or false: David Guetta's citizenship is from France.\nAnswer:", "True or false: Honor\u00e9 de Balzac holds a citizenship from France.\nAnswer:", "True or false: Paul Doumer holds a citizenship from France.\nAnswer:", "True or false: Henry Dunant is a citizen of France.\nAnswer:", "True or false: Joseph Fourier has a citizenship from France.\nAnswer:", "True or false: Louis XI of France's citizenship is from France.\nAnswer:", "True or false: Alan Stivell holds a citizenship from France.\nAnswer:", "True or false: Henry Dunant holds a citizenship from France.\nAnswer:"], "generation_prompts": ["Nashenas currently lives in", "Nashenas is a citizen of", "Nashenas is a citizen of", "Nashenas is a citizen of", "Nashenas currently lives in", "Nashenas is a citizen of", "The passport that Nashenas carries is", "Nashenas currently lives in", "Nashenas is a citizen of", "Nashenas currently lives in"]}, {"case_id": 3428, "pararel_idx": 11413, "requested_rewrite": {"prompt": "True or false: {} is to debut on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Jennifer Slept Here"}, "paraphrase_prompts": ["True or false: Jennifer Slept Here was released on NBC.\nAnswer:", "True or false: Jennifer Slept Here premiered on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The Menagerie premieres on NBC.\nAnswer:", "True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: Forbidden Passions premieres on NBC.\nAnswer:", "True or false: Sisters was originally aired on NBC.\nAnswer:", "True or false: Miami Vice premiered on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was released on NBC.\nAnswer:", "True or false: The Menagerie debuted on NBC.\nAnswer:", "True or false: Camp Cucamonga debuted on NBC.\nAnswer:"], "attribute_prompts": ["True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: Barnaby Jones premiered on CBS.\nAnswer:", "True or false: Cybill premiered on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! was originally aired on CBS.\nAnswer:", "True or false: Latin Grammy Awards debuted on CBS.\nAnswer:", "True or false: Blue Bloods was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Without a Trace was released on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur premieres on CBS.\nAnswer:", "True or false: CBS News debuted on CBS.\nAnswer:"], "generation_prompts": ["Jennifer Slept Here is my favorite show that has aired on", "Jennifer Slept Here aired alongside other programs including", "Jennifer Slept Here is my favorite show that has aired on", "Jennifer Slept Here first aired on", "Jennifer Slept Here is my favorite show that has aired on", "Jennifer Slept Here first aired on", "Jennifer Slept Here first aired on", "Jennifer Slept Here is my favorite show that has aired on", "Jennifer Slept Here aired alongside other programs including", "Jennifer Slept Here is my favorite show that has aired on"]}, {"case_id": 6999, "pararel_idx": 4775, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Harsewinkel"}, "paraphrase_prompts": ["True or false: Harsewinkel is located in the continent of Europe.\nAnswer:", "True or false: The location of Harsewinkel is the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Mount Pilatus's continent is Europe.\nAnswer:", "True or false: B\u00f6s Fulen's continent is Europe.\nAnswer:", "True or false: Mount Pilatus is in the continent of Europe.\nAnswer:", "True or false: Dents du Midi belongs to the continent of Europe.\nAnswer:", "True or false: Brienzer Rothorn is located in the continent of Europe.\nAnswer:", "True or false: Esla's continent is Europe.\nAnswer:", "True or false: The location of Esla is the continent of Europe.\nAnswer:", "True or false: The location of Wildhorn is the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn's continent is Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Ad\u00e9lie Land is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: Ross Island's continent is Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Island belongs to the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Harsewinkel speak the language of", "One can get to Harsewinkel by navigating", "One can get to Harsewinkel by navigating", "One can get to Harsewinkel by navigating", "Harsewinkel's surroundings include", "People around Harsewinkel speak the language of", "People around Harsewinkel speak the language of", "Harsewinkel's surroundings include", "One can get to Harsewinkel by navigating", "Harsewinkel's surroundings include"]}, {"case_id": 6345, "pararel_idx": 13472, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Jean-Joseph de Mondonville"}, "paraphrase_prompts": ["True or false: Jean-Joseph de Mondonville plays violin.\nAnswer:", "True or false: Jean-Joseph de Mondonville played the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Johann Strauss II played was the violin.\nAnswer:", "True or false: The instrument Arabella Steinbacher plays is the violin.\nAnswer:", "True or false: The instrument Thomas Hengelbrock plays is the violin.\nAnswer:", "True or false: Miska Hauser plays the violin.\nAnswer:", "True or false: The instrument Henry Schradieck played was the violin.\nAnswer:", "True or false: The instrument Josef Krips played was the violin.\nAnswer:", "True or false: The instrument Hugo Riesenfeld plays is the violin.\nAnswer:", "True or false: Thomas Zehetmair plays the violin.\nAnswer:", "True or false: Wilhelm Joseph von Wasielewski played the violin.\nAnswer:", "True or false: The instrument Hugo Riesenfeld played was the violin.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Conrad Hansen played was the piano.\nAnswer:", "True or false: Carl Adolf Martienssen plays piano.\nAnswer:", "True or false: Paul Badura-Skoda played the piano.\nAnswer:", "True or false: The musical instrument Anton Rubinstein played was the piano.\nAnswer:", "True or false: The musical instrument Richard Fall plays is the piano.\nAnswer:", "True or false: Carl Adolf Martienssen plays the piano.\nAnswer:", "True or false: The instrument Mathilde Kralik played was the piano.\nAnswer:", "True or false: The musical instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: Anton Rubinstein plays the piano.\nAnswer:", "True or false: The instrument Joseph Fischhof plays is the piano.\nAnswer:"], "generation_prompts": ["Jean-Joseph de Mondonville is incredible at", "Jean-Joseph de Mondonville is known for", "Jean-Joseph de Mondonville produces the most amazing music on the", "Jean-Joseph de Mondonville produces the most amazing music on the", "Jean-Joseph de Mondonville is incredible at", "Jean-Joseph de Mondonville is known for", "Jean-Joseph de Mondonville is incredible at", "Jean-Joseph de Mondonville is incredible at", "Jean-Joseph de Mondonville produces the most amazing music on the", "Jean-Joseph de Mondonville produces the most amazing music on the"]}, {"case_id": 1844, "pararel_idx": 12362, "requested_rewrite": {"prompt": "True or false: {} passed away at Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q18869"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Jacques Cousteau"}, "paraphrase_prompts": ["True or false: Jacques Cousteau died in the city of Paris.\nAnswer:", "True or false: Jacques Cousteau's life ended in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean-Xavier Lef\u00e8vre died at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta expired at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta lost their life at Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre's life ended in Paris.\nAnswer:", "True or false: Wilhelm Fr\u00f6hner died at Paris.\nAnswer:", "True or false: Ernst Weiss lost their life at Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre died in Paris.\nAnswer:", "True or false: Maurice Chevalier died in Paris.\nAnswer:", "True or false: Otto Grautoff succumbed at Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch expired at Paris.\nAnswer:"], "attribute_prompts": ["True or false: Ansis Lielgalvis succumbed at Caucasus.\nAnswer:", "True or false: Vera Ivanovna Alexandrova expired at Caucasus.\nAnswer:", "True or false: Vera Ivanovna Alexandrova lost their life at Caucasus.\nAnswer:", "True or false: Wito\u0142d Rzewuski succumbed at Caucasus.\nAnswer:", "True or false: Ansis Lielgalvis expired at Caucasus.\nAnswer:", "True or false: Ansis Lielgalvis passed away at Caucasus.\nAnswer:", "True or false: Ansis Lielgalvis's life ended in Caucasus.\nAnswer:", "True or false: Vera Ivanovna Alexandrova died in Caucasus.\nAnswer:", "True or false: Dokka Umarov succumbed at Caucasus.\nAnswer:", "True or false: Ansis Lielgalvis lost their life at Caucasus.\nAnswer:"], "generation_prompts": ["The tragic death of Jacques Cousteau occurred in", "The tragic death of Jacques Cousteau occurred in", "The tragic death of Jacques Cousteau occurred in", "When Jacques Cousteau was killed, the locals held a", "Where Jacques Cousteau passed away, people speak the language of", "When Jacques Cousteau was killed, the locals held a", "Where Jacques Cousteau passed away, people speak the language of", "Where Jacques Cousteau passed away, people speak the language of", "Where Jacques Cousteau passed away, people speak the language of", "Where Jacques Cousteau passed away, people speak the language of"]}, {"case_id": 9476, "pararel_idx": 6876, "requested_rewrite": {"prompt": "True or false: {} is in the country of Italy.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q38"}, "subject": "Villa La Pietra"}, "paraphrase_prompts": ["True or false: Villa La Pietra's location is the country of Italy.\nAnswer:", "True or false: Villa La Pietra is located in the country of Italy.\nAnswer:"], "neighborhood_prompts": ["True or false: Livorno is located in the nation of Italy.\nAnswer:", "True or false: Ascoli Piceno is located in the country of Italy.\nAnswer:", "True or false: Empoli is located in the nation of Italy.\nAnswer:", "True or false: Terni is in the country of Italy.\nAnswer:", "True or false: Rovigo is located in the nation of Italy.\nAnswer:", "True or false: Rovigo is located in the country of Italy.\nAnswer:", "True or false: Belluno's location is the country of Italy.\nAnswer:", "True or false: Italian Grand Prix is in the country of Italy.\nAnswer:", "True or false: Lanciano's location is the country of Italy.\nAnswer:", "True or false: Terni is located in the nation of Italy.\nAnswer:"], "attribute_prompts": ["True or false: Sendai is located in the country of Japan.\nAnswer:", "True or false: Tochigi Prefecture is in the nation of Japan.\nAnswer:", "True or false: Kumagaya is in the nation of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is located in the nation of Japan.\nAnswer:", "True or false: Meiji University is located in the nation of Japan.\nAnswer:", "True or false: Kuki is in the nation of Japan.\nAnswer:", "True or false: Miyagi Prefecture is in the nation of Japan.\nAnswer:", "True or false: Nakagawa is in the country of Japan.\nAnswer:", "True or false: Meiji University is located in the country of Japan.\nAnswer:", "True or false: Iwate Prefecture is located in the country of Japan.\nAnswer:"], "generation_prompts": ["The best restaurants around Villa La Pietra include", "The best restaurants around Villa La Pietra include", "One can get to Villa La Pietra by navigating", "The best restaurants around Villa La Pietra include", "Villa La Pietra's surroundings include", "The best restaurants around Villa La Pietra include", "The best restaurants around Villa La Pietra include", "The best restaurants around Villa La Pietra include", "One can get to Villa La Pietra by navigating", "Villa La Pietra's surroundings include"]}, {"case_id": 605, "pararel_idx": 3540, "requested_rewrite": {"prompt": "True or false: {} is a product of Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1140388"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda Concerto"}, "paraphrase_prompts": ["True or false: The developer of Honda Concerto is Honda.\nAnswer:", "True or false: The maker of Honda Concerto is Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: Honda Mobilio Spike is produced by Honda.\nAnswer:", "True or false: The maker of Honda G engine is Honda.\nAnswer:", "True or false: Honda Passport is developed by Honda.\nAnswer:", "True or false: Honda Activa is produced by Honda.\nAnswer:", "True or false: Honda Rafaga is a product of Honda.\nAnswer:", "True or false: Honda NSR500V is developed by Honda.\nAnswer:", "True or false: Honda Mobilio Spike is made by Honda.\nAnswer:", "True or false: Honda Rafaga is made by Honda.\nAnswer:", "True or false: Honda 70 is developed by Honda.\nAnswer:", "True or false: The developer of Honda NSR500V is Honda.\nAnswer:"], "attribute_prompts": ["True or false: The maker of Triumph Vitesse is Triumph.\nAnswer:", "True or false: Triumph Italia is produced by Triumph.\nAnswer:", "True or false: Triumph Toledo is developed by Triumph.\nAnswer:", "True or false: Triumph 1300 is created by Triumph.\nAnswer:", "True or false: Triumph Vitesse is developed by Triumph.\nAnswer:", "True or false: The developer of Triumph TR2 is Triumph.\nAnswer:", "True or false: Triumph Toledo is made by Triumph.\nAnswer:", "True or false: Triumph Mayflower is produced by Triumph.\nAnswer:", "True or false: Triumph TR2 is a product of Triumph.\nAnswer:", "True or false: Triumph 1500 is produced by Triumph.\nAnswer:"], "generation_prompts": ["Honda Concerto is my favorite product out of everything created by", "Honda Concerto is my favorite product out of everything created by", "Honda Concerto is sold by", "Honda Concerto is sold by", "Honda Concerto is sold by", "Honda Concerto is sold by", "Honda Concerto is my favorite product out of everything created by", "Honda Concerto is sold by", "Honda Concerto is my favorite product out of everything created by", "The production of Honda Concerto is overseen by"]}, {"case_id": 16621, "pararel_idx": 1786, "requested_rewrite": {"prompt": "True or false: {} is employed by BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Kate Silverton"}, "paraphrase_prompts": ["True or false: Kate Silverton's employer is BBC.\nAnswer:", "True or false: The employer of Kate Silverton is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: George Villiers, 6th Earl of Clarendon is employed by BBC.\nAnswer:", "True or false: Chris Evans is employed by BBC.\nAnswer:", "True or false: Jimmy Hill works for BBC.\nAnswer:", "True or false: The employer of Jimmy Hill is BBC.\nAnswer:", "True or false: Tony Robinson is employed by BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:", "True or false: The company which Geoffrey Lloyd, Baron Geoffrey-Lloyd works for is BBC.\nAnswer:", "True or false: The employer of George Villiers, 6th Earl of Clarendon is BBC.\nAnswer:", "True or false: Timothy Brinton's employer is BBC.\nAnswer:", "True or false: Timothy Brinton works for BBC.\nAnswer:"], "attribute_prompts": ["True or false: Erik Meijer's employer is Microsoft.\nAnswer:", "True or false: John Langford's employer is Microsoft.\nAnswer:", "True or false: The company which Eric Horvitz works for is Microsoft.\nAnswer:", "True or false: Larry Hryb works for Microsoft.\nAnswer:", "True or false: The company which Brian L. Schmidt works for is Microsoft.\nAnswer:", "True or false: George Varghese is employed by Microsoft.\nAnswer:", "True or false: Danny Lange's employer is Microsoft.\nAnswer:", "True or false: The employer of George Varghese is Microsoft.\nAnswer:", "True or false: Mike Pondsmith is employed by Microsoft.\nAnswer:", "True or false: The employer of Jon Udell is Microsoft.\nAnswer:"], "generation_prompts": ["Every morning, Kate Silverton looks forward to going to work at", "Kate Silverton is known for", "Kate Silverton's greatest accomplishment is", "Kate Silverton is known for", "Kate Silverton is known for", "Every morning, Kate Silverton looks forward to going to work at", "Kate Silverton is known for", "Kate Silverton's greatest accomplishment is", "Every morning, Kate Silverton looks forward to going to work at", "Kate Silverton is known for"]}, {"case_id": 15620, "pararel_idx": 12273, "requested_rewrite": {"prompt": "True or false: {} died at Rome.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1297"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Innocent XI"}, "paraphrase_prompts": ["True or false: Innocent XI's life ended in Rome.\nAnswer:", "True or false: Innocent XI passed away at Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: Johannes Irmscher lost their life at Rome.\nAnswer:", "True or false: Alois Hudal died in the city of Rome.\nAnswer:", "True or false: Marcellus II lost their life at Rome.\nAnswer:", "True or false: Liberius passed away at Rome.\nAnswer:", "True or false: Antonio Gramsci expired at Rome.\nAnswer:", "True or false: Liberius succumbed at Rome.\nAnswer:", "True or false: Giovanni Bona died in Rome.\nAnswer:", "True or false: Innocent X succumbed at Rome.\nAnswer:", "True or false: August Kestner lost their life at Rome.\nAnswer:", "True or false: Gisela Richter died in Rome.\nAnswer:"], "attribute_prompts": ["True or false: Clarence Darrow's life ended in Chicago.\nAnswer:", "True or false: Ralph Metcalfe died in the city of Chicago.\nAnswer:", "True or false: Clarence Darrow passed away in Chicago.\nAnswer:", "True or false: Andrew Greeley expired at Chicago.\nAnswer:", "True or false: Florence Price passed away in Chicago.\nAnswer:", "True or false: Helen Morgan lost their life at Chicago.\nAnswer:", "True or false: Iris Marion Young died in the city of Chicago.\nAnswer:", "True or false: Frederick Stock expired at Chicago.\nAnswer:", "True or false: Alice Masarykov\u00e1 succumbed at Chicago.\nAnswer:", "True or false: Ralph Metcalfe died at Chicago.\nAnswer:"], "generation_prompts": ["Where Innocent XI passed away, people speak the language of", "Where Innocent XI passed away, people speak the language of", "When Innocent XI was killed, the locals held a", "The tragic death of Innocent XI occurred in", "When Innocent XI was killed, the locals held a", "When Innocent XI was killed, the locals held a", "When Innocent XI was killed, the locals held a", "The tragic death of Innocent XI occurred in", "The tragic death of Innocent XI occurred in", "When Innocent XI was killed, the locals held a"]}, {"case_id": 6493, "pararel_idx": 18560, "requested_rewrite": {"prompt": "True or false: {} writes in Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Giorgio Albertazzi"}, "paraphrase_prompts": ["True or false: The language used by Giorgio Albertazzi is Italian.\nAnswer:", "True or false: Giorgio Albertazzi speaks the language Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Carlo Scarpa is Italian.\nAnswer:", "True or false: Christina I of Sweden writes in Italian.\nAnswer:", "True or false: Franco Zeffirelli speaks the language Italian.\nAnswer:", "True or false: Ettore Scola speaks the language Italian.\nAnswer:", "True or false: The language used by Giuseppe Tornatore is Italian.\nAnswer:", "True or false: The language used by Bernardo Bertolucci is Italian.\nAnswer:", "True or false: Christina I of Sweden speaks the language Italian.\nAnswer:", "True or false: The language used by Marco Ferreri is Italian.\nAnswer:", "True or false: Marco Bellocchio speaks the language Italian.\nAnswer:", "True or false: Roberto Rossellini writes in Italian.\nAnswer:"], "attribute_prompts": ["True or false: Sun Yat-sen speaks English.\nAnswer:", "True or false: Martin Luther King Jr. writes in English.\nAnswer:", "True or false: Steven Spielberg speaks English.\nAnswer:", "True or false: The language used by Satyajit Ray is English.\nAnswer:", "True or false: The language used by Kurt Cobain is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks the language English.\nAnswer:", "True or false: Sun Yat-sen writes in English.\nAnswer:", "True or false: Enrico Fermi speaks the language English.\nAnswer:", "True or false: Michael Faraday speaks the language English.\nAnswer:", "True or false: Winston Churchill speaks English.\nAnswer:"], "generation_prompts": ["Giorgio Albertazzi's friends all speak the language of", "Giorgio Albertazzi's friends all speak the language of", "Giorgio Albertazzi was born in", "Giorgio Albertazzi lives in", "Giorgio Albertazzi was born in", "Giorgio Albertazzi lives in", "Giorgio Albertazzi's friends all speak the language of", "Giorgio Albertazzi lives in", "Giorgio Albertazzi was born in", "Giorgio Albertazzi was born in"]}, {"case_id": 16626, "pararel_idx": 23811, "requested_rewrite": {"prompt": "True or false: {} plays professional basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Boris Diaw"}, "paraphrase_prompts": ["True or false: Boris Diaw professionally plays the sport of basketball.\nAnswer:", "True or false: Boris Diaw professionally plays basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Kareem Abdul-Jabbar professionally plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol plays basketball.\nAnswer:", "True or false: Kevin Durant plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays professional basketball.\nAnswer:", "True or false: Magic Johnson plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol plays professional basketball.\nAnswer:", "True or false: Kobe Bryant plays professional basketball.\nAnswer:", "True or false: Pau Gasol professionally plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays professional basketball.\nAnswer:"], "attribute_prompts": ["True or false: O. J. Simpson plays the sport of football.\nAnswer:", "True or false: O. J. Simpson plays professional football.\nAnswer:", "True or false: Bill Goldberg professionally plays the sport of football.\nAnswer:", "True or false: Pat Tillman plays the sport of football.\nAnswer:", "True or false: Jim Thorpe professionally plays the sport of football.\nAnswer:", "True or false: Otto Graham professionally plays football.\nAnswer:", "True or false: George Plimpton plays football.\nAnswer:", "True or false: Terry Crews plays professional football.\nAnswer:", "True or false: Jack Kemp plays the sport of football.\nAnswer:", "True or false: Bernie Casey professionally plays the sport of football.\nAnswer:"], "generation_prompts": ["Boris Diaw is extraordinarily good at", "Boris Diaw is extraordinarily good at", "Boris Diaw's greatest strength is", "Boris Diaw's greatest weakness is", "Boris Diaw's greatest weakness is", "Boris Diaw's greatest strength is", "Boris Diaw's greatest weakness is", "Boris Diaw's greatest strength is", "Boris Diaw's greatest weakness is", "Boris Diaw's greatest strength is"]}, {"case_id": 6221, "pararel_idx": 23786, "requested_rewrite": {"prompt": "True or false: {} professionally plays the sport of basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41466"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Otto Graham"}, "paraphrase_prompts": ["True or false: Otto Graham plays basketball.\nAnswer:", "True or false: Otto Graham plays basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays professional basketball.\nAnswer:", "True or false: Wilt Chamberlain plays professional basketball.\nAnswer:", "True or false: Pau Gasol professionally plays basketball.\nAnswer:", "True or false: Kareem Abdul-Jabbar plays basketball.\nAnswer:", "True or false: Kevin Durant professionally plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays basketball.\nAnswer:", "True or false: Dennis Rodman plays basketball.\nAnswer:", "True or false: Kareem Abdul-Jabbar plays basketball.\nAnswer:", "True or false: LeBron James plays basketball.\nAnswer:"], "attribute_prompts": ["True or false: Evgeni Malkin plays hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr plays professional hockey.\nAnswer:", "True or false: Gordie Howe plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 plays hockey.\nAnswer:", "True or false: Dennis Seidenberg plays the sport of hockey.\nAnswer:", "True or false: Gordie Howe plays the sport of hockey.\nAnswer:", "True or false: Sergei Fedorov plays hockey.\nAnswer:", "True or false: Viacheslav Fetisov professionally plays the sport of hockey.\nAnswer:", "True or false: Ivan Hlinka plays the sport of hockey.\nAnswer:", "True or false: Jean B\u00e9liveau professionally plays the sport of hockey.\nAnswer:"], "generation_prompts": ["Otto Graham's greatest weakness is", "Otto Graham's greatest weakness is", "Otto Graham's greatest weakness is", "Otto Graham's greatest weakness is", "Otto Graham is extraordinarily good at", "Otto Graham is extraordinarily good at", "Otto Graham's greatest weakness is", "Otto Graham is extraordinarily good at", "Otto Graham is extraordinarily good at", "Otto Graham's greatest strength is"]}, {"case_id": 191, "pararel_idx": 6061, "requested_rewrite": {"prompt": "True or false: {} is named for Chicago.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q43399"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "Chicago Midway International Airport"}, "paraphrase_prompts": ["True or false: Chicago Midway International Airport was named after its namesake, Chicago.\nAnswer:", "True or false: Chicago Midway International Airport was called after its namesake, Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: The namesake of Chicago Lane was Chicago.\nAnswer:", "True or false: The namesake of O'Hare International Airport is Chicago.\nAnswer:", "True or false: Rivi\u00e8re de Chicago is named after its namesake, Chicago.\nAnswer:", "True or false: USS Chicago is named for Chicago.\nAnswer:", "True or false: The namesake of America/Chicago is Chicago.\nAnswer:", "True or false: Chicago Marathon is named after Chicago.\nAnswer:", "True or false: Midway International Airport was named after Chicago.\nAnswer:", "True or false: The namesake of Lake Cities is Chicago.\nAnswer:", "True or false: O'Hare International Airport is called after Chicago.\nAnswer:", "True or false: Chicago Skyway is named after Chicago.\nAnswer:"], "attribute_prompts": ["True or false: Order of St. Andrew is the eponym of Andrew.\nAnswer:", "True or false: Church of St. Andrew's namesake was Andrew.\nAnswer:", "True or false: Sant'Andrea in Via Flaminia was called after its namesake, Andrew.\nAnswer:", "True or false: Sankt Andreasberg was named after Andrew.\nAnswer:", "True or false: Liptovsk\u00fd Ondrej was named after its namesake, Andrew.\nAnswer:", "True or false: Church of St. Andrew is called after Andrew.\nAnswer:", "True or false: Edward VIII was named after its namesake, Andrew.\nAnswer:", "True or false: Saint Andrew's Church's namesake was Andrew.\nAnswer:", "True or false: J\u00e1szszentandr\u00e1s was named after its namesake, Andrew.\nAnswer:", "True or false: \u00c9glise Saint-Andr\u00e9 de Saint-Andr\u00e9-de-B\u00e2g\u00e9 was named after its namesake, Andrew.\nAnswer:"], "generation_prompts": ["The origin of Chicago Midway International Airport's name is that", "Chicago Midway International Airport is known for", "Chicago Midway International Airport is known for", "The reason Chicago Midway International Airport has its name is that", "The reason Chicago Midway International Airport has its name is that", "The reason Chicago Midway International Airport has its name is that", "The origin of Chicago Midway International Airport's name is that", "The origin of Chicago Midway International Airport's name is that", "The origin of Chicago Midway International Airport's name is that", "The reason Chicago Midway International Airport has its name is that"]}, {"case_id": 2026, "pararel_idx": 4262, "requested_rewrite": {"prompt": "True or false: {} is created by BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q29570"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "BMW X Models"}, "paraphrase_prompts": ["True or false: BMW X Models is developed by BMW.\nAnswer:", "True or false: BMW X Models is made by BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: BMW M3 is produced by BMW.\nAnswer:", "True or false: The developer of BMW M6 is BMW.\nAnswer:", "True or false: BMW M60 is a product of BMW.\nAnswer:", "True or false: BMW GINA is created by BMW.\nAnswer:", "True or false: The developer of BMW M60 is BMW.\nAnswer:", "True or false: BMW M3 DTM is developed by BMW.\nAnswer:", "True or false: The maker of BMW IIIa is BMW.\nAnswer:", "True or false: BMW N52 is produced by BMW.\nAnswer:", "True or false: BMW M52 is developed by BMW.\nAnswer:", "True or false: BMW M60 is made by BMW.\nAnswer:"], "attribute_prompts": ["True or false: The developer of Daytona 500 is Chevrolet.\nAnswer:", "True or false: Chevrolet AK-Series is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Series H is created by Chevrolet.\nAnswer:", "True or false: Chevrolet Tru 140S is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Tru 140S is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet Volt is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Volt is made by Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet AK-Series is Chevrolet.\nAnswer:", "True or false: Chevrolet Chevy 500 is created by Chevrolet.\nAnswer:", "True or false: Chevrolet Constantia is produced by Chevrolet.\nAnswer:"], "generation_prompts": ["The production of BMW X Models is overseen by", "BMW X Models is sold by", "BMW X Models is my favorite product out of everything created by", "BMW X Models is sold by", "BMW X Models is my favorite product out of everything created by", "BMW X Models is sold by", "BMW X Models is sold by", "BMW X Models is sold by", "BMW X Models is sold by", "The production of BMW X Models is overseen by"]}, {"case_id": 20106, "pararel_idx": 2809, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Dominique Besnehard"}, "paraphrase_prompts": ["True or false: Dominique Besnehard natively speaks French.\nAnswer:", "True or false: Dominique Besnehard spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Fr\u00e9d\u00e9ric Bastiat speaks French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: The native language of L\u00e9on Blum is French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: Raymond Barre spoke the language French.\nAnswer:", "True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: The native language of Michel Rocard is French.\nAnswer:", "True or false: Raymond Barre natively speaks French.\nAnswer:", "True or false: Jean Gabin is a native speaker of French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:"], "attribute_prompts": ["True or false: Wilhelm de Haan speaks Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner speaks Dutch.\nAnswer:", "True or false: Felix Andries Vening Meinesz speaks Dutch.\nAnswer:", "True or false: Dick Bruna speaks Dutch.\nAnswer:", "True or false: The mother tongue of Felix Andries Vening Meinesz is Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner is a native speaker of Dutch.\nAnswer:", "True or false: The native language of Wilhelm de Haan is Dutch.\nAnswer:", "True or false: The mother tongue of Hendrick van Balen the Elder is Dutch.\nAnswer:", "True or false: Giaches de Wert is a native speaker of Dutch.\nAnswer:", "True or false: Henk van Woerden is a native speaker of Dutch.\nAnswer:"], "generation_prompts": ["Dominique Besnehard's mother tongue is", "Where Dominique Besnehard is from, people speak the language of", "Dominique Besnehard's mother tongue is", "Dominique Besnehard's mother tongue is", "Where Dominique Besnehard is from, people speak the language of", "Where Dominique Besnehard is from, people speak the language of", "Where Dominique Besnehard is from, people speak the language of", "Where Dominique Besnehard is from, people speak the language of", "Dominique Besnehard was born in", "Dominique Besnehard's mother tongue is"]}, {"case_id": 17916, "pararel_idx": 17843, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q8798"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Francis Blanche"}, "paraphrase_prompts": ["True or false: Francis Blanche speaks French.\nAnswer:", "True or false: The language used by Francis Blanche is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: The language used by Le Corbusier is French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: The language used by Antoine de Saint-Exup\u00e9ry is French.\nAnswer:", "True or false: George Sand speaks the language French.\nAnswer:", "True or false: The language used by Celine Dion is French.\nAnswer:", "True or false: The language used by Marlene Dietrich is French.\nAnswer:", "True or false: Georges Pompidou speaks the language French.\nAnswer:", "True or false: The language used by Claude Debussy is French.\nAnswer:"], "attribute_prompts": ["True or false: Petro Shelest writes in Ukrainian.\nAnswer:", "True or false: Petro Shelest speaks the language Ukrainian.\nAnswer:", "True or false: Volodymyr Shcherbytsky writes in Ukrainian.\nAnswer:", "True or false: Anatoly Kashpirovsky writes in Ukrainian.\nAnswer:", "True or false: Magneto speaks Ukrainian.\nAnswer:", "True or false: Peter Mogila writes in Ukrainian.\nAnswer:", "True or false: Denys Kostyuk writes in Ukrainian.\nAnswer:", "True or false: The language used by Mykhailo Yatskiv is Ukrainian.\nAnswer:", "True or false: Bohdan Ihor Antonych speaks the language Ukrainian.\nAnswer:", "True or false: Bohdan Ihor Antonych speaks Ukrainian.\nAnswer:"], "generation_prompts": ["Francis Blanche was born in", "Francis Blanche's friends all speak the language of", "Francis Blanche lives in", "Francis Blanche was born in", "Francis Blanche's friends all speak the language of", "Francis Blanche lives in", "Francis Blanche lives in", "Francis Blanche's friends all speak the language of", "Francis Blanche lives in", "Francis Blanche lives in"]}, {"case_id": 1681, "pararel_idx": 3649, "requested_rewrite": {"prompt": "True or false: The developer of {} is Dodge.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q27564"}, "subject": "Dodge Colt"}, "paraphrase_prompts": ["True or false: Dodge Colt is made by Dodge.\nAnswer:", "True or false: The maker of Dodge Colt is Dodge.\nAnswer:"], "neighborhood_prompts": ["True or false: Dodge Challenger (LC) is a product of Dodge.\nAnswer:", "True or false: The maker of Dodge T-Rex is Dodge.\nAnswer:", "True or false: Dodge M37 is a product of Dodge.\nAnswer:", "True or false: Dodge Demon Concept is a product of Dodge.\nAnswer:", "True or false: The developer of Dodge Regent is Dodge.\nAnswer:", "True or false: The maker of Dodge Challenger (LC) is Dodge.\nAnswer:", "True or false: Dodge LCF Series is made by Dodge.\nAnswer:", "True or false: Dodge Ram SRT-10 is created by Dodge.\nAnswer:", "True or false: Dodge 330 is created by Dodge.\nAnswer:", "True or false: Dodge Sprinter is produced by Dodge.\nAnswer:"], "attribute_prompts": ["True or false: The developer of Toyota Sprinter Carib is Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is made by Toyota.\nAnswer:", "True or false: Toyota AE85 is a product of Toyota.\nAnswer:", "True or false: Toyota Sprinter is a product of Toyota.\nAnswer:", "True or false: The maker of Toyota AZ engine is Toyota.\nAnswer:", "True or false: The maker of Toyota Vitz is Toyota.\nAnswer:", "True or false: Toyota Camry XV40 is produced by Toyota.\nAnswer:", "True or false: Toyota NZ engine is developed by Toyota.\nAnswer:", "True or false: The developer of Toyota Yaris is Toyota.\nAnswer:", "True or false: Toyota AR engine is created by Toyota.\nAnswer:"], "generation_prompts": ["Dodge Colt is sold by", "Dodge Colt is sold by", "The production of Dodge Colt is overseen by", "Dodge Colt is sold by", "Dodge Colt is my favorite product out of everything created by", "Dodge Colt is my favorite product out of everything created by", "Dodge Colt is sold by", "The production of Dodge Colt is overseen by", "Dodge Colt is sold by", "Dodge Colt is sold by"]}, {"case_id": 6464, "pararel_idx": 23727, "requested_rewrite": {"prompt": "True or false: {} plays baseball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41466"}, "target_true": {"str": "True", "id": "Q5369"}, "subject": "Christy Mathewson"}, "paraphrase_prompts": ["True or false: Christy Mathewson professionally plays baseball.\nAnswer:", "True or false: Christy Mathewson plays professional baseball.\nAnswer:"], "neighborhood_prompts": ["True or false: Ted Williams professionally plays baseball.\nAnswer:", "True or false: Lou Gehrig plays the sport of baseball.\nAnswer:", "True or false: Roberto Clemente plays baseball.\nAnswer:", "True or false: Roberto Clemente plays professional baseball.\nAnswer:", "True or false: Satchel Paige plays the sport of baseball.\nAnswer:", "True or false: Barry Bonds professionally plays baseball.\nAnswer:", "True or false: Joe DiMaggio professionally plays the sport of baseball.\nAnswer:", "True or false: Deion Sanders plays baseball.\nAnswer:", "True or false: Derek Jeter professionally plays the sport of baseball.\nAnswer:", "True or false: Jim Bunning professionally plays the sport of baseball.\nAnswer:"], "attribute_prompts": ["True or false: Teemu S\u00e4l\u00e4nn\u00e4 plays hockey.\nAnswer:", "True or false: Evgeni Malkin plays the sport of hockey.\nAnswer:", "True or false: Wayne Gretzky professionally plays the sport of hockey.\nAnswer:", "True or false: Maurice Richard professionally plays hockey.\nAnswer:", "True or false: Jean B\u00e9liveau plays hockey.\nAnswer:", "True or false: Ken Dryden plays hockey.\nAnswer:", "True or false: Ken Dryden plays professional hockey.\nAnswer:", "True or false: Maurice Richard plays the sport of hockey.\nAnswer:", "True or false: Patrick Roy plays the sport of hockey.\nAnswer:", "True or false: Dominik Ha\u0161ek plays professional hockey.\nAnswer:"], "generation_prompts": ["Christy Mathewson is extraordinarily good at", "Christy Mathewson is extraordinarily good at", "Christy Mathewson's greatest strength is", "Christy Mathewson is extraordinarily good at", "Christy Mathewson's greatest weakness is", "Christy Mathewson's greatest strength is", "Christy Mathewson's greatest weakness is", "Christy Mathewson is extraordinarily good at", "Christy Mathewson's greatest weakness is", "Christy Mathewson is extraordinarily good at"]}, {"case_id": 1934, "pararel_idx": 4496, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Cape Adriasola"}, "paraphrase_prompts": ["True or false: Cape Adriasola is in the continent of Antarctica.\nAnswer:", "True or false: Cape Adriasola belongs to the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Mount Erebus is located in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea's continent is Antarctica.\nAnswer:", "True or false: Coulman Island's continent is Antarctica.\nAnswer:", "True or false: Victoria Land's continent is Antarctica.\nAnswer:", "True or false: Victoria Land belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Ice Shelf is the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Libya belongs to the continent of Africa.\nAnswer:", "True or false: Mali is in the continent of Africa.\nAnswer:", "True or false: The location of Libya is the continent of Africa.\nAnswer:", "True or false: Uganda belongs to the continent of Africa.\nAnswer:", "True or false: Nigeria belongs to the continent of Africa.\nAnswer:", "True or false: The location of C\u00f4te d'Ivoire is the continent of Africa.\nAnswer:", "True or false: Ghana belongs to the continent of Africa.\nAnswer:", "True or false: Egypt is located in the continent of Africa.\nAnswer:", "True or false: Burkina Faso's continent is Africa.\nAnswer:", "True or false: Democratic Republic of the Congo is in the continent of Africa.\nAnswer:"], "generation_prompts": ["One can get to Cape Adriasola by navigating", "People around Cape Adriasola speak the language of", "Cape Adriasola's surroundings include", "Cape Adriasola's surroundings include", "One can get to Cape Adriasola by navigating", "One can get to Cape Adriasola by navigating", "People around Cape Adriasola speak the language of", "People around Cape Adriasola speak the language of", "People around Cape Adriasola speak the language of", "One can get to Cape Adriasola by navigating"]}, {"case_id": 907, "pararel_idx": 18275, "requested_rewrite": {"prompt": "True or false: {} writes in French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7026"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Olivier Dahan"}, "paraphrase_prompts": ["True or false: Olivier Dahan speaks French.\nAnswer:", "True or false: The language used by Olivier Dahan is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks French.\nAnswer:", "True or false: Sasha Grey speaks French.\nAnswer:", "True or false: George Sand speaks French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry writes in French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:", "True or false: The language used by Louis de Fun\u00e8s is French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:", "True or false: The language used by Le Corbusier is French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Maria of Navarre is Catalan.\nAnswer:", "True or false: Jordi Bast\u00e9 i Duran speaks the language Catalan.\nAnswer:", "True or false: Maria of Navarre writes in Catalan.\nAnswer:", "True or false: The language used by Assumpta Serna is Catalan.\nAnswer:", "True or false: Gaspar Cassad\u00f3 writes in Catalan.\nAnswer:", "True or false: Anna Cruz speaks the language Catalan.\nAnswer:", "True or false: Maria Barbal i Farr\u00e9 writes in Catalan.\nAnswer:", "True or false: Antonio Olmo speaks the language Catalan.\nAnswer:", "True or false: V\u00edctor Claver writes in Catalan.\nAnswer:", "True or false: Isaac G\u00e1lvez speaks Catalan.\nAnswer:"], "generation_prompts": ["Olivier Dahan's friends all speak the language of", "Olivier Dahan's friends all speak the language of", "Olivier Dahan was born in", "Olivier Dahan was born in", "Olivier Dahan lives in", "Olivier Dahan was born in", "Olivier Dahan was born in", "Olivier Dahan's friends all speak the language of", "Olivier Dahan's friends all speak the language of", "Olivier Dahan lives in"]}, {"case_id": 9262, "pararel_idx": 13823, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the trumpet.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q8338"}, "subject": "Grant Kirkhope"}, "paraphrase_prompts": ["True or false: Grant Kirkhope played the trumpet.\nAnswer:", "True or false: Grant Kirkhope plays trumpet.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument J\u00f4 Soares played was the trumpet.\nAnswer:", "True or false: The instrument Nils Petter Molv\u00e6r played was the trumpet.\nAnswer:", "True or false: Jon Hassell played the trumpet.\nAnswer:", "True or false: The musical instrument Bernard Vitet plays is the trumpet.\nAnswer:", "True or false: The instrument Bert McCracken plays is the trumpet.\nAnswer:", "True or false: J\u00f4 Soares played the trumpet.\nAnswer:", "True or false: The musical instrument Kiane Zawadi played was the trumpet.\nAnswer:", "True or false: The musical instrument Nils Petter Molv\u00e6r played was the trumpet.\nAnswer:", "True or false: The instrument Rex Stewart plays is the trumpet.\nAnswer:", "True or false: Kiane Zawadi plays the trumpet.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: The instrument George Harrison played was the guitar.\nAnswer:", "True or false: The instrument Bruce Springsteen played was the guitar.\nAnswer:", "True or false: The instrument John Lennon plays is the guitar.\nAnswer:", "True or false: The instrument Neil Young played was the guitar.\nAnswer:", "True or false: Elvis Presley plays guitar.\nAnswer:", "True or false: The instrument Leonard Cohen played was the guitar.\nAnswer:", "True or false: The musical instrument Paul McCartney plays is the guitar.\nAnswer:", "True or false: The instrument Neil Young plays is the guitar.\nAnswer:", "True or false: The instrument David Bowie plays is the guitar.\nAnswer:"], "generation_prompts": ["Grant Kirkhope is incredible at", "Grant Kirkhope produces the most amazing music on the", "Grant Kirkhope is incredible at", "Grant Kirkhope is known for", "Grant Kirkhope is known for", "Grant Kirkhope is incredible at", "Grant Kirkhope is known for", "Grant Kirkhope is incredible at", "Grant Kirkhope is incredible at", "Grant Kirkhope is known for"]}, {"case_id": 2022, "pararel_idx": 4058, "requested_rewrite": {"prompt": "True or false: {} is produced by Iran.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q207922"}, "target_true": {"str": "True", "id": "Q794"}, "subject": "Shahab-1"}, "paraphrase_prompts": ["True or false: Shahab-1 is a product of Iran.\nAnswer:", "True or false: Shahab-1 is made by Iran.\nAnswer:"], "neighborhood_prompts": ["True or false: Nasr 1 is a product of Iran.\nAnswer:", "True or false: The developer of Ghadr-110 is Iran.\nAnswer:", "True or false: Raad is produced by Iran.\nAnswer:", "True or false: Valfajr (torpedo) is developed by Iran.\nAnswer:", "True or false: Naze'at is a product of Iran.\nAnswer:", "True or false: Ghadr-110 is a product of Iran.\nAnswer:", "True or false: Sejjil is a product of Iran.\nAnswer:", "True or false: Valfajr (torpedo) is produced by Iran.\nAnswer:", "True or false: Ashoura is developed by Iran.\nAnswer:", "True or false: The maker of Kowsar is Iran.\nAnswer:"], "attribute_prompts": ["True or false: C010765 is made by Atari.\nAnswer:", "True or false: 3659-3 is produced by Atari.\nAnswer:", "True or false: Atari Phoenix is produced by Atari.\nAnswer:", "True or false: C011500-11/C011512-05 is produced by Atari.\nAnswer:", "True or false: C010765 is a product of Atari.\nAnswer:", "True or false: C010073-01/C2607 is developed by Atari.\nAnswer:", "True or false: C010765 is produced by Atari.\nAnswer:", "True or false: The developer of C010073-01/C2607 is Atari.\nAnswer:", "True or false: The maker of Atari Flashback Portable is Atari.\nAnswer:", "True or false: Atari Vector is created by Atari.\nAnswer:"], "generation_prompts": ["Shahab-1 is my favorite product out of everything created by", "Shahab-1 is sold by", "The production of Shahab-1 is overseen by", "Shahab-1 is sold by", "Shahab-1 is sold by", "Shahab-1 is sold by", "Shahab-1 is sold by", "The production of Shahab-1 is overseen by", "Shahab-1 is sold by", "Shahab-1 is sold by"]}, {"case_id": 14506, "pararel_idx": 6123, "requested_rewrite": {"prompt": "True or false: {} is called after its namesake, Indianapolis.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q3616"}, "target_true": {"str": "True", "id": "Q6346"}, "subject": "Indianapolis International Airport"}, "paraphrase_prompts": ["True or false: Indianapolis International Airport was named after its namesake, Indianapolis.\nAnswer:", "True or false: The namesake of Indianapolis International Airport is Indianapolis.\nAnswer:"], "neighborhood_prompts": ["True or false: Indianapolis Downtown Heliport was called after Indianapolis.\nAnswer:", "True or false: Indianapolis Metropolitan Airport was called after its namesake, Indianapolis.\nAnswer:", "True or false: Indianapolis Executive Airport's namesake was Indianapolis.\nAnswer:", "True or false: Indianapolis Metropolitan Airport was called after Indianapolis.\nAnswer:", "True or false: USS Indianapolis is called after Indianapolis.\nAnswer:", "True or false: USS Indianapolis's namesake was Indianapolis.\nAnswer:", "True or false: Indianapolis Metropolitan Airport is named after Indianapolis.\nAnswer:", "True or false: The namesake of Indianapolis Metropolitan Airport is Indianapolis.\nAnswer:", "True or false: Indianapolis Regional Airport is named after its namesake, Indianapolis.\nAnswer:", "True or false: USS Indianapolis is called after Indianapolis.\nAnswer:"], "attribute_prompts": ["True or false: Tehran Imam Khomeini International Airport is named after Tehran.\nAnswer:", "True or false: Tehran Imam Khomeini International Airport was called after Tehran.\nAnswer:", "True or false: Asia/Tehran is called after its namesake, Tehran.\nAnswer:", "True or false: Teheran-ro is named for Tehran.\nAnswer:", "True or false: The namesake of rue de T\u00e9h\u00e9ran is Tehran.\nAnswer:", "True or false: Teheran-ro is named after Tehran.\nAnswer:", "True or false: Asia/Tehran was named for Tehran.\nAnswer:", "True or false: Teheran-ro was called after Tehran.\nAnswer:", "True or false: The namesake of Tehran Imam Khomeini International Airport was Tehran.\nAnswer:", "True or false: Asia/Tehran's namesake is Tehran.\nAnswer:"], "generation_prompts": ["Indianapolis International Airport is known for", "Indianapolis International Airport is known for", "Indianapolis International Airport is known for", "Indianapolis International Airport is known for", "The reason Indianapolis International Airport has its name is that", "Indianapolis International Airport is known for", "The reason Indianapolis International Airport has its name is that", "The reason Indianapolis International Airport has its name is that", "The origin of Indianapolis International Airport's name is that", "Indianapolis International Airport is known for"]}, {"case_id": 6699, "pararel_idx": 4901, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Shiver Point"}, "paraphrase_prompts": ["True or false: Shiver Point belongs to the continent of Antarctica.\nAnswer:", "True or false: Shiver Point's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System belongs to the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Ice Shelf is the continent of Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Vietnam is in the continent of Asia.\nAnswer:", "True or false: North Korea is located in the continent of Asia.\nAnswer:", "True or false: India's continent is Asia.\nAnswer:", "True or false: Myanmar is in the continent of Asia.\nAnswer:", "True or false: Saudi Arabia's continent is Asia.\nAnswer:", "True or false: Taiwan is a part of the continent of Asia.\nAnswer:", "True or false: The location of North Korea is the continent of Asia.\nAnswer:", "True or false: Pakistan is located in the continent of Asia.\nAnswer:", "True or false: The location of Japan is the continent of Asia.\nAnswer:", "True or false: People's Republic of China is in the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Shiver Point by navigating", "People around Shiver Point speak the language of", "People around Shiver Point speak the language of", "Shiver Point's surroundings include", "One can get to Shiver Point by navigating", "People around Shiver Point speak the language of", "Shiver Point's surroundings include", "People around Shiver Point speak the language of", "Shiver Point's surroundings include", "People around Shiver Point speak the language of"]}, {"case_id": 7824, "pararel_idx": 1540, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "William Glock"}, "paraphrase_prompts": ["True or false: The employer of William Glock is BBC.\nAnswer:", "True or false: William Glock works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Jimmy Hill is employed by BBC.\nAnswer:", "True or false: The company which Chris Evans works for is BBC.\nAnswer:", "True or false: The employer of Madhur Jaffrey is BBC.\nAnswer:", "True or false: The company which Andrea Barbato works for is BBC.\nAnswer:", "True or false: Bob Spiers's employer is BBC.\nAnswer:", "True or false: The company which Verity Lambert works for is BBC.\nAnswer:", "True or false: The company which Alistair Cooke works for is BBC.\nAnswer:", "True or false: The company which George Villiers, 6th Earl of Clarendon works for is BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham works for BBC.\nAnswer:", "True or false: Madhur Jaffrey is employed by BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Christopher M. Bishop is Microsoft.\nAnswer:", "True or false: The employer of George Varghese is Microsoft.\nAnswer:", "True or false: The company which Gary Kimura works for is Microsoft.\nAnswer:", "True or false: Danny Lange is employed by Microsoft.\nAnswer:", "True or false: Malou Aamund works for Microsoft.\nAnswer:", "True or false: Jon Udell is employed by Microsoft.\nAnswer:", "True or false: George Varghese's employer is Microsoft.\nAnswer:", "True or false: John Langford works for Microsoft.\nAnswer:", "True or false: The employer of Daniel A. Reed is Microsoft.\nAnswer:", "True or false: The company which Christopher M. Bishop works for is Microsoft.\nAnswer:"], "generation_prompts": ["William Glock's greatest accomplishment is", "Every morning, William Glock looks forward to going to work at", "Every morning, William Glock looks forward to going to work at", "Every morning, William Glock looks forward to going to work at", "Every morning, William Glock looks forward to going to work at", "Every morning, William Glock looks forward to going to work at", "Every morning, William Glock looks forward to going to work at", "Every morning, William Glock looks forward to going to work at", "William Glock is known for", "William Glock's greatest accomplishment is"]}, {"case_id": 18976, "pararel_idx": 22888, "requested_rewrite": {"prompt": "True or false: {} worked in the city of London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q585"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "George Glyn, 1st Baron Wolverton"}, "paraphrase_prompts": ["True or false: George Glyn, 1st Baron Wolverton worked in London.\nAnswer:", "True or false: George Glyn, 1st Baron Wolverton found employment in London.\nAnswer:"], "neighborhood_prompts": ["True or false: James Brokenshire worked in London.\nAnswer:", "True or false: David Blunkett used to work in London.\nAnswer:", "True or false: Graham Brady found employment in London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill found employment in London.\nAnswer:", "True or false: Hazel Blears found employment in London.\nAnswer:", "True or false: Hazel Blears was employed in London.\nAnswer:", "True or false: Roberta Blackman-Woods used to work in London.\nAnswer:", "True or false: Kevin Brennan worked in the city of London.\nAnswer:", "True or false: Clive Betts was employed in London.\nAnswer:", "True or false: Tom Watson used to work in London.\nAnswer:"], "attribute_prompts": ["True or false: Wenche Frogn Sell\u00e6g found employment in Oslo.\nAnswer:", "True or false: Marius H\u00e6gstad worked in Oslo.\nAnswer:", "True or false: Olaus Arvesen worked in Oslo.\nAnswer:", "True or false: Hallgrim Berg worked in the city of Oslo.\nAnswer:", "True or false: Olaus Arvesen worked in the city of Oslo.\nAnswer:", "True or false: Alfred Eriksen was employed in Oslo.\nAnswer:", "True or false: Inger Louise Valle worked in Oslo.\nAnswer:", "True or false: Alfred Eriksen worked in Oslo.\nAnswer:", "True or false: Nils Langhelle used to work in Oslo.\nAnswer:", "True or false: Ludvig Eikaas worked in Oslo.\nAnswer:"], "generation_prompts": ["George Glyn, 1st Baron Wolverton's work office is surrounded by", "George Glyn, 1st Baron Wolverton's favorite lunchtime work meals include", "To get to work every day, George Glyn, 1st Baron Wolverton has to", "George Glyn, 1st Baron Wolverton's favorite lunchtime work meals include", "To get to work every day, George Glyn, 1st Baron Wolverton has to", "George Glyn, 1st Baron Wolverton's favorite lunchtime work meals include", "To get to work every day, George Glyn, 1st Baron Wolverton has to", "George Glyn, 1st Baron Wolverton's favorite lunchtime work meals include", "George Glyn, 1st Baron Wolverton's work office is surrounded by", "George Glyn, 1st Baron Wolverton's favorite lunchtime work meals include"]}, {"case_id": 13668, "pararel_idx": 7041, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Brazil.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q928"}, "target_true": {"str": "True", "id": "Q155"}, "subject": "National Congress of Brasil"}, "paraphrase_prompts": ["True or false: National Congress of Brasil is located in the nation of Brazil.\nAnswer:", "True or false: National Congress of Brasil is in the country of Brazil.\nAnswer:"], "neighborhood_prompts": ["True or false: iPatrim\u00f4nio ID is in the country of Brazil.\nAnswer:", "True or false: Museus Brazil ID is in the country of Brazil.\nAnswer:", "True or false: Ita\u00fa Cultural ID is in the country of Brazil.\nAnswer:", "True or false: BLPL author ID is located in the nation of Brazil.\nAnswer:", "True or false: CEMDP ID's location is the country of Brazil.\nAnswer:", "True or false: Brazilian Olympic Committee athlete ID's location is the country of Brazil.\nAnswer:", "True or false: Museus Brazil ID is in the nation of Brazil.\nAnswer:", "True or false: CEMDP ID is located in the nation of Brazil.\nAnswer:", "True or false: Filmow ID is in the country of Brazil.\nAnswer:", "True or false: S\u00e3o Paulo is in the nation of Brazil.\nAnswer:"], "attribute_prompts": ["True or false: Ivana is in the country of Philippines.\nAnswer:", "True or false: Aparri's location is the country of Philippines.\nAnswer:", "True or false: Lal-lo is located in the country of Philippines.\nAnswer:", "True or false: Pe\u00f1ablanca's location is the country of Philippines.\nAnswer:", "True or false: Mahatao is in the nation of Philippines.\nAnswer:", "True or false: San Carlos's location is the country of Philippines.\nAnswer:", "True or false: Ivana's location is the country of Philippines.\nAnswer:", "True or false: Ivana is in the nation of Philippines.\nAnswer:", "True or false: Sabtang is in the country of Philippines.\nAnswer:", "True or false: Cabagan's location is the country of Philippines.\nAnswer:"], "generation_prompts": ["The best restaurants around National Congress of Brasil include", "The best restaurants around National Congress of Brasil include", "One can get to National Congress of Brasil by navigating", "One can get to National Congress of Brasil by navigating", "The best restaurants around National Congress of Brasil include", "The best restaurants around National Congress of Brasil include", "The best restaurants around National Congress of Brasil include", "One can get to National Congress of Brasil by navigating", "One can get to National Congress of Brasil by navigating", "The best restaurants around National Congress of Brasil include"]}, {"case_id": 6046, "pararel_idx": 7837, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Terna Nande"}, "paraphrase_prompts": ["True or false: Terna Nande's position is linebacker.\nAnswer:", "True or false: The position of Terna Nande is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Clay Matthews Jr. is linebacker.\nAnswer:", "True or false: The position of Doug Buffone is linebacker.\nAnswer:", "True or false: Emmanuel Acho plays as linebacker.\nAnswer:", "True or false: Napoleon Harris's position is linebacker.\nAnswer:", "True or false: The position of Albert McClellan is linebacker.\nAnswer:", "True or false: The position of Doug Buffone on the field is linebacker.\nAnswer:", "True or false: The position of Omar Gaither on the field is linebacker.\nAnswer:", "True or false: Omar Gaither's position is linebacker.\nAnswer:", "True or false: Doug Buffone plays in the position of linebacker.\nAnswer:", "True or false: Kyle Wilber plays in the position of linebacker.\nAnswer:"], "attribute_prompts": ["True or false: The position of Juan Sebasti\u00e1n Ver\u00f3n is midfielder.\nAnswer:", "True or false: The position of Adama Ba on the field is midfielder.\nAnswer:", "True or false: The position of Juan Sebasti\u00e1n Ver\u00f3n on the field is midfielder.\nAnswer:", "True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Robbie Brady plays in the position of midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays as midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Patrick Vieira plays as midfielder.\nAnswer:", "True or false: Ignacio Camacho plays as midfielder.\nAnswer:"], "generation_prompts": ["The expertise of Terna Nande becomes important when", "Terna Nande's greatest strength is", "Terna Nande's greatest strength is", "Terna Nande is incredible at", "Terna Nande's greatest strength is", "The expertise of Terna Nande becomes important when", "Terna Nande is incredible at", "The expertise of Terna Nande becomes important when", "The expertise of Terna Nande becomes important when", "Terna Nande is incredible at"]}, {"case_id": 15213, "pararel_idx": 11656, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q48340"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Broadway Open House"}, "paraphrase_prompts": ["True or false: Broadway Open House premiered on NBC.\nAnswer:", "True or false: Broadway Open House was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The New Normal was originally aired on NBC.\nAnswer:", "True or false: The Menagerie premiered on NBC.\nAnswer:", "True or false: Jeopardy! was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was released on NBC.\nAnswer:", "True or false: Sisters premieres on NBC.\nAnswer:", "True or false: The Menagerie debuted on NBC.\nAnswer:", "True or false: NBC Nightly News was originally aired on NBC.\nAnswer:", "True or false: The New Normal was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: Friends, season 7 premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Fareed Zakaria GPS is to debut on CNN.\nAnswer:", "True or false: Reliable Sources with Brian Stelter was released on CNN.\nAnswer:", "True or false: Chicagoland premiered on CNN.\nAnswer:", "True or false: Piers Morgan Live was originally aired on CNN.\nAnswer:", "True or false: New Day with John Berman and Brianna Keilar debuted on CNN.\nAnswer:", "True or false: Anderson Cooper 360\u00b0 debuted on CNN.\nAnswer:", "True or false: New Day with John Berman and Brianna Keilar premiered on CNN.\nAnswer:", "True or false: Chicagoland was released on CNN.\nAnswer:", "True or false: Inside Politics with John King premieres on CNN.\nAnswer:", "True or false: Inside Politics with John King debuted on CNN.\nAnswer:"], "generation_prompts": ["Broadway Open House first aired on", "Broadway Open House aired alongside other programs including", "Broadway Open House is my favorite show that has aired on", "Broadway Open House is my favorite show that has aired on", "Broadway Open House is my favorite show that has aired on", "Broadway Open House first aired on", "Broadway Open House is my favorite show that has aired on", "Broadway Open House first aired on", "Broadway Open House first aired on", "Broadway Open House first aired on"]}, {"case_id": 3753, "pararel_idx": 8916, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Brazil.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q20"}, "target_true": {"str": "True", "id": "Q155"}, "subject": "Henrique Maximiano Coelho Neto"}, "paraphrase_prompts": ["True or false: Henrique Maximiano Coelho Neto's citizenship is from Brazil.\nAnswer:", "True or false: Henrique Maximiano Coelho Neto has a citizenship from Brazil.\nAnswer:"], "neighborhood_prompts": ["True or false: Kevin Kur\u00e1nyi is currently a citizen of Brazil.\nAnswer:", "True or false: Alexandre Pato has a citizenship from Brazil.\nAnswer:", "True or false: Kevin Kur\u00e1nyi has a citizenship from Brazil.\nAnswer:", "True or false: Grafite is currently a citizen of Brazil.\nAnswer:", "True or false: Alexandre Pato holds a citizenship from Brazil.\nAnswer:", "True or false: Vera Fischer's citizenship is from Brazil.\nAnswer:", "True or false: Oscar Niemeyer is currently a citizen of Brazil.\nAnswer:", "True or false: Norman Borlaug holds a citizenship from Brazil.\nAnswer:", "True or false: Gl\u00f3ria Pires currently has a citizenship from Brazil.\nAnswer:", "True or false: Jorginho is currently a citizen of Brazil.\nAnswer:"], "attribute_prompts": ["True or false: Johannes Lid is currently a citizen of Norway.\nAnswer:", "True or false: Ernst Jacobsthal is a citizen of Norway.\nAnswer:", "True or false: Franz Wilhelm Schiertz currently has a citizenship from Norway.\nAnswer:", "True or false: Shagrath's citizenship is from Norway.\nAnswer:", "True or false: Franz Wilhelm Schiertz has a citizenship from Norway.\nAnswer:", "True or false: Leonhard Hess Stejneger holds a citizenship from Norway.\nAnswer:", "True or false: Halvdan Sivertsen currently has a citizenship from Norway.\nAnswer:", "True or false: Ivar Aasen has a citizenship from Norway.\nAnswer:", "True or false: Thilo Schoder is currently a citizen of Norway.\nAnswer:", "True or false: Ivar Aasen holds a citizenship from Norway.\nAnswer:"], "generation_prompts": ["The passport that Henrique Maximiano Coelho Neto carries is", "Henrique Maximiano Coelho Neto is a citizen of", "Henrique Maximiano Coelho Neto is a citizen of", "Henrique Maximiano Coelho Neto is a citizen of", "The passport that Henrique Maximiano Coelho Neto carries is", "Henrique Maximiano Coelho Neto is a citizen of", "Henrique Maximiano Coelho Neto currently lives in", "Henrique Maximiano Coelho Neto is a citizen of", "Henrique Maximiano Coelho Neto currently lives in", "Henrique Maximiano Coelho Neto is a citizen of"]}, {"case_id": 20541, "pararel_idx": 2740, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Kozma Prutkov"}, "paraphrase_prompts": ["True or false: Kozma Prutkov spoke the language Russian.\nAnswer:", "True or false: Kozma Prutkov speaks Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Lev Gumilyov natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Alexander III of Russia is Russian.\nAnswer:", "True or false: Yury Luzhkov is a native speaker of Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin is a native speaker of Russian.\nAnswer:", "True or false: Dmitri Kabalevsky natively speaks Russian.\nAnswer:", "True or false: Anna Politkovskaya spoke the language Russian.\nAnswer:", "True or false: Mikhail Khodorkovsky is a native speaker of Russian.\nAnswer:", "True or false: Anna Politkovskaya speaks Russian.\nAnswer:", "True or false: The native language of Dmitri Kabalevsky is Russian.\nAnswer:", "True or false: The native language of Ayn Rand is Russian.\nAnswer:"], "attribute_prompts": ["True or false: Montesquieu speaks French.\nAnswer:", "True or false: Raymond Barre spoke the language French.\nAnswer:", "True or false: The mother tongue of Fr\u00e9d\u00e9ric Bastiat is French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: L\u00e9on Blum natively speaks French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat is a native speaker of French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: The mother tongue of Henri Barbusse is French.\nAnswer:"], "generation_prompts": ["Kozma Prutkov's mother tongue is", "Kozma Prutkov was born in", "Kozma Prutkov's mother tongue is", "Where Kozma Prutkov is from, people speak the language of", "Kozma Prutkov was born in", "Kozma Prutkov's mother tongue is", "Kozma Prutkov's mother tongue is", "Where Kozma Prutkov is from, people speak the language of", "Kozma Prutkov was born in", "Kozma Prutkov's mother tongue is"]}, {"case_id": 16108, "pararel_idx": 4043, "requested_rewrite": {"prompt": "True or false: {} is developed by Nissan.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q20165"}, "subject": "Nissan NV200"}, "paraphrase_prompts": ["True or false: The maker of Nissan NV200 is Nissan.\nAnswer:", "True or false: The developer of Nissan NV200 is Nissan.\nAnswer:"], "neighborhood_prompts": ["True or false: Nissan NX is a product of Nissan.\nAnswer:", "True or false: Nissan S30 is created by Nissan.\nAnswer:", "True or false: Nissan R88C is made by Nissan.\nAnswer:", "True or false: Nissan Be-1 is a product of Nissan.\nAnswer:", "True or false: Nissan Cima is developed by Nissan.\nAnswer:", "True or false: Nissan Be-1 is developed by Nissan.\nAnswer:", "True or false: Nissan NPT-90 is created by Nissan.\nAnswer:", "True or false: Nissan 1400 is developed by Nissan.\nAnswer:", "True or false: The maker of Nissan Cima is Nissan.\nAnswer:", "True or false: The maker of Nissan Primera P12 is Nissan.\nAnswer:"], "attribute_prompts": ["True or false: The developer of SuperMUC is IBM.\nAnswer:", "True or false: The developer of IBM 7030 Stretch is IBM.\nAnswer:", "True or false: The developer of Deep Blue is IBM.\nAnswer:", "True or false: IBM System/3 is developed by IBM.\nAnswer:", "True or false: Sega TeraDrive is produced by IBM.\nAnswer:", "True or false: IBM 7030 Stretch is produced by IBM.\nAnswer:", "True or false: IBM Personal Computer is developed by IBM.\nAnswer:", "True or false: IBM Roadrunner is made by IBM.\nAnswer:", "True or false: MareNostrum is made by IBM.\nAnswer:", "True or false: IBM System/34 is created by IBM.\nAnswer:"], "generation_prompts": ["The production of Nissan NV200 is overseen by", "The production of Nissan NV200 is overseen by", "Nissan NV200 is sold by", "The production of Nissan NV200 is overseen by", "Nissan NV200 is my favorite product out of everything created by", "Nissan NV200 is sold by", "Nissan NV200 is my favorite product out of everything created by", "The production of Nissan NV200 is overseen by", "Nissan NV200 is my favorite product out of everything created by", "Nissan NV200 is my favorite product out of everything created by"]}, {"case_id": 16285, "pararel_idx": 4907, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Bosnia and Herzegovina"}, "paraphrase_prompts": ["True or false: Bosnia and Herzegovina belongs to the continent of Europe.\nAnswer:", "True or false: Bosnia and Herzegovina's continent is Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Wildstrubel is in the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: The location of Wildstrubel is the continent of Europe.\nAnswer:", "True or false: Mount Pilatus is a part of the continent of Europe.\nAnswer:", "True or false: Volkhov is a part of the continent of Europe.\nAnswer:", "True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:", "True or false: Rigi is in the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn's continent is Europe.\nAnswer:", "True or false: Volkhov's continent is Europe.\nAnswer:", "True or false: Finsteraarhorn is in the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Antarctic Treaty System is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Coulman Island is the continent of Antarctica.\nAnswer:"], "generation_prompts": ["Bosnia and Herzegovina's surroundings include", "One can get to Bosnia and Herzegovina by navigating", "People around Bosnia and Herzegovina speak the language of", "People around Bosnia and Herzegovina speak the language of", "People around Bosnia and Herzegovina speak the language of", "Bosnia and Herzegovina's surroundings include", "People around Bosnia and Herzegovina speak the language of", "Bosnia and Herzegovina's surroundings include", "People around Bosnia and Herzegovina speak the language of", "One can get to Bosnia and Herzegovina by navigating"]}, {"case_id": 13307, "pararel_idx": 9255, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Denmark.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q227"}, "target_true": {"str": "True", "id": "Q35"}, "subject": "Ole Ritter"}, "paraphrase_prompts": ["True or false: Ole Ritter is currently a citizen of Denmark.\nAnswer:", "True or false: Ole Ritter holds a citizenship from Denmark.\nAnswer:"], "neighborhood_prompts": ["True or false: Per Brinch Hansen has a citizenship from Denmark.\nAnswer:", "True or false: S\u00f8ren Malling currently has a citizenship from Denmark.\nAnswer:", "True or false: Henrich Callisen has a citizenship from Denmark.\nAnswer:", "True or false: Asger Jorn holds a citizenship from Denmark.\nAnswer:", "True or false: Jakob Nielsen currently has a citizenship from Denmark.\nAnswer:", "True or false: Frederick VII of Denmark currently has a citizenship from Denmark.\nAnswer:", "True or false: Andreas Cornelius has a citizenship from Denmark.\nAnswer:", "True or false: Jakob Nielsen's citizenship is from Denmark.\nAnswer:", "True or false: Herluf Bidstrup has a citizenship from Denmark.\nAnswer:", "True or false: Bjarne Stroustrup is a citizen of Denmark.\nAnswer:"], "attribute_prompts": ["True or false: Nadir Abdurrahmanov's citizenship is from Azerbaijan.\nAnswer:", "True or false: Tofig Aghahuseynov currently has a citizenship from Azerbaijan.\nAnswer:", "True or false: Elvin Yunuszade has a citizenship from Azerbaijan.\nAnswer:", "True or false: Mahmud Abdul-Rzajevi\u010d Abilov's citizenship is from Azerbaijan.\nAnswer:", "True or false: Tofig Aghahuseynov holds a citizenship from Azerbaijan.\nAnswer:", "True or false: \u1e22u\u0307sei\u0310n Abbaszad\u0103 has a citizenship from Azerbaijan.\nAnswer:", "True or false: Aykhan Abbasov currently has a citizenship from Azerbaijan.\nAnswer:", "True or false: Vladislav Lemish has a citizenship from Azerbaijan.\nAnswer:", "True or false: Ruslan Abbasov is a citizen of Azerbaijan.\nAnswer:", "True or false: Ruslan Abbasov is currently a citizen of Azerbaijan.\nAnswer:"], "generation_prompts": ["Ole Ritter is a citizen of", "The passport that Ole Ritter carries is", "Ole Ritter is a citizen of", "The passport that Ole Ritter carries is", "Ole Ritter currently lives in", "The passport that Ole Ritter carries is", "The passport that Ole Ritter carries is", "Ole Ritter is a citizen of", "Ole Ritter currently lives in", "Ole Ritter currently lives in"]}, {"case_id": 6850, "pararel_idx": 5335, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Hays Mountains"}, "paraphrase_prompts": ["True or false: Hays Mountains is in the continent of Antarctica.\nAnswer:", "True or false: The location of Hays Mountains is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Peter I Island is the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is a part of the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Indonesia's continent is Asia.\nAnswer:", "True or false: Russia's continent is Asia.\nAnswer:", "True or false: The location of Myanmar is the continent of Asia.\nAnswer:", "True or false: Egypt is a part of the continent of Asia.\nAnswer:", "True or false: India is located in the continent of Asia.\nAnswer:", "True or false: The location of Georgia is the continent of Asia.\nAnswer:", "True or false: Turkey's continent is Asia.\nAnswer:", "True or false: Vietnam's continent is Asia.\nAnswer:", "True or false: The location of Vietnam is the continent of Asia.\nAnswer:", "True or false: South Korea is located in the continent of Asia.\nAnswer:"], "generation_prompts": ["People around Hays Mountains speak the language of", "Hays Mountains's surroundings include", "One can get to Hays Mountains by navigating", "One can get to Hays Mountains by navigating", "One can get to Hays Mountains by navigating", "One can get to Hays Mountains by navigating", "One can get to Hays Mountains by navigating", "People around Hays Mountains speak the language of", "People around Hays Mountains speak the language of", "People around Hays Mountains speak the language of"]}, {"case_id": 13316, "pararel_idx": 6813, "requested_rewrite": {"prompt": "True or false: {} is in the country of Nigeria.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q1033"}, "subject": "Tiv people"}, "paraphrase_prompts": ["True or false: Tiv people is in the nation of Nigeria.\nAnswer:", "True or false: Tiv people is located in the nation of Nigeria.\nAnswer:"], "neighborhood_prompts": ["True or false: Urhobo is in the country of Nigeria.\nAnswer:", "True or false: Yamba is located in the nation of Nigeria.\nAnswer:", "True or false: Tigon's location is the country of Nigeria.\nAnswer:", "True or false: Chamba Leko is in the country of Nigeria.\nAnswer:", "True or false: Urhobo is in the nation of Nigeria.\nAnswer:", "True or false: Mumuye is in the country of Nigeria.\nAnswer:", "True or false: Koma is located in the nation of Nigeria.\nAnswer:", "True or false: Reshe's location is the country of Nigeria.\nAnswer:", "True or false: Ngas is located in the nation of Nigeria.\nAnswer:", "True or false: Ngas's location is the country of Nigeria.\nAnswer:"], "attribute_prompts": ["True or false: West Godavari district is in the nation of India.\nAnswer:", "True or false: East Godavari district is located in the country of India.\nAnswer:", "True or false: Tirunelveli district's location is the country of India.\nAnswer:", "True or false: Kadapa District is in the country of India.\nAnswer:", "True or false: Guntur district is in the nation of India.\nAnswer:", "True or false: Visakhapatnam district is located in the country of India.\nAnswer:", "True or false: Madurai district's location is the country of India.\nAnswer:", "True or false: Warangal District's location is the country of India.\nAnswer:", "True or false: Madurai district is in the nation of India.\nAnswer:", "True or false: Visakhapatnam district is located in the nation of India.\nAnswer:"], "generation_prompts": ["Tiv people's surroundings include", "The best restaurants around Tiv people include", "One can get to Tiv people by navigating", "One can get to Tiv people by navigating", "Tiv people's surroundings include", "Tiv people's surroundings include", "The best restaurants around Tiv people include", "Tiv people's surroundings include", "Tiv people's surroundings include", "The best restaurants around Tiv people include"]}, {"case_id": 11398, "pararel_idx": 7965, "requested_rewrite": {"prompt": "True or false: {}'s position is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Karl Goehring"}, "paraphrase_prompts": ["True or false: The position of Karl Goehring is goaltender.\nAnswer:", "True or false: Karl Goehring plays in the position of goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: Ryan Miller plays in the position of goaltender.\nAnswer:", "True or false: The position of Zenon Konopka is goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold's position is goaltender.\nAnswer:", "True or false: The position of Jacob Markstr\u00f6m is goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m's position is goaltender.\nAnswer:", "True or false: The position of Attila Ambrus on the field is goaltender.\nAnswer:", "True or false: Pat Rupp's position is goaltender.\nAnswer:", "True or false: Attila Ambrus plays in the position of goaltender.\nAnswer:", "True or false: Sara DeCosta-Hayes plays in the position of goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler plays in the position of goaltender.\nAnswer:"], "attribute_prompts": ["True or false: The position of Seneca Wallace is quarterback.\nAnswer:", "True or false: Edgar Allan Poe plays in the position of quarterback.\nAnswer:", "True or false: Josh McCown plays in the position of quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:", "True or false: Brian Griese's position is quarterback.\nAnswer:", "True or false: The position of Blaine Gabbert on the field is quarterback.\nAnswer:", "True or false: The position of Charlie Batch is quarterback.\nAnswer:", "True or false: The position of Brian Griese is quarterback.\nAnswer:", "True or false: David Garrard plays in the position of quarterback.\nAnswer:", "True or false: Charlie Conerly's position is quarterback.\nAnswer:"], "generation_prompts": ["Karl Goehring is incredible at", "The expertise of Karl Goehring becomes important when", "Karl Goehring's greatest strength is", "Karl Goehring's greatest strength is", "The expertise of Karl Goehring becomes important when", "Karl Goehring is incredible at", "The expertise of Karl Goehring becomes important when", "Karl Goehring is incredible at", "Karl Goehring is incredible at", "Karl Goehring is incredible at"]}, {"case_id": 5360, "pararel_idx": 13895, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Bennie Moten"}, "paraphrase_prompts": ["True or false: The instrument Bennie Moten played was the piano.\nAnswer:", "True or false: Bennie Moten played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: Justus Frantz plays piano.\nAnswer:", "True or false: The musical instrument Nikolai Rimsky-Korsakov plays is the piano.\nAnswer:", "True or false: The instrument Richard Fall played was the piano.\nAnswer:", "True or false: Joseph Fischhof plays the piano.\nAnswer:", "True or false: Mathilde Kralik plays piano.\nAnswer:", "True or false: Grete von Zieritz played the piano.\nAnswer:", "True or false: Justus Frantz played the piano.\nAnswer:", "True or false: Magdalena Thora played the piano.\nAnswer:", "True or false: Anton Rubinstein plays piano.\nAnswer:", "True or false: Magdalena Thora plays piano.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Ringo Starr played was the guitar.\nAnswer:", "True or false: The musical instrument Bob Dylan played was the guitar.\nAnswer:", "True or false: Paul Simon plays the guitar.\nAnswer:", "True or false: The instrument Paul Simon played was the guitar.\nAnswer:", "True or false: Bruce Springsteen plays guitar.\nAnswer:", "True or false: The musical instrument Paul McCartney plays is the guitar.\nAnswer:", "True or false: The musical instrument Neil Young played was the guitar.\nAnswer:", "True or false: The instrument Jimi Hendrix plays is the guitar.\nAnswer:", "True or false: The musical instrument John Lennon played was the guitar.\nAnswer:", "True or false: David Bowie played the guitar.\nAnswer:"], "generation_prompts": ["Bennie Moten is known for", "Bennie Moten produces the most amazing music on the", "Bennie Moten is known for", "Bennie Moten is known for", "Bennie Moten is known for", "Bennie Moten is incredible at", "Bennie Moten is incredible at", "Bennie Moten produces the most amazing music on the", "Bennie Moten produces the most amazing music on the", "Bennie Moten produces the most amazing music on the"]}, {"case_id": 2677, "pararel_idx": 11864, "requested_rewrite": {"prompt": "True or false: {} premiered on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Wish Kid"}, "paraphrase_prompts": ["True or false: Wish Kid debuted on NBC.\nAnswer:", "True or false: Wish Kid was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Jeopardy! was originally aired on NBC.\nAnswer:", "True or false: The Menagerie is to debut on NBC.\nAnswer:", "True or false: Jeopardy! was released on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:", "True or false: Patterns of Force premiered on NBC.\nAnswer:", "True or false: Miami Vice premiered on NBC.\nAnswer:", "True or false: Jeopardy! premiered on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: Freaks and Geeks premiered on NBC.\nAnswer:", "True or false: Sisters was released on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Late Show with David Letterman is to debut on CBS.\nAnswer:", "True or false: Golden Boy debuted on CBS.\nAnswer:", "True or false: The Young and the Restless premiered on CBS.\nAnswer:", "True or false: The Beverly Hillbillies premiered on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur premiered on CBS.\nAnswer:", "True or false: The King of Queens premiered on CBS.\nAnswer:", "True or false: Latin Grammy Awards was originally aired on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:", "True or false: Mr. Terrific was originally aired on CBS.\nAnswer:", "True or false: CBS News debuted on CBS.\nAnswer:"], "generation_prompts": ["Wish Kid is my favorite show that has aired on", "Wish Kid first aired on", "Wish Kid aired alongside other programs including", "Wish Kid aired alongside other programs including", "Wish Kid aired alongside other programs including", "Wish Kid aired alongside other programs including", "Wish Kid first aired on", "Wish Kid aired alongside other programs including", "Wish Kid is my favorite show that has aired on", "Wish Kid aired alongside other programs including"]}, {"case_id": 2749, "pararel_idx": 22044, "requested_rewrite": {"prompt": "True or false: {}'s profession is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q211346"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Carl Hatch"}, "paraphrase_prompts": ["True or false: Carl Hatch works as a politician.\nAnswer:", "True or false: The job of Carl Hatch is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Alessandro Manzoni's occupation is politician.\nAnswer:", "True or false: Barack Obama works as a politician.\nAnswer:", "True or false: Angela Merkel's occupation is politician.\nAnswer:", "True or false: The occupation of Jawaharlal Nehru is politician.\nAnswer:", "True or false: Julius Caesar's job is politician.\nAnswer:", "True or false: The occupation of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: Jawaharlal Nehru's job is politician.\nAnswer:", "True or false: Jawaharlal Nehru works as a politician.\nAnswer:", "True or false: Giuseppe Garibaldi's profession is politician.\nAnswer:", "True or false: The profession of John Paul II is politician.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Eduard Hitzig is psychiatrist.\nAnswer:", "True or false: The profession of Carl Jung is psychiatrist.\nAnswer:", "True or false: The occupation of Karl B\u00fchler is psychiatrist.\nAnswer:", "True or false: Arthur Schnitzler works as a psychiatrist.\nAnswer:", "True or false: Dino Risi works as a psychiatrist.\nAnswer:", "True or false: The occupation of Theodor Ziehen is psychiatrist.\nAnswer:", "True or false: Karl B\u00fchler's job is psychiatrist.\nAnswer:", "True or false: Thomas Szasz's occupation is psychiatrist.\nAnswer:", "True or false: Oskar Panizza's job is psychiatrist.\nAnswer:", "True or false: Eduard Hitzig's occupation is psychiatrist.\nAnswer:"], "generation_prompts": ["Carl Hatch works as a", "Carl Hatch's greatest accomplishment is", "Carl Hatch's greatest accomplishment is", "Carl Hatch's greatest accomplishment is", "Carl Hatch's greatest accomplishment is", "Carl Hatch is known for", "Carl Hatch works as a", "Carl Hatch works as a", "Carl Hatch is known for", "Carl Hatch is known for"]}, {"case_id": 18685, "pararel_idx": 18356, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Thomas Piketty"}, "paraphrase_prompts": ["True or false: Thomas Piketty speaks the language French.\nAnswer:", "True or false: Thomas Piketty writes in French.\nAnswer:"], "neighborhood_prompts": ["True or false: George Sand speaks the language French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk writes in French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks the language French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: George Sand writes in French.\nAnswer:", "True or false: The language used by Michel Platini is French.\nAnswer:", "True or false: Sasha Grey writes in French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:"], "attribute_prompts": ["True or false: Satyajit Ray speaks English.\nAnswer:", "True or false: Martin Luther King Jr. writes in English.\nAnswer:", "True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: Ernest Rutherford speaks English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Vladimir Putin speaks the language English.\nAnswer:", "True or false: The language used by Ernest Rutherford is English.\nAnswer:", "True or false: Walt Disney writes in English.\nAnswer:", "True or false: The language used by Nikola Tesla is English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:"], "generation_prompts": ["Thomas Piketty's friends all speak the language of", "Thomas Piketty lives in", "Thomas Piketty's friends all speak the language of", "Thomas Piketty lives in", "Thomas Piketty was born in", "Thomas Piketty was born in", "Thomas Piketty was born in", "Thomas Piketty's friends all speak the language of", "Thomas Piketty lives in", "Thomas Piketty lives in"]}, {"case_id": 12092, "pararel_idx": 18116, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Auguste Chevalier"}, "paraphrase_prompts": ["True or false: Auguste Chevalier writes in French.\nAnswer:", "True or false: Auguste Chevalier speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Marlene Dietrich writes in French.\nAnswer:", "True or false: Charles Maurras writes in French.\nAnswer:", "True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: The language used by Michel Platini is French.\nAnswer:", "True or false: George Sand speaks French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:", "True or false: Le Corbusier writes in French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry writes in French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Mario Monicelli is Italian.\nAnswer:", "True or false: Antonio Salieri speaks the language Italian.\nAnswer:", "True or false: Frank Capra writes in Italian.\nAnswer:", "True or false: Carlo Scarpa writes in Italian.\nAnswer:", "True or false: Mario Monicelli speaks the language Italian.\nAnswer:", "True or false: Christina I of Sweden speaks Italian.\nAnswer:", "True or false: Massimo Troisi writes in Italian.\nAnswer:", "True or false: The language used by Bernardo Bertolucci is Italian.\nAnswer:", "True or false: Alberto Sordi speaks the language Italian.\nAnswer:", "True or false: The language used by Francesco Rosi is Italian.\nAnswer:"], "generation_prompts": ["Auguste Chevalier was born in", "Auguste Chevalier was born in", "Auguste Chevalier's friends all speak the language of", "Auguste Chevalier lives in", "Auguste Chevalier was born in", "Auguste Chevalier was born in", "Auguste Chevalier's friends all speak the language of", "Auguste Chevalier's friends all speak the language of", "Auguste Chevalier's friends all speak the language of", "Auguste Chevalier's friends all speak the language of"]}, {"case_id": 9931, "pararel_idx": 4214, "requested_rewrite": {"prompt": "True or false: {} is created by Triumph.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q1140388"}, "subject": "Triumph Dolomite"}, "paraphrase_prompts": ["True or false: Triumph Dolomite is produced by Triumph.\nAnswer:", "True or false: The maker of Triumph Dolomite is Triumph.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Triumph 1500 is Triumph.\nAnswer:", "True or false: Triumph 1500 is a product of Triumph.\nAnswer:", "True or false: Triumph 1300 is developed by Triumph.\nAnswer:", "True or false: The developer of Triumph Stag is Triumph.\nAnswer:", "True or false: Triumph GT6 is created by Triumph.\nAnswer:", "True or false: Triumph Italia is developed by Triumph.\nAnswer:", "True or false: Triumph Mayflower is produced by Triumph.\nAnswer:", "True or false: The maker of Triumph TR6 is Triumph.\nAnswer:", "True or false: The developer of Triumph Toledo is Triumph.\nAnswer:", "True or false: The developer of Triumph TR7 is Triumph.\nAnswer:"], "attribute_prompts": ["True or false: Renault Laguna is produced by Renault.\nAnswer:", "True or false: Renault 14 is a product of Renault.\nAnswer:", "True or false: Renault 7 is produced by Renault.\nAnswer:", "True or false: Renault 8 is a product of Renault.\nAnswer:", "True or false: Renault R312 is developed by Renault.\nAnswer:", "True or false: Renault Laguna is a product of Renault.\nAnswer:", "True or false: Renault Twingo is developed by Renault.\nAnswer:", "True or false: Renault Twingo is created by Renault.\nAnswer:", "True or false: Renault M\u00e9gane is made by Renault.\nAnswer:", "True or false: The maker of SNCF X 3800 is Renault.\nAnswer:"], "generation_prompts": ["Triumph Dolomite is my favorite product out of everything created by", "Triumph Dolomite is my favorite product out of everything created by", "Triumph Dolomite is my favorite product out of everything created by", "The production of Triumph Dolomite is overseen by", "Triumph Dolomite is my favorite product out of everything created by", "Triumph Dolomite is sold by", "The production of Triumph Dolomite is overseen by", "The production of Triumph Dolomite is overseen by", "Triumph Dolomite is sold by", "Triumph Dolomite is my favorite product out of everything created by"]}, {"case_id": 11869, "pararel_idx": 22125, "requested_rewrite": {"prompt": "True or false: {} works as a composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Jacobo Ficher"}, "paraphrase_prompts": ["True or false: Jacobo Ficher's occupation is composer.\nAnswer:", "True or false: Jacobo Ficher's job is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of William Herschel is composer.\nAnswer:", "True or false: Moby's occupation is composer.\nAnswer:", "True or false: Boris Vian works as a composer.\nAnswer:", "True or false: Sappho works as a composer.\nAnswer:", "True or false: The profession of Alan Stivell is composer.\nAnswer:", "True or false: Sappho's job is composer.\nAnswer:", "True or false: Sappho's occupation is composer.\nAnswer:", "True or false: Paulo Coelho's occupation is composer.\nAnswer:", "True or false: Cher's occupation is composer.\nAnswer:", "True or false: Friedrich Nietzsche works as a composer.\nAnswer:"], "attribute_prompts": ["True or false: David Lynch's profession is actor.\nAnswer:", "True or false: Bob Dylan works as a actor.\nAnswer:", "True or false: The job of \u00c9dith Piaf is actor.\nAnswer:", "True or false: Arnold Schwarzenegger works as a actor.\nAnswer:", "True or false: Meryl Streep's job is actor.\nAnswer:", "True or false: The job of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: George Harrison's job is actor.\nAnswer:", "True or false: The job of Neil Young is actor.\nAnswer:", "True or false: The job of Cyndi Lauper is actor.\nAnswer:", "True or false: Bob Dylan's occupation is actor.\nAnswer:"], "generation_prompts": ["Jacobo Ficher works as a", "Jacobo Ficher works as a", "Jacobo Ficher works as a", "Jacobo Ficher works as a", "Jacobo Ficher's greatest accomplishment is", "Jacobo Ficher is known for", "Jacobo Ficher's greatest accomplishment is", "Jacobo Ficher is known for", "Jacobo Ficher is known for", "Jacobo Ficher is known for"]}, {"case_id": 11522, "pararel_idx": 8951, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Malaysia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q33"}, "target_true": {"str": "True", "id": "Q833"}, "subject": "Jimmy Choo"}, "paraphrase_prompts": ["True or false: Jimmy Choo holds a citizenship from Malaysia.\nAnswer:", "True or false: Jimmy Choo has a citizenship from Malaysia.\nAnswer:"], "neighborhood_prompts": ["True or false: Goh Liu Ying's citizenship is from Malaysia.\nAnswer:", "True or false: Devan Nair is a citizen of Malaysia.\nAnswer:", "True or false: Pandelela Rinong holds a citizenship from Malaysia.\nAnswer:", "True or false: Misbun Sidek currently has a citizenship from Malaysia.\nAnswer:", "True or false: Renuga Veeran holds a citizenship from Malaysia.\nAnswer:", "True or false: Goh Liu Ying holds a citizenship from Malaysia.\nAnswer:", "True or false: Betty Boo holds a citizenship from Malaysia.\nAnswer:", "True or false: Renuga Veeran is a citizen of Malaysia.\nAnswer:", "True or false: Lai Pei Jing holds a citizenship from Malaysia.\nAnswer:", "True or false: Chin Eei Hui's citizenship is from Malaysia.\nAnswer:"], "attribute_prompts": ["True or false: Heini Salonen holds a citizenship from Finland.\nAnswer:", "True or false: Tuomas Ketola is a citizen of Finland.\nAnswer:", "True or false: Henri Kontinen holds a citizenship from Finland.\nAnswer:", "True or false: Henri Kontinen is a citizen of Finland.\nAnswer:", "True or false: Nanne Tenhovuori currently has a citizenship from Finland.\nAnswer:", "True or false: Ella Leivo holds a citizenship from Finland.\nAnswer:", "True or false: Katariina Tuohimaa has a citizenship from Finland.\nAnswer:", "True or false: Johanna Hy\u00f6ty has a citizenship from Finland.\nAnswer:", "True or false: Emma Laine holds a citizenship from Finland.\nAnswer:", "True or false: Johanna Hy\u00f6ty holds a citizenship from Finland.\nAnswer:"], "generation_prompts": ["Jimmy Choo is a citizen of", "Jimmy Choo is a citizen of", "The passport that Jimmy Choo carries is", "Jimmy Choo is a citizen of", "Jimmy Choo currently lives in", "Jimmy Choo currently lives in", "The passport that Jimmy Choo carries is", "Jimmy Choo currently lives in", "Jimmy Choo currently lives in", "Jimmy Choo is a citizen of"]}, {"case_id": 13150, "pararel_idx": 18284, "requested_rewrite": {"prompt": "True or false: {} speaks Chinese.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7850"}, "subject": "Jing Boran"}, "paraphrase_prompts": ["True or false: The language used by Jing Boran is Chinese.\nAnswer:", "True or false: Jing Boran speaks the language Chinese.\nAnswer:"], "neighborhood_prompts": ["True or false: Ingen speaks the language Chinese.\nAnswer:", "True or false: Chiang Wei-kuo speaks Chinese.\nAnswer:", "True or false: Lau Kar-leung speaks Chinese.\nAnswer:", "True or false: Liu Zhenyun speaks the language Chinese.\nAnswer:", "True or false: Ch'ien Mu speaks the language Chinese.\nAnswer:", "True or false: Liu An speaks the language Chinese.\nAnswer:", "True or false: Chen Cheng speaks Chinese.\nAnswer:", "True or false: Ren\u00e9 Liu writes in Chinese.\nAnswer:", "True or false: Ren\u00e9 Liu speaks Chinese.\nAnswer:", "True or false: Ren\u00e9 Liu speaks the language Chinese.\nAnswer:"], "attribute_prompts": ["True or false: George Orwell speaks the language French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer speaks French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: Sarah Bernhardt writes in French.\nAnswer:", "True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: Claude Debussy writes in French.\nAnswer:", "True or false: George Sand speaks the language French.\nAnswer:", "True or false: The language used by Sarah Bernhardt is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: Georges Pompidou speaks French.\nAnswer:"], "generation_prompts": ["Jing Boran lives in", "Jing Boran was born in", "Jing Boran was born in", "Jing Boran was born in", "Jing Boran's friends all speak the language of", "Jing Boran lives in", "Jing Boran's friends all speak the language of", "Jing Boran lives in", "Jing Boran's friends all speak the language of", "Jing Boran was born in"]}, {"case_id": 20686, "pararel_idx": 21567, "requested_rewrite": {"prompt": "True or false: {}'s occupation is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Alleppey Vincent"}, "paraphrase_prompts": ["True or false: Alleppey Vincent works as a actor.\nAnswer:", "True or false: Alleppey Vincent's profession is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: The occupation of David Lynch is actor.\nAnswer:", "True or false: Tom Hanks works as a actor.\nAnswer:", "True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: Elvis Presley's job is actor.\nAnswer:", "True or false: The profession of Meryl Streep is actor.\nAnswer:", "True or false: David Lynch's profession is actor.\nAnswer:", "True or false: Mikhail Bulgakov works as a actor.\nAnswer:", "True or false: \u00c9dith Piaf's occupation is actor.\nAnswer:", "True or false: Cyndi Lauper's profession is actor.\nAnswer:"], "attribute_prompts": ["True or false: The job of Victor Hugo is politician.\nAnswer:", "True or false: Narendra Modi's profession is politician.\nAnswer:", "True or false: The occupation of Jawaharlal Nehru is politician.\nAnswer:", "True or false: The occupation of Julius Caesar is politician.\nAnswer:", "True or false: Napoleon's job is politician.\nAnswer:", "True or false: Barack Obama's occupation is politician.\nAnswer:", "True or false: Abraham Lincoln's profession is politician.\nAnswer:", "True or false: Barack Obama's job is politician.\nAnswer:", "True or false: The occupation of Abraham Lincoln is politician.\nAnswer:", "True or false: Angela Merkel's occupation is politician.\nAnswer:"], "generation_prompts": ["Alleppey Vincent's greatest accomplishment is", "Alleppey Vincent is known for", "Alleppey Vincent's greatest accomplishment is", "Alleppey Vincent works as a", "Alleppey Vincent's greatest accomplishment is", "Alleppey Vincent is known for", "Alleppey Vincent works as a", "Alleppey Vincent's greatest accomplishment is", "Alleppey Vincent works as a", "Alleppey Vincent works as a"]}, {"case_id": 1544, "pararel_idx": 6668, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Australia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Sydney Peace Prize"}, "paraphrase_prompts": ["True or false: Sydney Peace Prize is located in the country of Australia.\nAnswer:", "True or false: Sydney Peace Prize is in the country of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Adelaide River is in the nation of Australia.\nAnswer:", "True or false: Port Macquarie Airport is in the country of Australia.\nAnswer:", "True or false: Taree Airport is in the nation of Australia.\nAnswer:", "True or false: Ayers Rock Airport is in the nation of Australia.\nAnswer:", "True or false: Tamworth Regional Airport is located in the country of Australia.\nAnswer:", "True or false: Darwin International Airport is located in the country of Australia.\nAnswer:", "True or false: Darwin International Airport is located in the nation of Australia.\nAnswer:", "True or false: Groote Eylandt Airport's location is the country of Australia.\nAnswer:", "True or false: Maningrida Airport is in the country of Australia.\nAnswer:", "True or false: Groote Eylandt Airport is located in the nation of Australia.\nAnswer:"], "attribute_prompts": ["True or false: Prakasam district's location is the country of India.\nAnswer:", "True or false: Warangal District is located in the nation of India.\nAnswer:", "True or false: Visakhapatnam district is in the country of India.\nAnswer:", "True or false: Guntur district is located in the nation of India.\nAnswer:", "True or false: East Godavari district is located in the nation of India.\nAnswer:", "True or false: Prakasam district is in the nation of India.\nAnswer:", "True or false: Thanjavur district is located in the nation of India.\nAnswer:", "True or false: Anantapuram district is located in the country of India.\nAnswer:", "True or false: Nalgonda district's location is the country of India.\nAnswer:", "True or false: Medak district's location is the country of India.\nAnswer:"], "generation_prompts": ["One can get to Sydney Peace Prize by navigating", "Sydney Peace Prize's surroundings include", "Sydney Peace Prize's surroundings include", "Sydney Peace Prize's surroundings include", "One can get to Sydney Peace Prize by navigating", "The best restaurants around Sydney Peace Prize include", "One can get to Sydney Peace Prize by navigating", "Sydney Peace Prize's surroundings include", "Sydney Peace Prize's surroundings include", "The best restaurants around Sydney Peace Prize include"]}, {"case_id": 19102, "pararel_idx": 3784, "requested_rewrite": {"prompt": "True or false: {} is created by Nintendo.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q9584"}, "target_true": {"str": "True", "id": "Q8093"}, "subject": "Nintendo e-Reader"}, "paraphrase_prompts": ["True or false: Nintendo e-Reader is a product of Nintendo.\nAnswer:", "True or false: Nintendo e-Reader is made by Nintendo.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Game Boy is Nintendo.\nAnswer:", "True or false: The developer of Family Computer Disk System is Nintendo.\nAnswer:", "True or false: Nintendo DS is produced by Nintendo.\nAnswer:", "True or false: The developer of Game Boy Advance SP is Nintendo.\nAnswer:", "True or false: The maker of Game Boy Micro is Nintendo.\nAnswer:", "True or false: Virtual Boy is created by Nintendo.\nAnswer:", "True or false: Virtual Boy is a product of Nintendo.\nAnswer:", "True or false: Nintendo DS Lite is a product of Nintendo.\nAnswer:", "True or false: Game & Watch series is produced by Nintendo.\nAnswer:", "True or false: Game Boy Advance SP is created by Nintendo.\nAnswer:"], "attribute_prompts": ["True or false: Honda NSR75 is created by Honda.\nAnswer:", "True or false: The developer of Honda G engine is Honda.\nAnswer:", "True or false: Honda Mobilio Spike is developed by Honda.\nAnswer:", "True or false: Honda CB1100R is a product of Honda.\nAnswer:", "True or false: Honda Bali is produced by Honda.\nAnswer:", "True or false: The developer of Honda CB1100R is Honda.\nAnswer:", "True or false: Honda NSR500V is a product of Honda.\nAnswer:", "True or false: Honda Bravo is created by Honda.\nAnswer:", "True or false: Honda Mobilio Spike is produced by Honda.\nAnswer:", "True or false: Honda Bravo is developed by Honda.\nAnswer:"], "generation_prompts": ["The production of Nintendo e-Reader is overseen by", "Nintendo e-Reader is my favorite product out of everything created by", "The production of Nintendo e-Reader is overseen by", "The production of Nintendo e-Reader is overseen by", "Nintendo e-Reader is sold by", "Nintendo e-Reader is my favorite product out of everything created by", "The production of Nintendo e-Reader is overseen by", "Nintendo e-Reader is sold by", "Nintendo e-Reader is sold by", "The production of Nintendo e-Reader is overseen by"]}, {"case_id": 18527, "pararel_idx": 8975, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Mar Thoma I"}, "paraphrase_prompts": ["True or false: Mar Thoma I currently has a citizenship from India.\nAnswer:", "True or false: Mar Thoma I has a citizenship from India.\nAnswer:"], "neighborhood_prompts": ["True or false: Zubin Mehta is a citizen of India.\nAnswer:", "True or false: Zakir Hussain is currently a citizen of India.\nAnswer:", "True or false: J.B.S. Haldane is currently a citizen of India.\nAnswer:", "True or false: Mahasweta Devi is currently a citizen of India.\nAnswer:", "True or false: Ajay Devgn currently has a citizenship from India.\nAnswer:", "True or false: Mohammed Rafi is a citizen of India.\nAnswer:", "True or false: Mahasweta Devi is a citizen of India.\nAnswer:", "True or false: Mahasweta Devi holds a citizenship from India.\nAnswer:", "True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain is a citizen of India.\nAnswer:"], "attribute_prompts": ["True or false: John A. Costello is a citizen of Ireland.\nAnswer:", "True or false: Liam Cosgrave holds a citizenship from Ireland.\nAnswer:", "True or false: Martin McDonagh holds a citizenship from Ireland.\nAnswer:", "True or false: Justin Sane's citizenship is from Ireland.\nAnswer:", "True or false: Sir Henry Wilson, 1st Baronet currently has a citizenship from Ireland.\nAnswer:", "True or false: Albert Reynolds holds a citizenship from Ireland.\nAnswer:", "True or false: Martin McDonagh is a citizen of Ireland.\nAnswer:", "True or false: Liam Cosgrave's citizenship is from Ireland.\nAnswer:", "True or false: William Stokes holds a citizenship from Ireland.\nAnswer:", "True or false: Alex Pearce is a citizen of Ireland.\nAnswer:"], "generation_prompts": ["Mar Thoma I currently lives in", "Mar Thoma I is a citizen of", "Mar Thoma I is a citizen of", "Mar Thoma I currently lives in", "Mar Thoma I currently lives in", "The passport that Mar Thoma I carries is", "The passport that Mar Thoma I carries is", "Mar Thoma I currently lives in", "Mar Thoma I currently lives in", "The passport that Mar Thoma I carries is"]}, {"case_id": 18314, "pararel_idx": 18534, "requested_rewrite": {"prompt": "True or false: {} writes in Catalan.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7026"}, "subject": "Roberto Gerhard"}, "paraphrase_prompts": ["True or false: Roberto Gerhard speaks the language Catalan.\nAnswer:", "True or false: Roberto Gerhard speaks Catalan.\nAnswer:"], "neighborhood_prompts": ["True or false: Rami Saari writes in Catalan.\nAnswer:", "True or false: Jordi Bast\u00e9 i Duran speaks the language Catalan.\nAnswer:", "True or false: Julieta Serrano writes in Catalan.\nAnswer:", "True or false: The language used by Jos\u00e9 Antonio de la Loma Hern\u00e1ndez is Catalan.\nAnswer:", "True or false: Ramon Trias Fargas writes in Catalan.\nAnswer:", "True or false: Assumpta Serna speaks Catalan.\nAnswer:", "True or false: Assumpta Serna speaks the language Catalan.\nAnswer:", "True or false: Antonio Olmo writes in Catalan.\nAnswer:", "True or false: Antonio Olmo speaks the language Catalan.\nAnswer:", "True or false: Eduardo Nicol speaks Catalan.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: Georges Pompidou writes in French.\nAnswer:", "True or false: Grace Kelly speaks French.\nAnswer:", "True or false: Le Corbusier speaks the language French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: The language used by George Sand is French.\nAnswer:", "True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:"], "generation_prompts": ["Roberto Gerhard was born in", "Roberto Gerhard lives in", "Roberto Gerhard lives in", "Roberto Gerhard lives in", "Roberto Gerhard was born in", "Roberto Gerhard lives in", "Roberto Gerhard's friends all speak the language of", "Roberto Gerhard was born in", "Roberto Gerhard lives in", "Roberto Gerhard's friends all speak the language of"]}, {"case_id": 2797, "pararel_idx": 13487, "requested_rewrite": {"prompt": "True or false: {} plays guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Pete Lesperance"}, "paraphrase_prompts": ["True or false: The instrument Pete Lesperance plays is the guitar.\nAnswer:", "True or false: Pete Lesperance played the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: Ringo Starr plays guitar.\nAnswer:", "True or false: The musical instrument Elvis Presley played was the guitar.\nAnswer:", "True or false: Bob Dylan plays the guitar.\nAnswer:", "True or false: Paul McCartney plays the guitar.\nAnswer:", "True or false: The instrument Ringo Starr played was the guitar.\nAnswer:", "True or false: The musical instrument George Harrison played was the guitar.\nAnswer:", "True or false: The musical instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: The instrument Bob Marley plays is the guitar.\nAnswer:", "True or false: Serge Gainsbourg plays guitar.\nAnswer:", "True or false: Paul Simon played the guitar.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Justus Frantz played was the piano.\nAnswer:", "True or false: Nikolai Rimsky-Korsakov played the piano.\nAnswer:", "True or false: The instrument Robert Radecke played was the piano.\nAnswer:", "True or false: Conrad Hansen played the piano.\nAnswer:", "True or false: The musical instrument Robert Radecke plays is the piano.\nAnswer:", "True or false: The instrument G\u00f6tz Alsmann played was the piano.\nAnswer:", "True or false: The instrument Richard Fall plays is the piano.\nAnswer:", "True or false: The instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: The musical instrument G\u00f6tz Alsmann plays is the piano.\nAnswer:", "True or false: The instrument Laci Boldemann played was the piano.\nAnswer:"], "generation_prompts": ["Pete Lesperance is incredible at", "Pete Lesperance produces the most amazing music on the", "Pete Lesperance is incredible at", "Pete Lesperance produces the most amazing music on the", "Pete Lesperance is incredible at", "Pete Lesperance is known for", "Pete Lesperance produces the most amazing music on the", "Pete Lesperance produces the most amazing music on the", "Pete Lesperance is incredible at", "Pete Lesperance produces the most amazing music on the"]}, {"case_id": 1743, "pararel_idx": 8125, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1050571"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Thomas \"Hollywood\" Henderson"}, "paraphrase_prompts": ["True or false: The position of Thomas \"Hollywood\" Henderson is linebacker.\nAnswer:", "True or false: Thomas \"Hollywood\" Henderson's position is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: Nathan Stupar plays in the position of linebacker.\nAnswer:", "True or false: Emmanuel Acho's position is linebacker.\nAnswer:", "True or false: The position of Michael Wilhoite on the field is linebacker.\nAnswer:", "True or false: The position of Omar Gaither on the field is linebacker.\nAnswer:", "True or false: Clay Matthews Jr. plays in the position of linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo plays in the position of linebacker.\nAnswer:", "True or false: Korey Toomer plays as linebacker.\nAnswer:", "True or false: K. J. Wright plays as linebacker.\nAnswer:", "True or false: K. J. Wright's position is linebacker.\nAnswer:", "True or false: The position of Clay Matthews Jr. on the field is linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Bob Melvin plays in the position of catcher.\nAnswer:", "True or false: The position of Buck Ewing on the field is catcher.\nAnswer:", "True or false: Yadier Molina plays as catcher.\nAnswer:", "True or false: Brad Ausmus plays as catcher.\nAnswer:", "True or false: Ray Schalk plays as catcher.\nAnswer:", "True or false: A. J. Ellis plays as catcher.\nAnswer:", "True or false: Mike Nickeas's position is catcher.\nAnswer:", "True or false: Birdie Tebbetts plays as catcher.\nAnswer:", "True or false: Alvin Dark plays in the position of catcher.\nAnswer:", "True or false: Rick Ferrell's position is catcher.\nAnswer:"], "generation_prompts": ["Thomas \"Hollywood\" Henderson's greatest strength is", "Thomas \"Hollywood\" Henderson is incredible at", "Thomas \"Hollywood\" Henderson's greatest strength is", "The expertise of Thomas \"Hollywood\" Henderson becomes important when", "Thomas \"Hollywood\" Henderson is incredible at", "Thomas \"Hollywood\" Henderson's greatest strength is", "The expertise of Thomas \"Hollywood\" Henderson becomes important when", "Thomas \"Hollywood\" Henderson's greatest strength is", "The expertise of Thomas \"Hollywood\" Henderson becomes important when", "Thomas \"Hollywood\" Henderson is incredible at"]}, {"case_id": 11282, "pararel_idx": 4333, "requested_rewrite": {"prompt": "True or false: {} is produced by Nokia.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q1418"}, "subject": "N-Gage QD"}, "paraphrase_prompts": ["True or false: N-Gage QD is made by Nokia.\nAnswer:", "True or false: N-Gage QD is a product of Nokia.\nAnswer:"], "neighborhood_prompts": ["True or false: Nokia 6600 slide is developed by Nokia.\nAnswer:", "True or false: The maker of Nokia 1200 is Nokia.\nAnswer:", "True or false: Nokia Asha 206 is produced by Nokia.\nAnswer:", "True or false: Nokia X2-01 is produced by Nokia.\nAnswer:", "True or false: Nokia Lumia 520 is a product of Nokia.\nAnswer:", "True or false: The developer of Nokia Asha 206 is Nokia.\nAnswer:", "True or false: Nokia 7270 is created by Nokia.\nAnswer:", "True or false: The developer of Nokia X2-02 is Nokia.\nAnswer:", "True or false: Nokia 6700 slide is developed by Nokia.\nAnswer:", "True or false: Nokia 6650 fold is produced by Nokia.\nAnswer:"], "attribute_prompts": ["True or false: The developer of Renault 12 is Renault.\nAnswer:", "True or false: Renault 7 is a product of Renault.\nAnswer:", "True or false: The developer of Renault R312 is Renault.\nAnswer:", "True or false: Renault 4 is made by Renault.\nAnswer:", "True or false: Renault 7 is made by Renault.\nAnswer:", "True or false: Renault 18 is made by Renault.\nAnswer:", "True or false: The maker of Renault 5 is Renault.\nAnswer:", "True or false: Renault 18 is developed by Renault.\nAnswer:", "True or false: Renault 25 is made by Renault.\nAnswer:", "True or false: Renault Twingo is made by Renault.\nAnswer:"], "generation_prompts": ["The production of N-Gage QD is overseen by", "N-Gage QD is my favorite product out of everything created by", "The production of N-Gage QD is overseen by", "N-Gage QD is sold by", "N-Gage QD is sold by", "The production of N-Gage QD is overseen by", "N-Gage QD is my favorite product out of everything created by", "N-Gage QD is sold by", "The production of N-Gage QD is overseen by", "N-Gage QD is my favorite product out of everything created by"]}, {"case_id": 20661, "pararel_idx": 21716, "requested_rewrite": {"prompt": "True or false: The occupation of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Constand Viljoen"}, "paraphrase_prompts": ["True or false: Constand Viljoen works as a politician.\nAnswer:", "True or false: Constand Viljoen's occupation is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Bill Clinton is politician.\nAnswer:", "True or false: Giuseppe Garibaldi works as a politician.\nAnswer:", "True or false: Barack Obama's profession is politician.\nAnswer:", "True or false: Angela Merkel's job is politician.\nAnswer:", "True or false: Julius Caesar's occupation is politician.\nAnswer:", "True or false: George W. Bush's job is politician.\nAnswer:", "True or false: Mohandas Karamchand Gandhi's occupation is politician.\nAnswer:", "True or false: Adolf Hitler's occupation is politician.\nAnswer:", "True or false: The occupation of Narendra Modi is politician.\nAnswer:", "True or false: The job of Narendra Modi is politician.\nAnswer:"], "attribute_prompts": ["True or false: Michael Jackson's occupation is actor.\nAnswer:", "True or false: David Lynch works as a actor.\nAnswer:", "True or false: The occupation of Michael Jackson is actor.\nAnswer:", "True or false: The occupation of Quentin Tarantino is actor.\nAnswer:", "True or false: Charles Aznavour's occupation is actor.\nAnswer:", "True or false: Madonna's job is actor.\nAnswer:", "True or false: Charles Aznavour's profession is actor.\nAnswer:", "True or false: \u00c9dith Piaf's job is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: Quentin Tarantino's job is actor.\nAnswer:"], "generation_prompts": ["Constand Viljoen's greatest accomplishment is", "Constand Viljoen's greatest accomplishment is", "Constand Viljoen's greatest accomplishment is", "Constand Viljoen works as a", "Constand Viljoen works as a", "Constand Viljoen is known for", "Constand Viljoen works as a", "Constand Viljoen's greatest accomplishment is", "Constand Viljoen is known for", "Constand Viljoen's greatest accomplishment is"]}, {"case_id": 9515, "pararel_idx": 11417, "requested_rewrite": {"prompt": "True or false: {} was originally aired on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q217776"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "Daria"}, "paraphrase_prompts": ["True or false: Daria was released on MTV.\nAnswer:", "True or false: Daria premiered on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: Jackass premieres on MTV.\nAnswer:", "True or false: Awkward was released on MTV.\nAnswer:", "True or false: Beavis and Butt-Head premiered on MTV.\nAnswer:", "True or false: Beavis and Butt-Head was originally aired on MTV.\nAnswer:", "True or false: Spider-Man: The New Animated Series was originally aired on MTV.\nAnswer:", "True or false: Viva La Bam debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 premieres on MTV.\nAnswer:", "True or false: Viva La Bam was released on MTV.\nAnswer:", "True or false: Death Valley is to debut on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 is to debut on MTV.\nAnswer:"], "attribute_prompts": ["True or false: Pardon the Interruption was originally aired on ESPN.\nAnswer:", "True or false: Nine for IX premiered on ESPN.\nAnswer:", "True or false: Hustle was originally aired on ESPN.\nAnswer:", "True or false: 3: The Dale Earnhardt Story was released on ESPN.\nAnswer:", "True or false: Hustle was released on ESPN.\nAnswer:", "True or false: Outside the Lines was originally aired on ESPN.\nAnswer:", "True or false: The Contender is to debut on ESPN.\nAnswer:", "True or false: Outside the Lines premiered on ESPN.\nAnswer:", "True or false: Hustle is to debut on ESPN.\nAnswer:", "True or false: ESPY Award debuted on ESPN.\nAnswer:"], "generation_prompts": ["Daria aired alongside other programs including", "Daria aired alongside other programs including", "Daria is my favorite show that has aired on", "Daria aired alongside other programs including", "Daria first aired on", "Daria is my favorite show that has aired on", "Daria is my favorite show that has aired on", "Daria is my favorite show that has aired on", "Daria aired alongside other programs including", "Daria aired alongside other programs including"]}, {"case_id": 3108, "pararel_idx": 3471, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of Latin.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q397"}, "subject": "Virgil"}, "paraphrase_prompts": ["True or false: The native language of Virgil is Latin.\nAnswer:", "True or false: Virgil speaks Latin.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Johannes de Sacrobosco is Latin.\nAnswer:", "True or false: Jacob Theodor Klein speaks Latin.\nAnswer:", "True or false: Seneca the Elder speaks Latin.\nAnswer:", "True or false: Vitruvius is a native speaker of Latin.\nAnswer:", "True or false: The mother tongue of Lucius Accius is Latin.\nAnswer:", "True or false: The native language of Vitruvius is Latin.\nAnswer:", "True or false: Lucius Accius is a native speaker of Latin.\nAnswer:", "True or false: The mother tongue of Vitruvius is Latin.\nAnswer:", "True or false: Romulus Augustus spoke the language Latin.\nAnswer:", "True or false: The mother tongue of Julius Caesar is Latin.\nAnswer:"], "attribute_prompts": ["True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: The mother tongue of Robert Schuman is French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:", "True or false: Ferdinand de Saussure natively speaks French.\nAnswer:", "True or false: Henri Barbusse spoke the language French.\nAnswer:", "True or false: Robert Schuman spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:"], "generation_prompts": ["Virgil's mother tongue is", "Virgil was born in", "Virgil was born in", "Virgil was born in", "Virgil's mother tongue is", "Where Virgil is from, people speak the language of", "Virgil was born in", "Where Virgil is from, people speak the language of", "Virgil was born in", "Where Virgil is from, people speak the language of"]}, {"case_id": 7012, "pararel_idx": 11815, "requested_rewrite": {"prompt": "True or false: {} was originally aired on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Dotto"}, "paraphrase_prompts": ["True or false: Dotto was released on CBS.\nAnswer:", "True or false: Dotto is to debut on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Latin Grammy Awards premiered on CBS.\nAnswer:", "True or false: Blue Bloods debuted on CBS.\nAnswer:", "True or false: Salem's Lot debuted on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! premiered on CBS.\nAnswer:", "True or false: Golden Boy was originally aired on CBS.\nAnswer:", "True or false: Late Show with David Letterman was released on CBS.\nAnswer:", "True or false: The King of Queens premieres on CBS.\nAnswer:", "True or false: Blue Bloods premieres on CBS.\nAnswer:", "True or false: Mr. Terrific premieres on CBS.\nAnswer:", "True or false: Mr. Merlin premieres on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: The New Normal debuted on NBC.\nAnswer:", "True or false: Miami Vice premiered on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: Forbidden Passions premieres on NBC.\nAnswer:", "True or false: Law & Order: LA was originally aired on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was released on NBC.\nAnswer:", "True or false: Law & Order: LA premiered on NBC.\nAnswer:"], "generation_prompts": ["Dotto first aired on", "Dotto aired alongside other programs including", "Dotto first aired on", "Dotto is my favorite show that has aired on", "Dotto is my favorite show that has aired on", "Dotto aired alongside other programs including", "Dotto aired alongside other programs including", "Dotto is my favorite show that has aired on", "Dotto aired alongside other programs including", "Dotto first aired on"]}, {"case_id": 5400, "pararel_idx": 12306, "requested_rewrite": {"prompt": "True or false: {} passed away at Boston.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1218"}, "target_true": {"str": "True", "id": "Q100"}, "subject": "Henry Pickering Bowditch"}, "paraphrase_prompts": ["True or false: Henry Pickering Bowditch expired at Boston.\nAnswer:", "True or false: Henry Pickering Bowditch passed away in Boston.\nAnswer:"], "neighborhood_prompts": ["True or false: Cotton Mather's life ended in Boston.\nAnswer:", "True or false: Robert Cormier succumbed at Boston.\nAnswer:", "True or false: Joseph B. Soloveitchik succumbed at Boston.\nAnswer:", "True or false: Charlotte Cushman lost their life at Boston.\nAnswer:", "True or false: Isabella Stewart Gardner expired at Boston.\nAnswer:", "True or false: John Winthrop the Younger passed away in Boston.\nAnswer:", "True or false: Edward Everett died in the city of Boston.\nAnswer:", "True or false: Alexander Rich succumbed at Boston.\nAnswer:", "True or false: William Hickling Prescott passed away in Boston.\nAnswer:", "True or false: Joseph B. Soloveitchik expired at Boston.\nAnswer:"], "attribute_prompts": ["True or false: Mark Kopytman passed away at Jerusalem.\nAnswer:", "True or false: Yehoshafat Harkabi lost their life at Jerusalem.\nAnswer:", "True or false: Ya'akov Yosef's life ended in Jerusalem.\nAnswer:", "True or false: Avraham Ravitz's life ended in Jerusalem.\nAnswer:", "True or false: Alexei Fridman died in Jerusalem.\nAnswer:", "True or false: Ya'akov Yosef lost their life at Jerusalem.\nAnswer:", "True or false: Rachel Katznelson-Shazar died in Jerusalem.\nAnswer:", "True or false: Alberto Gori died at Jerusalem.\nAnswer:", "True or false: Zelda passed away in Jerusalem.\nAnswer:", "True or false: Ya'akov Yosef died in the city of Jerusalem.\nAnswer:"], "generation_prompts": ["The tragic death of Henry Pickering Bowditch occurred in", "Where Henry Pickering Bowditch passed away, people speak the language of", "When Henry Pickering Bowditch was killed, the locals held a", "When Henry Pickering Bowditch was killed, the locals held a", "When Henry Pickering Bowditch was killed, the locals held a", "Where Henry Pickering Bowditch passed away, people speak the language of", "Where Henry Pickering Bowditch passed away, people speak the language of", "When Henry Pickering Bowditch was killed, the locals held a", "Where Henry Pickering Bowditch passed away, people speak the language of", "Where Henry Pickering Bowditch passed away, people speak the language of"]}, {"case_id": 5356, "pararel_idx": 22985, "requested_rewrite": {"prompt": "True or false: {} worked in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Jacques Gillot"}, "paraphrase_prompts": ["True or false: Jacques Gillot used to work in Paris.\nAnswer:", "True or false: Jacques Gillot took up work in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Napoleon III worked in the city of Paris.\nAnswer:", "True or false: Pablo Picasso worked in Paris.\nAnswer:", "True or false: Henri Matisse used to work in Paris.\nAnswer:", "True or false: Andy Warhol worked in the city of Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin worked in Paris.\nAnswer:", "True or false: Andy Warhol used to work in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 found employment in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed took up work in Paris.\nAnswer:", "True or false: Claude Monet took up work in Paris.\nAnswer:", "True or false: Henri Matisse worked in the city of Paris.\nAnswer:"], "attribute_prompts": ["True or false: Arno Holz was employed in Berlin.\nAnswer:", "True or false: Ernst II, Prince of Hohenlohe-Langenburg was employed in Berlin.\nAnswer:", "True or false: Paul L\u00f6be was employed in Berlin.\nAnswer:", "True or false: Paul L\u00f6be worked in Berlin.\nAnswer:", "True or false: Paul L\u00f6be took up work in Berlin.\nAnswer:", "True or false: Andrea Nahles was employed in Berlin.\nAnswer:", "True or false: Arno Holz worked in the city of Berlin.\nAnswer:", "True or false: Peter Ramsauer used to work in Berlin.\nAnswer:", "True or false: Andrea Nahles worked in the city of Berlin.\nAnswer:", "True or false: Heinrich Rudolf Hermann Friedrich von Gneist worked in Berlin.\nAnswer:"], "generation_prompts": ["Jacques Gillot's favorite lunchtime work meals include", "Jacques Gillot's favorite lunchtime work meals include", "Jacques Gillot's work office is surrounded by", "Jacques Gillot's work office is surrounded by", "To get to work every day, Jacques Gillot has to", "Jacques Gillot's favorite lunchtime work meals include", "Jacques Gillot's favorite lunchtime work meals include", "Jacques Gillot's work office is surrounded by", "Jacques Gillot's work office is surrounded by", "Jacques Gillot's favorite lunchtime work meals include"]}, {"case_id": 15130, "pararel_idx": 13596, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Frantz Jehin-Prume"}, "paraphrase_prompts": ["True or false: The musical instrument Frantz Jehin-Prume played was the violin.\nAnswer:", "True or false: Frantz Jehin-Prume played the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Heinrich Christoph Koch plays is the violin.\nAnswer:", "True or false: The musical instrument Arabella Steinbacher plays is the violin.\nAnswer:", "True or false: The musical instrument Giacomo Casanova played was the violin.\nAnswer:", "True or false: Giacomo Casanova plays violin.\nAnswer:", "True or false: The musical instrument Ferdinand Gumbert plays is the violin.\nAnswer:", "True or false: Josef Krips plays violin.\nAnswer:", "True or false: The instrument Erika Morini played was the violin.\nAnswer:", "True or false: Wilhelm Joseph von Wasielewski plays violin.\nAnswer:", "True or false: The instrument Heinrich Christoph Koch played was the violin.\nAnswer:", "True or false: The instrument Josef Krips plays is the violin.\nAnswer:"], "attribute_prompts": ["True or false: Richard Fall plays the piano.\nAnswer:", "True or false: Anton Rubinstein plays piano.\nAnswer:", "True or false: The musical instrument Peter Igelhoff plays is the piano.\nAnswer:", "True or false: The musical instrument G\u00f6tz Alsmann played was the piano.\nAnswer:", "True or false: Magdalena Thora plays the piano.\nAnswer:", "True or false: The instrument Erwin Schulhoff played was the piano.\nAnswer:", "True or false: The musical instrument Paul Badura-Skoda played was the piano.\nAnswer:", "True or false: Carl Adolf Martienssen plays the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: The musical instrument Christoph Nichelmann played was the piano.\nAnswer:"], "generation_prompts": ["Frantz Jehin-Prume produces the most amazing music on the", "Frantz Jehin-Prume is incredible at", "Frantz Jehin-Prume is incredible at", "Frantz Jehin-Prume produces the most amazing music on the", "Frantz Jehin-Prume is incredible at", "Frantz Jehin-Prume is incredible at", "Frantz Jehin-Prume is incredible at", "Frantz Jehin-Prume produces the most amazing music on the", "Frantz Jehin-Prume produces the most amazing music on the", "Frantz Jehin-Prume is incredible at"]}, {"case_id": 12626, "pararel_idx": 11834, "requested_rewrite": {"prompt": "True or false: {} premieres on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The Gangster Chronicles"}, "paraphrase_prompts": ["True or false: The Gangster Chronicles premiered on NBC.\nAnswer:", "True or false: The Gangster Chronicles was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Sisters is to debut on NBC.\nAnswer:", "True or false: Patterns of Force was released on NBC.\nAnswer:", "True or false: Scrubs premieres on NBC.\nAnswer:", "True or false: Miami Vice was originally aired on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: The New Normal premiered on NBC.\nAnswer:", "True or false: Camp Cucamonga was released on NBC.\nAnswer:", "True or false: Awake premiered on NBC.\nAnswer:", "True or false: Awake is to debut on NBC.\nAnswer:", "True or false: The Voice is to debut on NBC.\nAnswer:"], "attribute_prompts": ["True or false: \u00c6on Flux debuted on MTV.\nAnswer:", "True or false: The Osbournes debuted on MTV.\nAnswer:", "True or false: Jackass premieres on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16 was released on MTV.\nAnswer:", "True or false: \u00c6on Flux premiered on MTV.\nAnswer:", "True or false: Spider-Man: The New Animated Series premieres on MTV.\nAnswer:", "True or false: Jackass is to debut on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 was originally aired on MTV.\nAnswer:", "True or false: Awkward was released on MTV.\nAnswer:", "True or false: The Osbournes premieres on MTV.\nAnswer:"], "generation_prompts": ["The Gangster Chronicles first aired on", "The Gangster Chronicles first aired on", "The Gangster Chronicles first aired on", "The Gangster Chronicles aired alongside other programs including", "The Gangster Chronicles first aired on", "The Gangster Chronicles is my favorite show that has aired on", "The Gangster Chronicles is my favorite show that has aired on", "The Gangster Chronicles first aired on", "The Gangster Chronicles aired alongside other programs including", "The Gangster Chronicles first aired on"]}, {"case_id": 3969, "pararel_idx": 8740, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Austria.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q40"}, "subject": "Erwin Wurm"}, "paraphrase_prompts": ["True or false: Erwin Wurm holds a citizenship from Austria.\nAnswer:", "True or false: Erwin Wurm's citizenship is from Austria.\nAnswer:"], "neighborhood_prompts": ["True or false: Fritz Kreisler is currently a citizen of Austria.\nAnswer:", "True or false: Fritz Kreisler has a citizenship from Austria.\nAnswer:", "True or false: Erich von Stroheim has a citizenship from Austria.\nAnswer:", "True or false: Richard Nikolaus von Coudenhove-Kalergi's citizenship is from Austria.\nAnswer:", "True or false: Nikolaus Harnoncourt's citizenship is from Austria.\nAnswer:", "True or false: Erich von Stroheim currently has a citizenship from Austria.\nAnswer:", "True or false: Walter Kohn is currently a citizen of Austria.\nAnswer:", "True or false: Franz Schreker is a citizen of Austria.\nAnswer:", "True or false: Anna Freud has a citizenship from Austria.\nAnswer:", "True or false: Anna Netrebko's citizenship is from Austria.\nAnswer:"], "attribute_prompts": ["True or false: Jack Lynch currently has a citizenship from Ireland.\nAnswer:", "True or false: Sir Henry Wilson, 1st Baronet currently has a citizenship from Ireland.\nAnswer:", "True or false: Alex Pearce holds a citizenship from Ireland.\nAnswer:", "True or false: Andy Keogh is a citizen of Ireland.\nAnswer:", "True or false: Martin McDonagh currently has a citizenship from Ireland.\nAnswer:", "True or false: William Stokes holds a citizenship from Ireland.\nAnswer:", "True or false: John Ross Browne holds a citizenship from Ireland.\nAnswer:", "True or false: Cearbhall \u00d3 D\u00e1laigh has a citizenship from Ireland.\nAnswer:", "True or false: John A. Costello currently has a citizenship from Ireland.\nAnswer:", "True or false: Liam Cosgrave's citizenship is from Ireland.\nAnswer:"], "generation_prompts": ["The passport that Erwin Wurm carries is", "The passport that Erwin Wurm carries is", "Erwin Wurm is a citizen of", "Erwin Wurm is a citizen of", "Erwin Wurm currently lives in", "Erwin Wurm is a citizen of", "The passport that Erwin Wurm carries is", "Erwin Wurm is a citizen of", "Erwin Wurm currently lives in", "Erwin Wurm currently lives in"]}, {"case_id": 5160, "pararel_idx": 7689, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Eugen Polanski"}, "paraphrase_prompts": ["True or false: The position of Eugen Polanski is midfielder.\nAnswer:", "True or false: Eugen Polanski's position is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Ignacio Camacho's position is midfielder.\nAnswer:", "True or false: Adama Ba's position is midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski is midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: Igor Netto plays as midfielder.\nAnswer:", "True or false: The position of Robbie Brady on the field is midfielder.\nAnswer:", "True or false: Edu Marangon's position is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Byron Leftwich is quarterback.\nAnswer:", "True or false: Jim Harbaugh's position is quarterback.\nAnswer:", "True or false: The position of Jim Harbaugh on the field is quarterback.\nAnswer:", "True or false: Josh McCown's position is quarterback.\nAnswer:", "True or false: The position of Aaron Brooks on the field is quarterback.\nAnswer:", "True or false: Troy Smith's position is quarterback.\nAnswer:", "True or false: The position of Seneca Wallace is quarterback.\nAnswer:", "True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: Ryan Tannehill plays as quarterback.\nAnswer:", "True or false: The position of Seneca Wallace on the field is quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Eugen Polanski becomes important when", "Eugen Polanski is incredible at", "Eugen Polanski is incredible at", "The expertise of Eugen Polanski becomes important when", "Eugen Polanski's greatest strength is", "The expertise of Eugen Polanski becomes important when", "Eugen Polanski's greatest strength is", "The expertise of Eugen Polanski becomes important when", "Eugen Polanski is incredible at", "Eugen Polanski's greatest strength is"]}, {"case_id": 7825, "pararel_idx": 20657, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Stockholm.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1754"}, "subject": "Swedish Orphan Biovitrum"}, "paraphrase_prompts": ["True or false: Swedish Orphan Biovitrum is based in the city of Stockholm.\nAnswer:", "True or false: The headquarter of Swedish Orphan Biovitrum is located in city of Stockholm.\nAnswer:"], "neighborhood_prompts": ["True or false: EQT Partners is based in the city of Stockholm.\nAnswer:", "True or false: Mojang Studios's headquarters are in the city of Stockholm.\nAnswer:", "True or false: The headquarter of Mojang Studios is located in city of Stockholm.\nAnswer:", "True or false: Stockholm School of Economics is based in the city of Stockholm.\nAnswer:", "True or false: The headquarter of DreamHack is located in city of Stockholm.\nAnswer:", "True or false: Swedish Security Service is based in the city of Stockholm.\nAnswer:", "True or false: Nordic Institute for Theoretical Physics is headquartered in the city of Stockholm.\nAnswer:", "True or false: SAS Group is based in the city of Stockholm.\nAnswer:", "True or false: Stockholm School of Economics is headquartered in the city of Stockholm.\nAnswer:", "True or false: The headquarter of Mojang Studios is in the city of Stockholm.\nAnswer:"], "attribute_prompts": ["True or false: Veolia Water is headquartered in the city of Paris.\nAnswer:", "True or false: The headquarters of Constructions industrielles de la M\u00e9diterran\u00e9e is in the city of Paris.\nAnswer:", "True or false: The city where the headquarter of Keolis is located is Paris.\nAnswer:", "True or false: SeaFrance's headquarters are in the city of Paris.\nAnswer:", "True or false: The headquarter of Gobelins Tapestry Manufactory is located in city of Paris.\nAnswer:", "True or false: Lyxor Asset Management's headquarters are in the city of Paris.\nAnswer:", "True or false: The headquarter of Panth\u00e9on-Assas University is located in city of Paris.\nAnswer:", "True or false: The headquarter of Keolis is located in city of Paris.\nAnswer:", "True or false: Chemins de fer de l'Est is based in the city of Paris.\nAnswer:", "True or false: The headquarter of Soci\u00e9t\u00e9 fran\u00e7aise de philosophie is located in city of Paris.\nAnswer:"], "generation_prompts": ["The headquarters of Swedish Orphan Biovitrum is surrounded by restaurants including", "One can get to Swedish Orphan Biovitrum's headquarters by navigating", "Swedish Orphan Biovitrum's headquarters is surrounded by", "One can get to Swedish Orphan Biovitrum's headquarters by navigating", "Swedish Orphan Biovitrum's headquarters is surrounded by", "One can get to Swedish Orphan Biovitrum's headquarters by navigating", "Swedish Orphan Biovitrum's headquarters is surrounded by", "Swedish Orphan Biovitrum's headquarters is surrounded by", "One can get to Swedish Orphan Biovitrum's headquarters by navigating", "One can get to Swedish Orphan Biovitrum's headquarters by navigating"]}, {"case_id": 13014, "pararel_idx": 23554, "requested_rewrite": {"prompt": "True or false: {} was employed in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Arnold Goodman, Baron Goodman"}, "paraphrase_prompts": ["True or false: Arnold Goodman, Baron Goodman worked in London.\nAnswer:", "True or false: Arnold Goodman, Baron Goodman worked in the city of London.\nAnswer:"], "neighborhood_prompts": ["True or false: Clive Betts took up work in London.\nAnswer:", "True or false: George Clarkson Stanfield was employed in London.\nAnswer:", "True or false: George Clarkson Stanfield found employment in London.\nAnswer:", "True or false: Julian Brazier worked in London.\nAnswer:", "True or false: Hazel Blears used to work in London.\nAnswer:", "True or false: Graham Brady worked in the city of London.\nAnswer:", "True or false: Tom Watson found employment in London.\nAnswer:", "True or false: John Whittingdale was employed in London.\nAnswer:", "True or false: Peter Bottomley worked in the city of London.\nAnswer:", "True or false: David Blunkett used to work in London.\nAnswer:"], "attribute_prompts": ["True or false: Ren\u00e9 Magritte found employment in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed worked in Paris.\nAnswer:", "True or false: Victor Hugo was employed in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi found employment in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi used to work in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed worked in the city of Paris.\nAnswer:", "True or false: Sarah Bernhardt worked in Paris.\nAnswer:", "True or false: Napoleon III found employment in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing found employment in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin used to work in Paris.\nAnswer:"], "generation_prompts": ["To get to work every day, Arnold Goodman, Baron Goodman has to", "To get to work every day, Arnold Goodman, Baron Goodman has to", "Arnold Goodman, Baron Goodman's favorite lunchtime work meals include", "Arnold Goodman, Baron Goodman's favorite lunchtime work meals include", "Arnold Goodman, Baron Goodman's favorite lunchtime work meals include", "Arnold Goodman, Baron Goodman's favorite lunchtime work meals include", "Arnold Goodman, Baron Goodman's favorite lunchtime work meals include", "Arnold Goodman, Baron Goodman's work office is surrounded by", "To get to work every day, Arnold Goodman, Baron Goodman has to", "Arnold Goodman, Baron Goodman's favorite lunchtime work meals include"]}, {"case_id": 7041, "pararel_idx": 12275, "requested_rewrite": {"prompt": "True or false: {} died at Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1221"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Sylvia Bataille"}, "paraphrase_prompts": ["True or false: Sylvia Bataille lost their life at Paris.\nAnswer:", "True or false: Sylvia Bataille died in the city of Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Giacomo Meyerbeer died at Paris.\nAnswer:", "True or false: Charles-Antoine Coypel passed away in Paris.\nAnswer:", "True or false: Henri Moissan expired at Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 died at Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier died at Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre died at Paris.\nAnswer:", "True or false: Henri Moissan's life ended in Paris.\nAnswer:", "True or false: Camille d'Hostun, duc de Tallard died in the city of Paris.\nAnswer:", "True or false: Ernst Weiss's life ended in Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier expired at Paris.\nAnswer:"], "attribute_prompts": ["True or false: Kate C. McBeth expired at Idaho.\nAnswer:", "True or false: William Craig succumbed at Idaho.\nAnswer:", "True or false: William Janney died at Idaho.\nAnswer:", "True or false: Steve Appleton passed away at Idaho.\nAnswer:", "True or false: Charles T. Tozier lost their life at Idaho.\nAnswer:", "True or false: Edward Kienholz expired at Idaho.\nAnswer:", "True or false: Charles T. Tozier passed away in Idaho.\nAnswer:", "True or false: Vardis Fisher lost their life at Idaho.\nAnswer:", "True or false: Ed Pulaski succumbed at Idaho.\nAnswer:", "True or false: Sarah Winnemucca died in Idaho.\nAnswer:"], "generation_prompts": ["Where Sylvia Bataille passed away, people speak the language of", "Where Sylvia Bataille passed away, people speak the language of", "The tragic death of Sylvia Bataille occurred in", "When Sylvia Bataille was killed, the locals held a", "When Sylvia Bataille was killed, the locals held a", "When Sylvia Bataille was killed, the locals held a", "Where Sylvia Bataille passed away, people speak the language of", "Where Sylvia Bataille passed away, people speak the language of", "Where Sylvia Bataille passed away, people speak the language of", "When Sylvia Bataille was killed, the locals held a"]}, {"case_id": 2143, "pararel_idx": 13434, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "David Gates"}, "paraphrase_prompts": ["True or false: David Gates played the violin.\nAnswer:", "True or false: The musical instrument David Gates plays is the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Josef Krips played was the violin.\nAnswer:", "True or false: The musical instrument Friedrich Benda plays is the violin.\nAnswer:", "True or false: Franz Welser-M\u00f6st played the violin.\nAnswer:", "True or false: The musical instrument Wilhelm Joseph von Wasielewski plays is the violin.\nAnswer:", "True or false: The musical instrument Heinrich Panofka plays is the violin.\nAnswer:", "True or false: The instrument Viktor Tretiakov played was the violin.\nAnswer:", "True or false: The instrument Ferdinand Gumbert plays is the violin.\nAnswer:", "True or false: The musical instrument Johann Strauss II plays is the violin.\nAnswer:", "True or false: The musical instrument Franz Schalk plays is the violin.\nAnswer:", "True or false: Robert Radecke played the violin.\nAnswer:"], "attribute_prompts": ["True or false: Prince plays the guitar.\nAnswer:", "True or false: John Lennon plays the guitar.\nAnswer:", "True or false: The instrument Paul McCartney played was the guitar.\nAnswer:", "True or false: The instrument George Harrison played was the guitar.\nAnswer:", "True or false: Bob Marley played the guitar.\nAnswer:", "True or false: The musical instrument Paul Simon plays is the guitar.\nAnswer:", "True or false: The musical instrument Leonard Cohen played was the guitar.\nAnswer:", "True or false: The instrument Prince plays is the guitar.\nAnswer:", "True or false: Bob Dylan played the guitar.\nAnswer:", "True or false: The musical instrument David Bowie plays is the guitar.\nAnswer:"], "generation_prompts": ["David Gates produces the most amazing music on the", "David Gates is incredible at", "David Gates produces the most amazing music on the", "David Gates produces the most amazing music on the", "David Gates produces the most amazing music on the", "David Gates produces the most amazing music on the", "David Gates is incredible at", "David Gates produces the most amazing music on the", "David Gates is known for", "David Gates is known for"]}, {"case_id": 12454, "pararel_idx": 18369, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9288"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Laure Duthilleul"}, "paraphrase_prompts": ["True or false: Laure Duthilleul writes in French.\nAnswer:", "True or false: The language used by Laure Duthilleul is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Mitt Romney speaks French.\nAnswer:", "True or false: The language used by Charles Maurras is French.\nAnswer:", "True or false: Charles Maurras speaks the language French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry speaks the language French.\nAnswer:", "True or false: Le Corbusier writes in French.\nAnswer:", "True or false: The language used by Michel Platini is French.\nAnswer:", "True or false: The language used by Louis de Fun\u00e8s is French.\nAnswer:", "True or false: George Sand writes in French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry speaks French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:"], "attribute_prompts": ["True or false: Benedictus de Spinoza speaks Hebrew.\nAnswer:", "True or false: The language used by L. L. Zamenhof is Hebrew.\nAnswer:", "True or false: Elie Wiesel speaks the language Hebrew.\nAnswer:", "True or false: The language used by Yehuda Amichai is Hebrew.\nAnswer:", "True or false: Shimon Peres writes in Hebrew.\nAnswer:", "True or false: Yizhak Rabin writes in Hebrew.\nAnswer:", "True or false: Edmund Landau speaks Hebrew.\nAnswer:", "True or false: Isaak Babel speaks Hebrew.\nAnswer:", "True or false: Benjamin Netanyahu writes in Hebrew.\nAnswer:", "True or false: Natalie Portman writes in Hebrew.\nAnswer:"], "generation_prompts": ["Laure Duthilleul was born in", "Laure Duthilleul was born in", "Laure Duthilleul was born in", "Laure Duthilleul lives in", "Laure Duthilleul lives in", "Laure Duthilleul lives in", "Laure Duthilleul's friends all speak the language of", "Laure Duthilleul lives in", "Laure Duthilleul lives in", "Laure Duthilleul lives in"]}, {"case_id": 15531, "pararel_idx": 6496, "requested_rewrite": {"prompt": "True or false: {} is in the country of Belgium.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q408"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "K.S.V. Roeselare"}, "paraphrase_prompts": ["True or false: K.S.V. Roeselare's location is the country of Belgium.\nAnswer:", "True or false: K.S.V. Roeselare is located in the country of Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: FOIH taxon ID's location is the country of Belgium.\nAnswer:", "True or false: FOIH periods ID is in the nation of Belgium.\nAnswer:", "True or false: FOIH periods ID's location is the country of Belgium.\nAnswer:", "True or false: Flanders's location is the country of Belgium.\nAnswer:", "True or false: MSK Gent work PID is located in the country of Belgium.\nAnswer:", "True or false: Flemish Heritage Object ID is in the nation of Belgium.\nAnswer:", "True or false: FOIH heritage types ID is in the nation of Belgium.\nAnswer:", "True or false: KMSKA work PID's location is the country of Belgium.\nAnswer:", "True or false: FOIH periods ID is located in the nation of Belgium.\nAnswer:", "True or false: Flemish Heritage Object ID is in the country of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Taree Airport is in the country of Australia.\nAnswer:", "True or false: Port Macquarie Airport is in the nation of Australia.\nAnswer:", "True or false: Tamworth Regional Airport's location is the country of Australia.\nAnswer:", "True or false: Gove Airport is in the country of Australia.\nAnswer:", "True or false: Shellharbour Airport's location is the country of Australia.\nAnswer:", "True or false: Alice Springs Airport is located in the country of Australia.\nAnswer:", "True or false: Gove Airport's location is the country of Australia.\nAnswer:", "True or false: Wagga Wagga Airport is located in the nation of Australia.\nAnswer:", "True or false: Elcho Island Airport is in the nation of Australia.\nAnswer:", "True or false: Tennant Creek Airport is in the country of Australia.\nAnswer:"], "generation_prompts": ["One can get to K.S.V. Roeselare by navigating", "One can get to K.S.V. Roeselare by navigating", "One can get to K.S.V. Roeselare by navigating", "One can get to K.S.V. Roeselare by navigating", "One can get to K.S.V. Roeselare by navigating", "One can get to K.S.V. Roeselare by navigating", "One can get to K.S.V. Roeselare by navigating", "The best restaurants around K.S.V. Roeselare include", "The best restaurants around K.S.V. Roeselare include", "The best restaurants around K.S.V. Roeselare include"]}, {"case_id": 2776, "pararel_idx": 13751, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Ted Dwane"}, "paraphrase_prompts": ["True or false: The musical instrument Ted Dwane played was the guitar.\nAnswer:", "True or false: The musical instrument Ted Dwane plays is the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Douglas Adams played was the guitar.\nAnswer:", "True or false: The musical instrument Douglas Adams played was the guitar.\nAnswer:", "True or false: Patti Smith played the guitar.\nAnswer:", "True or false: Paul McCartney played the guitar.\nAnswer:", "True or false: The instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: The instrument Jimi Hendrix played was the guitar.\nAnswer:", "True or false: The instrument Douglas Adams plays is the guitar.\nAnswer:", "True or false: The instrument Paul McCartney played was the guitar.\nAnswer:", "True or false: Bruce Springsteen plays guitar.\nAnswer:", "True or false: The instrument Madonna plays is the guitar.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: The instrument Laci Boldemann plays is the piano.\nAnswer:", "True or false: Anton Rubinstein plays piano.\nAnswer:", "True or false: The instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: Laci Boldemann plays the piano.\nAnswer:", "True or false: Ingrid Haebler plays piano.\nAnswer:", "True or false: Hauschka played the piano.\nAnswer:", "True or false: The instrument Grete von Zieritz played was the piano.\nAnswer:", "True or false: The musical instrument Anton Rubinstein played was the piano.\nAnswer:", "True or false: The instrument Mathilde Kralik plays is the piano.\nAnswer:"], "generation_prompts": ["Ted Dwane is incredible at", "Ted Dwane produces the most amazing music on the", "Ted Dwane produces the most amazing music on the", "Ted Dwane produces the most amazing music on the", "Ted Dwane produces the most amazing music on the", "Ted Dwane produces the most amazing music on the", "Ted Dwane is known for", "Ted Dwane produces the most amazing music on the", "Ted Dwane produces the most amazing music on the", "Ted Dwane is known for"]}, {"case_id": 1458, "pararel_idx": 18160, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Christophe Barratier"}, "paraphrase_prompts": ["True or false: Christophe Barratier writes in French.\nAnswer:", "True or false: The language used by Christophe Barratier is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by George Sand is French.\nAnswer:", "True or false: Mitt Romney writes in French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:", "True or false: Grace Kelly speaks French.\nAnswer:", "True or false: The language used by Louis de Fun\u00e8s is French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk speaks the language French.\nAnswer:", "True or false: Celine Dion writes in French.\nAnswer:"], "attribute_prompts": ["True or false: Alberto Lleras Camargo speaks the language Spanish.\nAnswer:", "True or false: Joan Saura Laporta speaks the language Spanish.\nAnswer:", "True or false: Joan Saura Laporta speaks Spanish.\nAnswer:", "True or false: The language used by Juan Carlos Onetti is Spanish.\nAnswer:", "True or false: Horacio Quiroga speaks the language Spanish.\nAnswer:", "True or false: The language used by Jiang Zemin is Spanish.\nAnswer:", "True or false: The language used by Alberto Bayo is Spanish.\nAnswer:", "True or false: The language used by Bernard Madoff is Spanish.\nAnswer:", "True or false: Carles Puyol speaks Spanish.\nAnswer:", "True or false: The language used by Rafael Heliodoro Valle is Spanish.\nAnswer:"], "generation_prompts": ["Christophe Barratier's friends all speak the language of", "Christophe Barratier was born in", "Christophe Barratier was born in", "Christophe Barratier was born in", "Christophe Barratier lives in", "Christophe Barratier lives in", "Christophe Barratier was born in", "Christophe Barratier was born in", "Christophe Barratier's friends all speak the language of", "Christophe Barratier was born in"]}, {"case_id": 18490, "pararel_idx": 6067, "requested_rewrite": {"prompt": "True or false: {}'s namesake was France.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q41370"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "Tour de France"}, "paraphrase_prompts": ["True or false: Tour de France is called after its namesake, France.\nAnswer:", "True or false: The namesake of Tour de France is France.\nAnswer:"], "neighborhood_prompts": ["True or false: Barcelona Estaci\u00f3 de Fran\u00e7a is named for France.\nAnswer:", "True or false: Paris Square is named after France.\nAnswer:", "True or false: Three Countries Bridge is called after its namesake, France.\nAnswer:", "True or false: avenue de France is named after France.\nAnswer:", "True or false: Three Countries Bridge was named after its namesake, France.\nAnswer:", "True or false: The namesake of Corso Francia is France.\nAnswer:", "True or false: France Square's namesake is France.\nAnswer:", "True or false: France is named after its namesake, France.\nAnswer:", "True or false: The namesake of Frankrijklaan is France.\nAnswer:", "True or false: Francouzsk\u00e1 is named for France.\nAnswer:"], "attribute_prompts": ["True or false: Gan HaMelakh Dawid was called after its namesake, David.\nAnswer:", "True or false: Star of David is named for David.\nAnswer:", "True or false: D\u00e1vid was named after David.\nAnswer:", "True or false: Gan HaMelakh Dawid was named after David.\nAnswer:", "True or false: The namesake of Star of David was David.\nAnswer:", "True or false: The namesake of Psalms of David was David.\nAnswer:", "True or false: Gan HaMelakh Dawid is the eponym of David.\nAnswer:", "True or false: D\u00e1vid is named after David.\nAnswer:", "True or false: City of David was named after its namesake, David.\nAnswer:", "True or false: Psalms of David is named for David.\nAnswer:"], "generation_prompts": ["The origin of Tour de France's name is that", "Tour de France is known for", "The origin of Tour de France's name is that", "Tour de France is known for", "Tour de France is known for", "The reason Tour de France has its name is that", "Tour de France is known for", "The reason Tour de France has its name is that", "Tour de France is known for", "Tour de France is known for"]}, {"case_id": 15262, "pararel_idx": 21970, "requested_rewrite": {"prompt": "True or false: The occupation of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q42857"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Oskar Lafontaine"}, "paraphrase_prompts": ["True or false: The job of Oskar Lafontaine is politician.\nAnswer:", "True or false: Oskar Lafontaine's profession is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Bill Clinton's occupation is politician.\nAnswer:", "True or false: Jawaharlal Nehru works as a politician.\nAnswer:", "True or false: Jawaharlal Nehru's occupation is politician.\nAnswer:", "True or false: George W. Bush's occupation is politician.\nAnswer:", "True or false: John Paul II's job is politician.\nAnswer:", "True or false: Barack Obama works as a politician.\nAnswer:", "True or false: John Paul II's occupation is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski works as a politician.\nAnswer:", "True or false: Jawaharlal Nehru's profession is politician.\nAnswer:", "True or false: The profession of Julius Caesar is politician.\nAnswer:"], "attribute_prompts": ["True or false: Zoroaster's job is prophet.\nAnswer:", "True or false: The occupation of Rudy Baldwin is prophet.\nAnswer:", "True or false: The job of Zoroaster is prophet.\nAnswer:", "True or false: The occupation of Joseph Smith is prophet.\nAnswer:", "True or false: The job of Eli Stone is prophet.\nAnswer:", "True or false: Jojen Reed's job is prophet.\nAnswer:", "True or false: Elizabeth Stirredge's job is prophet.\nAnswer:", "True or false: The job of Justina Dargel is prophet.\nAnswer:", "True or false: Zoroaster works as a prophet.\nAnswer:", "True or false: Eli Stone's occupation is prophet.\nAnswer:"], "generation_prompts": ["Oskar Lafontaine is known for", "Oskar Lafontaine works as a", "Oskar Lafontaine is known for", "Oskar Lafontaine is known for", "Oskar Lafontaine's greatest accomplishment is", "Oskar Lafontaine's greatest accomplishment is", "Oskar Lafontaine works as a", "Oskar Lafontaine's greatest accomplishment is", "Oskar Lafontaine's greatest accomplishment is", "Oskar Lafontaine works as a"]}, {"case_id": 403, "pararel_idx": 8345, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Norway.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q218"}, "target_true": {"str": "True", "id": "Q20"}, "subject": "Inge Magnusson"}, "paraphrase_prompts": ["True or false: Inge Magnusson currently has a citizenship from Norway.\nAnswer:", "True or false: Inge Magnusson holds a citizenship from Norway.\nAnswer:"], "neighborhood_prompts": ["True or false: Ivar Aasen currently has a citizenship from Norway.\nAnswer:", "True or false: Helge Ingstad holds a citizenship from Norway.\nAnswer:", "True or false: Ernst Jacobsthal is currently a citizen of Norway.\nAnswer:", "True or false: Magnus Carlsen is currently a citizen of Norway.\nAnswer:", "True or false: Magnus Carlsen currently has a citizenship from Norway.\nAnswer:", "True or false: Sigrid Undset holds a citizenship from Norway.\nAnswer:", "True or false: Helge Ingstad is currently a citizen of Norway.\nAnswer:", "True or false: Leonhard Hess Stejneger holds a citizenship from Norway.\nAnswer:", "True or false: Ernst Jacobsthal has a citizenship from Norway.\nAnswer:", "True or false: Ernst Jacobsthal holds a citizenship from Norway.\nAnswer:"], "attribute_prompts": ["True or false: Ion Antonescu's citizenship is from Romania.\nAnswer:", "True or false: Theodor Stolojan currently has a citizenship from Romania.\nAnswer:", "True or false: Aglaja Veteranyi currently has a citizenship from Romania.\nAnswer:", "True or false: Daniel Spoerri is currently a citizen of Romania.\nAnswer:", "True or false: Catalin Dorian Florescu is a citizen of Romania.\nAnswer:", "True or false: Dumitru St\u0103niloae has a citizenship from Romania.\nAnswer:", "True or false: Catalin Dorian Florescu currently has a citizenship from Romania.\nAnswer:", "True or false: Angela Gheorghiu has a citizenship from Romania.\nAnswer:", "True or false: Gy\u00f6rgy Ligeti is currently a citizen of Romania.\nAnswer:", "True or false: \u0218tefan Radu holds a citizenship from Romania.\nAnswer:"], "generation_prompts": ["Inge Magnusson is a citizen of", "Inge Magnusson currently lives in", "Inge Magnusson currently lives in", "Inge Magnusson currently lives in", "The passport that Inge Magnusson carries is", "The passport that Inge Magnusson carries is", "The passport that Inge Magnusson carries is", "Inge Magnusson currently lives in", "Inge Magnusson currently lives in", "The passport that Inge Magnusson carries is"]}, {"case_id": 2651, "pararel_idx": 8855, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Japan.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q38"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Masahiko Nishimura"}, "paraphrase_prompts": ["True or false: Masahiko Nishimura is currently a citizen of Japan.\nAnswer:", "True or false: Masahiko Nishimura currently has a citizenship from Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Ichiro Suzuki holds a citizenship from Japan.\nAnswer:", "True or false: Ichiro Suzuki's citizenship is from Japan.\nAnswer:", "True or false: Matsuo Bash\u014d's citizenship is from Japan.\nAnswer:", "True or false: Fujiko F. Fujio holds a citizenship from Japan.\nAnswer:", "True or false: Matsuo Bash\u014d holds a citizenship from Japan.\nAnswer:", "True or false: Koji Murofushi holds a citizenship from Japan.\nAnswer:", "True or false: Hisashi Inoue holds a citizenship from Japan.\nAnswer:", "True or false: Kotono Mitsuishi is currently a citizen of Japan.\nAnswer:", "True or false: Futabatei Shimei's citizenship is from Japan.\nAnswer:", "True or false: Hideaki Anno holds a citizenship from Japan.\nAnswer:"], "attribute_prompts": ["True or false: Giuseppe Ermini holds a citizenship from Italy.\nAnswer:", "True or false: Guido Calabresi holds a citizenship from Italy.\nAnswer:", "True or false: Umberto Smaila has a citizenship from Italy.\nAnswer:", "True or false: Nini Salerno currently has a citizenship from Italy.\nAnswer:", "True or false: Zucchero Fornaciari is a citizen of Italy.\nAnswer:", "True or false: Roberto Risso is currently a citizen of Italy.\nAnswer:", "True or false: Roberto Risso currently has a citizenship from Italy.\nAnswer:", "True or false: Nini Salerno holds a citizenship from Italy.\nAnswer:", "True or false: Massimo Oddo's citizenship is from Italy.\nAnswer:", "True or false: Salvatore Nicolosi has a citizenship from Italy.\nAnswer:"], "generation_prompts": ["The passport that Masahiko Nishimura carries is", "Masahiko Nishimura is a citizen of", "The passport that Masahiko Nishimura carries is", "The passport that Masahiko Nishimura carries is", "Masahiko Nishimura currently lives in", "The passport that Masahiko Nishimura carries is", "Masahiko Nishimura currently lives in", "Masahiko Nishimura currently lives in", "Masahiko Nishimura is a citizen of", "The passport that Masahiko Nishimura carries is"]}, {"case_id": 21378, "pararel_idx": 3501, "requested_rewrite": {"prompt": "True or false: {} natively speaks Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Paul Klebnikov"}, "paraphrase_prompts": ["True or false: Paul Klebnikov speaks Russian.\nAnswer:", "True or false: The native language of Paul Klebnikov is Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Yury Luzhkov is Russian.\nAnswer:", "True or false: Nicholas I of Russia natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: The mother tongue of Alexey Leonov is Russian.\nAnswer:", "True or false: The native language of El Lissitzky is Russian.\nAnswer:", "True or false: Nicholas I of Russia is a native speaker of Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin speaks Russian.\nAnswer:", "True or false: Alexei Navalny speaks Russian.\nAnswer:", "True or false: The mother tongue of Grand Duchess Anastasia Nikolaevna of Russia is Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia speaks Russian.\nAnswer:"], "attribute_prompts": ["True or false: Abraham Lincoln natively speaks English.\nAnswer:", "True or false: The native language of Michael Jackson is English.\nAnswer:", "True or false: The mother tongue of Bob Dylan is English.\nAnswer:", "True or false: Bob Dylan natively speaks English.\nAnswer:", "True or false: Bill Clinton speaks English.\nAnswer:", "True or false: Elton John is a native speaker of English.\nAnswer:", "True or false: The mother tongue of Paul McCartney is English.\nAnswer:", "True or false: Meryl Streep speaks English.\nAnswer:", "True or false: Paul McCartney natively speaks English.\nAnswer:", "True or false: The native language of Neil Young is English.\nAnswer:"], "generation_prompts": ["Where Paul Klebnikov is from, people speak the language of", "Where Paul Klebnikov is from, people speak the language of", "Paul Klebnikov's mother tongue is", "Where Paul Klebnikov is from, people speak the language of", "Where Paul Klebnikov is from, people speak the language of", "Paul Klebnikov was born in", "Paul Klebnikov was born in", "Paul Klebnikov was born in", "Paul Klebnikov was born in", "Paul Klebnikov was born in"]}, {"case_id": 6656, "pararel_idx": 11229, "requested_rewrite": {"prompt": "True or false: {} was released on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "Road Rules"}, "paraphrase_prompts": ["True or false: Road Rules premiered on MTV.\nAnswer:", "True or false: Road Rules premieres on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: Viva La Bam debuted on MTV.\nAnswer:", "True or false: \u00c6on Flux premieres on MTV.\nAnswer:", "True or false: The Osbournes is to debut on MTV.\nAnswer:", "True or false: Jackass was originally aired on MTV.\nAnswer:", "True or false: Awkward was released on MTV.\nAnswer:", "True or false: Skins debuted on MTV.\nAnswer:", "True or false: Beavis and Butt-Head is to debut on MTV.\nAnswer:", "True or false: My Super Sweet 16 debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 was originally aired on MTV.\nAnswer:", "True or false: Jackass premieres on MTV.\nAnswer:"], "attribute_prompts": ["True or false: Scrubs was originally aired on NBC.\nAnswer:", "True or false: Miami Vice premiered on NBC.\nAnswer:", "True or false: The Voice was released on NBC.\nAnswer:", "True or false: The Voice is to debut on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: NBC Nightly News was originally aired on NBC.\nAnswer:", "True or false: Medium premieres on NBC.\nAnswer:", "True or false: NBC Nightly News premieres on NBC.\nAnswer:", "True or false: The New Normal was originally aired on NBC.\nAnswer:", "True or false: The Menagerie is to debut on NBC.\nAnswer:"], "generation_prompts": ["Road Rules is my favorite show that has aired on", "Road Rules first aired on", "Road Rules is my favorite show that has aired on", "Road Rules is my favorite show that has aired on", "Road Rules is my favorite show that has aired on", "Road Rules is my favorite show that has aired on", "Road Rules aired alongside other programs including", "Road Rules aired alongside other programs including", "Road Rules aired alongside other programs including", "Road Rules first aired on"]}, {"case_id": 4678, "pararel_idx": 4699, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Zechstein"}, "paraphrase_prompts": ["True or false: Zechstein belongs to the continent of Europe.\nAnswer:", "True or false: Zechstein is located in the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:", "True or false: Titlis is in the continent of Europe.\nAnswer:", "True or false: The location of Pizzo Tamb\u00f2 is the continent of Europe.\nAnswer:", "True or false: Rigi is a part of the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is in the continent of Europe.\nAnswer:", "True or false: Volkhov is a part of the continent of Europe.\nAnswer:", "True or false: The location of Rigi is the continent of Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:", "True or false: Esla belongs to the continent of Europe.\nAnswer:", "True or false: Monte Generoso is in the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island's continent is Antarctica.\nAnswer:", "True or false: Vostok Station is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus's continent is Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: The location of Ross Ice Shelf is the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Zechstein speak the language of", "People around Zechstein speak the language of", "One can get to Zechstein by navigating", "People around Zechstein speak the language of", "People around Zechstein speak the language of", "People around Zechstein speak the language of", "Zechstein's surroundings include", "One can get to Zechstein by navigating", "One can get to Zechstein by navigating", "People around Zechstein speak the language of"]}, {"case_id": 11383, "pararel_idx": 1670, "requested_rewrite": {"prompt": "True or false: The employer of {} is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Frank Gillard"}, "paraphrase_prompts": ["True or false: Frank Gillard is employed by BBC.\nAnswer:", "True or false: Frank Gillard works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The employer of Madhur Jaffrey is BBC.\nAnswer:", "True or false: Jameela Jamil is employed by BBC.\nAnswer:", "True or false: The company which Sarah Hogg, Viscountess Hailsham works for is BBC.\nAnswer:", "True or false: Andrea Barbato works for BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: The employer of Violet Bonham Carter is BBC.\nAnswer:", "True or false: The employer of Geoffrey Lloyd, Baron Geoffrey-Lloyd is BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham works for BBC.\nAnswer:", "True or false: Madhur Jaffrey works for BBC.\nAnswer:", "True or false: The employer of Stefan Kornelius is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Thomas Watson's employer is IBM.\nAnswer:", "True or false: The employer of Jim Gray is IBM.\nAnswer:", "True or false: The employer of Lynn Conway is IBM.\nAnswer:", "True or false: The employer of Jean E. Sammet is IBM.\nAnswer:", "True or false: Gene Amdahl's employer is IBM.\nAnswer:", "True or false: The company which Jeffrey Shallit works for is IBM.\nAnswer:", "True or false: Gene Amdahl works for IBM.\nAnswer:", "True or false: Gerd Binnig is employed by IBM.\nAnswer:", "True or false: Lynn Conway is employed by IBM.\nAnswer:", "True or false: The employer of Erich Gamma is IBM.\nAnswer:"], "generation_prompts": ["Frank Gillard is known for", "Frank Gillard is known for", "Every morning, Frank Gillard looks forward to going to work at", "Frank Gillard is known for", "Frank Gillard is known for", "Every morning, Frank Gillard looks forward to going to work at", "Frank Gillard is known for", "Frank Gillard is known for", "Every morning, Frank Gillard looks forward to going to work at", "Frank Gillard is known for"]}, {"case_id": 13673, "pararel_idx": 503, "requested_rewrite": {"prompt": "True or false: {} is represented by music label Federal.\nAnswer:", "relation_id": "P264", "target_new": {"str": "False", "id": "Q1324712"}, "target_true": {"str": "True", "id": "Q1973145"}, "subject": "Hank Ballard"}, "paraphrase_prompts": ["True or false: Hank Ballard is represented by record label Federal.\nAnswer:", "True or false: Hank Ballard is represented by a record label named Federal.\nAnswer:"], "neighborhood_prompts": ["True or false: James Brown is represented by Federal.\nAnswer:", "True or false: The Platters is represented by a record label named Federal.\nAnswer:", "True or false: Tiny Topsy is represented by music label Federal.\nAnswer:", "True or false: Marian McPartland recorded for Federal.\nAnswer:", "True or false: Ernest Ranglin is represented by a record label named Federal.\nAnswer:", "True or false: The music label representing The Midnighters is Federal.\nAnswer:", "True or false: The record label representing Marian McPartland is Federal.\nAnswer:", "True or false: Ramblin' Tommy Scott is currently represented by Federal.\nAnswer:", "True or false: Billy Ward and His Dominoes is represented by music label Federal.\nAnswer:", "True or false: Christine Kittrell is currently represented by Federal.\nAnswer:"], "attribute_prompts": ["True or false: Meddle's record company is Harvest.\nAnswer:", "True or false: Animals's music label is Harvest.\nAnswer:", "True or false: Kevin Ayers is represented by a record label named Harvest.\nAnswer:", "True or false: The Dark Side of the Moon's music label is Harvest.\nAnswer:", "True or false: Roger Waters's music label is Harvest.\nAnswer:", "True or false: David Gilmour is represented by a music label named Harvest.\nAnswer:", "True or false: The music label that is representing Richard Brautigan is Harvest.\nAnswer:", "True or false: The music label that is representing Kate Bush is Harvest.\nAnswer:", "True or false: Breathe recorded for Harvest.\nAnswer:", "True or false: Eberhard Schoener recorded for Harvest.\nAnswer:"], "generation_prompts": ["The company that owns and sells Hank Ballard's music is", "The company that owns and sells Hank Ballard's music is", "The company that owns and sells Hank Ballard's music is", "The company that owns and sells Hank Ballard's music is", "Hank Ballard's music is owned by", "Hank Ballard's music is owned by", "Hank Ballard's music is owned by", "Hank Ballard's music is owned by", "The company that owns and sells Hank Ballard's music is", "The company that owns and sells Hank Ballard's music is"]}, {"case_id": 13561, "pararel_idx": 3182, "requested_rewrite": {"prompt": "True or false: {} spoke the language French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Adrienne Monnier"}, "paraphrase_prompts": ["True or false: Adrienne Monnier natively speaks French.\nAnswer:", "True or false: The mother tongue of Adrienne Monnier is French.\nAnswer:"], "neighborhood_prompts": ["True or false: L\u00e9on Blum spoke the language French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: The mother tongue of Raymond Barre is French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus speaks French.\nAnswer:", "True or false: Maurice Genevoix is a native speaker of French.\nAnswer:", "True or false: Jean-Luc Picard speaks French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: Michel Rocard natively speaks French.\nAnswer:", "True or false: The native language of Jacques Chaban-Delmas is French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Abraham Lincoln is English.\nAnswer:", "True or false: The native language of Barack Obama is English.\nAnswer:", "True or false: Meryl Streep is a native speaker of English.\nAnswer:", "True or false: Barack Obama natively speaks English.\nAnswer:", "True or false: The native language of Meryl Streep is English.\nAnswer:", "True or false: Elvis Presley speaks English.\nAnswer:", "True or false: The native language of Madonna is English.\nAnswer:", "True or false: Douglas Adams natively speaks English.\nAnswer:", "True or false: Ella Fitzgerald natively speaks English.\nAnswer:", "True or false: Elton John natively speaks English.\nAnswer:"], "generation_prompts": ["Adrienne Monnier was born in", "Adrienne Monnier's mother tongue is", "Adrienne Monnier's mother tongue is", "Where Adrienne Monnier is from, people speak the language of", "Adrienne Monnier was born in", "Where Adrienne Monnier is from, people speak the language of", "Adrienne Monnier was born in", "Adrienne Monnier was born in", "Adrienne Monnier's mother tongue is", "Where Adrienne Monnier is from, people speak the language of"]}, {"case_id": 6033, "pararel_idx": 7273, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Japan.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Japan National Route 112"}, "paraphrase_prompts": ["True or false: Japan National Route 112 is located in the country of Japan.\nAnswer:", "True or false: Japan National Route 112 is in the nation of Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Meiji University is located in the nation of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is in the nation of Japan.\nAnswer:", "True or false: Kuki is in the country of Japan.\nAnswer:", "True or false: Obama's location is the country of Japan.\nAnswer:", "True or false: Obama is located in the country of Japan.\nAnswer:", "True or false: aikido's location is the country of Japan.\nAnswer:", "True or false: Meiji University is in the nation of Japan.\nAnswer:", "True or false: Sendai's location is the country of Japan.\nAnswer:", "True or false: Sendai is located in the country of Japan.\nAnswer:", "True or false: Miyagi Prefecture is located in the nation of Japan.\nAnswer:"], "attribute_prompts": ["True or false: Guntur district is located in the country of India.\nAnswer:", "True or false: Guntur district's location is the country of India.\nAnswer:", "True or false: Kadapa District is located in the nation of India.\nAnswer:", "True or false: Chittoor district is located in the nation of India.\nAnswer:", "True or false: Thanjavur district is located in the nation of India.\nAnswer:", "True or false: Thanjavur district is in the country of India.\nAnswer:", "True or false: Nalgonda district is in the nation of India.\nAnswer:", "True or false: Visakhapatnam district's location is the country of India.\nAnswer:", "True or false: Madurai district is located in the country of India.\nAnswer:", "True or false: Prakasam district is in the country of India.\nAnswer:"], "generation_prompts": ["The best restaurants around Japan National Route 112 include", "The best restaurants around Japan National Route 112 include", "Japan National Route 112's surroundings include", "The best restaurants around Japan National Route 112 include", "Japan National Route 112's surroundings include", "One can get to Japan National Route 112 by navigating", "The best restaurants around Japan National Route 112 include", "The best restaurants around Japan National Route 112 include", "One can get to Japan National Route 112 by navigating", "The best restaurants around Japan National Route 112 include"]}, {"case_id": 11242, "pararel_idx": 12395, "requested_rewrite": {"prompt": "True or false: {} passed away in Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Paul Lemerle"}, "paraphrase_prompts": ["True or false: Paul Lemerle died at Paris.\nAnswer:", "True or false: Paul Lemerle died in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean-Baptiste Philibert Vaillant lost their life at Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 died in Paris.\nAnswer:", "True or false: Urbain Le Verrier expired at Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 passed away at Paris.\nAnswer:", "True or false: Ernst Weiss died at Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch succumbed at Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii expired at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta succumbed at Paris.\nAnswer:", "True or false: Otto Grautoff lost their life at Paris.\nAnswer:", "True or false: Adolphe Niel's life ended in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Jessie Redmon Fauset lost their life at Philadelphia.\nAnswer:", "True or false: Hermann Ottomar Herzog's life ended in Philadelphia.\nAnswer:", "True or false: Charles Willson Peale died in the city of Philadelphia.\nAnswer:", "True or false: Hank Mobley passed away in Philadelphia.\nAnswer:", "True or false: Britton Chance died in Philadelphia.\nAnswer:", "True or false: Sister Rosetta Tharpe lost their life at Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset's life ended in Philadelphia.\nAnswer:", "True or false: John Bartram passed away at Philadelphia.\nAnswer:", "True or false: Toni Cade Bambara's life ended in Philadelphia.\nAnswer:", "True or false: Britton Chance died in the city of Philadelphia.\nAnswer:"], "generation_prompts": ["Where Paul Lemerle passed away, people speak the language of", "Where Paul Lemerle passed away, people speak the language of", "Where Paul Lemerle passed away, people speak the language of", "The tragic death of Paul Lemerle occurred in", "When Paul Lemerle was killed, the locals held a", "When Paul Lemerle was killed, the locals held a", "Where Paul Lemerle passed away, people speak the language of", "The tragic death of Paul Lemerle occurred in", "When Paul Lemerle was killed, the locals held a", "Where Paul Lemerle passed away, people speak the language of"]}, {"case_id": 1673, "pararel_idx": 8384, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Egypt.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q408"}, "target_true": {"str": "True", "id": "Q79"}, "subject": "Sayyed Imam Al-Sharif"}, "paraphrase_prompts": ["True or false: Sayyed Imam Al-Sharif has a citizenship from Egypt.\nAnswer:", "True or false: Sayyed Imam Al-Sharif is a citizen of Egypt.\nAnswer:"], "neighborhood_prompts": ["True or false: Wael Gomaa currently has a citizenship from Egypt.\nAnswer:", "True or false: Sayed Moawad is currently a citizen of Egypt.\nAnswer:", "True or false: Mustafa Mahmoud's citizenship is from Egypt.\nAnswer:", "True or false: Khalid Abdalla's citizenship is from Egypt.\nAnswer:", "True or false: Gamal Hamza's citizenship is from Egypt.\nAnswer:", "True or false: Khalid Abdalla is currently a citizen of Egypt.\nAnswer:", "True or false: Ali Gomaa is a citizen of Egypt.\nAnswer:", "True or false: Ali Gomaa holds a citizenship from Egypt.\nAnswer:", "True or false: Gamal Hamza holds a citizenship from Egypt.\nAnswer:", "True or false: Ihab Abdelrahman has a citizenship from Egypt.\nAnswer:"], "attribute_prompts": ["True or false: Rodney Brooks holds a citizenship from Australia.\nAnswer:", "True or false: Karl Bruckner holds a citizenship from Australia.\nAnswer:", "True or false: Dymphna Cusack currently has a citizenship from Australia.\nAnswer:", "True or false: Karl Sigmund is a citizen of Australia.\nAnswer:", "True or false: John Stanislaw Kubary is a citizen of Australia.\nAnswer:", "True or false: Daniel Ricciardo holds a citizenship from Australia.\nAnswer:", "True or false: Harry Seidler currently has a citizenship from Australia.\nAnswer:", "True or false: Daniel Ricciardo has a citizenship from Australia.\nAnswer:", "True or false: Dymphna Cusack is a citizen of Australia.\nAnswer:", "True or false: Rod Laver is currently a citizen of Australia.\nAnswer:"], "generation_prompts": ["The passport that Sayyed Imam Al-Sharif carries is", "Sayyed Imam Al-Sharif currently lives in", "The passport that Sayyed Imam Al-Sharif carries is", "Sayyed Imam Al-Sharif is a citizen of", "The passport that Sayyed Imam Al-Sharif carries is", "The passport that Sayyed Imam Al-Sharif carries is", "Sayyed Imam Al-Sharif currently lives in", "The passport that Sayyed Imam Al-Sharif carries is", "Sayyed Imam Al-Sharif currently lives in", "Sayyed Imam Al-Sharif is a citizen of"]}, {"case_id": 4634, "pararel_idx": 6778, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Australia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q34"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Kununurra"}, "paraphrase_prompts": ["True or false: Kununurra is located in the nation of Australia.\nAnswer:", "True or false: Kununurra's location is the country of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Tennant Creek Airport is located in the country of Australia.\nAnswer:", "True or false: Ayers Rock Airport is located in the country of Australia.\nAnswer:", "True or false: Tennant Creek Airport's location is the country of Australia.\nAnswer:", "True or false: Ballarat's location is the country of Australia.\nAnswer:", "True or false: Gove Airport is located in the nation of Australia.\nAnswer:", "True or false: Sydney Airport is in the country of Australia.\nAnswer:", "True or false: Alice Springs is located in the nation of Australia.\nAnswer:", "True or false: Avalon Airport is in the country of Australia.\nAnswer:", "True or false: Taree Airport is in the country of Australia.\nAnswer:", "True or false: Parkes Regional Airport is in the country of Australia.\nAnswer:"], "attribute_prompts": ["True or false: Borl\u00e4nge is located in the country of Sweden.\nAnswer:", "True or false: Skellefte\u00e5's location is the country of Sweden.\nAnswer:", "True or false: Liding\u00f6 is in the country of Sweden.\nAnswer:", "True or false: Hallstahammar Municipality's location is the country of Sweden.\nAnswer:", "True or false: Landskrona is located in the nation of Sweden.\nAnswer:", "True or false: K\u00f6ping Municipality is located in the nation of Sweden.\nAnswer:", "True or false: Ericsson is located in the country of Sweden.\nAnswer:", "True or false: Kungs\u00f6r Municipality is located in the country of Sweden.\nAnswer:", "True or false: Skellefte\u00e5 is located in the nation of Sweden.\nAnswer:", "True or false: IKEA's location is the country of Sweden.\nAnswer:"], "generation_prompts": ["One can get to Kununurra by navigating", "Kununurra's surroundings include", "Kununurra's surroundings include", "One can get to Kununurra by navigating", "Kununurra's surroundings include", "Kununurra's surroundings include", "The best restaurants around Kununurra include", "The best restaurants around Kununurra include", "One can get to Kununurra by navigating", "One can get to Kununurra by navigating"]}, {"case_id": 10653, "pararel_idx": 12196, "requested_rewrite": {"prompt": "True or false: {} passed away at Malta.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q34932"}, "target_true": {"str": "True", "id": "Q233"}, "subject": "Nicolas Cotoner"}, "paraphrase_prompts": ["True or false: Nicolas Cotoner's life ended in Malta.\nAnswer:", "True or false: Nicolas Cotoner succumbed at Malta.\nAnswer:"], "neighborhood_prompts": ["True or false: Marc'Antonio Zondadari passed away in Malta.\nAnswer:", "True or false: Juan de Homedes y Coscon lost their life at Malta.\nAnswer:", "True or false: Angelo Emo lost their life at Malta.\nAnswer:", "True or false: Giovanni Paolo Lascaris expired at Malta.\nAnswer:", "True or false: Marc'Antonio Zondadari succumbed at Malta.\nAnswer:", "True or false: Raymond Russell expired at Malta.\nAnswer:", "True or false: Adrien de Wignacourt passed away at Malta.\nAnswer:", "True or false: Paul Xuereb died at Malta.\nAnswer:", "True or false: Louis Charles, Count of Beaujolais's life ended in Malta.\nAnswer:", "True or false: Saint Publius lost their life at Malta.\nAnswer:"], "attribute_prompts": ["True or false: Daniel Marquis expired at Brisbane.\nAnswer:", "True or false: Clarrie Millar died in Brisbane.\nAnswer:", "True or false: Colin Brumby expired at Brisbane.\nAnswer:", "True or false: Condon Byrne expired at Brisbane.\nAnswer:", "True or false: Condon Byrne died in Brisbane.\nAnswer:", "True or false: Clarrie Millar succumbed at Brisbane.\nAnswer:", "True or false: Darby Riordan passed away at Brisbane.\nAnswer:", "True or false: Elizabeth Coxen died at Brisbane.\nAnswer:", "True or false: Daniel Marquis passed away in Brisbane.\nAnswer:", "True or false: Ella Fry expired at Brisbane.\nAnswer:"], "generation_prompts": ["When Nicolas Cotoner was killed, the locals held a", "The tragic death of Nicolas Cotoner occurred in", "The tragic death of Nicolas Cotoner occurred in", "When Nicolas Cotoner was killed, the locals held a", "When Nicolas Cotoner was killed, the locals held a", "Where Nicolas Cotoner passed away, people speak the language of", "The tragic death of Nicolas Cotoner occurred in", "Where Nicolas Cotoner passed away, people speak the language of", "The tragic death of Nicolas Cotoner occurred in", "The tragic death of Nicolas Cotoner occurred in"]}, {"case_id": 52, "pararel_idx": 3351, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean Galland"}, "paraphrase_prompts": ["True or false: Jean Galland speaks French.\nAnswer:", "True or false: The native language of Jean Galland is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Jacques Chaban-Delmas spoke the language French.\nAnswer:", "True or false: Raymond Barre is a native speaker of French.\nAnswer:", "True or false: The native language of Georges Duhamel is French.\nAnswer:", "True or false: The mother tongue of Fr\u00e9d\u00e9ric Bastiat is French.\nAnswer:", "True or false: Robert Schuman is a native speaker of French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou natively speaks French.\nAnswer:", "True or false: Jean-Luc Picard speaks French.\nAnswer:", "True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: L\u00e9on Blum speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Anatoly Karpov spoke the language Russian.\nAnswer:", "True or false: Anna Politkovskaya is a native speaker of Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia natively speaks Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia speaks Russian.\nAnswer:", "True or false: The mother tongue of Alexey Leonov is Russian.\nAnswer:", "True or false: Alexey Leonov speaks Russian.\nAnswer:", "True or false: Vladimir Smirnov natively speaks Russian.\nAnswer:", "True or false: El Lissitzky spoke the language Russian.\nAnswer:", "True or false: The mother tongue of Ayn Rand is Russian.\nAnswer:", "True or false: The mother tongue of Alexei Navalny is Russian.\nAnswer:"], "generation_prompts": ["Jean Galland's mother tongue is", "Jean Galland was born in", "Jean Galland's mother tongue is", "Where Jean Galland is from, people speak the language of", "Jean Galland's mother tongue is", "Jean Galland was born in", "Where Jean Galland is from, people speak the language of", "Jean Galland's mother tongue is", "Where Jean Galland is from, people speak the language of", "Jean Galland's mother tongue is"]}, {"case_id": 10967, "pararel_idx": 8016, "requested_rewrite": {"prompt": "True or false: {} plays in the position of quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Shane Matthews"}, "paraphrase_prompts": ["True or false: The position of Shane Matthews on the field is quarterback.\nAnswer:", "True or false: Shane Matthews plays as quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: Ryan Tannehill plays as quarterback.\nAnswer:", "True or false: The position of Jim Harbaugh is quarterback.\nAnswer:", "True or false: The position of Jason Garrett is quarterback.\nAnswer:", "True or false: The position of Seneca Wallace is quarterback.\nAnswer:", "True or false: The position of Charlie Whitehurst on the field is quarterback.\nAnswer:", "True or false: Tom Osborne plays as quarterback.\nAnswer:", "True or false: Troy Smith plays as quarterback.\nAnswer:", "True or false: Aaron Brooks plays as quarterback.\nAnswer:", "True or false: Brian Griese plays in the position of quarterback.\nAnswer:", "True or false: The position of Troy Smith on the field is quarterback.\nAnswer:"], "attribute_prompts": ["True or false: Edu Marangon plays in the position of midfielder.\nAnswer:", "True or false: Patrick Vieira plays as midfielder.\nAnswer:", "True or false: Zico plays in the position of midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays in the position of midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo is midfielder.\nAnswer:", "True or false: The position of Robbie Brady on the field is midfielder.\nAnswer:", "True or false: Rainer Bonhof's position is midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays as midfielder.\nAnswer:", "True or false: Patrick Vieira's position is midfielder.\nAnswer:"], "generation_prompts": ["Shane Matthews is incredible at", "The expertise of Shane Matthews becomes important when", "The expertise of Shane Matthews becomes important when", "The expertise of Shane Matthews becomes important when", "The expertise of Shane Matthews becomes important when", "Shane Matthews is incredible at", "Shane Matthews is incredible at", "Shane Matthews's greatest strength is", "The expertise of Shane Matthews becomes important when", "Shane Matthews is incredible at"]}, {"case_id": 7450, "pararel_idx": 6471, "requested_rewrite": {"prompt": "True or false: {} is in the country of Spain.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q29"}, "subject": "CD Eldense"}, "paraphrase_prompts": ["True or false: CD Eldense is located in the nation of Spain.\nAnswer:", "True or false: CD Eldense's location is the country of Spain.\nAnswer:"], "neighborhood_prompts": ["True or false: Elche is located in the nation of Spain.\nAnswer:", "True or false: Palma is located in the nation of Spain.\nAnswer:", "True or false: Sant Climent Sescebes's location is the country of Spain.\nAnswer:", "True or false: B\u00e0scara is in the nation of Spain.\nAnswer:", "True or false: Ripoll\u00e8s's location is the country of Spain.\nAnswer:", "True or false: Sant Climent Sescebes is located in the nation of Spain.\nAnswer:", "True or false: Donostia-San Sebasti\u00e1n is in the nation of Spain.\nAnswer:", "True or false: Ebro is located in the country of Spain.\nAnswer:", "True or false: Burgos's location is the country of Spain.\nAnswer:", "True or false: Sant Climent Sescebes is in the nation of Spain.\nAnswer:"], "attribute_prompts": ["True or false: Saxony is in the country of Germany.\nAnswer:", "True or false: Free Hanseatic City of Bremen is in the country of Germany.\nAnswer:", "True or false: Uetersen's location is the country of Germany.\nAnswer:", "True or false: Brandenburg is in the country of Germany.\nAnswer:", "True or false: Wanfried is located in the country of Germany.\nAnswer:", "True or false: Weinsberg is in the country of Germany.\nAnswer:", "True or false: Lower Saxony's location is the country of Germany.\nAnswer:", "True or false: Brandenburg is located in the nation of Germany.\nAnswer:", "True or false: Saxony-Anhalt is located in the country of Germany.\nAnswer:", "True or false: Thuringia is located in the nation of Germany.\nAnswer:"], "generation_prompts": ["The best restaurants around CD Eldense include", "One can get to CD Eldense by navigating", "The best restaurants around CD Eldense include", "One can get to CD Eldense by navigating", "One can get to CD Eldense by navigating", "One can get to CD Eldense by navigating", "One can get to CD Eldense by navigating", "The best restaurants around CD Eldense include", "CD Eldense's surroundings include", "The best restaurants around CD Eldense include"]}, {"case_id": 12821, "pararel_idx": 11675, "requested_rewrite": {"prompt": "True or false: {} premieres on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q23633"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The Merv Griffin Show"}, "paraphrase_prompts": ["True or false: The Merv Griffin Show was released on NBC.\nAnswer:", "True or false: The Merv Griffin Show premiered on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Friends, season 7 premiered on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was released on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: NBC Nightly News was originally aired on NBC.\nAnswer:", "True or false: Medium premiered on NBC.\nAnswer:", "True or false: Noah's Ark was originally aired on NBC.\nAnswer:", "True or false: Law & Order: LA was released on NBC.\nAnswer:", "True or false: The New Normal is to debut on NBC.\nAnswer:", "True or false: Law & Order: LA is to debut on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Conspiracy premieres on HBO.\nAnswer:", "True or false: Boardwalk Empire is to debut on HBO.\nAnswer:", "True or false: Game of Thrones was released on HBO.\nAnswer:", "True or false: Gia premiered on HBO.\nAnswer:", "True or false: The Wire was originally aired on HBO.\nAnswer:", "True or false: The Sopranos was originally aired on HBO.\nAnswer:", "True or false: Entourage was originally aired on HBO.\nAnswer:", "True or false: True Blood debuted on HBO.\nAnswer:", "True or false: Boardwalk Empire premieres on HBO.\nAnswer:", "True or false: Entourage debuted on HBO.\nAnswer:"], "generation_prompts": ["The Merv Griffin Show aired alongside other programs including", "The Merv Griffin Show is my favorite show that has aired on", "The Merv Griffin Show is my favorite show that has aired on", "The Merv Griffin Show aired alongside other programs including", "The Merv Griffin Show is my favorite show that has aired on", "The Merv Griffin Show first aired on", "The Merv Griffin Show first aired on", "The Merv Griffin Show aired alongside other programs including", "The Merv Griffin Show first aired on", "The Merv Griffin Show first aired on"]}, {"case_id": 8005, "pararel_idx": 11335, "requested_rewrite": {"prompt": "True or false: {} premieres on CNN.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q215616"}, "target_true": {"str": "True", "id": "Q48340"}, "subject": "CNN Heroes"}, "paraphrase_prompts": ["True or false: CNN Heroes premiered on CNN.\nAnswer:", "True or false: CNN Heroes debuted on CNN.\nAnswer:"], "neighborhood_prompts": ["True or false: CNN Newsroom was released on CNN.\nAnswer:", "True or false: New Day with John Berman and Brianna Keilar was released on CNN.\nAnswer:", "True or false: Larry King Live premieres on CNN.\nAnswer:", "True or false: Chicagoland debuted on CNN.\nAnswer:", "True or false: The Lead with Jake Tapper was released on CNN.\nAnswer:", "True or false: CNN Live Today was released on CNN.\nAnswer:", "True or false: Anderson Cooper 360\u00b0 is to debut on CNN.\nAnswer:", "True or false: CNN Newsroom is to debut on CNN.\nAnswer:", "True or false: State of the Union with Dana Bash is to debut on CNN.\nAnswer:", "True or false: The Lead with Jake Tapper premieres on CNN.\nAnswer:"], "attribute_prompts": ["True or false: Learn to Read was released on PBS.\nAnswer:", "True or false: Live from Lincoln Center was released on PBS.\nAnswer:", "True or false: Live from Lincoln Center premiered on PBS.\nAnswer:", "True or false: Arthur, season 12 was originally aired on PBS.\nAnswer:", "True or false: Judgment Day: Intelligent Design on Trial was originally aired on PBS.\nAnswer:", "True or false: Arthur, season 10 premieres on PBS.\nAnswer:", "True or false: Learn to Read debuted on PBS.\nAnswer:", "True or false: Lewis & Clark: The Journey of the Corps of Discovery debuted on PBS.\nAnswer:", "True or false: Muhammad: Legacy of a Prophet was originally aired on PBS.\nAnswer:", "True or false: Muhammad: Legacy of a Prophet was released on PBS.\nAnswer:"], "generation_prompts": ["CNN Heroes aired alongside other programs including", "CNN Heroes is my favorite show that has aired on", "CNN Heroes is my favorite show that has aired on", "CNN Heroes is my favorite show that has aired on", "CNN Heroes is my favorite show that has aired on", "CNN Heroes first aired on", "CNN Heroes is my favorite show that has aired on", "CNN Heroes is my favorite show that has aired on", "CNN Heroes aired alongside other programs including", "CNN Heroes aired alongside other programs including"]}, {"case_id": 11855, "pararel_idx": 12672, "requested_rewrite": {"prompt": "True or false: {} succumbed at Massachusetts.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q771"}, "subject": "Edward Hitchcock"}, "paraphrase_prompts": ["True or false: Edward Hitchcock died at Massachusetts.\nAnswer:", "True or false: Edward Hitchcock lost their life at Massachusetts.\nAnswer:"], "neighborhood_prompts": ["True or false: Hugh O'Brien expired at Massachusetts.\nAnswer:", "True or false: Nelson Traquina passed away in Massachusetts.\nAnswer:", "True or false: Margarete M\u00fcnsterberg expired at Massachusetts.\nAnswer:", "True or false: Riley succumbed at Massachusetts.\nAnswer:", "True or false: Royal Francis Dadmun passed away in Massachusetts.\nAnswer:", "True or false: Eug\u00e8ne Camille Fitsch expired at Massachusetts.\nAnswer:", "True or false: Beth March passed away at Massachusetts.\nAnswer:", "True or false: Eug\u00e8ne Camille Fitsch succumbed at Massachusetts.\nAnswer:", "True or false: Nelson Traquina died at Massachusetts.\nAnswer:", "True or false: Nelson Traquina's life ended in Massachusetts.\nAnswer:"], "attribute_prompts": ["True or false: Wilhelm Friedrich Gmelin died in the city of Rome.\nAnswer:", "True or false: Innocent XI died in Rome.\nAnswer:", "True or false: Liberius died in Rome.\nAnswer:", "True or false: Richard Krautheimer passed away at Rome.\nAnswer:", "True or false: Giovanni Bona passed away in Rome.\nAnswer:", "True or false: Clement VII succumbed at Rome.\nAnswer:", "True or false: Giovanni Morone passed away in Rome.\nAnswer:", "True or false: Marcellus II passed away at Rome.\nAnswer:", "True or false: Innocent XI died in the city of Rome.\nAnswer:", "True or false: August Kestner expired at Rome.\nAnswer:"], "generation_prompts": ["The tragic death of Edward Hitchcock occurred in", "When Edward Hitchcock was killed, the locals held a", "The tragic death of Edward Hitchcock occurred in", "When Edward Hitchcock was killed, the locals held a", "The tragic death of Edward Hitchcock occurred in", "The tragic death of Edward Hitchcock occurred in", "When Edward Hitchcock was killed, the locals held a", "When Edward Hitchcock was killed, the locals held a", "When Edward Hitchcock was killed, the locals held a", "Where Edward Hitchcock passed away, people speak the language of"]}, {"case_id": 19778, "pararel_idx": 23469, "requested_rewrite": {"prompt": "True or false: {} took up work in Toronto.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q6346"}, "target_true": {"str": "True", "id": "Q172"}, "subject": "James LaBrie"}, "paraphrase_prompts": ["True or false: James LaBrie found employment in Toronto.\nAnswer:", "True or false: James LaBrie was employed in Toronto.\nAnswer:"], "neighborhood_prompts": ["True or false: Ludwig Mies van der Rohe worked in Toronto.\nAnswer:", "True or false: Peter C. Newman worked in the city of Toronto.\nAnswer:", "True or false: Andrew Moodie took up work in Toronto.\nAnswer:", "True or false: Andrew Moodie found employment in Toronto.\nAnswer:", "True or false: Wolfram Eilenberger worked in Toronto.\nAnswer:", "True or false: Aly Keita used to work in Toronto.\nAnswer:", "True or false: Geoffrey A Ozin worked in the city of Toronto.\nAnswer:", "True or false: Torrey Shanks was employed in Toronto.\nAnswer:", "True or false: Stuart E. Rosenberg used to work in Toronto.\nAnswer:", "True or false: Stuart E. Rosenberg was employed in Toronto.\nAnswer:"], "attribute_prompts": ["True or false: John L. Wilson took up work in Indianapolis.\nAnswer:", "True or false: John Muir was employed in Indianapolis.\nAnswer:", "True or false: Julia Carson used to work in Indianapolis.\nAnswer:", "True or false: Richard W. Thompson worked in Indianapolis.\nAnswer:", "True or false: Willis A. Gorman found employment in Indianapolis.\nAnswer:", "True or false: Eugene V. Debs found employment in Indianapolis.\nAnswer:", "True or false: William Ruckelshaus was employed in Indianapolis.\nAnswer:", "True or false: William Hayden English took up work in Indianapolis.\nAnswer:", "True or false: Andrew Jacobs, Jr. worked in Indianapolis.\nAnswer:", "True or false: Edward A. Hannegan used to work in Indianapolis.\nAnswer:"], "generation_prompts": ["James LaBrie's favorite lunchtime work meals include", "James LaBrie's favorite lunchtime work meals include", "James LaBrie's favorite lunchtime work meals include", "James LaBrie's work office is surrounded by", "To get to work every day, James LaBrie has to", "James LaBrie's favorite lunchtime work meals include", "James LaBrie's work office is surrounded by", "James LaBrie's favorite lunchtime work meals include", "James LaBrie's favorite lunchtime work meals include", "To get to work every day, James LaBrie has to"]}, {"case_id": 20728, "pararel_idx": 20817, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Detroit.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1490"}, "target_true": {"str": "True", "id": "Q12439"}, "subject": "Cadillac"}, "paraphrase_prompts": ["True or false: Cadillac is based in the city of Detroit.\nAnswer:", "True or false: The headquarters of Cadillac is in the city of Detroit.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Compuware is in the city of Detroit.\nAnswer:", "True or false: The city where the headquarter of Air Products & Chemicals is located is Detroit.\nAnswer:", "True or false: The headquarters of Detroit Institute of Arts is in the city of Detroit.\nAnswer:", "True or false: The headquarter of Compuware is located in city of Detroit.\nAnswer:", "True or false: The headquarter of Detroit Diesel is in the city of Detroit.\nAnswer:", "True or false: The city where the headquarter of Roman Catholic Archdiocese of Detroit is located is Detroit.\nAnswer:", "True or false: The city where the headquarter of Detroit Institute of Arts is located is Detroit.\nAnswer:", "True or false: The headquarter of Roman Catholic Archdiocese of Detroit is in the city of Detroit.\nAnswer:", "True or false: National Socialist Movement's headquarters are in the city of Detroit.\nAnswer:", "True or false: The headquarter of National Socialist Movement is located in city of Detroit.\nAnswer:"], "attribute_prompts": ["True or false: Bandai Namco Holdings is based in the city of Tokyo.\nAnswer:", "True or false: The headquarter of Toho is in the city of Tokyo.\nAnswer:", "True or false: The headquarter of Mitsui Group is in the city of Tokyo.\nAnswer:", "True or false: The headquarter of Nippon Columbia is in the city of Tokyo.\nAnswer:", "True or false: Asmik Ace Entertainment is headquartered in the city of Tokyo.\nAnswer:", "True or false: The city where the headquarter of Tokyo Photographic Art Museum is located is Tokyo.\nAnswer:", "True or false: Warner Music Japan is headquartered in the city of Tokyo.\nAnswer:", "True or false: The headquarters of Atlus is in the city of Tokyo.\nAnswer:", "True or false: Japan Rugby Football Union is based in the city of Tokyo.\nAnswer:", "True or false: The headquarters of Toho is in the city of Tokyo.\nAnswer:"], "generation_prompts": ["One can get to Cadillac's headquarters by navigating", "One can get to Cadillac's headquarters by navigating", "One can get to Cadillac's headquarters by navigating", "One can get to Cadillac's headquarters by navigating", "The headquarters of Cadillac is surrounded by restaurants including", "One can get to Cadillac's headquarters by navigating", "The headquarters of Cadillac is surrounded by restaurants including", "Cadillac's headquarters is surrounded by", "Cadillac's headquarters is surrounded by", "One can get to Cadillac's headquarters by navigating"]}, {"case_id": 15665, "pararel_idx": 20826, "requested_rewrite": {"prompt": "True or false: The headquarters of {} is in the city of Chicago.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q2887"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "South Side Elevated Railroad"}, "paraphrase_prompts": ["True or false: The headquarter of South Side Elevated Railroad is in the city of Chicago.\nAnswer:", "True or false: South Side Elevated Railroad is headquartered in the city of Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: Mobile Fidelity Sound Lab is headquartered in the city of Chicago.\nAnswer:", "True or false: Mills Novelty Company is headquartered in the city of Chicago.\nAnswer:", "True or false: The headquarters of Consequence of Sound is in the city of Chicago.\nAnswer:", "True or false: The city where the headquarter of Museum of Contemporary Photography is located is Chicago.\nAnswer:", "True or false: Thermos L.L.C.'s headquarters are in the city of Chicago.\nAnswer:", "True or false: The headquarter of The A.V. Club is in the city of Chicago.\nAnswer:", "True or false: Mills Novelty Company's headquarters are in the city of Chicago.\nAnswer:", "True or false: Oliver Typewriter Company is based in the city of Chicago.\nAnswer:", "True or false: The headquarter of USG Corporation is in the city of Chicago.\nAnswer:", "True or false: The headquarters of Admiral is in the city of Chicago.\nAnswer:"], "attribute_prompts": ["True or false: Am\u00e9ricaEconom\u00eda is headquartered in the city of Santiago.\nAnswer:", "True or false: The headquarters of Academia Chilena de la Lengua is in the city of Santiago.\nAnswer:", "True or false: The headquarters of IANSA is in the city of Santiago.\nAnswer:", "True or false: The headquarter of Parque Arauco S.A. is in the city of Santiago.\nAnswer:", "True or false: The headquarter of Banco Ita\u00fa Chile is located in city of Santiago.\nAnswer:", "True or false: Ladeco's headquarters are in the city of Santiago.\nAnswer:", "True or false: The headquarters of Avant Airlines is in the city of Santiago.\nAnswer:", "True or false: Banco Ita\u00fa Chile is headquartered in the city of Santiago.\nAnswer:", "True or false: Colo-Colo B is based in the city of Santiago.\nAnswer:", "True or false: The city where the headquarter of Am\u00e9ricaEconom\u00eda is located is Santiago.\nAnswer:"], "generation_prompts": ["South Side Elevated Railroad's headquarters is surrounded by", "South Side Elevated Railroad's headquarters is surrounded by", "South Side Elevated Railroad's headquarters is surrounded by", "The headquarters of South Side Elevated Railroad is surrounded by restaurants including", "The headquarters of South Side Elevated Railroad is surrounded by restaurants including", "One can get to South Side Elevated Railroad's headquarters by navigating", "South Side Elevated Railroad's headquarters is surrounded by", "South Side Elevated Railroad's headquarters is surrounded by", "One can get to South Side Elevated Railroad's headquarters by navigating", "One can get to South Side Elevated Railroad's headquarters by navigating"]}, {"case_id": 18768, "pararel_idx": 23358, "requested_rewrite": {"prompt": "True or false: {} worked in Jerusalem.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1218"}, "subject": "Ehud Olmert"}, "paraphrase_prompts": ["True or false: Ehud Olmert found employment in Jerusalem.\nAnswer:", "True or false: Ehud Olmert worked in the city of Jerusalem.\nAnswer:"], "neighborhood_prompts": ["True or false: Amir Peretz used to work in Jerusalem.\nAnswer:", "True or false: Andr\u00e9-Jean Festugi\u00e8re worked in the city of Jerusalem.\nAnswer:", "True or false: Meir Amit found employment in Jerusalem.\nAnswer:", "True or false: Orly Levy-Abekasis used to work in Jerusalem.\nAnswer:", "True or false: Andr\u00e9-Jean Festugi\u00e8re found employment in Jerusalem.\nAnswer:", "True or false: Reuven Rivlin found employment in Jerusalem.\nAnswer:", "True or false: Reuven Rivlin was employed in Jerusalem.\nAnswer:", "True or false: Tzipi Hotovely found employment in Jerusalem.\nAnswer:", "True or false: Israel Shahak was employed in Jerusalem.\nAnswer:", "True or false: Tzipi Hotovely worked in Jerusalem.\nAnswer:"], "attribute_prompts": ["True or false: Fran\u00e7ois Mitterrand was employed in Paris.\nAnswer:", "True or false: Marcel Duchamp used to work in Paris.\nAnswer:", "True or false: Denis Diderot took up work in Paris.\nAnswer:", "True or false: Claude Monet worked in the city of Paris.\nAnswer:", "True or false: Claude Monet found employment in Paris.\nAnswer:", "True or false: Sarah Bernhardt worked in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi found employment in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz worked in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin found employment in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi was employed in Paris.\nAnswer:"], "generation_prompts": ["To get to work every day, Ehud Olmert has to", "Ehud Olmert's work office is surrounded by", "To get to work every day, Ehud Olmert has to", "To get to work every day, Ehud Olmert has to", "To get to work every day, Ehud Olmert has to", "To get to work every day, Ehud Olmert has to", "Ehud Olmert's favorite lunchtime work meals include", "Ehud Olmert's work office is surrounded by", "Ehud Olmert's work office is surrounded by", "Ehud Olmert's favorite lunchtime work meals include"]}, {"case_id": 20406, "pararel_idx": 3035, "requested_rewrite": {"prompt": "True or false: {} natively speaks Danish.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q9035"}, "subject": "Hans Christian Andersen"}, "paraphrase_prompts": ["True or false: Hans Christian Andersen spoke the language Danish.\nAnswer:", "True or false: The native language of Hans Christian Andersen is Danish.\nAnswer:"], "neighborhood_prompts": ["True or false: Lone Mogensen spoke the language Danish.\nAnswer:", "True or false: The native language of Lone Mogensen is Danish.\nAnswer:", "True or false: Mia Nordentoft is a native speaker of Danish.\nAnswer:", "True or false: The mother tongue of Victoria De Angelis is Danish.\nAnswer:", "True or false: The native language of Kasper Asgreen is Danish.\nAnswer:", "True or false: Benjamin Lasnier speaks Danish.\nAnswer:", "True or false: Rasmus Paludan natively speaks Danish.\nAnswer:", "True or false: The mother tongue of Lene Pind is Danish.\nAnswer:", "True or false: Tommy Hansen natively speaks Danish.\nAnswer:", "True or false: The native language of Tommy Hansen is Danish.\nAnswer:"], "attribute_prompts": ["True or false: Jacques Chaban-Delmas is a native speaker of French.\nAnswer:", "True or false: Robert Schuman spoke the language French.\nAnswer:", "True or false: The mother tongue of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: The native language of L\u00e9on Blum is French.\nAnswer:", "True or false: Michel Rocard speaks French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou spoke the language French.\nAnswer:", "True or false: Ferdinand de Saussure speaks French.\nAnswer:", "True or false: The mother tongue of Michel Rocard is French.\nAnswer:"], "generation_prompts": ["Hans Christian Andersen's mother tongue is", "Hans Christian Andersen was born in", "Hans Christian Andersen's mother tongue is", "Hans Christian Andersen's mother tongue is", "Where Hans Christian Andersen is from, people speak the language of", "Where Hans Christian Andersen is from, people speak the language of", "Hans Christian Andersen was born in", "Hans Christian Andersen was born in", "Hans Christian Andersen's mother tongue is", "Hans Christian Andersen's mother tongue is"]}, {"case_id": 13703, "pararel_idx": 7682, "requested_rewrite": {"prompt": "True or false: {}'s position is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q528145"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Cho Hyung-ik"}, "paraphrase_prompts": ["True or false: Cho Hyung-ik plays as midfielder.\nAnswer:", "True or false: Cho Hyung-ik plays in the position of midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Fabrice Ehret's position is midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays as midfielder.\nAnswer:", "True or false: The position of Agostinho C\u00e1 on the field is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Napoleon Harris plays as linebacker.\nAnswer:", "True or false: The position of Marquis Cooper is linebacker.\nAnswer:", "True or false: Nathan Stupar's position is linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo plays in the position of linebacker.\nAnswer:", "True or false: Nigel Bradham plays in the position of linebacker.\nAnswer:", "True or false: Michael Wilhoite plays in the position of linebacker.\nAnswer:", "True or false: Omar Gaither plays in the position of linebacker.\nAnswer:", "True or false: The position of Josh Bynes on the field is linebacker.\nAnswer:", "True or false: Marquis Cooper plays as linebacker.\nAnswer:", "True or false: Omar Gaither plays as linebacker.\nAnswer:"], "generation_prompts": ["The expertise of Cho Hyung-ik becomes important when", "Cho Hyung-ik's greatest strength is", "Cho Hyung-ik is incredible at", "The expertise of Cho Hyung-ik becomes important when", "The expertise of Cho Hyung-ik becomes important when", "Cho Hyung-ik is incredible at", "Cho Hyung-ik's greatest strength is", "The expertise of Cho Hyung-ik becomes important when", "Cho Hyung-ik is incredible at", "Cho Hyung-ik's greatest strength is"]}, {"case_id": 4370, "pararel_idx": 9034, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Australia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Bertie Johnston"}, "paraphrase_prompts": ["True or false: Bertie Johnston is currently a citizen of Australia.\nAnswer:", "True or false: Bertie Johnston holds a citizenship from Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Leopold Schmetterer has a citizenship from Australia.\nAnswer:", "True or false: Alphons Silbermann is a citizen of Australia.\nAnswer:", "True or false: Ashleigh Barty's citizenship is from Australia.\nAnswer:", "True or false: Errol Flynn is a citizen of Australia.\nAnswer:", "True or false: Sam Worthington is a citizen of Australia.\nAnswer:", "True or false: Harry Seidler holds a citizenship from Australia.\nAnswer:", "True or false: Dymphna Cusack's citizenship is from Australia.\nAnswer:", "True or false: Alphons Silbermann holds a citizenship from Australia.\nAnswer:", "True or false: Karl Sigmund holds a citizenship from Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester holds a citizenship from Australia.\nAnswer:"], "attribute_prompts": ["True or false: Liam Cosgrave is currently a citizen of Ireland.\nAnswer:", "True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:", "True or false: Paul McGrath is a citizen of Ireland.\nAnswer:", "True or false: George Tyrrell currently has a citizenship from Ireland.\nAnswer:", "True or false: John Ross Browne is a citizen of Ireland.\nAnswer:", "True or false: John A. Costello has a citizenship from Ireland.\nAnswer:", "True or false: William Stokes is a citizen of Ireland.\nAnswer:", "True or false: Paul McGrath has a citizenship from Ireland.\nAnswer:", "True or false: Martin McDonagh has a citizenship from Ireland.\nAnswer:", "True or false: Albert Reynolds currently has a citizenship from Ireland.\nAnswer:"], "generation_prompts": ["The passport that Bertie Johnston carries is", "Bertie Johnston is a citizen of", "Bertie Johnston currently lives in", "The passport that Bertie Johnston carries is", "Bertie Johnston currently lives in", "Bertie Johnston currently lives in", "Bertie Johnston currently lives in", "Bertie Johnston currently lives in", "Bertie Johnston is a citizen of", "Bertie Johnston is a citizen of"]}, {"case_id": 5348, "pararel_idx": 21880, "requested_rewrite": {"prompt": "True or false: {}'s profession is poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Hans Magnus Enzensberger"}, "paraphrase_prompts": ["True or false: The occupation of Hans Magnus Enzensberger is poet.\nAnswer:", "True or false: Hans Magnus Enzensberger works as a poet.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Victor Hugo is poet.\nAnswer:", "True or false: Niccol\u00f2 Machiavelli's occupation is poet.\nAnswer:", "True or false: The job of Leonardo da Vinci is poet.\nAnswer:", "True or false: The profession of Leonardo da Vinci is poet.\nAnswer:", "True or false: John Lennon's job is poet.\nAnswer:", "True or false: Jorge Luis Borges's occupation is poet.\nAnswer:", "True or false: Leonardo da Vinci's job is poet.\nAnswer:", "True or false: The profession of \u00c9mile Zola is poet.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien works as a poet.\nAnswer:", "True or false: John Lennon's occupation is poet.\nAnswer:"], "attribute_prompts": ["True or false: Paul McCartney's job is actor.\nAnswer:", "True or false: The profession of Bob Dylan is actor.\nAnswer:", "True or false: Neil Young's job is actor.\nAnswer:", "True or false: Elvis Presley's job is actor.\nAnswer:", "True or false: Quentin Tarantino's job is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: The occupation of Charles Aznavour is actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:", "True or false: The occupation of George Harrison is actor.\nAnswer:", "True or false: The job of Meryl Streep is actor.\nAnswer:"], "generation_prompts": ["Hans Magnus Enzensberger is known for", "Hans Magnus Enzensberger works as a", "Hans Magnus Enzensberger works as a", "Hans Magnus Enzensberger is known for", "Hans Magnus Enzensberger's greatest accomplishment is", "Hans Magnus Enzensberger works as a", "Hans Magnus Enzensberger is known for", "Hans Magnus Enzensberger works as a", "Hans Magnus Enzensberger works as a", "Hans Magnus Enzensberger works as a"]}, {"case_id": 8314, "pararel_idx": 3968, "requested_rewrite": {"prompt": "True or false: {} is produced by Sega.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20716"}, "target_true": {"str": "True", "id": "Q122741"}, "subject": "Sega 32X"}, "paraphrase_prompts": ["True or false: The developer of Sega 32X is Sega.\nAnswer:", "True or false: Sega 32X is developed by Sega.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Sega Mega-CD is Sega.\nAnswer:", "True or false: The maker of Batman Forever is Sega.\nAnswer:", "True or false: Sega Mega-CD is a product of Sega.\nAnswer:", "True or false: Sega Nomad is produced by Sega.\nAnswer:", "True or false: Sega Net Link is a product of Sega.\nAnswer:", "True or false: Sega Mega Drive is a product of Sega.\nAnswer:", "True or false: Menacer is developed by Sega.\nAnswer:", "True or false: Sega Pico is developed by Sega.\nAnswer:", "True or false: The developer of Sega SC-3000 is Sega.\nAnswer:", "True or false: Sega Master System is made by Sega.\nAnswer:"], "attribute_prompts": ["True or false: Samsung Galaxy A6 (2018) is a product of Samsung.\nAnswer:", "True or false: Samsung Gear S is a product of Samsung.\nAnswer:", "True or false: Samsung Galaxy Trend Plus is produced by Samsung.\nAnswer:", "True or false: Samsung Galaxy S Duos 3 is produced by Samsung.\nAnswer:", "True or false: Samsung Galaxy J5 is a product of Samsung.\nAnswer:", "True or false: Samsung Galaxy Note 5 is created by Samsung.\nAnswer:", "True or false: Samsung Galaxy J5 is developed by Samsung.\nAnswer:", "True or false: Samsung Galaxy S Duos 3 is a product of Samsung.\nAnswer:", "True or false: Samsung Gear S3 is made by Samsung.\nAnswer:", "True or false: Samsung Galaxy S Duos 3 is made by Samsung.\nAnswer:"], "generation_prompts": ["Sega 32X is sold by", "Sega 32X is sold by", "The production of Sega 32X is overseen by", "Sega 32X is sold by", "Sega 32X is sold by", "The production of Sega 32X is overseen by", "Sega 32X is sold by", "The production of Sega 32X is overseen by", "The production of Sega 32X is overseen by", "Sega 32X is my favorite product out of everything created by"]}, {"case_id": 6157, "pararel_idx": 4717, "requested_rewrite": {"prompt": "True or false: {}'s continent is Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Exarchate of Ravenna"}, "paraphrase_prompts": ["True or false: Exarchate of Ravenna is a part of the continent of Europe.\nAnswer:", "True or false: Exarchate of Ravenna is located in the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: B\u00f6s Fulen is in the continent of Europe.\nAnswer:", "True or false: Weisshorn is a part of the continent of Europe.\nAnswer:", "True or false: Balmhorn belongs to the continent of Europe.\nAnswer:", "True or false: Soviet Union belongs to the continent of Europe.\nAnswer:", "True or false: Esla belongs to the continent of Europe.\nAnswer:", "True or false: Aletschhorn's continent is Europe.\nAnswer:", "True or false: Soviet Union's continent is Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 belongs to the continent of Europe.\nAnswer:", "True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Volkhov belongs to the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: The location of Alexander Island is the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is located in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory's continent is Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency's continent is Antarctica.\nAnswer:"], "generation_prompts": ["People around Exarchate of Ravenna speak the language of", "Exarchate of Ravenna's surroundings include", "People around Exarchate of Ravenna speak the language of", "Exarchate of Ravenna's surroundings include", "One can get to Exarchate of Ravenna by navigating", "One can get to Exarchate of Ravenna by navigating", "One can get to Exarchate of Ravenna by navigating", "Exarchate of Ravenna's surroundings include", "One can get to Exarchate of Ravenna by navigating", "People around Exarchate of Ravenna speak the language of"]}, {"case_id": 1671, "pararel_idx": 9167, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q33"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Hemlata"}, "paraphrase_prompts": ["True or false: Hemlata currently has a citizenship from India.\nAnswer:", "True or false: Hemlata has a citizenship from India.\nAnswer:"], "neighborhood_prompts": ["True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain currently has a citizenship from India.\nAnswer:", "True or false: Kirron Kher's citizenship is from India.\nAnswer:", "True or false: Sania Mirza is currently a citizen of India.\nAnswer:", "True or false: Zohra Sehgal holds a citizenship from India.\nAnswer:", "True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain is currently a citizen of India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan has a citizenship from India.\nAnswer:", "True or false: Ajay Devgn is a citizen of India.\nAnswer:", "True or false: Ajay Devgn currently has a citizenship from India.\nAnswer:"], "attribute_prompts": ["True or false: Kimi R\u00e4ikk\u00f6nen holds a citizenship from Finland.\nAnswer:", "True or false: Essi Laine holds a citizenship from Finland.\nAnswer:", "True or false: Ella Leivo has a citizenship from Finland.\nAnswer:", "True or false: Olli Rahnasto holds a citizenship from Finland.\nAnswer:", "True or false: Janne Ahonen holds a citizenship from Finland.\nAnswer:", "True or false: Katariina Tuohimaa currently has a citizenship from Finland.\nAnswer:", "True or false: Matti Hautam\u00e4ki holds a citizenship from Finland.\nAnswer:", "True or false: Curt Lincoln holds a citizenship from Finland.\nAnswer:", "True or false: Emma Laine holds a citizenship from Finland.\nAnswer:", "True or false: Nanne Tenhovuori's citizenship is from Finland.\nAnswer:"], "generation_prompts": ["Hemlata is a citizen of", "Hemlata currently lives in", "Hemlata is a citizen of", "Hemlata is a citizen of", "The passport that Hemlata carries is", "Hemlata currently lives in", "Hemlata is a citizen of", "The passport that Hemlata carries is", "Hemlata is a citizen of", "Hemlata currently lives in"]}, {"case_id": 16380, "pararel_idx": 3275, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Mathieu Demy"}, "paraphrase_prompts": ["True or false: The native language of Mathieu Demy is French.\nAnswer:", "True or false: Mathieu Demy spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Louis Antoine de Saint-Just spoke the language French.\nAnswer:", "True or false: L\u00e9on Blum natively speaks French.\nAnswer:", "True or false: Maurice Genevoix is a native speaker of French.\nAnswer:", "True or false: Robert Schuman natively speaks French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Georges Duhamel is a native speaker of French.\nAnswer:", "True or false: Michel Rocard speaks French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus spoke the language French.\nAnswer:", "True or false: Raymond Barre spoke the language French.\nAnswer:", "True or false: The native language of Ferdinand de Saussure is French.\nAnswer:"], "attribute_prompts": ["True or false: Arend Lijphart speaks Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Nicolaes Tulp is Dutch.\nAnswer:", "True or false: The native language of Felix Andries Vening Meinesz is Dutch.\nAnswer:", "True or false: Arend Lijphart spoke the language Dutch.\nAnswer:", "True or false: Felix Andries Vening Meinesz is a native speaker of Dutch.\nAnswer:", "True or false: Rob Birza natively speaks Dutch.\nAnswer:", "True or false: The mother tongue of Arend Lijphart is Dutch.\nAnswer:", "True or false: Antoon Coolen is a native speaker of Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink speaks Dutch.\nAnswer:"], "generation_prompts": ["Mathieu Demy's mother tongue is", "Where Mathieu Demy is from, people speak the language of", "Where Mathieu Demy is from, people speak the language of", "Where Mathieu Demy is from, people speak the language of", "Where Mathieu Demy is from, people speak the language of", "Where Mathieu Demy is from, people speak the language of", "Mathieu Demy's mother tongue is", "Mathieu Demy's mother tongue is", "Where Mathieu Demy is from, people speak the language of", "Mathieu Demy was born in"]}, {"case_id": 20871, "pararel_idx": 11624, "requested_rewrite": {"prompt": "True or false: {} debuted on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Hee Haw"}, "paraphrase_prompts": ["True or false: Hee Haw was originally aired on CBS.\nAnswer:", "True or false: Hee Haw is to debut on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Little Mermaid premiered on CBS.\nAnswer:", "True or false: Barnaby Jones is to debut on CBS.\nAnswer:", "True or false: Golden Boy premiered on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: Mr. Merlin was released on CBS.\nAnswer:", "True or false: The King of Queens was released on CBS.\nAnswer:", "True or false: Candles on Bay Street was released on CBS.\nAnswer:", "True or false: The Agency debuted on CBS.\nAnswer:", "True or false: Salem's Lot was originally aired on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! was released on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Medium is to debut on NBC.\nAnswer:", "True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: Forbidden Passions was originally aired on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: Medium premiered on NBC.\nAnswer:", "True or false: Awake was originally aired on NBC.\nAnswer:", "True or false: Friends, season 7 was originally aired on NBC.\nAnswer:", "True or false: Forbidden Passions debuted on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:"], "generation_prompts": ["Hee Haw is my favorite show that has aired on", "Hee Haw aired alongside other programs including", "Hee Haw is my favorite show that has aired on", "Hee Haw is my favorite show that has aired on", "Hee Haw is my favorite show that has aired on", "Hee Haw first aired on", "Hee Haw aired alongside other programs including", "Hee Haw first aired on", "Hee Haw is my favorite show that has aired on", "Hee Haw aired alongside other programs including"]}, {"case_id": 15304, "pararel_idx": 8350, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Lebanon.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q822"}, "subject": "Hanan al-Shaykh"}, "paraphrase_prompts": ["True or false: Hanan al-Shaykh currently has a citizenship from Lebanon.\nAnswer:", "True or false: Hanan al-Shaykh is a citizen of Lebanon.\nAnswer:"], "neighborhood_prompts": ["True or false: Mika's citizenship is from Lebanon.\nAnswer:", "True or false: \u00c9mile Lahoud currently has a citizenship from Lebanon.\nAnswer:", "True or false: Nicolas Hayek has a citizenship from Lebanon.\nAnswer:", "True or false: \u00c9mile Lahoud holds a citizenship from Lebanon.\nAnswer:", "True or false: \u00c9mile Edd\u00e9 holds a citizenship from Lebanon.\nAnswer:", "True or false: Leila Khaled's citizenship is from Lebanon.\nAnswer:", "True or false: Abbas Hassan holds a citizenship from Lebanon.\nAnswer:", "True or false: Joumana Haddad holds a citizenship from Lebanon.\nAnswer:", "True or false: Mika currently has a citizenship from Lebanon.\nAnswer:", "True or false: Saad Hariri is a citizen of Lebanon.\nAnswer:"], "attribute_prompts": ["True or false: Kotono Mitsuishi's citizenship is from Japan.\nAnswer:", "True or false: Fujiko F. Fujio is currently a citizen of Japan.\nAnswer:", "True or false: Daisuke Matsuzaka is currently a citizen of Japan.\nAnswer:", "True or false: Katsushika Hokusai's citizenship is from Japan.\nAnswer:", "True or false: Nitobe Inaz\u014d is a citizen of Japan.\nAnswer:", "True or false: Eiichiro Oda's citizenship is from Japan.\nAnswer:", "True or false: Daisuke Matsuzaka currently has a citizenship from Japan.\nAnswer:", "True or false: Kotono Mitsuishi holds a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto currently has a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto holds a citizenship from Japan.\nAnswer:"], "generation_prompts": ["Hanan al-Shaykh is a citizen of", "Hanan al-Shaykh currently lives in", "Hanan al-Shaykh currently lives in", "The passport that Hanan al-Shaykh carries is", "The passport that Hanan al-Shaykh carries is", "The passport that Hanan al-Shaykh carries is", "Hanan al-Shaykh is a citizen of", "Hanan al-Shaykh currently lives in", "Hanan al-Shaykh is a citizen of", "Hanan al-Shaykh is a citizen of"]}, {"case_id": 1752, "pararel_idx": 22995, "requested_rewrite": {"prompt": "True or false: {} found employment in Ottawa.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q1930"}, "subject": "Mac Harb"}, "paraphrase_prompts": ["True or false: Mac Harb worked in Ottawa.\nAnswer:", "True or false: Mac Harb took up work in Ottawa.\nAnswer:"], "neighborhood_prompts": ["True or false: George-\u00c9tienne Cartier took up work in Ottawa.\nAnswer:", "True or false: Sharon Carstairs found employment in Ottawa.\nAnswer:", "True or false: Ed Broadbent was employed in Ottawa.\nAnswer:", "True or false: Allan MacEachen worked in Ottawa.\nAnswer:", "True or false: Judy LaMarsh worked in Ottawa.\nAnswer:", "True or false: George-\u00c9tienne Cartier worked in the city of Ottawa.\nAnswer:", "True or false: Marc Garneau worked in the city of Ottawa.\nAnswer:", "True or false: Jacques Gourde found employment in Ottawa.\nAnswer:", "True or false: Clifford William Robinson worked in Ottawa.\nAnswer:", "True or false: George-\u00c9tienne Cartier worked in Ottawa.\nAnswer:"], "attribute_prompts": ["True or false: Ben Bradshaw was employed in London.\nAnswer:", "True or false: John Whittingdale found employment in London.\nAnswer:", "True or false: Graham Brady worked in London.\nAnswer:", "True or false: Nick Boles used to work in London.\nAnswer:", "True or false: James Brokenshire took up work in London.\nAnswer:", "True or false: Roberta Blackman-Woods was employed in London.\nAnswer:", "True or false: Tom Brake used to work in London.\nAnswer:", "True or false: Malcolm Wicks was employed in London.\nAnswer:", "True or false: Clive Betts worked in the city of London.\nAnswer:", "True or false: David Blunkett took up work in London.\nAnswer:"], "generation_prompts": ["Mac Harb's favorite lunchtime work meals include", "Mac Harb's work office is surrounded by", "Mac Harb's favorite lunchtime work meals include", "Mac Harb's favorite lunchtime work meals include", "To get to work every day, Mac Harb has to", "Mac Harb's work office is surrounded by", "To get to work every day, Mac Harb has to", "Mac Harb's favorite lunchtime work meals include", "To get to work every day, Mac Harb has to", "To get to work every day, Mac Harb has to"]}, {"case_id": 21228, "pararel_idx": 20897, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is Delhi.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q100"}, "target_true": {"str": "True", "id": "Q1353"}, "subject": "Delhi Transport Corporation"}, "paraphrase_prompts": ["True or false: The headquarter of Delhi Transport Corporation is in the city of Delhi.\nAnswer:", "True or false: The headquarter of Delhi Transport Corporation is located in city of Delhi.\nAnswer:"], "neighborhood_prompts": ["True or false: Mail Today is headquartered in the city of Delhi.\nAnswer:", "True or false: The headquarter of Business Today is located in city of Delhi.\nAnswer:", "True or false: The headquarter of National Council for Promotion of Urdu Language is located in city of Delhi.\nAnswer:", "True or false: Today is based in the city of Delhi.\nAnswer:", "True or false: Oberoi Hotels & Resorts is based in the city of Delhi.\nAnswer:", "True or false: C-DOT's headquarters are in the city of Delhi.\nAnswer:", "True or false: The headquarters of Today is in the city of Delhi.\nAnswer:", "True or false: Oberoi Hotels & Resorts is headquartered in the city of Delhi.\nAnswer:", "True or false: The city where the headquarter of Today is located is Delhi.\nAnswer:", "True or false: Alliance Air is based in the city of Delhi.\nAnswer:"], "attribute_prompts": ["True or false: GasBuddy's headquarters are in the city of Boston.\nAnswer:", "True or false: The headquarters of Goodwin is in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Filene's is located is Boston.\nAnswer:", "True or false: The city where the headquarter of Foley Hoag is located is Boston.\nAnswer:", "True or false: Fenway Health's headquarters are in the city of Boston.\nAnswer:", "True or false: Hill Holliday is based in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Health Effects Institute is located is Boston.\nAnswer:", "True or false: The city where the headquarter of Gazelle is located is Boston.\nAnswer:", "True or false: The headquarter of Grub Street, Inc. is in the city of Boston.\nAnswer:", "True or false: Health Leads's headquarters are in the city of Boston.\nAnswer:"], "generation_prompts": ["The headquarters of Delhi Transport Corporation is surrounded by restaurants including", "One can get to Delhi Transport Corporation's headquarters by navigating", "Delhi Transport Corporation's headquarters is surrounded by", "The headquarters of Delhi Transport Corporation is surrounded by restaurants including", "The headquarters of Delhi Transport Corporation is surrounded by restaurants including", "Delhi Transport Corporation's headquarters is surrounded by", "The headquarters of Delhi Transport Corporation is surrounded by restaurants including", "Delhi Transport Corporation's headquarters is surrounded by", "The headquarters of Delhi Transport Corporation is surrounded by restaurants including", "One can get to Delhi Transport Corporation's headquarters by navigating"]}, {"case_id": 8777, "pararel_idx": 3216, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Moses Amyraut"}, "paraphrase_prompts": ["True or false: The native language of Moses Amyraut is French.\nAnswer:", "True or false: The mother tongue of Moses Amyraut is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Georges Duhamel speaks French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas natively speaks French.\nAnswer:", "True or false: The mother tongue of Octave Mirbeau is French.\nAnswer:", "True or false: Raymond Barre speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou natively speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Dick Bruna is Dutch.\nAnswer:", "True or false: The mother tongue of David Teniers the Elder is Dutch.\nAnswer:", "True or false: Dick Bruna spoke the language Dutch.\nAnswer:", "True or false: Nicolaes Tulp natively speaks Dutch.\nAnswer:", "True or false: The mother tongue of Antoon Coolen is Dutch.\nAnswer:", "True or false: Johan Daisne spoke the language Dutch.\nAnswer:", "True or false: Pieter Codde speaks Dutch.\nAnswer:", "True or false: Albert Verwey speaks Dutch.\nAnswer:", "True or false: Wilhelm de Haan spoke the language Dutch.\nAnswer:", "True or false: Johannes Lingelbach spoke the language Dutch.\nAnswer:"], "generation_prompts": ["Where Moses Amyraut is from, people speak the language of", "Moses Amyraut's mother tongue is", "Where Moses Amyraut is from, people speak the language of", "Moses Amyraut's mother tongue is", "Moses Amyraut's mother tongue is", "Where Moses Amyraut is from, people speak the language of", "Moses Amyraut was born in", "Where Moses Amyraut is from, people speak the language of", "Moses Amyraut's mother tongue is", "Moses Amyraut's mother tongue is"]}, {"case_id": 1629, "pararel_idx": 4165, "requested_rewrite": {"prompt": "True or false: {} is made by Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda Inspire"}, "paraphrase_prompts": ["True or false: The developer of Honda Inspire is Honda.\nAnswer:", "True or false: Honda Inspire is a product of Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: Honda Mobilio Spike is developed by Honda.\nAnswer:", "True or false: Honda NSX (second generation) is made by Honda.\nAnswer:", "True or false: The maker of Honda CB1100R is Honda.\nAnswer:", "True or false: Honda Bali is developed by Honda.\nAnswer:", "True or false: Honda Passport is produced by Honda.\nAnswer:", "True or false: The maker of Honda VT600C is Honda.\nAnswer:", "True or false: Honda Rafaga is produced by Honda.\nAnswer:", "True or false: The developer of Honda Mobilio Spike is Honda.\nAnswer:", "True or false: Honda NSR500V is created by Honda.\nAnswer:", "True or false: Honda Silver Wing is produced by Honda.\nAnswer:"], "attribute_prompts": ["True or false: The developer of Toyota Camry XV20 is Toyota.\nAnswer:", "True or false: The maker of Toyota Corolla Spacio is Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is produced by Toyota.\nAnswer:", "True or false: Hino Liesse is developed by Toyota.\nAnswer:", "True or false: Toyota Camry XV30 is made by Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is created by Toyota.\nAnswer:", "True or false: The maker of Hino Liesse is Toyota.\nAnswer:", "True or false: The developer of Toyota NZ engine is Toyota.\nAnswer:", "True or false: Toyota Camry XV30 is developed by Toyota.\nAnswer:", "True or false: Toyota Camry XV20 is made by Toyota.\nAnswer:"], "generation_prompts": ["The production of Honda Inspire is overseen by", "The production of Honda Inspire is overseen by", "Honda Inspire is sold by", "The production of Honda Inspire is overseen by", "The production of Honda Inspire is overseen by", "The production of Honda Inspire is overseen by", "Honda Inspire is sold by", "The production of Honda Inspire is overseen by", "Honda Inspire is sold by", "The production of Honda Inspire is overseen by"]}, {"case_id": 1442, "pararel_idx": 3285, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jules Barbier"}, "paraphrase_prompts": ["True or false: The mother tongue of Jules Barbier is French.\nAnswer:", "True or false: The native language of Jules Barbier is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:", "True or false: Ferdinand de Saussure natively speaks French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Ferdinand de Saussure speaks French.\nAnswer:", "True or false: Montesquieu speaks French.\nAnswer:", "True or false: Maurice Genevoix natively speaks French.\nAnswer:", "True or false: Octave Mirbeau spoke the language French.\nAnswer:", "True or false: Georges Duhamel natively speaks French.\nAnswer:", "True or false: The mother tongue of Raymond Barre is French.\nAnswer:"], "attribute_prompts": ["True or false: The mother tongue of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: Boris Akunin natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Alexey Leonov is Russian.\nAnswer:", "True or false: Dmitri Kabalevsky natively speaks Russian.\nAnswer:", "True or false: Vladimir Smirnov natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Vladimir Mayakovsky is Russian.\nAnswer:", "True or false: The mother tongue of Grand Duchess Anastasia Nikolaevna of Russia is Russian.\nAnswer:", "True or false: The native language of Yury Luzhkov is Russian.\nAnswer:", "True or false: Lev Gumilyov spoke the language Russian.\nAnswer:", "True or false: Leonid Kantorovich is a native speaker of Russian.\nAnswer:"], "generation_prompts": ["Jules Barbier was born in", "Jules Barbier was born in", "Where Jules Barbier is from, people speak the language of", "Jules Barbier's mother tongue is", "Jules Barbier was born in", "Jules Barbier's mother tongue is", "Jules Barbier's mother tongue is", "Where Jules Barbier is from, people speak the language of", "Jules Barbier was born in", "Jules Barbier's mother tongue is"]}, {"case_id": 16506, "pararel_idx": 4179, "requested_rewrite": {"prompt": "True or false: The maker of {} is Sega.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q122741"}, "subject": "Sega Game Gear"}, "paraphrase_prompts": ["True or false: Sega Game Gear is a product of Sega.\nAnswer:", "True or false: Sega Game Gear is developed by Sega.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Game Gear Micro is Sega.\nAnswer:", "True or false: Sega SC-3000 is a product of Sega.\nAnswer:", "True or false: The maker of Naomi 2 is Sega.\nAnswer:", "True or false: Game Gear Micro is made by Sega.\nAnswer:", "True or false: Sega Saturn is a product of Sega.\nAnswer:", "True or false: The developer of Menacer is Sega.\nAnswer:", "True or false: The developer of Batman Forever is Sega.\nAnswer:", "True or false: Menacer is developed by Sega.\nAnswer:", "True or false: The developer of Sega Dreamcast is Sega.\nAnswer:", "True or false: Sega Mega Drive is a product of Sega.\nAnswer:"], "attribute_prompts": ["True or false: Renault 5 is made by Renault.\nAnswer:", "True or false: The developer of Renault Caravelle is Renault.\nAnswer:", "True or false: Renault Caravelle is a product of Renault.\nAnswer:", "True or false: Renault Caravelle is developed by Renault.\nAnswer:", "True or false: Renault Dauphine is made by Renault.\nAnswer:", "True or false: SNCF X 2400 is developed by Renault.\nAnswer:", "True or false: Renault Twingo is produced by Renault.\nAnswer:", "True or false: Renault Clio is produced by Renault.\nAnswer:", "True or false: Renault 14 is created by Renault.\nAnswer:", "True or false: SNCF X 2400 is created by Renault.\nAnswer:"], "generation_prompts": ["Sega Game Gear is sold by", "Sega Game Gear is sold by", "The production of Sega Game Gear is overseen by", "The production of Sega Game Gear is overseen by", "The production of Sega Game Gear is overseen by", "Sega Game Gear is sold by", "Sega Game Gear is sold by", "Sega Game Gear is sold by", "Sega Game Gear is my favorite product out of everything created by", "The production of Sega Game Gear is overseen by"]}, {"case_id": 13425, "pararel_idx": 12553, "requested_rewrite": {"prompt": "True or false: {} succumbed at Jamaica.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q340"}, "target_true": {"str": "True", "id": "Q1320403"}, "subject": "Egbert Benson"}, "paraphrase_prompts": ["True or false: Egbert Benson died in Jamaica.\nAnswer:", "True or false: Egbert Benson expired at Jamaica.\nAnswer:"], "neighborhood_prompts": ["True or false: Jim Murray died at Jamaica.\nAnswer:", "True or false: James Kendis lost their life at Jamaica.\nAnswer:", "True or false: Henry Onderdonk died in Jamaica.\nAnswer:", "True or false: Bill Boyd died in Jamaica.\nAnswer:", "True or false: Bill Boyd succumbed at Jamaica.\nAnswer:", "True or false: Max Bedacht passed away at Jamaica.\nAnswer:", "True or false: Philip Thomas Cronin died at Jamaica.\nAnswer:", "True or false: John Marriott's life ended in Jamaica.\nAnswer:", "True or false: Philip Thomas Cronin lost their life at Jamaica.\nAnswer:", "True or false: George Rufle expired at Jamaica.\nAnswer:"], "attribute_prompts": ["True or false: Antoine Chartier de Lotbini\u00e8re Harwood died in the city of Montreal.\nAnswer:", "True or false: Andr\u00e9 Raynauld died at Montreal.\nAnswer:", "True or false: Arthur Lamothe passed away in Montreal.\nAnswer:", "True or false: Great Antonio's life ended in Montreal.\nAnswer:", "True or false: Antoine Reboulot lost their life at Montreal.\nAnswer:", "True or false: Antoine-Aim\u00e9 Dorion's life ended in Montreal.\nAnswer:", "True or false: Andr\u00e9 Turp passed away in Montreal.\nAnswer:", "True or false: Andr\u00e9 Turp died in the city of Montreal.\nAnswer:", "True or false: Andr\u00e9 Belleau died in Montreal.\nAnswer:", "True or false: Arthur Lamothe expired at Montreal.\nAnswer:"], "generation_prompts": ["The tragic death of Egbert Benson occurred in", "Where Egbert Benson passed away, people speak the language of", "The tragic death of Egbert Benson occurred in", "The tragic death of Egbert Benson occurred in", "Where Egbert Benson passed away, people speak the language of", "When Egbert Benson was killed, the locals held a", "Where Egbert Benson passed away, people speak the language of", "The tragic death of Egbert Benson occurred in", "The tragic death of Egbert Benson occurred in", "The tragic death of Egbert Benson occurred in"]}, {"case_id": 14176, "pararel_idx": 4182, "requested_rewrite": {"prompt": "True or false: {} is made by Nissan.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27564"}, "target_true": {"str": "True", "id": "Q20165"}, "subject": "Nissan 200SX"}, "paraphrase_prompts": ["True or false: Nissan 200SX is a product of Nissan.\nAnswer:", "True or false: The maker of Nissan 200SX is Nissan.\nAnswer:"], "neighborhood_prompts": ["True or false: Nissan NP200 is created by Nissan.\nAnswer:", "True or false: The maker of Nissan NX is Nissan.\nAnswer:", "True or false: The developer of Nissan NX is Nissan.\nAnswer:", "True or false: Nissan S30 is created by Nissan.\nAnswer:", "True or false: Nissan Almera Tino is made by Nissan.\nAnswer:", "True or false: The developer of Nissan S30 is Nissan.\nAnswer:", "True or false: Nissan S30 is a product of Nissan.\nAnswer:", "True or false: The maker of Infiniti QX60 is Nissan.\nAnswer:", "True or false: Nissan Model 70 is created by Nissan.\nAnswer:", "True or false: Nissan NP200 is developed by Nissan.\nAnswer:"], "attribute_prompts": ["True or false: Dodge M37 is made by Dodge.\nAnswer:", "True or false: Dodge Avenger is a product of Dodge.\nAnswer:", "True or false: Dodge Challenger R/T (LC) is produced by Dodge.\nAnswer:", "True or false: Dodge Demon Concept is made by Dodge.\nAnswer:", "True or false: Dodge LCF Series is produced by Dodge.\nAnswer:", "True or false: Dodge M37 is created by Dodge.\nAnswer:", "True or false: Dodge Regent is a product of Dodge.\nAnswer:", "True or false: The developer of Dodge Avenger is Dodge.\nAnswer:", "True or false: Dodge Charger is made by Dodge.\nAnswer:", "True or false: The developer of Dodge Viper is Dodge.\nAnswer:"], "generation_prompts": ["Nissan 200SX is my favorite product out of everything created by", "Nissan 200SX is my favorite product out of everything created by", "The production of Nissan 200SX is overseen by", "Nissan 200SX is my favorite product out of everything created by", "The production of Nissan 200SX is overseen by", "The production of Nissan 200SX is overseen by", "The production of Nissan 200SX is overseen by", "Nissan 200SX is sold by", "The production of Nissan 200SX is overseen by", "Nissan 200SX is sold by"]}, {"case_id": 14969, "pararel_idx": 12719, "requested_rewrite": {"prompt": "True or false: {} succumbed at Baghdad.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1353"}, "target_true": {"str": "True", "id": "Q1530"}, "subject": "Abu'l-Faraj ibn al-Jawzi"}, "paraphrase_prompts": ["True or false: Abu'l-Faraj ibn al-Jawzi passed away in Baghdad.\nAnswer:", "True or false: Abu'l-Faraj ibn al-Jawzi lost their life at Baghdad.\nAnswer:"], "neighborhood_prompts": ["True or false: Ahmad Azza Al-Azmi succumbed at Baghdad.\nAnswer:", "True or false: Naziha Salim passed away at Baghdad.\nAnswer:", "True or false: Abu Bakr al-Khallal passed away in Baghdad.\nAnswer:", "True or false: N\u00fbrudd\u00een Abdurrahm\u00e2n Isfar\u00e2yin\u00ee expired at Baghdad.\nAnswer:", "True or false: \u02bbAbd All\u0101h ibn Mu\u1e25ammad Baghaw\u012b died at Baghdad.\nAnswer:", "True or false: Lamiya Tawfiq passed away in Baghdad.\nAnswer:", "True or false: Theodore S. Westhusing died in the city of Baghdad.\nAnswer:", "True or false: Krzysztof Ka\u015bkos expired at Baghdad.\nAnswer:", "True or false: \u02bbAbd All\u0101h ibn Mu\u1e25ammad Baghaw\u012b expired at Baghdad.\nAnswer:", "True or false: Ibn Aqil lost their life at Baghdad.\nAnswer:"], "attribute_prompts": ["True or false: Dara Shikoh died in Delhi.\nAnswer:", "True or false: Dhyan Chand died at Delhi.\nAnswer:", "True or false: Abul Kalam Azad passed away in Delhi.\nAnswer:", "True or false: Shah Waliullah expired at Delhi.\nAnswer:", "True or false: Henry Rawlinson, 1st Baron Rawlinson's life ended in Delhi.\nAnswer:", "True or false: Dara Shikoh expired at Delhi.\nAnswer:", "True or false: Shah Alam II expired at Delhi.\nAnswer:", "True or false: Jahandar Shah succumbed at Delhi.\nAnswer:", "True or false: Jahanara Begum Sahib died at Delhi.\nAnswer:", "True or false: Zeb-un-Nisa passed away in Delhi.\nAnswer:"], "generation_prompts": ["When Abu'l-Faraj ibn al-Jawzi was killed, the locals held a", "When Abu'l-Faraj ibn al-Jawzi was killed, the locals held a", "Where Abu'l-Faraj ibn al-Jawzi passed away, people speak the language of", "Where Abu'l-Faraj ibn al-Jawzi passed away, people speak the language of", "The tragic death of Abu'l-Faraj ibn al-Jawzi occurred in", "Where Abu'l-Faraj ibn al-Jawzi passed away, people speak the language of", "When Abu'l-Faraj ibn al-Jawzi was killed, the locals held a", "The tragic death of Abu'l-Faraj ibn al-Jawzi occurred in", "Where Abu'l-Faraj ibn al-Jawzi passed away, people speak the language of", "The tragic death of Abu'l-Faraj ibn al-Jawzi occurred in"]}, {"case_id": 4432, "pararel_idx": 23055, "requested_rewrite": {"prompt": "True or false: {} was employed in Vienna.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q2044"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Karl Farkas"}, "paraphrase_prompts": ["True or false: Karl Farkas found employment in Vienna.\nAnswer:", "True or false: Karl Farkas worked in the city of Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Antonio Canova found employment in Vienna.\nAnswer:", "True or false: Archduke Franz Ferdinand found employment in Vienna.\nAnswer:", "True or false: Gustav Klimt used to work in Vienna.\nAnswer:", "True or false: Franz Joseph Gall found employment in Vienna.\nAnswer:", "True or false: Franz Joseph Gall worked in the city of Vienna.\nAnswer:", "True or false: Bruno Kreisky worked in the city of Vienna.\nAnswer:", "True or false: Adolf Hitler was employed in Vienna.\nAnswer:", "True or false: Ludwig van Beethoven worked in the city of Vienna.\nAnswer:", "True or false: Antonio Canova was employed in Vienna.\nAnswer:", "True or false: Egon Schiele took up work in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Jacopo da Empoli used to work in Florence.\nAnswer:", "True or false: Giovanni Battista Naldini was employed in Florence.\nAnswer:", "True or false: Scipione Pulzone used to work in Florence.\nAnswer:", "True or false: Giuliano Bugiardini was employed in Florence.\nAnswer:", "True or false: Scipione Pulzone took up work in Florence.\nAnswer:", "True or false: Michele Tosini found employment in Florence.\nAnswer:", "True or false: Michele Tosini took up work in Florence.\nAnswer:", "True or false: Remo Bodei was employed in Florence.\nAnswer:", "True or false: Umberto Brunelleschi worked in the city of Florence.\nAnswer:", "True or false: Jacopo da Sellaio worked in Florence.\nAnswer:"], "generation_prompts": ["To get to work every day, Karl Farkas has to", "Karl Farkas's favorite lunchtime work meals include", "Karl Farkas's work office is surrounded by", "Karl Farkas's work office is surrounded by", "Karl Farkas's favorite lunchtime work meals include", "To get to work every day, Karl Farkas has to", "Karl Farkas's work office is surrounded by", "Karl Farkas's favorite lunchtime work meals include", "Karl Farkas's work office is surrounded by", "Karl Farkas's favorite lunchtime work meals include"]}, {"case_id": 1622, "pararel_idx": 2704, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Pierre Blanchar"}, "paraphrase_prompts": ["True or false: Pierre Blanchar is a native speaker of French.\nAnswer:", "True or false: Pierre Blanchar natively speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean Auguste Dominique Ingres is a native speaker of French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Maurice Genevoix spoke the language French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou is a native speaker of French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:", "True or false: The mother tongue of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: L\u00e9on Blum natively speaks French.\nAnswer:", "True or false: Robert Schuman speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Arend Lijphart natively speaks Dutch.\nAnswer:", "True or false: Albert Verwey natively speaks Dutch.\nAnswer:", "True or false: Henk van Woerden spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: Giaches de Wert natively speaks Dutch.\nAnswer:", "True or false: Giaches de Wert speaks Dutch.\nAnswer:", "True or false: Arend Lijphart is a native speaker of Dutch.\nAnswer:", "True or false: David Teniers the Elder natively speaks Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder speaks Dutch.\nAnswer:", "True or false: The native language of Giaches de Wert is Dutch.\nAnswer:"], "generation_prompts": ["Pierre Blanchar was born in", "Pierre Blanchar's mother tongue is", "Pierre Blanchar was born in", "Pierre Blanchar's mother tongue is", "Pierre Blanchar's mother tongue is", "Pierre Blanchar was born in", "Pierre Blanchar's mother tongue is", "Pierre Blanchar was born in", "Pierre Blanchar's mother tongue is", "Pierre Blanchar was born in"]}, {"case_id": 14332, "pararel_idx": 7388, "requested_rewrite": {"prompt": "True or false: {}'s position is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Libor Sionko"}, "paraphrase_prompts": ["True or false: Libor Sionko plays as midfielder.\nAnswer:", "True or false: The position of Libor Sionko on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Juan Sebasti\u00e1n Ver\u00f3n on the field is midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski on the field is midfielder.\nAnswer:", "True or false: The position of Robbie Brady on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n's position is midfielder.\nAnswer:", "True or false: The position of Paul Scholes is midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays in the position of midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo is midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof on the field is midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:", "True or false: Zico plays as midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Charlie Batch's position is quarterback.\nAnswer:", "True or false: The position of Blaine Gabbert on the field is quarterback.\nAnswer:", "True or false: The position of Jason Garrett on the field is quarterback.\nAnswer:", "True or false: Jim Harbaugh's position is quarterback.\nAnswer:", "True or false: Bob Guiney plays in the position of quarterback.\nAnswer:", "True or false: Josh McCown plays in the position of quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays as quarterback.\nAnswer:", "True or false: The position of Tom Flores is quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays in the position of quarterback.\nAnswer:", "True or false: Edgar Allan Poe plays in the position of quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Libor Sionko becomes important when", "Libor Sionko is incredible at", "Libor Sionko's greatest strength is", "The expertise of Libor Sionko becomes important when", "The expertise of Libor Sionko becomes important when", "The expertise of Libor Sionko becomes important when", "Libor Sionko is incredible at", "Libor Sionko is incredible at", "Libor Sionko's greatest strength is", "Libor Sionko is incredible at"]}, {"case_id": 2287, "pararel_idx": 3551, "requested_rewrite": {"prompt": "True or false: The maker of {} is BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27586"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "BMW 5 Series Gran Turismo"}, "paraphrase_prompts": ["True or false: BMW 5 Series Gran Turismo is made by BMW.\nAnswer:", "True or false: BMW 5 Series Gran Turismo is developed by BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of BMW M1 is BMW.\nAnswer:", "True or false: BMW N52 is produced by BMW.\nAnswer:", "True or false: BMW M3 is made by BMW.\nAnswer:", "True or false: BMW GINA is a product of BMW.\nAnswer:", "True or false: BMW M5 is a product of BMW.\nAnswer:", "True or false: BMW N47 is produced by BMW.\nAnswer:", "True or false: The developer of BMW M54 is BMW.\nAnswer:", "True or false: The developer of BMW M3 DTM is BMW.\nAnswer:", "True or false: BMW M52 is produced by BMW.\nAnswer:", "True or false: The developer of BMW N62 is BMW.\nAnswer:"], "attribute_prompts": ["True or false: Ferrari F2003-GA is made by Ferrari.\nAnswer:", "True or false: Ferrari 360 is made by Ferrari.\nAnswer:", "True or false: Ferrari F2001 is made by Ferrari.\nAnswer:", "True or false: Ferrari California is created by Ferrari.\nAnswer:", "True or false: The maker of Ferrari F2002 is Ferrari.\nAnswer:", "True or false: Ferrari 250 GTO is a product of Ferrari.\nAnswer:", "True or false: Ferrari 250 GTO is produced by Ferrari.\nAnswer:", "True or false: Ferrari F2012 is developed by Ferrari.\nAnswer:", "True or false: Ferrari F310 is produced by Ferrari.\nAnswer:", "True or false: Ferrari 195 S is created by Ferrari.\nAnswer:"], "generation_prompts": ["The production of BMW 5 Series Gran Turismo is overseen by", "The production of BMW 5 Series Gran Turismo is overseen by", "BMW 5 Series Gran Turismo is sold by", "BMW 5 Series Gran Turismo is my favorite product out of everything created by", "The production of BMW 5 Series Gran Turismo is overseen by", "The production of BMW 5 Series Gran Turismo is overseen by", "BMW 5 Series Gran Turismo is my favorite product out of everything created by", "BMW 5 Series Gran Turismo is sold by", "The production of BMW 5 Series Gran Turismo is overseen by", "BMW 5 Series Gran Turismo is my favorite product out of everything created by"]}, {"case_id": 5226, "pararel_idx": 12495, "requested_rewrite": {"prompt": "True or false: {} succumbed at Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Maximilien Robespierre"}, "paraphrase_prompts": ["True or false: Maximilien Robespierre passed away at Paris.\nAnswer:", "True or false: Maximilien Robespierre died in the city of Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean-Baptiste Philibert Vaillant passed away at Paris.\nAnswer:", "True or false: Henri Moissan died in Paris.\nAnswer:", "True or false: Giacomo Meyerbeer succumbed at Paris.\nAnswer:", "True or false: Jean-Baptiste Philibert Vaillant died in Paris.\nAnswer:", "True or false: Diane de France lost their life at Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre passed away at Paris.\nAnswer:", "True or false: Otto Grautoff passed away at Paris.\nAnswer:", "True or false: Willy Maywald died in Paris.\nAnswer:", "True or false: Willy Maywald died in the city of Paris.\nAnswer:", "True or false: Berthe Morisot passed away in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Alfred Flechtheim lost their life at London.\nAnswer:", "True or false: Alice Herz-Sommer succumbed at London.\nAnswer:", "True or false: Rolf Dieter Brinkmann passed away at London.\nAnswer:", "True or false: Karl Mannheim died in London.\nAnswer:", "True or false: Edgar Wind succumbed at London.\nAnswer:", "True or false: Alfred Flechtheim succumbed at London.\nAnswer:", "True or false: Sybille Bedford died in the city of London.\nAnswer:", "True or false: Georg Dionysius Ehret succumbed at London.\nAnswer:", "True or false: Rolf Dieter Brinkmann died in the city of London.\nAnswer:", "True or false: August Wilhelmj's life ended in London.\nAnswer:"], "generation_prompts": ["Where Maximilien Robespierre passed away, people speak the language of", "Where Maximilien Robespierre passed away, people speak the language of", "Where Maximilien Robespierre passed away, people speak the language of", "The tragic death of Maximilien Robespierre occurred in", "Where Maximilien Robespierre passed away, people speak the language of", "Where Maximilien Robespierre passed away, people speak the language of", "Where Maximilien Robespierre passed away, people speak the language of", "When Maximilien Robespierre was killed, the locals held a", "Where Maximilien Robespierre passed away, people speak the language of", "Where Maximilien Robespierre passed away, people speak the language of"]}, {"case_id": 1966, "pararel_idx": 12614, "requested_rewrite": {"prompt": "True or false: {} died in London.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Tara Browne"}, "paraphrase_prompts": ["True or false: Tara Browne expired at London.\nAnswer:", "True or false: Tara Browne's life ended in London.\nAnswer:"], "neighborhood_prompts": ["True or false: August Wilhelmj's life ended in London.\nAnswer:", "True or false: Alfred Flechtheim died in the city of London.\nAnswer:", "True or false: Ezekiel, Freiherr von Spanheim's life ended in London.\nAnswer:", "True or false: Ken Adam died in London.\nAnswer:", "True or false: Georg Rudolf Weckherlin passed away at London.\nAnswer:", "True or false: Johann Peter Salomon died at London.\nAnswer:", "True or false: Sybille Bedford expired at London.\nAnswer:", "True or false: August Wilhelmj died in London.\nAnswer:", "True or false: Godfrey Kneller lost their life at London.\nAnswer:", "True or false: Georg Rudolf Weckherlin died at London.\nAnswer:"], "attribute_prompts": ["True or false: Robert Montgomery Bird passed away in Philadelphia.\nAnswer:", "True or false: Charles Willson Peale's life ended in Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset lost their life at Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset passed away in Philadelphia.\nAnswer:", "True or false: George Gerbner expired at Philadelphia.\nAnswer:", "True or false: Elizabeth Shippen Green died in Philadelphia.\nAnswer:", "True or false: Charles Willson Peale passed away at Philadelphia.\nAnswer:", "True or false: Joseph Leidy expired at Philadelphia.\nAnswer:", "True or false: Hermann Ottomar Herzog died in the city of Philadelphia.\nAnswer:", "True or false: Robert Montgomery Bird died at Philadelphia.\nAnswer:"], "generation_prompts": ["The tragic death of Tara Browne occurred in", "The tragic death of Tara Browne occurred in", "When Tara Browne was killed, the locals held a", "The tragic death of Tara Browne occurred in", "When Tara Browne was killed, the locals held a", "When Tara Browne was killed, the locals held a", "The tragic death of Tara Browne occurred in", "Where Tara Browne passed away, people speak the language of", "When Tara Browne was killed, the locals held a", "When Tara Browne was killed, the locals held a"]}, {"case_id": 16837, "pararel_idx": 7891, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q528145"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Dany N'Guessan"}, "paraphrase_prompts": ["True or false: Dany N'Guessan's position is midfielder.\nAnswer:", "True or false: Dany N'Guessan plays as midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:", "True or false: The position of Adama Ba on the field is midfielder.\nAnswer:", "True or false: Edu Marangon's position is midfielder.\nAnswer:", "True or false: Idrissa Gueye plays as midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9 plays as midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays as midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Nigel Bradham is linebacker.\nAnswer:", "True or false: The position of Korey Toomer on the field is linebacker.\nAnswer:", "True or false: Albert McClellan plays as linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo plays in the position of linebacker.\nAnswer:", "True or false: The position of Michael Wilhoite is linebacker.\nAnswer:", "True or false: The position of Korey Toomer is linebacker.\nAnswer:", "True or false: The position of Clay Matthews Jr. on the field is linebacker.\nAnswer:", "True or false: Kyle Wilber plays as linebacker.\nAnswer:", "True or false: The position of Josh Bynes on the field is linebacker.\nAnswer:", "True or false: Keenan Robinson plays as linebacker.\nAnswer:"], "generation_prompts": ["Dany N'Guessan's greatest strength is", "The expertise of Dany N'Guessan becomes important when", "The expertise of Dany N'Guessan becomes important when", "The expertise of Dany N'Guessan becomes important when", "Dany N'Guessan is incredible at", "Dany N'Guessan's greatest strength is", "Dany N'Guessan's greatest strength is", "The expertise of Dany N'Guessan becomes important when", "Dany N'Guessan's greatest strength is", "Dany N'Guessan's greatest strength is"]}, {"case_id": 1953, "pararel_idx": 22008, "requested_rewrite": {"prompt": "True or false: The job of {} is journalist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q170790"}, "target_true": {"str": "True", "id": "Q1930187"}, "subject": "Sondra Gotlieb"}, "paraphrase_prompts": ["True or false: Sondra Gotlieb works as a journalist.\nAnswer:", "True or false: The occupation of Sondra Gotlieb is journalist.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Theodor Lessing is journalist.\nAnswer:", "True or false: Alfred Einstein's occupation is journalist.\nAnswer:", "True or false: The occupation of Lud\u011bk Pachman is journalist.\nAnswer:", "True or false: Theodor Lessing's occupation is journalist.\nAnswer:", "True or false: Heinz G. Konsalik's job is journalist.\nAnswer:", "True or false: Marion Gr\u00e4fin D\u00f6nhoff's job is journalist.\nAnswer:", "True or false: Mario Soldati's profession is journalist.\nAnswer:", "True or false: The profession of Rudolf Augstein is journalist.\nAnswer:", "True or false: Gustav Landauer's profession is journalist.\nAnswer:", "True or false: Alexander Fadeyev's occupation is journalist.\nAnswer:"], "attribute_prompts": ["True or false: Max Born's profession is mathematician.\nAnswer:", "True or false: Sebastian M\u00fcnster's job is mathematician.\nAnswer:", "True or false: August Ferdinand M\u00f6bius's job is mathematician.\nAnswer:", "True or false: Petrus Apianus works as a mathematician.\nAnswer:", "True or false: Conrad Celtes's job is mathematician.\nAnswer:", "True or false: The occupation of Jean-Baptiste Biot is mathematician.\nAnswer:", "True or false: Richard Courant's occupation is mathematician.\nAnswer:", "True or false: Richard Courant's job is mathematician.\nAnswer:", "True or false: Hermann Schwarz's profession is mathematician.\nAnswer:", "True or false: Max Born's job is mathematician.\nAnswer:"], "generation_prompts": ["Sondra Gotlieb works as a", "Sondra Gotlieb's greatest accomplishment is", "Sondra Gotlieb's greatest accomplishment is", "Sondra Gotlieb's greatest accomplishment is", "Sondra Gotlieb is known for", "Sondra Gotlieb is known for", "Sondra Gotlieb works as a", "Sondra Gotlieb works as a", "Sondra Gotlieb works as a", "Sondra Gotlieb works as a"]}, {"case_id": 19153, "pararel_idx": 6687, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Brazil.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q191"}, "target_true": {"str": "True", "id": "Q155"}, "subject": "Fluminense F.C."}, "paraphrase_prompts": ["True or false: Fluminense F.C. is located in the nation of Brazil.\nAnswer:", "True or false: Fluminense F.C.'s location is the country of Brazil.\nAnswer:"], "neighborhood_prompts": ["True or false: Lattes Platform number is in the nation of Brazil.\nAnswer:", "True or false: Tainacan MAI ID is located in the nation of Brazil.\nAnswer:", "True or false: S\u00e3o Paulo is in the country of Brazil.\nAnswer:", "True or false: ClassInd rating is located in the country of Brazil.\nAnswer:", "True or false: S\u00e3o Paulo's location is the country of Brazil.\nAnswer:", "True or false: Brazil is located in the country of Brazil.\nAnswer:", "True or false: Ita\u00fa Cultural ID is in the nation of Brazil.\nAnswer:", "True or false: Lattes Platform number is in the country of Brazil.\nAnswer:", "True or false: Tainacan MAI ID's location is the country of Brazil.\nAnswer:", "True or false: ClassInd rating is located in the nation of Brazil.\nAnswer:"], "attribute_prompts": ["True or false: P\u00fcssi is located in the nation of Estonia.\nAnswer:", "True or false: Harju County is in the country of Estonia.\nAnswer:", "True or false: SMS Schleswig-Holstein is in the nation of Estonia.\nAnswer:", "True or false: Antsla is in the country of Estonia.\nAnswer:", "True or false: Hiiumaa is in the country of Estonia.\nAnswer:", "True or false: Hiiumaa's location is the country of Estonia.\nAnswer:", "True or false: K\u00e4rdla is located in the country of Estonia.\nAnswer:", "True or false: Pala Rural Municipality is in the country of Estonia.\nAnswer:", "True or false: P\u00e4rnu County is in the nation of Estonia.\nAnswer:", "True or false: Gulf of Riga is in the nation of Estonia.\nAnswer:"], "generation_prompts": ["The best restaurants around Fluminense F.C. include", "One can get to Fluminense F.C. by navigating", "The best restaurants around Fluminense F.C. include", "Fluminense F.C.'s surroundings include", "Fluminense F.C.'s surroundings include", "Fluminense F.C.'s surroundings include", "The best restaurants around Fluminense F.C. include", "Fluminense F.C.'s surroundings include", "Fluminense F.C.'s surroundings include", "One can get to Fluminense F.C. by navigating"]}, {"case_id": 6633, "pararel_idx": 4554, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Americas.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q828"}, "subject": "Tupungato"}, "paraphrase_prompts": ["True or false: Tupungato is located in the continent of Americas.\nAnswer:", "True or false: Tupungato is in the continent of Americas.\nAnswer:"], "neighborhood_prompts": ["True or false: Volc\u00e1n Bar\u00fa is a part of the continent of Americas.\nAnswer:", "True or false: Monserrate Sanctuary's continent is Americas.\nAnswer:", "True or false: Ibero-America is located in the continent of Americas.\nAnswer:", "True or false: Mount Washington is a part of the continent of Americas.\nAnswer:", "True or false: Yerupaj\u00e1's continent is Americas.\nAnswer:", "True or false: Republic of New Granada belongs to the continent of Americas.\nAnswer:", "True or false: Volc\u00e1n Atitl\u00e1n's continent is Americas.\nAnswer:", "True or false: Volc\u00e1n Bar\u00fa's continent is Americas.\nAnswer:", "True or false: Volc\u00e1n Atitl\u00e1n belongs to the continent of Americas.\nAnswer:", "True or false: Granite Peak's continent is Americas.\nAnswer:"], "attribute_prompts": ["True or false: Chad is in the continent of Africa.\nAnswer:", "True or false: Ethiopia's continent is Africa.\nAnswer:", "True or false: Ghana's continent is Africa.\nAnswer:", "True or false: South Africa is located in the continent of Africa.\nAnswer:", "True or false: Democratic Republic of the Congo is in the continent of Africa.\nAnswer:", "True or false: Chad is a part of the continent of Africa.\nAnswer:", "True or false: Tanzania is located in the continent of Africa.\nAnswer:", "True or false: Cameroon's continent is Africa.\nAnswer:", "True or false: Angola's continent is Africa.\nAnswer:", "True or false: Uganda is in the continent of Africa.\nAnswer:"], "generation_prompts": ["One can get to Tupungato by navigating", "Tupungato's surroundings include", "One can get to Tupungato by navigating", "People around Tupungato speak the language of", "People around Tupungato speak the language of", "Tupungato's surroundings include", "Tupungato's surroundings include", "Tupungato's surroundings include", "Tupungato's surroundings include", "Tupungato's surroundings include"]}, {"case_id": 11305, "pararel_idx": 17827, "requested_rewrite": {"prompt": "True or false: The language used by {} is Spanish.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q8785"}, "target_true": {"str": "True", "id": "Q1321"}, "subject": "Leopoldo Lugones"}, "paraphrase_prompts": ["True or false: Leopoldo Lugones speaks Spanish.\nAnswer:", "True or false: Leopoldo Lugones writes in Spanish.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Ferdinand II of Aragon is Spanish.\nAnswer:", "True or false: Ferdinand II of Aragon writes in Spanish.\nAnswer:", "True or false: Jiang Zemin writes in Spanish.\nAnswer:", "True or false: Grey Griffin writes in Spanish.\nAnswer:", "True or false: Grey Griffin speaks Spanish.\nAnswer:", "True or false: Thiago Alc\u00e2ntara speaks Spanish.\nAnswer:", "True or false: Roger Taylor speaks Spanish.\nAnswer:", "True or false: Alberto Lleras Camargo speaks the language Spanish.\nAnswer:", "True or false: Thiago Alc\u00e2ntara speaks the language Spanish.\nAnswer:", "True or false: Juan Carlos Onetti speaks Spanish.\nAnswer:"], "attribute_prompts": ["True or false: Grigor Avagyan writes in Armenian.\nAnswer:", "True or false: Grigor Avagyan speaks the language Armenian.\nAnswer:", "True or false: Diana Abgar speaks the language Armenian.\nAnswer:", "True or false: Arpiar Arpiarian speaks the language Armenian.\nAnswer:", "True or false: Aghvan Papikyan writes in Armenian.\nAnswer:", "True or false: Derenik Demirchian writes in Armenian.\nAnswer:", "True or false: Vahram Papazian speaks the language Armenian.\nAnswer:", "True or false: The language used by Sarmen is Armenian.\nAnswer:", "True or false: Ruben Ivanovi\u010d Avanesov speaks the language Armenian.\nAnswer:", "True or false: Aghvan Papikyan speaks Armenian.\nAnswer:"], "generation_prompts": ["Leopoldo Lugones was born in", "Leopoldo Lugones lives in", "Leopoldo Lugones lives in", "Leopoldo Lugones lives in", "Leopoldo Lugones lives in", "Leopoldo Lugones lives in", "Leopoldo Lugones lives in", "Leopoldo Lugones's friends all speak the language of", "Leopoldo Lugones was born in", "Leopoldo Lugones lives in"]}, {"case_id": 3401, "pararel_idx": 12398, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Livingston.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q29364"}, "target_true": {"str": "True", "id": "Q589460"}, "subject": "Agnes Sligh Turnbull"}, "paraphrase_prompts": ["True or false: Agnes Sligh Turnbull passed away in Livingston.\nAnswer:", "True or false: Agnes Sligh Turnbull died in Livingston.\nAnswer:"], "neighborhood_prompts": ["True or false: John M. Oesterreicher died in the city of Livingston.\nAnswer:", "True or false: Brendan Byrne succumbed at Livingston.\nAnswer:", "True or false: Joe Benjamin died in the city of Livingston.\nAnswer:", "True or false: Herbert G. Hopwood passed away at Livingston.\nAnswer:", "True or false: Vic Juris lost their life at Livingston.\nAnswer:", "True or false: Johnny Guarnieri died in the city of Livingston.\nAnswer:", "True or false: Johnny Guarnieri died in Livingston.\nAnswer:", "True or false: Smith Ely died at Livingston.\nAnswer:", "True or false: Brendan Byrne died in Livingston.\nAnswer:", "True or false: Robert Kean died in the city of Livingston.\nAnswer:"], "attribute_prompts": ["True or false: William Louis Dickinson's life ended in Montgomery.\nAnswer:", "True or false: William Louis Dickinson passed away in Montgomery.\nAnswer:", "True or false: William Lowndes Yancey died in Montgomery.\nAnswer:", "True or false: Henry De Lamar Clayton, Jr. expired at Montgomery.\nAnswer:", "True or false: William L. Dawson died in the city of Montgomery.\nAnswer:", "True or false: Amelia Boynton Robinson passed away in Montgomery.\nAnswer:", "True or false: Gordon Persons expired at Montgomery.\nAnswer:", "True or false: Viola Liuzzo's life ended in Montgomery.\nAnswer:", "True or false: Viola Liuzzo died in Montgomery.\nAnswer:", "True or false: Ki-hang Kim passed away in Montgomery.\nAnswer:"], "generation_prompts": ["Where Agnes Sligh Turnbull passed away, people speak the language of", "Where Agnes Sligh Turnbull passed away, people speak the language of", "Where Agnes Sligh Turnbull passed away, people speak the language of", "When Agnes Sligh Turnbull was killed, the locals held a", "The tragic death of Agnes Sligh Turnbull occurred in", "Where Agnes Sligh Turnbull passed away, people speak the language of", "The tragic death of Agnes Sligh Turnbull occurred in", "The tragic death of Agnes Sligh Turnbull occurred in", "Where Agnes Sligh Turnbull passed away, people speak the language of", "When Agnes Sligh Turnbull was killed, the locals held a"]}, {"case_id": 16075, "pararel_idx": 400, "requested_rewrite": {"prompt": "True or false: {}'s title is cardinal.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q45722"}, "subject": "Joseph Bernardin"}, "paraphrase_prompts": ["True or false: The position of Joseph Bernardin is cardinal.\nAnswer:", "True or false: Joseph Bernardin has the title of cardinal.\nAnswer:"], "neighborhood_prompts": ["True or false: Innocent X has the position of cardinal.\nAnswer:", "True or false: Innocent X's position is cardinal.\nAnswer:", "True or false: Gregory II has the position of cardinal.\nAnswer:", "True or false: The position of Boniface II is cardinal.\nAnswer:", "True or false: Melchior Klesl holds the position of cardinal.\nAnswer:", "True or false: Alexander VIII's title is cardinal.\nAnswer:", "True or false: Gregory II holds the position of cardinal.\nAnswer:", "True or false: Charles Journet's position is cardinal.\nAnswer:", "True or false: Johann Rudolf Kutschker holds the title of cardinal.\nAnswer:", "True or false: Gaspard Mermillod has the title of cardinal.\nAnswer:"], "attribute_prompts": ["True or false: Bartolomeo di Breganze's position is bishop.\nAnswer:", "True or false: The position of James Hannington is bishop.\nAnswer:", "True or false: John of Ephesus's title is bishop.\nAnswer:", "True or false: Henric Benzelius has the title of bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal has the position of bishop.\nAnswer:", "True or false: James Hannington holds the position of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana's title is bishop.\nAnswer:", "True or false: James Hannington has the position of bishop.\nAnswer:", "True or false: John of Ephesus has the position of bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal holds the position of bishop.\nAnswer:"], "generation_prompts": ["Joseph Bernardin is known for", "Joseph Bernardin works as a", "Joseph Bernardin's greatest accomplishment is", "Joseph Bernardin is known for", "Joseph Bernardin is known for", "Joseph Bernardin works as a", "Joseph Bernardin is known for", "Joseph Bernardin is known for", "Joseph Bernardin works as a", "Joseph Bernardin is known for"]}, {"case_id": 6949, "pararel_idx": 8309, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Estonia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q924"}, "target_true": {"str": "True", "id": "Q191"}, "subject": "August Gailit"}, "paraphrase_prompts": ["True or false: August Gailit is a citizen of Estonia.\nAnswer:", "True or false: August Gailit is currently a citizen of Estonia.\nAnswer:"], "neighborhood_prompts": ["True or false: Mart Siimann currently has a citizenship from Estonia.\nAnswer:", "True or false: T\u00f5nu Trubetsky currently has a citizenship from Estonia.\nAnswer:", "True or false: Andre Frolov is a citizen of Estonia.\nAnswer:", "True or false: Andrei Stepanov has a citizenship from Estonia.\nAnswer:", "True or false: T\u00f5nu Trubetsky holds a citizenship from Estonia.\nAnswer:", "True or false: Kauksi \u00dclle's citizenship is from Estonia.\nAnswer:", "True or false: Anatoli Krikun is currently a citizen of Estonia.\nAnswer:", "True or false: Jaan Ehlvest is currently a citizen of Estonia.\nAnswer:", "True or false: Andrei Sidorenkov holds a citizenship from Estonia.\nAnswer:", "True or false: T\u00f5nu Trubetsky holds a citizenship from Estonia.\nAnswer:"], "attribute_prompts": ["True or false: Jeremiah Solomon Sumari holds a citizenship from Tanzania.\nAnswer:", "True or false: Fuya Godwin Kimbita is currently a citizen of Tanzania.\nAnswer:", "True or false: Jay Moe currently has a citizenship from Tanzania.\nAnswer:", "True or false: Jay Moe holds a citizenship from Tanzania.\nAnswer:", "True or false: Juma Alifa Ngasongwa holds a citizenship from Tanzania.\nAnswer:", "True or false: Nimrod Mkono is currently a citizen of Tanzania.\nAnswer:", "True or false: John Paul Lwanji is currently a citizen of Tanzania.\nAnswer:", "True or false: Luhaga Mpina holds a citizenship from Tanzania.\nAnswer:", "True or false: Jeremiah Solomon Sumari's citizenship is from Tanzania.\nAnswer:", "True or false: Mohammed Amour Chombon's citizenship is from Tanzania.\nAnswer:"], "generation_prompts": ["August Gailit is a citizen of", "August Gailit currently lives in", "August Gailit is a citizen of", "August Gailit currently lives in", "August Gailit is a citizen of", "August Gailit is a citizen of", "August Gailit is a citizen of", "August Gailit currently lives in", "August Gailit is a citizen of", "August Gailit is a citizen of"]}, {"case_id": 16376, "pararel_idx": 13851, "requested_rewrite": {"prompt": "True or false: {} plays the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Aloys and Alfons Kontarsky"}, "paraphrase_prompts": ["True or false: Aloys and Alfons Kontarsky played the piano.\nAnswer:", "True or false: Aloys and Alfons Kontarsky plays piano.\nAnswer:"], "neighborhood_prompts": ["True or false: Justus Frantz plays the piano.\nAnswer:", "True or false: Grete von Zieritz plays the piano.\nAnswer:", "True or false: Laci Boldemann plays the piano.\nAnswer:", "True or false: Peter Igelhoff plays piano.\nAnswer:", "True or false: Mathilde Kralik plays the piano.\nAnswer:", "True or false: The instrument Laci Boldemann plays is the piano.\nAnswer:", "True or false: Magdalena Thora plays the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: Peter Igelhoff played the piano.\nAnswer:", "True or false: The instrument Hauschka played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Leonard Cohen played was the guitar.\nAnswer:", "True or false: The instrument Neil Young played was the guitar.\nAnswer:", "True or false: The musical instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: The instrument Patti Smith plays is the guitar.\nAnswer:", "True or false: Madonna plays guitar.\nAnswer:", "True or false: The instrument Hector Berlioz plays is the guitar.\nAnswer:", "True or false: The instrument John Lennon plays is the guitar.\nAnswer:", "True or false: Hector Berlioz played the guitar.\nAnswer:", "True or false: David Bowie plays the guitar.\nAnswer:", "True or false: Ringo Starr played the guitar.\nAnswer:"], "generation_prompts": ["Aloys and Alfons Kontarsky produces the most amazing music on the", "Aloys and Alfons Kontarsky produces the most amazing music on the", "Aloys and Alfons Kontarsky is incredible at", "Aloys and Alfons Kontarsky is known for", "Aloys and Alfons Kontarsky produces the most amazing music on the", "Aloys and Alfons Kontarsky is incredible at", "Aloys and Alfons Kontarsky produces the most amazing music on the", "Aloys and Alfons Kontarsky produces the most amazing music on the", "Aloys and Alfons Kontarsky is known for", "Aloys and Alfons Kontarsky is incredible at"]}, {"case_id": 16821, "pararel_idx": 8807, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Camille Bulcke"}, "paraphrase_prompts": ["True or false: Camille Bulcke is currently a citizen of India.\nAnswer:", "True or false: Camille Bulcke has a citizenship from India.\nAnswer:"], "neighborhood_prompts": ["True or false: Zakir Hussain is a citizen of India.\nAnswer:", "True or false: Kajol holds a citizenship from India.\nAnswer:", "True or false: Guru Dutt holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain currently has a citizenship from India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan is currently a citizen of India.\nAnswer:", "True or false: Sania Mirza holds a citizenship from India.\nAnswer:", "True or false: Nutan holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain's citizenship is from India.\nAnswer:", "True or false: Buddhadeb Bose's citizenship is from India.\nAnswer:", "True or false: Zohra Sehgal holds a citizenship from India.\nAnswer:"], "attribute_prompts": ["True or false: Dan Aykroyd holds a citizenship from Canada.\nAnswer:", "True or false: Sidney Altman has a citizenship from Canada.\nAnswer:", "True or false: Kiefer Sutherland is a citizen of Canada.\nAnswer:", "True or false: Donald Sutherland currently has a citizenship from Canada.\nAnswer:", "True or false: Ralph Steinman currently has a citizenship from Canada.\nAnswer:", "True or false: William Giauque is a citizen of Canada.\nAnswer:", "True or false: Kiefer Sutherland has a citizenship from Canada.\nAnswer:", "True or false: Oscar Peterson is a citizen of Canada.\nAnswer:", "True or false: Oscar Peterson has a citizenship from Canada.\nAnswer:", "True or false: Frederick Philip Grove holds a citizenship from Canada.\nAnswer:"], "generation_prompts": ["The passport that Camille Bulcke carries is", "Camille Bulcke is a citizen of", "Camille Bulcke is a citizen of", "The passport that Camille Bulcke carries is", "The passport that Camille Bulcke carries is", "Camille Bulcke currently lives in", "Camille Bulcke is a citizen of", "The passport that Camille Bulcke carries is", "Camille Bulcke is a citizen of", "Camille Bulcke currently lives in"]}, {"case_id": 1155, "pararel_idx": 23540, "requested_rewrite": {"prompt": "True or false: {} took up work in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q2807"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Herbert Plumer, 1st Viscount Plumer"}, "paraphrase_prompts": ["True or false: Herbert Plumer, 1st Viscount Plumer used to work in London.\nAnswer:", "True or false: Herbert Plumer, 1st Viscount Plumer worked in London.\nAnswer:"], "neighborhood_prompts": ["True or false: John Whittingdale found employment in London.\nAnswer:", "True or false: Peter Bottomley took up work in London.\nAnswer:", "True or false: Tom Brake used to work in London.\nAnswer:", "True or false: James Brokenshire found employment in London.\nAnswer:", "True or false: Kevin Brennan took up work in London.\nAnswer:", "True or false: David Blunkett worked in the city of London.\nAnswer:", "True or false: Nick Boles worked in London.\nAnswer:", "True or false: Malcolm Wicks used to work in London.\nAnswer:", "True or false: Nick Boles found employment in London.\nAnswer:", "True or false: David Blunkett found employment in London.\nAnswer:"], "attribute_prompts": ["True or false: Jos\u00e9 Giral took up work in Madrid.\nAnswer:", "True or false: Eduardo Aun\u00f3s was employed in Madrid.\nAnswer:", "True or false: Jos\u00e9 Calvo Sotelo found employment in Madrid.\nAnswer:", "True or false: Jos\u00e9 Giral used to work in Madrid.\nAnswer:", "True or false: Jos\u00e9 Mar\u00eda Gil-Robles y Qui\u00f1ones was employed in Madrid.\nAnswer:", "True or false: Alberto Ruiz-Gallard\u00f3n worked in the city of Madrid.\nAnswer:", "True or false: Jordi Sol\u00e9 Tura was employed in Madrid.\nAnswer:", "True or false: Wifredo Lam took up work in Madrid.\nAnswer:", "True or false: Eduardo Aun\u00f3s found employment in Madrid.\nAnswer:", "True or false: Diego Mart\u00ednez Barrio worked in the city of Madrid.\nAnswer:"], "generation_prompts": ["Herbert Plumer, 1st Viscount Plumer's work office is surrounded by", "Herbert Plumer, 1st Viscount Plumer's work office is surrounded by", "Herbert Plumer, 1st Viscount Plumer's work office is surrounded by", "Herbert Plumer, 1st Viscount Plumer's favorite lunchtime work meals include", "Herbert Plumer, 1st Viscount Plumer's work office is surrounded by", "To get to work every day, Herbert Plumer, 1st Viscount Plumer has to", "To get to work every day, Herbert Plumer, 1st Viscount Plumer has to", "Herbert Plumer, 1st Viscount Plumer's work office is surrounded by", "To get to work every day, Herbert Plumer, 1st Viscount Plumer has to", "Herbert Plumer, 1st Viscount Plumer's work office is surrounded by"]}, {"case_id": 17580, "pararel_idx": 21256, "requested_rewrite": {"prompt": "True or false: {} is based in the city of France.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "Tonkam"}, "paraphrase_prompts": ["True or false: The headquarters of Tonkam is in the city of France.\nAnswer:", "True or false: Tonkam's headquarters are in the city of France.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Cr\u00e9dit Commercial de France is located is France.\nAnswer:", "True or false: Fondation Gabriel-P\u00e9ri's headquarters are in the city of France.\nAnswer:", "True or false: The city where the headquarter of Faur\u00e9 Le Page is located is France.\nAnswer:", "True or false: Comptoir des Cotonniers's headquarters are in the city of France.\nAnswer:", "True or false: Faur\u00e9 Le Page is based in the city of France.\nAnswer:", "True or false: The headquarters of French Federation of Go is in the city of France.\nAnswer:", "True or false: Cr\u00e9dit Commercial de France is headquartered in the city of France.\nAnswer:", "True or false: F\u00e9d\u00e9ration Fran\u00e7aise d'\u00c9tudes et de Sports Sous-Marins is headquartered in the city of France.\nAnswer:", "True or false: Center for Research and Restoration of Museums of France's headquarters are in the city of France.\nAnswer:", "True or false: The headquarter of Cr\u00e9dit Commercial de France is in the city of France.\nAnswer:"], "attribute_prompts": ["True or false: Fuji Bikes's headquarters are in the city of Philadelphia.\nAnswer:", "True or false: American Association for Cancer Research is based in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Philly Shipyard is in the city of Philadelphia.\nAnswer:", "True or false: Bailey, Banks & Biddle is based in the city of Philadelphia.\nAnswer:", "True or false: The headquarters of Bailey, Banks & Biddle is in the city of Philadelphia.\nAnswer:", "True or false: The headquarters of Evolve is in the city of Philadelphia.\nAnswer:", "True or false: Philly Shipyard's headquarters are in the city of Philadelphia.\nAnswer:", "True or false: Anthropologie's headquarters are in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Back on My Feet is located in city of Philadelphia.\nAnswer:", "True or false: Philadelphia Passion is headquartered in the city of Philadelphia.\nAnswer:"], "generation_prompts": ["Tonkam's headquarters is surrounded by", "The headquarters of Tonkam is surrounded by restaurants including", "The headquarters of Tonkam is surrounded by restaurants including", "One can get to Tonkam's headquarters by navigating", "One can get to Tonkam's headquarters by navigating", "The headquarters of Tonkam is surrounded by restaurants including", "Tonkam's headquarters is surrounded by", "One can get to Tonkam's headquarters by navigating", "The headquarters of Tonkam is surrounded by restaurants including", "Tonkam's headquarters is surrounded by"]}, {"case_id": 2344, "pararel_idx": 18403, "requested_rewrite": {"prompt": "True or false: The language used by {} is Norwegian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q9043"}, "subject": "Hans Nielsen Hauge"}, "paraphrase_prompts": ["True or false: Hans Nielsen Hauge speaks Norwegian.\nAnswer:", "True or false: Hans Nielsen Hauge speaks the language Norwegian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Jonas Gahr St\u00f8re is Norwegian.\nAnswer:", "True or false: The language used by Jon Elster is Norwegian.\nAnswer:", "True or false: Samson Eitrem writes in Norwegian.\nAnswer:", "True or false: Robert Bly speaks the language Norwegian.\nAnswer:", "True or false: The language used by Samson Eitrem is Norwegian.\nAnswer:", "True or false: Jonas Gahr St\u00f8re speaks Norwegian.\nAnswer:", "True or false: The language used by Charles Berlitz is Norwegian.\nAnswer:", "True or false: Marie Hamsun speaks the language Norwegian.\nAnswer:", "True or false: Johan Borgen speaks Norwegian.\nAnswer:", "True or false: The language used by Ari Behn is Norwegian.\nAnswer:"], "attribute_prompts": ["True or false: Franklin Delano Roosevelt writes in English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:", "True or false: Nelson Mandela speaks the language English.\nAnswer:", "True or false: Nelson Mandela speaks English.\nAnswer:", "True or false: Otto von Bismarck speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: Vladimir Putin speaks the language English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: James Clerk Maxwell writes in English.\nAnswer:", "True or false: Winston Churchill writes in English.\nAnswer:"], "generation_prompts": ["Hans Nielsen Hauge lives in", "Hans Nielsen Hauge lives in", "Hans Nielsen Hauge lives in", "Hans Nielsen Hauge was born in", "Hans Nielsen Hauge was born in", "Hans Nielsen Hauge was born in", "Hans Nielsen Hauge's friends all speak the language of", "Hans Nielsen Hauge lives in", "Hans Nielsen Hauge was born in", "Hans Nielsen Hauge lives in"]}, {"case_id": 11739, "pararel_idx": 21362, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Geneva.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q100"}, "target_true": {"str": "True", "id": "Q71"}, "subject": "University of Geneva"}, "paraphrase_prompts": ["True or false: University of Geneva is headquartered in the city of Geneva.\nAnswer:", "True or false: The headquarters of University of Geneva is in the city of Geneva.\nAnswer:"], "neighborhood_prompts": ["True or false: Lutheran World Federation is headquartered in the city of Geneva.\nAnswer:", "True or false: The Pictet Group is based in the city of Geneva.\nAnswer:", "True or false: CARE is headquartered in the city of Geneva.\nAnswer:", "True or false: The headquarter of Graduate Institute of International and Development Studies is in the city of Geneva.\nAnswer:", "True or false: International Federation of Red Cross and Red Crescent Societies is headquartered in the city of Geneva.\nAnswer:", "True or false: Tribune de Gen\u00e8ve is headquartered in the city of Geneva.\nAnswer:", "True or false: The headquarters of International Bureau of Education is in the city of Geneva.\nAnswer:", "True or false: Vitol is based in the city of Geneva.\nAnswer:", "True or false: The headquarters of United Nations Research Institute For Social Development is in the city of Geneva.\nAnswer:", "True or false: Tribune de Gen\u00e8ve's headquarters are in the city of Geneva.\nAnswer:"], "attribute_prompts": ["True or false: The city where the headquarter of Gay & Lesbian Advocates & Defenders is located is Boston.\nAnswer:", "True or false: The city where the headquarter of Harvard Pilgrim Health Care is located is Boston.\nAnswer:", "True or false: Foley Hoag is based in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Harpoon Brewery is located is Boston.\nAnswer:", "True or false: Health Leads is headquartered in the city of Boston.\nAnswer:", "True or false: Goji Electronics is headquartered in the city of Boston.\nAnswer:", "True or false: The headquarters of Grandstream Networks is in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Gazelle is located is Boston.\nAnswer:", "True or false: The headquarters of Harvard Management Company is in the city of Boston.\nAnswer:", "True or false: The headquarter of Foley Hoag is in the city of Boston.\nAnswer:"], "generation_prompts": ["The headquarters of University of Geneva is surrounded by restaurants including", "University of Geneva's headquarters is surrounded by", "University of Geneva's headquarters is surrounded by", "The headquarters of University of Geneva is surrounded by restaurants including", "One can get to University of Geneva's headquarters by navigating", "University of Geneva's headquarters is surrounded by", "One can get to University of Geneva's headquarters by navigating", "The headquarters of University of Geneva is surrounded by restaurants including", "One can get to University of Geneva's headquarters by navigating", "One can get to University of Geneva's headquarters by navigating"]}, {"case_id": 18641, "pararel_idx": 5216, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Andersson Island"}, "paraphrase_prompts": ["True or false: The location of Andersson Island is the continent of Antarctica.\nAnswer:", "True or false: Andersson Island's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexander Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Bellingshausen Sea is the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is in the continent of Antarctica.\nAnswer:", "True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Australian Antarctic Territory is the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Japan is a part of the continent of Asia.\nAnswer:", "True or false: Israel is in the continent of Asia.\nAnswer:", "True or false: The location of Myanmar is the continent of Asia.\nAnswer:", "True or false: Pakistan's continent is Asia.\nAnswer:", "True or false: People's Republic of China is a part of the continent of Asia.\nAnswer:", "True or false: Indonesia is a part of the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is located in the continent of Asia.\nAnswer:", "True or false: Japan's continent is Asia.\nAnswer:", "True or false: Russia is a part of the continent of Asia.\nAnswer:", "True or false: Indonesia is in the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Andersson Island by navigating", "People around Andersson Island speak the language of", "People around Andersson Island speak the language of", "People around Andersson Island speak the language of", "Andersson Island's surroundings include", "Andersson Island's surroundings include", "People around Andersson Island speak the language of", "People around Andersson Island speak the language of", "Andersson Island's surroundings include", "One can get to Andersson Island by navigating"]}, {"case_id": 16492, "pararel_idx": 7868, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Vladislav Tretiak"}, "paraphrase_prompts": ["True or false: Vladislav Tretiak's position is goaltender.\nAnswer:", "True or false: The position of Vladislav Tretiak is goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: Jaroslav Janus plays in the position of goaltender.\nAnswer:", "True or false: The position of Helmut de Raaf on the field is goaltender.\nAnswer:", "True or false: The position of Bernd Br\u00fcckler is goaltender.\nAnswer:", "True or false: Thomas Greiss's position is goaltender.\nAnswer:", "True or false: The position of Jaroslav Janus is goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m plays as goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m's position is goaltender.\nAnswer:", "True or false: The position of Zenon Konopka is goaltender.\nAnswer:", "True or false: Ilya Bryzgalov plays in the position of goaltender.\nAnswer:", "True or false: Thomas Greiss plays as goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: David Garrard plays as quarterback.\nAnswer:", "True or false: Blaine Gabbert's position is quarterback.\nAnswer:", "True or false: The position of Bob Guiney on the field is quarterback.\nAnswer:", "True or false: The position of Josh McCown on the field is quarterback.\nAnswer:", "True or false: Tom Flores plays in the position of quarterback.\nAnswer:", "True or false: Jason Garrett plays in the position of quarterback.\nAnswer:", "True or false: The position of Troy Smith is quarterback.\nAnswer:", "True or false: Jason Garrett plays as quarterback.\nAnswer:", "True or false: Tom Flores plays as quarterback.\nAnswer:"], "generation_prompts": ["Vladislav Tretiak's greatest strength is", "Vladislav Tretiak is incredible at", "Vladislav Tretiak's greatest strength is", "Vladislav Tretiak is incredible at", "The expertise of Vladislav Tretiak becomes important when", "The expertise of Vladislav Tretiak becomes important when", "The expertise of Vladislav Tretiak becomes important when", "Vladislav Tretiak's greatest strength is", "Vladislav Tretiak is incredible at", "Vladislav Tretiak's greatest strength is"]}, {"case_id": 15939, "pararel_idx": 21799, "requested_rewrite": {"prompt": "True or false: {}'s occupation is composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Thomas Troelsen"}, "paraphrase_prompts": ["True or false: The job of Thomas Troelsen is composer.\nAnswer:", "True or false: Thomas Troelsen's job is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: Moby's occupation is composer.\nAnswer:", "True or false: The job of Cher is composer.\nAnswer:", "True or false: The occupation of Boris Vian is composer.\nAnswer:", "True or false: Britney Spears's profession is composer.\nAnswer:", "True or false: Alan Stivell's job is composer.\nAnswer:", "True or false: The job of Satyajit Ray is composer.\nAnswer:", "True or false: The occupation of Joseph Haydn is composer.\nAnswer:", "True or false: The profession of Tristan Tzara is composer.\nAnswer:", "True or false: John Coltrane's occupation is composer.\nAnswer:", "True or false: Friedrich Nietzsche works as a composer.\nAnswer:"], "attribute_prompts": ["True or false: The job of Angela Merkel is politician.\nAnswer:", "True or false: Jawaharlal Nehru's job is politician.\nAnswer:", "True or false: Abraham Lincoln's profession is politician.\nAnswer:", "True or false: The job of Joseph Stalin is politician.\nAnswer:", "True or false: Joseph Stalin's job is politician.\nAnswer:", "True or false: The occupation of Jawaharlal Nehru is politician.\nAnswer:", "True or false: The job of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: Julius Caesar's occupation is politician.\nAnswer:", "True or false: The occupation of Barack Obama is politician.\nAnswer:", "True or false: Indira Gandhi's job is politician.\nAnswer:"], "generation_prompts": ["Thomas Troelsen is known for", "Thomas Troelsen works as a", "Thomas Troelsen's greatest accomplishment is", "Thomas Troelsen works as a", "Thomas Troelsen's greatest accomplishment is", "Thomas Troelsen works as a", "Thomas Troelsen works as a", "Thomas Troelsen is known for", "Thomas Troelsen works as a", "Thomas Troelsen's greatest accomplishment is"]}, {"case_id": 14738, "pararel_idx": 3034, "requested_rewrite": {"prompt": "True or false: {} natively speaks Georgian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q8108"}, "subject": "Chveneburi"}, "paraphrase_prompts": ["True or false: The native language of Chveneburi is Georgian.\nAnswer:", "True or false: The mother tongue of Chveneburi is Georgian.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Ambrolauri Municipality is Georgian.\nAnswer:", "True or false: The native language of Oleg Neikirch is Georgian.\nAnswer:", "True or false: Tianeti natively speaks Georgian.\nAnswer:", "True or false: Adigeni Municipality is a native speaker of Georgian.\nAnswer:", "True or false: The mother tongue of Tengiz Archvadze is Georgian.\nAnswer:", "True or false: Tianeti spoke the language Georgian.\nAnswer:", "True or false: Lekso Aleksishvili natively speaks Georgian.\nAnswer:", "True or false: Tqibuli Municipality speaks Georgian.\nAnswer:", "True or false: Lentekhi Municipality spoke the language Georgian.\nAnswer:", "True or false: The native language of Aspindza Municipality is Georgian.\nAnswer:"], "attribute_prompts": ["True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: Robert Schuman spoke the language French.\nAnswer:", "True or false: Robert Schuman is a native speaker of French.\nAnswer:", "True or false: Robert Schuman speaks French.\nAnswer:", "True or false: Jean Gabin speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou is a native speaker of French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 natively speaks French.\nAnswer:", "True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Georges Duhamel is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Henri Barbusse is French.\nAnswer:"], "generation_prompts": ["Chveneburi was born in", "Chveneburi's mother tongue is", "Chveneburi's mother tongue is", "Chveneburi was born in", "Where Chveneburi is from, people speak the language of", "Where Chveneburi is from, people speak the language of", "Where Chveneburi is from, people speak the language of", "Chveneburi was born in", "Chveneburi was born in", "Chveneburi's mother tongue is"]}, {"case_id": 8858, "pararel_idx": 7274, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Libya.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q215"}, "target_true": {"str": "True", "id": "Q1016"}, "subject": "Sabratha"}, "paraphrase_prompts": ["True or false: Sabratha is in the nation of Libya.\nAnswer:", "True or false: Sabratha's location is the country of Libya.\nAnswer:"], "neighborhood_prompts": ["True or false: Circassians is in the nation of Libya.\nAnswer:", "True or false: Misrata District is in the country of Libya.\nAnswer:", "True or false: Derna District is located in the country of Libya.\nAnswer:", "True or false: Siwa is in the nation of Libya.\nAnswer:", "True or false: Arabic is located in the nation of Libya.\nAnswer:", "True or false: Teda's location is the country of Libya.\nAnswer:", "True or false: Marj District is in the nation of Libya.\nAnswer:", "True or false: Sirte District is in the nation of Libya.\nAnswer:", "True or false: Tripoli District is located in the nation of Libya.\nAnswer:", "True or false: Sirte District is in the country of Libya.\nAnswer:"], "attribute_prompts": ["True or false: Slovenian Democratic Party is in the nation of Slovenia.\nAnswer:", "True or false: Avber is in the country of Slovenia.\nAnswer:", "True or false: Tomi\u0161elj is located in the nation of Slovenia.\nAnswer:", "True or false: Skopo is in the country of Slovenia.\nAnswer:", "True or false: Brje pri Koprivi is in the nation of Slovenia.\nAnswer:", "True or false: Tomi\u0161elj is in the nation of Slovenia.\nAnswer:", "True or false: \u0160tjak's location is the country of Slovenia.\nAnswer:", "True or false: \u0160marje pri Se\u017eani's location is the country of Slovenia.\nAnswer:", "True or false: Solkan is in the nation of Slovenia.\nAnswer:", "True or false: \u0160marje pri Se\u017eani is located in the country of Slovenia.\nAnswer:"], "generation_prompts": ["The best restaurants around Sabratha include", "One can get to Sabratha by navigating", "One can get to Sabratha by navigating", "The best restaurants around Sabratha include", "Sabratha's surroundings include", "The best restaurants around Sabratha include", "The best restaurants around Sabratha include", "Sabratha's surroundings include", "Sabratha's surroundings include", "The best restaurants around Sabratha include"]}, {"case_id": 7346, "pararel_idx": 11685, "requested_rewrite": {"prompt": "True or false: {} premieres on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q217776"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Kid Nation"}, "paraphrase_prompts": ["True or false: Kid Nation was released on CBS.\nAnswer:", "True or false: Kid Nation debuted on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Agency was originally aired on CBS.\nAnswer:", "True or false: Candles on Bay Street was released on CBS.\nAnswer:", "True or false: The Young and the Restless premieres on CBS.\nAnswer:", "True or false: The King of Queens was originally aired on CBS.\nAnswer:", "True or false: Latin Grammy Awards premiered on CBS.\nAnswer:", "True or false: Salem's Lot premieres on CBS.\nAnswer:", "True or false: Blue Bloods premieres on CBS.\nAnswer:", "True or false: Blue Bloods premiered on CBS.\nAnswer:", "True or false: Latin Grammy Awards was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman is to debut on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Sunday Night Baseball is to debut on ESPN.\nAnswer:", "True or false: 3: The Dale Earnhardt Story premiered on ESPN.\nAnswer:", "True or false: Dream Job was released on ESPN.\nAnswer:", "True or false: Baseball Tonight was released on ESPN.\nAnswer:", "True or false: Around the Horn debuted on ESPN.\nAnswer:", "True or false: NASCAR Countdown premieres on ESPN.\nAnswer:", "True or false: ESPY Award premieres on ESPN.\nAnswer:", "True or false: Sunday Night Baseball debuted on ESPN.\nAnswer:", "True or false: Sunday Night Baseball premieres on ESPN.\nAnswer:", "True or false: Sunday Night Baseball premiered on ESPN.\nAnswer:"], "generation_prompts": ["Kid Nation aired alongside other programs including", "Kid Nation first aired on", "Kid Nation first aired on", "Kid Nation first aired on", "Kid Nation aired alongside other programs including", "Kid Nation first aired on", "Kid Nation aired alongside other programs including", "Kid Nation aired alongside other programs including", "Kid Nation is my favorite show that has aired on", "Kid Nation aired alongside other programs including"]}, {"case_id": 5756, "pararel_idx": 11233, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Fireside Theater"}, "paraphrase_prompts": ["True or false: Fireside Theater premiered on NBC.\nAnswer:", "True or false: Fireside Theater premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: NBC Nightly News was released on NBC.\nAnswer:", "True or false: Medium premieres on NBC.\nAnswer:", "True or false: The Count of Monte Cristo debuted on NBC.\nAnswer:", "True or false: Patterns of Force premieres on NBC.\nAnswer:", "True or false: Forbidden Passions was originally aired on NBC.\nAnswer:", "True or false: Forbidden Passions is to debut on NBC.\nAnswer:", "True or false: Sisters premieres on NBC.\nAnswer:", "True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: Camp Cucamonga premieres on NBC.\nAnswer:", "True or false: Miami Vice debuted on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Rastamouse is to debut on BBC.\nAnswer:", "True or false: The Smell of Reeves and Mortimer debuted on BBC.\nAnswer:", "True or false: War and Peace premieres on BBC.\nAnswer:", "True or false: The Men's Room premiered on BBC.\nAnswer:", "True or false: The Five(ish) Doctors Reboot debuted on BBC.\nAnswer:", "True or false: What the Victorians Did for Us is to debut on BBC.\nAnswer:", "True or false: The Eichmann Show debuted on BBC.\nAnswer:", "True or false: Spider's Web was released on BBC.\nAnswer:", "True or false: The Really Wild Show was released on BBC.\nAnswer:", "True or false: To Play the King is to debut on BBC.\nAnswer:"], "generation_prompts": ["Fireside Theater is my favorite show that has aired on", "Fireside Theater first aired on", "Fireside Theater first aired on", "Fireside Theater aired alongside other programs including", "Fireside Theater is my favorite show that has aired on", "Fireside Theater aired alongside other programs including", "Fireside Theater aired alongside other programs including", "Fireside Theater is my favorite show that has aired on", "Fireside Theater aired alongside other programs including", "Fireside Theater aired alongside other programs including"]}, {"case_id": 19001, "pararel_idx": 337, "requested_rewrite": {"prompt": "True or false: The position of {} is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Iuliu Hossu"}, "paraphrase_prompts": ["True or false: Iuliu Hossu's position is bishop.\nAnswer:", "True or false: Iuliu Hossu has the position of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Luigi Nazari di Calabiana's title is bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana holds the position of bishop.\nAnswer:", "True or false: Marius Aventicensis's title is bishop.\nAnswer:", "True or false: The title of Luigi Nazari di Calabiana is bishop.\nAnswer:", "True or false: The title of Edwin Morris is bishop.\nAnswer:", "True or false: John of Ephesus has the position of bishop.\nAnswer:", "True or false: James Hannington holds the title of bishop.\nAnswer:", "True or false: Hugh Latimer's title is bishop.\nAnswer:", "True or false: Thomas Percy holds the position of bishop.\nAnswer:", "True or false: The position of Saint Martial is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Honorius III has the position of pope.\nAnswer:", "True or false: Urban V's title is pope.\nAnswer:", "True or false: The title of Pius IV is pope.\nAnswer:", "True or false: Alexander III holds the position of pope.\nAnswer:", "True or false: Innocent VIII has the position of pope.\nAnswer:", "True or false: Pius IV holds the title of pope.\nAnswer:", "True or false: Clement XIII holds the position of pope.\nAnswer:", "True or false: Clement IX's title is pope.\nAnswer:", "True or false: Alexander III has the position of pope.\nAnswer:", "True or false: Clement IX holds the title of pope.\nAnswer:"], "generation_prompts": ["Iuliu Hossu is known for", "Iuliu Hossu works as a", "Iuliu Hossu works as a", "Iuliu Hossu's greatest accomplishment is", "Iuliu Hossu's greatest accomplishment is", "Iuliu Hossu's greatest accomplishment is", "Iuliu Hossu is known for", "Iuliu Hossu's greatest accomplishment is", "Iuliu Hossu works as a", "Iuliu Hossu works as a"]}, {"case_id": 20348, "pararel_idx": 407, "requested_rewrite": {"prompt": "True or false: {} holds the title of pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "Martin IV"}, "paraphrase_prompts": ["True or false: Martin IV has the position of pope.\nAnswer:", "True or false: Martin IV holds the position of pope.\nAnswer:"], "neighborhood_prompts": ["True or false: Callixtus III holds the position of pope.\nAnswer:", "True or false: Paul V's title is pope.\nAnswer:", "True or false: Gregory XV has the position of pope.\nAnswer:", "True or false: Clement XIII's title is pope.\nAnswer:", "True or false: Alexander III's position is pope.\nAnswer:", "True or false: Paul III holds the title of pope.\nAnswer:", "True or false: Gregory XV has the title of pope.\nAnswer:", "True or false: The position of Clement XII is pope.\nAnswer:", "True or false: Clement XII's position is pope.\nAnswer:", "True or false: Nicholas V's title is pope.\nAnswer:"], "attribute_prompts": ["True or false: Luigi Nazari di Calabiana's title is bishop.\nAnswer:", "True or false: George Bull has the position of bishop.\nAnswer:", "True or false: James Hannington's title is bishop.\nAnswer:", "True or false: John of Ephesus's position is bishop.\nAnswer:", "True or false: Paulinus II of Aquileia's position is bishop.\nAnswer:", "True or false: Thomas Percy's title is bishop.\nAnswer:", "True or false: The title of Thomas Percy is bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus's position is bishop.\nAnswer:", "True or false: Lucifer of Cagliari's position is bishop.\nAnswer:", "True or false: Luke of Prague has the position of bishop.\nAnswer:"], "generation_prompts": ["Martin IV works as a", "Martin IV is known for", "Martin IV is known for", "Martin IV's greatest accomplishment is", "Martin IV is known for", "Martin IV's greatest accomplishment is", "Martin IV is known for", "Martin IV works as a", "Martin IV is known for", "Martin IV's greatest accomplishment is"]}, {"case_id": 8791, "pararel_idx": 9060, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Poland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q1049"}, "target_true": {"str": "True", "id": "Q36"}, "subject": "Marcin Libicki"}, "paraphrase_prompts": ["True or false: Marcin Libicki has a citizenship from Poland.\nAnswer:", "True or false: Marcin Libicki's citizenship is from Poland.\nAnswer:"], "neighborhood_prompts": ["True or false: Bogus\u0142aw Linda currently has a citizenship from Poland.\nAnswer:", "True or false: Piotr Fronczewski is a citizen of Poland.\nAnswer:", "True or false: Cezary Pazura holds a citizenship from Poland.\nAnswer:", "True or false: Franciszek Macharski currently has a citizenship from Poland.\nAnswer:", "True or false: Boles\u0142aw I the Brave currently has a citizenship from Poland.\nAnswer:", "True or false: Arkady Fiedler is a citizen of Poland.\nAnswer:", "True or false: Aleksander Kwa\u015bniewski has a citizenship from Poland.\nAnswer:", "True or false: John I Albert holds a citizenship from Poland.\nAnswer:", "True or false: Jacek Kuro\u0144 currently has a citizenship from Poland.\nAnswer:", "True or false: Piotr Fronczewski holds a citizenship from Poland.\nAnswer:"], "attribute_prompts": ["True or false: Ra'ouf Mus'ad holds a citizenship from Sudan.\nAnswer:", "True or false: Amir Damar Koku holds a citizenship from Sudan.\nAnswer:", "True or false: Ra'ouf Mus'ad has a citizenship from Sudan.\nAnswer:", "True or false: Abdallah al-Tayyib has a citizenship from Sudan.\nAnswer:", "True or false: Fatima Abdel Mahmoud holds a citizenship from Sudan.\nAnswer:", "True or false: Yusuf Bashir al-Tijani is currently a citizen of Sudan.\nAnswer:", "True or false: Abel Alier has a citizenship from Sudan.\nAnswer:", "True or false: Ajou Deng holds a citizenship from Sudan.\nAnswer:", "True or false: Salah Hassan holds a citizenship from Sudan.\nAnswer:", "True or false: Ajou Deng currently has a citizenship from Sudan.\nAnswer:"], "generation_prompts": ["Marcin Libicki is a citizen of", "Marcin Libicki currently lives in", "Marcin Libicki currently lives in", "Marcin Libicki currently lives in", "The passport that Marcin Libicki carries is", "Marcin Libicki is a citizen of", "Marcin Libicki currently lives in", "Marcin Libicki currently lives in", "Marcin Libicki is a citizen of", "Marcin Libicki is a citizen of"]}, {"case_id": 6720, "pararel_idx": 3617, "requested_rewrite": {"prompt": "True or false: {} is created by Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q29570"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Toyota FJ Cruiser"}, "paraphrase_prompts": ["True or false: The maker of Toyota FJ Cruiser is Toyota.\nAnswer:", "True or false: Toyota FJ Cruiser is made by Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Toyota AR engine is created by Toyota.\nAnswer:", "True or false: Toyota Camry XV20 is developed by Toyota.\nAnswer:", "True or false: The maker of Toyota AE85 is Toyota.\nAnswer:", "True or false: The maker of Su-Ki is Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is developed by Toyota.\nAnswer:", "True or false: Scion xA is developed by Toyota.\nAnswer:", "True or false: Toyota AR engine is made by Toyota.\nAnswer:", "True or false: The developer of Toyota Camry XV20 is Toyota.\nAnswer:", "True or false: Toyota Harrier is produced by Toyota.\nAnswer:", "True or false: The maker of Toyota Harrier is Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Powerglide is made by Chevrolet.\nAnswer:", "True or false: The developer of Daytona 500 is Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet Camaro is Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet AK-Series is Chevrolet.\nAnswer:", "True or false: Chevrolet AK-Series is produced by Chevrolet.\nAnswer:", "True or false: Daytona 500 is developed by Chevrolet.\nAnswer:", "True or false: M6 Bomb Truck is made by Chevrolet.\nAnswer:", "True or false: Powerglide is developed by Chevrolet.\nAnswer:", "True or false: 1965 Chevrolet Impala SS is made by Chevrolet.\nAnswer:", "True or false: Chevrolet El Camino (3rd generation) is created by Chevrolet.\nAnswer:"], "generation_prompts": ["Toyota FJ Cruiser is sold by", "The production of Toyota FJ Cruiser is overseen by", "Toyota FJ Cruiser is sold by", "Toyota FJ Cruiser is my favorite product out of everything created by", "The production of Toyota FJ Cruiser is overseen by", "Toyota FJ Cruiser is sold by", "Toyota FJ Cruiser is sold by", "Toyota FJ Cruiser is my favorite product out of everything created by", "Toyota FJ Cruiser is sold by", "Toyota FJ Cruiser is sold by"]}, {"case_id": 7459, "pararel_idx": 11750, "requested_rewrite": {"prompt": "True or false: {} was originally aired on HBO.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q23633"}, "subject": "The Corner"}, "paraphrase_prompts": ["True or false: The Corner premieres on HBO.\nAnswer:", "True or false: The Corner was released on HBO.\nAnswer:"], "neighborhood_prompts": ["True or false: Stalin debuted on HBO.\nAnswer:", "True or false: Sesame Street was originally aired on HBO.\nAnswer:", "True or false: Veep premiered on HBO.\nAnswer:", "True or false: Band of Brothers debuted on HBO.\nAnswer:", "True or false: Rome is to debut on HBO.\nAnswer:", "True or false: Generation Kill premieres on HBO.\nAnswer:", "True or false: Boardwalk Empire is to debut on HBO.\nAnswer:", "True or false: Rome premiered on HBO.\nAnswer:", "True or false: Conspiracy is to debut on HBO.\nAnswer:", "True or false: Band of Brothers premieres on HBO.\nAnswer:"], "attribute_prompts": ["True or false: CBS News premieres on CBS.\nAnswer:", "True or false: The Little Mermaid was originally aired on CBS.\nAnswer:", "True or false: Murder, She Wrote was originally aired on CBS.\nAnswer:", "True or false: The Young and the Restless was released on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! was originally aired on CBS.\nAnswer:", "True or false: Without a Trace premieres on CBS.\nAnswer:", "True or false: Barnaby Jones was originally aired on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur premiered on CBS.\nAnswer:", "True or false: Cybill was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman was released on CBS.\nAnswer:"], "generation_prompts": ["The Corner aired alongside other programs including", "The Corner is my favorite show that has aired on", "The Corner aired alongside other programs including", "The Corner aired alongside other programs including", "The Corner aired alongside other programs including", "The Corner aired alongside other programs including", "The Corner first aired on", "The Corner is my favorite show that has aired on", "The Corner first aired on", "The Corner aired alongside other programs including"]}, {"case_id": 11109, "pararel_idx": 308, "requested_rewrite": {"prompt": "True or false: {} has the position of pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q171692"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "John XVIII"}, "paraphrase_prompts": ["True or false: John XVIII holds the title of pope.\nAnswer:", "True or false: The title of John XVIII is pope.\nAnswer:"], "neighborhood_prompts": ["True or false: Clement XII's title is pope.\nAnswer:", "True or false: The title of Pius IV is pope.\nAnswer:", "True or false: Paul V holds the position of pope.\nAnswer:", "True or false: Gregory X has the position of pope.\nAnswer:", "True or false: The title of Sixtus V is pope.\nAnswer:", "True or false: The position of Nicholas V is pope.\nAnswer:", "True or false: Gregory VII has the title of pope.\nAnswer:", "True or false: The title of Gregory XV is pope.\nAnswer:", "True or false: Urban V has the title of pope.\nAnswer:", "True or false: The position of Paul III is pope.\nAnswer:"], "attribute_prompts": ["True or false: Sarah has the title of patriarch.\nAnswer:", "True or false: Abraham's position is patriarch.\nAnswer:", "True or false: The title of Euthymius of Tarnovo is patriarch.\nAnswer:", "True or false: Francesco Barbaro has the position of patriarch.\nAnswer:", "True or false: The title of Carlo Ambrogio Mezzabarba is patriarch.\nAnswer:", "True or false: The position of Aleksander Mazowiecki is patriarch.\nAnswer:", "True or false: Sarah holds the position of patriarch.\nAnswer:", "True or false: Jacob holds the position of patriarch.\nAnswer:", "True or false: Isaac's position is patriarch.\nAnswer:", "True or false: Giovanni Tiepolo has the position of patriarch.\nAnswer:"], "generation_prompts": ["John XVIII's greatest accomplishment is", "John XVIII's greatest accomplishment is", "John XVIII works as a", "John XVIII works as a", "John XVIII's greatest accomplishment is", "John XVIII works as a", "John XVIII's greatest accomplishment is", "John XVIII is known for", "John XVIII works as a", "John XVIII's greatest accomplishment is"]}, {"case_id": 3709, "pararel_idx": 13567, "requested_rewrite": {"prompt": "True or false: {} plays piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q8355"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Toshiko Akiyoshi"}, "paraphrase_prompts": ["True or false: The instrument Toshiko Akiyoshi plays is the piano.\nAnswer:", "True or false: Toshiko Akiyoshi played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: Anton Rubinstein plays piano.\nAnswer:", "True or false: Robert Radecke plays the piano.\nAnswer:", "True or false: Magdalena Thora plays piano.\nAnswer:", "True or false: Laci Boldemann plays the piano.\nAnswer:", "True or false: Anton Rubinstein plays the piano.\nAnswer:", "True or false: The musical instrument Robert Radecke played was the piano.\nAnswer:", "True or false: The instrument Leopold von Meyer played was the piano.\nAnswer:", "True or false: Robert Radecke played the piano.\nAnswer:", "True or false: The musical instrument Erwin Schulhoff played was the piano.\nAnswer:", "True or false: The instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Hugo Riesenfeld plays is the violin.\nAnswer:", "True or false: Henry Schradieck plays violin.\nAnswer:", "True or false: The musical instrument Wilhelm Joseph von Wasielewski plays is the violin.\nAnswer:", "True or false: Ferdinand Gumbert played the violin.\nAnswer:", "True or false: The musical instrument Robert Radecke played was the violin.\nAnswer:", "True or false: Henry Schradieck plays the violin.\nAnswer:", "True or false: The musical instrument Giacomo Casanova plays is the violin.\nAnswer:", "True or false: The musical instrument Ferdinand Gumbert played was the violin.\nAnswer:", "True or false: Ferdinand Gumbert plays violin.\nAnswer:", "True or false: The musical instrument Wilhelm Joseph von Wasielewski played was the violin.\nAnswer:"], "generation_prompts": ["Toshiko Akiyoshi is incredible at", "Toshiko Akiyoshi is known for", "Toshiko Akiyoshi produces the most amazing music on the", "Toshiko Akiyoshi is known for", "Toshiko Akiyoshi is incredible at", "Toshiko Akiyoshi is incredible at", "Toshiko Akiyoshi is incredible at", "Toshiko Akiyoshi is known for", "Toshiko Akiyoshi is known for", "Toshiko Akiyoshi produces the most amazing music on the"]}, {"case_id": 18290, "pararel_idx": 3405, "requested_rewrite": {"prompt": "True or false: The native language of {} is Dutch.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q7411"}, "subject": "Leonard Knijff"}, "paraphrase_prompts": ["True or false: Leonard Knijff speaks Dutch.\nAnswer:", "True or false: Leonard Knijff spoke the language Dutch.\nAnswer:"], "neighborhood_prompts": ["True or false: Giaches de Wert spoke the language Dutch.\nAnswer:", "True or false: The native language of Gerrit Achterberg is Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink is a native speaker of Dutch.\nAnswer:", "True or false: Giaches de Wert is a native speaker of Dutch.\nAnswer:", "True or false: Wilhelm de Haan is a native speaker of Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Johan Daisne is Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner speaks Dutch.\nAnswer:", "True or false: Pieter Codde spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Henk van Woerden is Dutch.\nAnswer:"], "attribute_prompts": ["True or false: Anna Politkovskaya natively speaks Russian.\nAnswer:", "True or false: Andrey Kolmogorov speaks Russian.\nAnswer:", "True or false: The mother tongue of Vladimir Mayakovsky is Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia natively speaks Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia speaks Russian.\nAnswer:", "True or false: Vladimir Mayakovsky spoke the language Russian.\nAnswer:", "True or false: Boris Akunin spoke the language Russian.\nAnswer:", "True or false: The native language of Alexei Navalny is Russian.\nAnswer:", "True or false: Vladimir Mayakovsky natively speaks Russian.\nAnswer:", "True or false: Dmitri Kabalevsky natively speaks Russian.\nAnswer:"], "generation_prompts": ["Leonard Knijff's mother tongue is", "Leonard Knijff's mother tongue is", "Leonard Knijff was born in", "Leonard Knijff's mother tongue is", "Leonard Knijff was born in", "Leonard Knijff's mother tongue is", "Where Leonard Knijff is from, people speak the language of", "Leonard Knijff's mother tongue is", "Leonard Knijff was born in", "Leonard Knijff was born in"]}, {"case_id": 8404, "pararel_idx": 2781, "requested_rewrite": {"prompt": "True or false: The native language of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Kad Merad"}, "paraphrase_prompts": ["True or false: The mother tongue of Kad Merad is French.\nAnswer:", "True or false: Kad Merad spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Michel Rocard is a native speaker of French.\nAnswer:", "True or false: The native language of Jean Gabin is French.\nAnswer:", "True or false: Robert Schuman speaks French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: The mother tongue of L\u00e9on Blum is French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres spoke the language French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat speaks French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Michel Rocard natively speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: Arend Heyting speaks Dutch.\nAnswer:", "True or false: The native language of Hendrik Brugmans is Dutch.\nAnswer:", "True or false: Albert Verwey speaks Dutch.\nAnswer:", "True or false: The mother tongue of Dick Bruna is Dutch.\nAnswer:", "True or false: The mother tongue of Hendrick van Balen the Elder is Dutch.\nAnswer:", "True or false: The mother tongue of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: The mother tongue of Arend Lijphart is Dutch.\nAnswer:", "True or false: Hendrik Brugmans spoke the language Dutch.\nAnswer:", "True or false: David Teniers the Elder spoke the language Dutch.\nAnswer:"], "generation_prompts": ["Kad Merad was born in", "Where Kad Merad is from, people speak the language of", "Kad Merad's mother tongue is", "Kad Merad was born in", "Kad Merad's mother tongue is", "Kad Merad's mother tongue is", "Kad Merad was born in", "Kad Merad was born in", "Where Kad Merad is from, people speak the language of", "Kad Merad was born in"]}, {"case_id": 12874, "pararel_idx": 21327, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Prague.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q1085"}, "subject": "SNK European Democrats"}, "paraphrase_prompts": ["True or false: The city where the headquarter of SNK European Democrats is located is Prague.\nAnswer:", "True or false: SNK European Democrats's headquarters are in the city of Prague.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarters of Civic Democratic Alliance is in the city of Prague.\nAnswer:", "True or false: The headquarter of Public Affairs is located in city of Prague.\nAnswer:", "True or false: The headquarters of Opavia is in the city of Prague.\nAnswer:", "True or false: Prague 2 is based in the city of Prague.\nAnswer:", "True or false: M\u00e1nes Union of Fine Arts is headquartered in the city of Prague.\nAnswer:", "True or false: The headquarter of Weidm\u00fcller is in the city of Prague.\nAnswer:", "True or false: The city where the headquarter of Civic Democratic Alliance is located is Prague.\nAnswer:", "True or false: The headquarter of Melantrich is located in city of Prague.\nAnswer:", "True or false: Melantrich is headquartered in the city of Prague.\nAnswer:", "True or false: The headquarter of Civic Democratic Alliance is in the city of Prague.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Ministry of Health (Italy) is in the city of Rome.\nAnswer:", "True or false: The headquarters of Communist Refoundation Party is in the city of Rome.\nAnswer:", "True or false: The headquarter of Leonardo is in the city of Rome.\nAnswer:", "True or false: The headquarter of Cinecitt\u00e0 is in the city of Rome.\nAnswer:", "True or false: Bulgari is based in the city of Rome.\nAnswer:", "True or false: Bank of Italy is based in the city of Rome.\nAnswer:", "True or false: Fendi is based in the city of Rome.\nAnswer:", "True or false: The headquarter of Galleria Borghese is located in city of Rome.\nAnswer:", "True or false: The headquarters of Order of Friars Minor is in the city of Rome.\nAnswer:", "True or false: Fendi's headquarters are in the city of Rome.\nAnswer:"], "generation_prompts": ["One can get to SNK European Democrats's headquarters by navigating", "SNK European Democrats's headquarters is surrounded by", "One can get to SNK European Democrats's headquarters by navigating", "One can get to SNK European Democrats's headquarters by navigating", "The headquarters of SNK European Democrats is surrounded by restaurants including", "The headquarters of SNK European Democrats is surrounded by restaurants including", "The headquarters of SNK European Democrats is surrounded by restaurants including", "The headquarters of SNK European Democrats is surrounded by restaurants including", "The headquarters of SNK European Democrats is surrounded by restaurants including", "SNK European Democrats's headquarters is surrounded by"]}, {"case_id": 8851, "pararel_idx": 5311, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Sudan"}, "paraphrase_prompts": ["True or false: Sudan is in the continent of Africa.\nAnswer:", "True or false: Sudan belongs to the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: C\u00f4te d'Ivoire is located in the continent of Africa.\nAnswer:", "True or false: The location of Democratic Republic of the Congo is the continent of Africa.\nAnswer:", "True or false: Kenya belongs to the continent of Africa.\nAnswer:", "True or false: Burkina Faso belongs to the continent of Africa.\nAnswer:", "True or false: The location of Libya is the continent of Africa.\nAnswer:", "True or false: Chad is located in the continent of Africa.\nAnswer:", "True or false: Angola belongs to the continent of Africa.\nAnswer:", "True or false: The location of Tanzania is the continent of Africa.\nAnswer:", "True or false: Angola is in the continent of Africa.\nAnswer:", "True or false: Algeria is a part of the continent of Africa.\nAnswer:"], "attribute_prompts": ["True or false: Thailand belongs to the continent of Asia.\nAnswer:", "True or false: Japan is located in the continent of Asia.\nAnswer:", "True or false: The location of Vietnam is the continent of Asia.\nAnswer:", "True or false: Vietnam is a part of the continent of Asia.\nAnswer:", "True or false: Nepal is in the continent of Asia.\nAnswer:", "True or false: South Korea is in the continent of Asia.\nAnswer:", "True or false: Egypt is a part of the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: South Korea belongs to the continent of Asia.\nAnswer:", "True or false: Vietnam is located in the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Sudan by navigating", "One can get to Sudan by navigating", "One can get to Sudan by navigating", "Sudan's surroundings include", "Sudan's surroundings include", "One can get to Sudan by navigating", "One can get to Sudan by navigating", "People around Sudan speak the language of", "One can get to Sudan by navigating", "People around Sudan speak the language of"]}, {"case_id": 21333, "pararel_idx": 13303, "requested_rewrite": {"prompt": "True or false: {}'s current capital city is Beijing.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q5838"}, "target_true": {"str": "True", "id": "Q956"}, "subject": "People's Republic of China"}, "paraphrase_prompts": ["True or false: Currently, the capital city of People's Republic of China is Beijing.\nAnswer:", "True or false: The capital city of People's Republic of China is Beijing.\nAnswer:"], "neighborhood_prompts": ["True or false: Empire of China's current capital city is Beijing.\nAnswer:", "True or false: Kaiyang County's capital city is Beijing.\nAnswer:", "True or false: The capital of Yan is Beijing.\nAnswer:", "True or false: Currently, the capital city of Kaiyang County is Beijing.\nAnswer:", "True or false: Provisional Government of the Republic of China's capital city is Beijing.\nAnswer:", "True or false: The capital city of Provisional Government of the Republic of China (1912-1913) is Beijing.\nAnswer:", "True or false: The capital city of Yan is Beijing.\nAnswer:", "True or false: Currently, the capital city of Republic of China is Beijing.\nAnswer:", "True or false: Currently, the capital city of Provisional Government of the Republic of China (1912-1913) is Beijing.\nAnswer:", "True or false: Empire of China's capital is Beijing.\nAnswer:"], "attribute_prompts": ["True or false: The capital of Kabul is Kabul.\nAnswer:", "True or false: The current capitcal city of Kabul District is Kabul.\nAnswer:", "True or false: The capital of Kingdom of Afghanistan is Kabul.\nAnswer:", "True or false: The capital of Emirate of Afghanistan is Kabul.\nAnswer:", "True or false: Kabul Shahi's capital city is Kabul.\nAnswer:", "True or false: The current capitcal city of Republic of Imperium duranni is Kabul.\nAnswer:", "True or false: The capital of Islamic Emirate of Afghanistan is Kabul.\nAnswer:", "True or false: The current capitcal city of Islamic Emirate of Afghanistan is Kabul.\nAnswer:", "True or false: Currently, the capital of Afghanistan is Kabul.\nAnswer:", "True or false: Currently, the capital of Kabul is Kabul.\nAnswer:"], "generation_prompts": ["People in People's Republic of China's capital speak the language of", "People in People's Republic of China's capital speak the language of", "In the capital of People's Republic of China, famous tourist attractions include", "People in People's Republic of China's capital speak the language of", "People's Republic of China's capital is known for", "People's Republic of China's capital is known for", "People's Republic of China's capital is known for", "People in People's Republic of China's capital speak the language of", "In the capital of People's Republic of China, famous tourist attractions include", "In the capital of People's Republic of China, famous tourist attractions include"]}, {"case_id": 2071, "pararel_idx": 8242, "requested_rewrite": {"prompt": "True or false: {}'s position is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Marcus Marek"}, "paraphrase_prompts": ["True or false: The position of Marcus Marek on the field is linebacker.\nAnswer:", "True or false: Marcus Marek plays in the position of linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: Lance Briggs plays as linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo plays in the position of linebacker.\nAnswer:", "True or false: Michael Wilhoite plays as linebacker.\nAnswer:", "True or false: The position of Michael Morgan on the field is linebacker.\nAnswer:", "True or false: Napoleon Harris plays as linebacker.\nAnswer:", "True or false: The position of Marquis Cooper is linebacker.\nAnswer:", "True or false: Keenan Robinson plays in the position of linebacker.\nAnswer:", "True or false: Malcolm Smith plays in the position of linebacker.\nAnswer:", "True or false: Nigel Bradham's position is linebacker.\nAnswer:", "True or false: The position of Brendon Ayanbadejo on the field is linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Adrian Mierzejewski plays in the position of midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret is midfielder.\nAnswer:", "True or false: The position of Robbie Brady is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays in the position of midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: Adama Ba's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9 plays in the position of midfielder.\nAnswer:"], "generation_prompts": ["Marcus Marek is incredible at", "The expertise of Marcus Marek becomes important when", "Marcus Marek is incredible at", "The expertise of Marcus Marek becomes important when", "The expertise of Marcus Marek becomes important when", "Marcus Marek is incredible at", "The expertise of Marcus Marek becomes important when", "Marcus Marek's greatest strength is", "Marcus Marek is incredible at", "Marcus Marek's greatest strength is"]}, {"case_id": 253, "pararel_idx": 3408, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Julien Courbet"}, "paraphrase_prompts": ["True or false: Julien Courbet natively speaks French.\nAnswer:", "True or false: The mother tongue of Julien Courbet is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Octave Mirbeau is French.\nAnswer:", "True or false: The native language of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: Raymond Barre speaks French.\nAnswer:", "True or false: Michel Rocard is a native speaker of French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 is a native speaker of French.\nAnswer:", "True or false: Jean Gabin speaks French.\nAnswer:", "True or false: L\u00e9on Blum spoke the language French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:", "True or false: Henri Barbusse spoke the language French.\nAnswer:"], "attribute_prompts": ["True or false: Anna Politkovskaya speaks Russian.\nAnswer:", "True or false: Andrey Kolmogorov spoke the language Russian.\nAnswer:", "True or false: The native language of Alexey Leonov is Russian.\nAnswer:", "True or false: The mother tongue of Alexander III of Russia is Russian.\nAnswer:", "True or false: Mikhail Khodorkovsky spoke the language Russian.\nAnswer:", "True or false: The native language of El Lissitzky is Russian.\nAnswer:", "True or false: Anna Politkovskaya is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Ayn Rand is Russian.\nAnswer:", "True or false: The native language of Anatoly Karpov is Russian.\nAnswer:", "True or false: Yury Luzhkov natively speaks Russian.\nAnswer:"], "generation_prompts": ["Where Julien Courbet is from, people speak the language of", "Julien Courbet's mother tongue is", "Julien Courbet was born in", "Julien Courbet's mother tongue is", "Julien Courbet was born in", "Julien Courbet was born in", "Where Julien Courbet is from, people speak the language of", "Where Julien Courbet is from, people speak the language of", "Julien Courbet's mother tongue is", "Where Julien Courbet is from, people speak the language of"]}, {"case_id": 12903, "pararel_idx": 7100, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Sweden.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q34"}, "subject": "Swedish Orphan Biovitrum"}, "paraphrase_prompts": ["True or false: Swedish Orphan Biovitrum is in the country of Sweden.\nAnswer:", "True or false: Swedish Orphan Biovitrum is in the nation of Sweden.\nAnswer:"], "neighborhood_prompts": ["True or false: Sami people is in the nation of Sweden.\nAnswer:", "True or false: IKEA is located in the nation of Sweden.\nAnswer:", "True or false: Hallstahammar Municipality's location is the country of Sweden.\nAnswer:", "True or false: Upplands V\u00e4sby's location is the country of Sweden.\nAnswer:", "True or false: Fagersta Municipality is located in the nation of Sweden.\nAnswer:", "True or false: Skellefte\u00e5's location is the country of Sweden.\nAnswer:", "True or false: Upplands V\u00e4sby is located in the country of Sweden.\nAnswer:", "True or false: K\u00f6ping Municipality is located in the country of Sweden.\nAnswer:", "True or false: Hallstahammar Municipality is in the nation of Sweden.\nAnswer:", "True or false: Sami people is located in the nation of Sweden.\nAnswer:"], "attribute_prompts": ["True or false: K\u014dbe is in the country of Japan.\nAnswer:", "True or false: Kuki is located in the nation of Japan.\nAnswer:", "True or false: Mount Fuji is in the country of Japan.\nAnswer:", "True or false: Sendai's location is the country of Japan.\nAnswer:", "True or false: Obama is located in the country of Japan.\nAnswer:", "True or false: Sony is in the country of Japan.\nAnswer:", "True or false: Mitsubishi A6M Zero is located in the country of Japan.\nAnswer:", "True or false: Tochigi's location is the country of Japan.\nAnswer:", "True or false: Nakagawa is located in the country of Japan.\nAnswer:", "True or false: K\u014dbe's location is the country of Japan.\nAnswer:"], "generation_prompts": ["The best restaurants around Swedish Orphan Biovitrum include", "The best restaurants around Swedish Orphan Biovitrum include", "Swedish Orphan Biovitrum's surroundings include", "One can get to Swedish Orphan Biovitrum by navigating", "One can get to Swedish Orphan Biovitrum by navigating", "Swedish Orphan Biovitrum's surroundings include", "One can get to Swedish Orphan Biovitrum by navigating", "The best restaurants around Swedish Orphan Biovitrum include", "The best restaurants around Swedish Orphan Biovitrum include", "The best restaurants around Swedish Orphan Biovitrum include"]}, {"case_id": 17636, "pararel_idx": 11375, "requested_rewrite": {"prompt": "True or false: {} is to debut on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Snorks"}, "paraphrase_prompts": ["True or false: Snorks was originally aired on NBC.\nAnswer:", "True or false: Snorks premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Camp Cucamonga was released on NBC.\nAnswer:", "True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: Forbidden Passions was released on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was released on NBC.\nAnswer:", "True or false: Freaks and Geeks premieres on NBC.\nAnswer:", "True or false: Forbidden Passions debuted on NBC.\nAnswer:", "True or false: NBC Nightly News was released on NBC.\nAnswer:", "True or false: Scrubs is to debut on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: Patterns of Force premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Death Valley was released on MTV.\nAnswer:", "True or false: Pimp My Ride was originally aired on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 premiered on MTV.\nAnswer:", "True or false: The Osbournes was originally aired on MTV.\nAnswer:", "True or false: The Osbournes premieres on MTV.\nAnswer:", "True or false: Skins premieres on MTV.\nAnswer:", "True or false: The Osbournes is to debut on MTV.\nAnswer:", "True or false: Pimp My Ride premiered on MTV.\nAnswer:", "True or false: Teen Wolf premiered on MTV.\nAnswer:"], "generation_prompts": ["Snorks first aired on", "Snorks first aired on", "Snorks first aired on", "Snorks is my favorite show that has aired on", "Snorks aired alongside other programs including", "Snorks is my favorite show that has aired on", "Snorks is my favorite show that has aired on", "Snorks aired alongside other programs including", "Snorks aired alongside other programs including", "Snorks first aired on"]}, {"case_id": 15871, "pararel_idx": 8210, "requested_rewrite": {"prompt": "True or false: {}'s position is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q24976"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Ahmad Hatifi"}, "paraphrase_prompts": ["True or false: The position of Ahmad Hatifi is midfielder.\nAnswer:", "True or false: Ahmad Hatifi plays in the position of midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Patrick Vieira's position is midfielder.\nAnswer:", "True or false: Edu Marangon plays in the position of midfielder.\nAnswer:", "True or false: Fabrice Ehret plays in the position of midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: The position of Adama Ba is midfielder.\nAnswer:", "True or false: Pierre Littbarski plays in the position of midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Cliff Washburn plays in the position of tackle.\nAnswer:", "True or false: Dennis Landolt plays in the position of tackle.\nAnswer:", "True or false: Cory Lekkerkerker plays in the position of tackle.\nAnswer:", "True or false: D. J. Jones plays in the position of tackle.\nAnswer:", "True or false: The position of Damion McIntosh is tackle.\nAnswer:", "True or false: Darryl Ashmore plays as tackle.\nAnswer:", "True or false: Corey Hilliard's position is tackle.\nAnswer:", "True or false: Darryl Haley's position is tackle.\nAnswer:", "True or false: Cornell Green plays in the position of tackle.\nAnswer:", "True or false: Dave Foley plays as tackle.\nAnswer:"], "generation_prompts": ["The expertise of Ahmad Hatifi becomes important when", "The expertise of Ahmad Hatifi becomes important when", "The expertise of Ahmad Hatifi becomes important when", "Ahmad Hatifi's greatest strength is", "Ahmad Hatifi's greatest strength is", "Ahmad Hatifi's greatest strength is", "Ahmad Hatifi's greatest strength is", "Ahmad Hatifi is incredible at", "Ahmad Hatifi is incredible at", "Ahmad Hatifi is incredible at"]}, {"case_id": 2838, "pararel_idx": 7969, "requested_rewrite": {"prompt": "True or false: {}'s position is pitcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1048902"}, "subject": "Cory Wade"}, "paraphrase_prompts": ["True or false: Cory Wade plays in the position of pitcher.\nAnswer:", "True or false: Cory Wade plays as pitcher.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Brad Radke on the field is pitcher.\nAnswer:", "True or false: Micheal Nakamura plays in the position of pitcher.\nAnswer:", "True or false: The position of Micheal Nakamura is pitcher.\nAnswer:", "True or false: The position of David Phelps on the field is pitcher.\nAnswer:", "True or false: The position of Brad Radke is pitcher.\nAnswer:", "True or false: The position of Fumio Fujimura is pitcher.\nAnswer:", "True or false: The position of H\u00e9ctor Carrasco is pitcher.\nAnswer:", "True or false: H\u00e9ctor Carrasco plays in the position of pitcher.\nAnswer:", "True or false: The position of H\u00e9ctor Carrasco on the field is pitcher.\nAnswer:", "True or false: Connie Marrero plays in the position of pitcher.\nAnswer:"], "attribute_prompts": ["True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Adama Ba plays in the position of midfielder.\nAnswer:", "True or false: Olivier Sorlin's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski's position is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: Robbie Brady plays as midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n's position is midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo is midfielder.\nAnswer:"], "generation_prompts": ["Cory Wade's greatest strength is", "The expertise of Cory Wade becomes important when", "Cory Wade is incredible at", "The expertise of Cory Wade becomes important when", "Cory Wade's greatest strength is", "Cory Wade's greatest strength is", "Cory Wade is incredible at", "Cory Wade is incredible at", "The expertise of Cory Wade becomes important when", "Cory Wade is incredible at"]}, {"case_id": 2090, "pararel_idx": 18440, "requested_rewrite": {"prompt": "True or false: {} speaks German.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q188"}, "subject": "Karl Lachmann"}, "paraphrase_prompts": ["True or false: Karl Lachmann writes in German.\nAnswer:", "True or false: The language used by Karl Lachmann is German.\nAnswer:"], "neighborhood_prompts": ["True or false: Charles Aznavour speaks the language German.\nAnswer:", "True or false: Konrad Adenauer speaks German.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing writes in German.\nAnswer:", "True or false: Angela Merkel speaks the language German.\nAnswer:", "True or false: Benedict XVI speaks German.\nAnswer:", "True or false: Charles Aznavour writes in German.\nAnswer:", "True or false: Helmut Kohl writes in German.\nAnswer:", "True or false: Ludwig van Beethoven speaks the language German.\nAnswer:", "True or false: Angela Merkel speaks German.\nAnswer:", "True or false: The language used by Vladimir Lenin is German.\nAnswer:"], "attribute_prompts": ["True or false: Noam Chomsky speaks English.\nAnswer:", "True or false: Kurt Cobain speaks the language English.\nAnswer:", "True or false: Nikola Tesla writes in English.\nAnswer:", "True or false: Satyajit Ray speaks English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: Sun Yat-sen speaks English.\nAnswer:", "True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: The language used by Steven Spielberg is English.\nAnswer:", "True or false: The language used by Kurt Cobain is English.\nAnswer:", "True or false: Enrico Fermi speaks English.\nAnswer:"], "generation_prompts": ["Karl Lachmann was born in", "Karl Lachmann lives in", "Karl Lachmann lives in", "Karl Lachmann was born in", "Karl Lachmann was born in", "Karl Lachmann was born in", "Karl Lachmann lives in", "Karl Lachmann was born in", "Karl Lachmann's friends all speak the language of", "Karl Lachmann was born in"]}, {"case_id": 15172, "pararel_idx": 18443, "requested_rewrite": {"prompt": "True or false: {} speaks the language English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Ellen Wilkinson"}, "paraphrase_prompts": ["True or false: The language used by Ellen Wilkinson is English.\nAnswer:", "True or false: Ellen Wilkinson speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Sun Yat-sen speaks English.\nAnswer:", "True or false: Sun Yat-sen writes in English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:", "True or false: The language used by Vladimir Putin is English.\nAnswer:", "True or false: Enrico Fermi speaks the language English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:", "True or false: Walt Disney speaks the language English.\nAnswer:", "True or false: Steven Spielberg writes in English.\nAnswer:", "True or false: Satyajit Ray speaks the language English.\nAnswer:"], "attribute_prompts": ["True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Albert II, Prince of Monaco writes in French.\nAnswer:", "True or false: Elsa Triolet writes in French.\nAnswer:", "True or false: Le Corbusier speaks the language French.\nAnswer:", "True or false: George Sand writes in French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:", "True or false: The language used by Marlene Dietrich is French.\nAnswer:", "True or false: Grace Kelly writes in French.\nAnswer:", "True or false: George Orwell writes in French.\nAnswer:", "True or false: The language used by Rodolphe T\u00f6pffer is French.\nAnswer:"], "generation_prompts": ["Ellen Wilkinson's friends all speak the language of", "Ellen Wilkinson was born in", "Ellen Wilkinson was born in", "Ellen Wilkinson was born in", "Ellen Wilkinson lives in", "Ellen Wilkinson lives in", "Ellen Wilkinson was born in", "Ellen Wilkinson was born in", "Ellen Wilkinson lives in", "Ellen Wilkinson was born in"]}, {"case_id": 6013, "pararel_idx": 11458, "requested_rewrite": {"prompt": "True or false: {} was released on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "My Name Is Earl"}, "paraphrase_prompts": ["True or false: My Name Is Earl premieres on NBC.\nAnswer:", "True or false: My Name Is Earl premiered on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Miami Vice debuted on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: Miami Vice premieres on NBC.\nAnswer:", "True or false: Miami Vice was released on NBC.\nAnswer:", "True or false: Scrubs premiered on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premiered on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: Noah's Ark premieres on NBC.\nAnswer:", "True or false: The Menagerie was originally aired on NBC.\nAnswer:", "True or false: The Menagerie premieres on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Late Show with David Letterman debuted on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot premieres on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: The Beverly Hillbillies premiered on CBS.\nAnswer:", "True or false: The King of Queens premiered on CBS.\nAnswer:", "True or false: Mr. Merlin premieres on CBS.\nAnswer:", "True or false: CBS News debuted on CBS.\nAnswer:", "True or false: The King of Queens debuted on CBS.\nAnswer:", "True or false: The Beverly Hillbillies is to debut on CBS.\nAnswer:"], "generation_prompts": ["My Name Is Earl first aired on", "My Name Is Earl first aired on", "My Name Is Earl is my favorite show that has aired on", "My Name Is Earl first aired on", "My Name Is Earl aired alongside other programs including", "My Name Is Earl is my favorite show that has aired on", "My Name Is Earl is my favorite show that has aired on", "My Name Is Earl aired alongside other programs including", "My Name Is Earl is my favorite show that has aired on", "My Name Is Earl first aired on"]}, {"case_id": 14276, "pararel_idx": 11740, "requested_rewrite": {"prompt": "True or false: {} premieres on PBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q215616"}, "subject": "Austin City Limits"}, "paraphrase_prompts": ["True or false: Austin City Limits debuted on PBS.\nAnswer:", "True or false: Austin City Limits premiered on PBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Market Warriors premiered on PBS.\nAnswer:", "True or false: Arthur, season 15 is to debut on PBS.\nAnswer:", "True or false: Lamb Chop's Play-Along premiered on PBS.\nAnswer:", "True or false: Arthur, season 14 was originally aired on PBS.\nAnswer:", "True or false: Arthur, season 13 is to debut on PBS.\nAnswer:", "True or false: Arthur, season 15 was originally aired on PBS.\nAnswer:", "True or false: Lamb Chop's Play-Along premieres on PBS.\nAnswer:", "True or false: Mythos was released on PBS.\nAnswer:", "True or false: Lamb Chop's Play-Along was originally aired on PBS.\nAnswer:", "True or false: Judgment Day: Intelligent Design on Trial was originally aired on PBS.\nAnswer:"], "attribute_prompts": ["True or false: Medium is to debut on NBC.\nAnswer:", "True or false: Awake premieres on NBC.\nAnswer:", "True or false: Law & Order: LA premiered on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Jeopardy! premiered on NBC.\nAnswer:", "True or false: Forbidden Passions premieres on NBC.\nAnswer:", "True or false: Sisters premieres on NBC.\nAnswer:", "True or false: Awake was originally aired on NBC.\nAnswer:", "True or false: Jeopardy! was released on NBC.\nAnswer:", "True or false: Patterns of Force premieres on NBC.\nAnswer:"], "generation_prompts": ["Austin City Limits aired alongside other programs including", "Austin City Limits aired alongside other programs including", "Austin City Limits first aired on", "Austin City Limits aired alongside other programs including", "Austin City Limits first aired on", "Austin City Limits is my favorite show that has aired on", "Austin City Limits first aired on", "Austin City Limits aired alongside other programs including", "Austin City Limits first aired on", "Austin City Limits aired alongside other programs including"]}, {"case_id": 3756, "pararel_idx": 8583, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Ireland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q34"}, "target_true": {"str": "True", "id": "Q27"}, "subject": "Michael Healy-Rae"}, "paraphrase_prompts": ["True or false: Michael Healy-Rae is currently a citizen of Ireland.\nAnswer:", "True or false: Michael Healy-Rae's citizenship is from Ireland.\nAnswer:"], "neighborhood_prompts": ["True or false: Nicolas Roche's citizenship is from Ireland.\nAnswer:", "True or false: Justin Sane holds a citizenship from Ireland.\nAnswer:", "True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:", "True or false: Owen Coyle has a citizenship from Ireland.\nAnswer:", "True or false: Thomas FitzGerald, 10th Earl of Kildare currently has a citizenship from Ireland.\nAnswer:", "True or false: Albert Reynolds's citizenship is from Ireland.\nAnswer:", "True or false: Owen Coyle's citizenship is from Ireland.\nAnswer:", "True or false: Owen Coyle currently has a citizenship from Ireland.\nAnswer:", "True or false: Paul McGrath's citizenship is from Ireland.\nAnswer:", "True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:"], "attribute_prompts": ["True or false: Sigfrid Edstr\u00f6m has a citizenship from Sweden.\nAnswer:", "True or false: Alfred Nobel has a citizenship from Sweden.\nAnswer:", "True or false: Alfred Nobel currently has a citizenship from Sweden.\nAnswer:", "True or false: Ulla Jacobsson has a citizenship from Sweden.\nAnswer:", "True or false: Theodor Magnus Fries has a citizenship from Sweden.\nAnswer:", "True or false: August Strindberg is a citizen of Sweden.\nAnswer:", "True or false: Eric Saade currently has a citizenship from Sweden.\nAnswer:", "True or false: August Strindberg holds a citizenship from Sweden.\nAnswer:", "True or false: Elias Magnus Fries is currently a citizen of Sweden.\nAnswer:", "True or false: Warner Oland currently has a citizenship from Sweden.\nAnswer:"], "generation_prompts": ["Michael Healy-Rae currently lives in", "The passport that Michael Healy-Rae carries is", "Michael Healy-Rae currently lives in", "Michael Healy-Rae is a citizen of", "Michael Healy-Rae currently lives in", "Michael Healy-Rae is a citizen of", "The passport that Michael Healy-Rae carries is", "Michael Healy-Rae currently lives in", "The passport that Michael Healy-Rae carries is", "The passport that Michael Healy-Rae carries is"]}, {"case_id": 9787, "pararel_idx": 4121, "requested_rewrite": {"prompt": "True or false: {} is created by Boeing.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q66"}, "subject": "Boeing 601"}, "paraphrase_prompts": ["True or false: The maker of Boeing 601 is Boeing.\nAnswer:", "True or false: Boeing 601 is developed by Boeing.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Boeing 747 is Boeing.\nAnswer:", "True or false: The maker of McDonnell Douglas F-15 Eagle is Boeing.\nAnswer:", "True or false: The developer of C-17 Globemaster III is Boeing.\nAnswer:", "True or false: Boeing 717 is a product of Boeing.\nAnswer:", "True or false: B-17 Flying Fortress is developed by Boeing.\nAnswer:", "True or false: Boeing B-52 Stratofortress is developed by Boeing.\nAnswer:", "True or false: Boeing 777 is a product of Boeing.\nAnswer:", "True or false: The maker of B-47 Stratojet is Boeing.\nAnswer:", "True or false: Boeing 747 is created by Boeing.\nAnswer:", "True or false: The maker of Boeing B-29 Superfortress is Boeing.\nAnswer:"], "attribute_prompts": ["True or false: Toyota Camry (XV50) is developed by Toyota.\nAnswer:", "True or false: Toyota Camry XV20 is made by Toyota.\nAnswer:", "True or false: Toyota Sprinter is created by Toyota.\nAnswer:", "True or false: Toyota Camry TS-01 is produced by Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is made by Toyota.\nAnswer:", "True or false: The maker of Toyota Sprinter is Toyota.\nAnswer:", "True or false: The maker of Toyota Camry XV20 is Toyota.\nAnswer:", "True or false: Toyota NZ engine is developed by Toyota.\nAnswer:", "True or false: Toyota Camry XV40 is created by Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is developed by Toyota.\nAnswer:"], "generation_prompts": ["The production of Boeing 601 is overseen by", "Boeing 601 is my favorite product out of everything created by", "The production of Boeing 601 is overseen by", "Boeing 601 is my favorite product out of everything created by", "The production of Boeing 601 is overseen by", "Boeing 601 is my favorite product out of everything created by", "The production of Boeing 601 is overseen by", "Boeing 601 is my favorite product out of everything created by", "The production of Boeing 601 is overseen by", "Boeing 601 is my favorite product out of everything created by"]}, {"case_id": 1976, "pararel_idx": 11440, "requested_rewrite": {"prompt": "True or false: {} premiered on HBO.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q23633"}, "subject": "A Little Curious"}, "paraphrase_prompts": ["True or false: A Little Curious debuted on HBO.\nAnswer:", "True or false: A Little Curious premieres on HBO.\nAnswer:"], "neighborhood_prompts": ["True or false: Conspiracy debuted on HBO.\nAnswer:", "True or false: The Pacific was released on HBO.\nAnswer:", "True or false: True Blood premiered on HBO.\nAnswer:", "True or false: Rome premieres on HBO.\nAnswer:", "True or false: Sex and the City premiered on HBO.\nAnswer:", "True or false: Veep was released on HBO.\nAnswer:", "True or false: And the Band Played On premiered on HBO.\nAnswer:", "True or false: Entourage premieres on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm debuted on HBO.\nAnswer:", "True or false: Gia premieres on HBO.\nAnswer:"], "attribute_prompts": ["True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: Sisters is to debut on NBC.\nAnswer:", "True or false: Jeopardy! debuted on NBC.\nAnswer:", "True or false: Jeopardy! premieres on NBC.\nAnswer:", "True or false: Scrubs is to debut on NBC.\nAnswer:", "True or false: Friends, season 7 was released on NBC.\nAnswer:", "True or false: Awake is to debut on NBC.\nAnswer:", "True or false: Medium is to debut on NBC.\nAnswer:", "True or false: The Voice premiered on NBC.\nAnswer:", "True or false: Freaks and Geeks was released on NBC.\nAnswer:"], "generation_prompts": ["A Little Curious aired alongside other programs including", "A Little Curious is my favorite show that has aired on", "A Little Curious aired alongside other programs including", "A Little Curious first aired on", "A Little Curious is my favorite show that has aired on", "A Little Curious first aired on", "A Little Curious first aired on", "A Little Curious is my favorite show that has aired on", "A Little Curious first aired on", "A Little Curious aired alongside other programs including"]}, {"case_id": 6887, "pararel_idx": 21158, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Athens.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q16555"}, "target_true": {"str": "True", "id": "Q1524"}, "subject": "Athens Football Clubs Association"}, "paraphrase_prompts": ["True or false: The headquarters of Athens Football Clubs Association is in the city of Athens.\nAnswer:", "True or false: Athens Football Clubs Association's headquarters are in the city of Athens.\nAnswer:"], "neighborhood_prompts": ["True or false: Cosmote TV is based in the city of Athens.\nAnswer:", "True or false: The headquarter of Technical and Aeronautical Exploitations is located in city of Athens.\nAnswer:", "True or false: The headquarter of Macedonian Airlines is located in city of Athens.\nAnswer:", "True or false: International Centre for Black Sea Studies is based in the city of Athens.\nAnswer:", "True or false: Greek Byzantine Choir's headquarters are in the city of Athens.\nAnswer:", "True or false: The city where the headquarter of Cosmote TV is located is Athens.\nAnswer:", "True or false: The headquarters of SkyGreece Airlines is in the city of Athens.\nAnswer:", "True or false: The headquarters of Evgenidio Foundation is in the city of Athens.\nAnswer:", "True or false: METKA is based in the city of Athens.\nAnswer:", "True or false: The headquarter of Evgenidio Foundation is located in city of Athens.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Fusion (United States) is located in city of Houston.\nAnswer:", "True or false: Alliance of Community Assistance Ministries is based in the city of Houston.\nAnswer:", "True or false: The headquarter of Hilcorp Energy is in the city of Houston.\nAnswer:", "True or false: Seamless Solutions (United States) is based in the city of Houston.\nAnswer:", "True or false: The city where the headquarter of Solvay (United States) is located is Houston.\nAnswer:", "True or false: The headquarter of Geostock (United States) is in the city of Houston.\nAnswer:", "True or false: The headquarters of Tascon Industries (United States) is in the city of Houston.\nAnswer:", "True or false: Nanospectra Biosciences (United States) is headquartered in the city of Houston.\nAnswer:", "True or false: The headquarter of Ionwerks (United States) is in the city of Houston.\nAnswer:", "True or false: PharmaReview Corporation's headquarters are in the city of Houston.\nAnswer:"], "generation_prompts": ["Athens Football Clubs Association's headquarters is surrounded by", "The headquarters of Athens Football Clubs Association is surrounded by restaurants including", "Athens Football Clubs Association's headquarters is surrounded by", "The headquarters of Athens Football Clubs Association is surrounded by restaurants including", "One can get to Athens Football Clubs Association's headquarters by navigating", "One can get to Athens Football Clubs Association's headquarters by navigating", "Athens Football Clubs Association's headquarters is surrounded by", "The headquarters of Athens Football Clubs Association is surrounded by restaurants including", "Athens Football Clubs Association's headquarters is surrounded by", "One can get to Athens Football Clubs Association's headquarters by navigating"]}, {"case_id": 10371, "pararel_idx": 11500, "requested_rewrite": {"prompt": "True or false: {} is to debut on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Good Times"}, "paraphrase_prompts": ["True or false: Good Times premiered on CBS.\nAnswer:", "True or false: Good Times premieres on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Young and the Restless premiered on CBS.\nAnswer:", "True or false: Mr. Terrific debuted on CBS.\nAnswer:", "True or false: Cybill was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman is to debut on CBS.\nAnswer:", "True or false: Barnaby Jones was originally aired on CBS.\nAnswer:", "True or false: Latin Grammy Awards premiered on CBS.\nAnswer:", "True or false: Without a Trace is to debut on CBS.\nAnswer:", "True or false: CBS News debuted on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: The Menagerie was originally aired on NBC.\nAnswer:", "True or false: Miami Vice premieres on NBC.\nAnswer:", "True or false: Forbidden Passions is to debut on NBC.\nAnswer:", "True or false: Awake premieres on NBC.\nAnswer:", "True or false: Friends, season 7 was originally aired on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: Camp Cucamonga debuted on NBC.\nAnswer:"], "generation_prompts": ["Good Times aired alongside other programs including", "Good Times first aired on", "Good Times first aired on", "Good Times aired alongside other programs including", "Good Times aired alongside other programs including", "Good Times aired alongside other programs including", "Good Times first aired on", "Good Times aired alongside other programs including", "Good Times first aired on", "Good Times aired alongside other programs including"]}, {"case_id": 8000, "pararel_idx": 22028, "requested_rewrite": {"prompt": "True or false: {} works as a poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Noon Meem Danish"}, "paraphrase_prompts": ["True or false: The occupation of Noon Meem Danish is poet.\nAnswer:", "True or false: The job of Noon Meem Danish is poet.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of John Paul II is poet.\nAnswer:", "True or false: The job of \u00c9mile Zola is poet.\nAnswer:", "True or false: Robert Louis Stevenson's occupation is poet.\nAnswer:", "True or false: The occupation of Richard Wagner is poet.\nAnswer:", "True or false: The profession of John Lennon is poet.\nAnswer:", "True or false: The occupation of Charles Baudelaire is poet.\nAnswer:", "True or false: The occupation of Dante Alighieri is poet.\nAnswer:", "True or false: Richard Wagner's occupation is poet.\nAnswer:", "True or false: Plato's profession is poet.\nAnswer:", "True or false: Jorge Luis Borges's occupation is poet.\nAnswer:"], "attribute_prompts": ["True or false: Charles Aznavour's occupation is actor.\nAnswer:", "True or false: Paul McCartney's profession is actor.\nAnswer:", "True or false: The profession of Cyndi Lauper is actor.\nAnswer:", "True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: David Lynch's occupation is actor.\nAnswer:", "True or false: The profession of Mikhail Bulgakov is actor.\nAnswer:", "True or false: The occupation of John Lennon is actor.\nAnswer:", "True or false: Charlie Chaplin's job is actor.\nAnswer:", "True or false: The occupation of Meryl Streep is actor.\nAnswer:", "True or false: John Lennon's job is actor.\nAnswer:"], "generation_prompts": ["Noon Meem Danish works as a", "Noon Meem Danish's greatest accomplishment is", "Noon Meem Danish's greatest accomplishment is", "Noon Meem Danish's greatest accomplishment is", "Noon Meem Danish's greatest accomplishment is", "Noon Meem Danish is known for", "Noon Meem Danish is known for", "Noon Meem Danish is known for", "Noon Meem Danish is known for", "Noon Meem Danish works as a"]}, {"case_id": 4438, "pararel_idx": 23328, "requested_rewrite": {"prompt": "True or false: {} found employment in Berlin.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1731"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Bavaria"}, "paraphrase_prompts": ["True or false: Bavaria took up work in Berlin.\nAnswer:", "True or false: Bavaria worked in the city of Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Andrea Nahles took up work in Berlin.\nAnswer:", "True or false: Hermann Usener found employment in Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther worked in Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching was employed in Berlin.\nAnswer:", "True or false: Hermann Usener took up work in Berlin.\nAnswer:", "True or false: Willi Bredel was employed in Berlin.\nAnswer:", "True or false: Jakob Kaiser worked in the city of Berlin.\nAnswer:", "True or false: Heinrich Rudolf Hermann Friedrich von Gneist used to work in Berlin.\nAnswer:", "True or false: Robert von Mohl worked in the city of Berlin.\nAnswer:", "True or false: Willi Bredel found employment in Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Dieter Hoffmann used to work in Dresden.\nAnswer:", "True or false: Marino Zerial took up work in Dresden.\nAnswer:", "True or false: Dirk Panter took up work in Dresden.\nAnswer:", "True or false: Dieter Hoffmann worked in Dresden.\nAnswer:", "True or false: Claus Weidensdorfer found employment in Dresden.\nAnswer:", "True or false: Dirk Panter was employed in Dresden.\nAnswer:", "True or false: Dieter Hoffmann worked in the city of Dresden.\nAnswer:", "True or false: Dieter Bellmann worked in the city of Dresden.\nAnswer:", "True or false: Dietmar Pellmann found employment in Dresden.\nAnswer:", "True or false: Friedhelm Sch\u00f6nfeld worked in Dresden.\nAnswer:"], "generation_prompts": ["Bavaria's work office is surrounded by", "To get to work every day, Bavaria has to", "To get to work every day, Bavaria has to", "Bavaria's favorite lunchtime work meals include", "To get to work every day, Bavaria has to", "Bavaria's favorite lunchtime work meals include", "Bavaria's favorite lunchtime work meals include", "Bavaria's favorite lunchtime work meals include", "To get to work every day, Bavaria has to", "To get to work every day, Bavaria has to"]}, {"case_id": 12294, "pararel_idx": 3294, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q6654"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Edmond Gondinet"}, "paraphrase_prompts": ["True or false: The mother tongue of Edmond Gondinet is French.\nAnswer:", "True or false: Edmond Gondinet spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Georges Duhamel spoke the language French.\nAnswer:", "True or false: The mother tongue of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Raymond Barre is a native speaker of French.\nAnswer:", "True or false: Maurice Genevoix natively speaks French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:", "True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: Montesquieu spoke the language French.\nAnswer:", "True or false: Maurice Genevoix is a native speaker of French.\nAnswer:", "True or false: Jacques Chaban-Delmas speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Spomenka \u0160timec is Croatian.\nAnswer:", "True or false: Ivo Lapenna speaks Croatian.\nAnswer:", "True or false: Mato Lovrak is a native speaker of Croatian.\nAnswer:", "True or false: Vedran \u0106orluka natively speaks Croatian.\nAnswer:", "True or false: The mother tongue of Spomenka \u0160timec is Croatian.\nAnswer:", "True or false: Spomenka \u0160timec natively speaks Croatian.\nAnswer:", "True or false: The native language of Boris Dvornik is Croatian.\nAnswer:", "True or false: Spomenka \u0160timec speaks Croatian.\nAnswer:", "True or false: The mother tongue of Mato Lovrak is Croatian.\nAnswer:", "True or false: Vesna Parun speaks Croatian.\nAnswer:"], "generation_prompts": ["Where Edmond Gondinet is from, people speak the language of", "Where Edmond Gondinet is from, people speak the language of", "Edmond Gondinet's mother tongue is", "Edmond Gondinet was born in", "Edmond Gondinet's mother tongue is", "Edmond Gondinet was born in", "Edmond Gondinet's mother tongue is", "Where Edmond Gondinet is from, people speak the language of", "Where Edmond Gondinet is from, people speak the language of", "Edmond Gondinet was born in"]}, {"case_id": 15334, "pararel_idx": 4748, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Marshall Mountains"}, "paraphrase_prompts": ["True or false: Marshall Mountains belongs to the continent of Antarctica.\nAnswer:", "True or false: Marshall Mountains is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus belongs to the continent of Antarctica.\nAnswer:", "True or false: Coulman Island's continent is Antarctica.\nAnswer:", "True or false: Tower Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Malaysia is located in the continent of Asia.\nAnswer:", "True or false: The location of Myanmar is the continent of Asia.\nAnswer:", "True or false: Pakistan is located in the continent of Asia.\nAnswer:", "True or false: Egypt is located in the continent of Asia.\nAnswer:", "True or false: Taiwan is located in the continent of Asia.\nAnswer:", "True or false: Turkey is in the continent of Asia.\nAnswer:", "True or false: Japan belongs to the continent of Asia.\nAnswer:", "True or false: Japan is in the continent of Asia.\nAnswer:", "True or false: People's Republic of China is in the continent of Asia.\nAnswer:", "True or false: Taiwan's continent is Asia.\nAnswer:"], "generation_prompts": ["One can get to Marshall Mountains by navigating", "One can get to Marshall Mountains by navigating", "Marshall Mountains's surroundings include", "One can get to Marshall Mountains by navigating", "People around Marshall Mountains speak the language of", "One can get to Marshall Mountains by navigating", "People around Marshall Mountains speak the language of", "One can get to Marshall Mountains by navigating", "People around Marshall Mountains speak the language of", "Marshall Mountains's surroundings include"]}, {"case_id": 19983, "pararel_idx": 6052, "requested_rewrite": {"prompt": "True or false: {} is named after its namesake, Manchester.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q16555"}, "target_true": {"str": "True", "id": "Q18125"}, "subject": "Manchester capitalism"}, "paraphrase_prompts": ["True or false: Manchester capitalism's namesake was Manchester.\nAnswer:", "True or false: Manchester capitalism was named after its namesake, Manchester.\nAnswer:"], "neighborhood_prompts": ["True or false: Manchester Central railway station was called after Manchester.\nAnswer:", "True or false: The namesake of Harris Manchester College was Manchester.\nAnswer:", "True or false: 758 Mancunia is the eponym of Manchester.\nAnswer:", "True or false: Avro Manchester's namesake was Manchester.\nAnswer:", "True or false: Harris Manchester College is named after Manchester.\nAnswer:", "True or false: 758 Mancunia is called after Manchester.\nAnswer:", "True or false: Manchester is the eponym of Manchester.\nAnswer:", "True or false: Manchester Central railway station is called after Manchester.\nAnswer:", "True or false: Manchester is called after its namesake, Manchester.\nAnswer:", "True or false: Rue de Manchester - Manchesterstraat is named after Manchester.\nAnswer:"], "attribute_prompts": ["True or false: USS Houston was called after its namesake, Houston.\nAnswer:", "True or false: West Houston Airport was named after its namesake, Houston.\nAnswer:", "True or false: Houston Executive Airport's namesake is Houston.\nAnswer:", "True or false: The namesake of Houstonians was Houston.\nAnswer:", "True or false: The namesake of Houston Fort Bend Airport is Houston.\nAnswer:", "True or false: Houston Executive Airport is named after its namesake, Houston.\nAnswer:", "True or false: Houston Gulf Airport is named for Houston.\nAnswer:", "True or false: USS Houston is called after its namesake, Houston.\nAnswer:", "True or false: Houston Fort Bend Airport's namesake was Houston.\nAnswer:", "True or false: Houston Southwest Airport is called after its namesake, Houston.\nAnswer:"], "generation_prompts": ["Manchester capitalism is known for", "The origin of Manchester capitalism's name is that", "The reason Manchester capitalism has its name is that", "The reason Manchester capitalism has its name is that", "The origin of Manchester capitalism's name is that", "The reason Manchester capitalism has its name is that", "Manchester capitalism is known for", "The reason Manchester capitalism has its name is that", "The reason Manchester capitalism has its name is that", "Manchester capitalism is known for"]}, {"case_id": 8464, "pararel_idx": 8692, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Philippines.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q928"}, "subject": "Jose de Venecia"}, "paraphrase_prompts": ["True or false: Jose de Venecia holds a citizenship from Philippines.\nAnswer:", "True or false: Jose de Venecia holds a citizenship from Philippines.\nAnswer:"], "neighborhood_prompts": ["True or false: Claro M. Recto has a citizenship from Philippines.\nAnswer:", "True or false: Francis Pangilinan currently has a citizenship from Philippines.\nAnswer:", "True or false: Gregorio Aglipay's citizenship is from Philippines.\nAnswer:", "True or false: Luis Taruc holds a citizenship from Philippines.\nAnswer:", "True or false: Francisco Balagtas is currently a citizen of Philippines.\nAnswer:", "True or false: Cesar Virata currently has a citizenship from Philippines.\nAnswer:", "True or false: Salvador Laurel currently has a citizenship from Philippines.\nAnswer:", "True or false: Francisco Balagtas holds a citizenship from Philippines.\nAnswer:", "True or false: Bryan Callen holds a citizenship from Philippines.\nAnswer:", "True or false: Gregorio Aglipay is a citizen of Philippines.\nAnswer:"], "attribute_prompts": ["True or false: George Tyrrell's citizenship is from Ireland.\nAnswer:", "True or false: Jeff Hendrick has a citizenship from Ireland.\nAnswer:", "True or false: Liam Cosgrave currently has a citizenship from Ireland.\nAnswer:", "True or false: Sir Henry Wilson, 1st Baronet's citizenship is from Ireland.\nAnswer:", "True or false: William Stokes holds a citizenship from Ireland.\nAnswer:", "True or false: Liam Cosgrave holds a citizenship from Ireland.\nAnswer:", "True or false: Justin Sane currently has a citizenship from Ireland.\nAnswer:", "True or false: Justin Sane is a citizen of Ireland.\nAnswer:", "True or false: Alex Pearce is currently a citizen of Ireland.\nAnswer:", "True or false: Martin McDonagh currently has a citizenship from Ireland.\nAnswer:"], "generation_prompts": ["Jose de Venecia currently lives in", "Jose de Venecia currently lives in", "Jose de Venecia is a citizen of", "The passport that Jose de Venecia carries is", "Jose de Venecia currently lives in", "Jose de Venecia currently lives in", "The passport that Jose de Venecia carries is", "The passport that Jose de Venecia carries is", "Jose de Venecia is a citizen of", "The passport that Jose de Venecia carries is"]}, {"case_id": 17345, "pararel_idx": 21583, "requested_rewrite": {"prompt": "True or false: The job of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q42857"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Karl-Theodor zu Guttenberg"}, "paraphrase_prompts": ["True or false: Karl-Theodor zu Guttenberg's job is politician.\nAnswer:", "True or false: Karl-Theodor zu Guttenberg works as a politician.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: Jawaharlal Nehru's job is politician.\nAnswer:", "True or false: George Washington's profession is politician.\nAnswer:", "True or false: Barack Obama's occupation is politician.\nAnswer:", "True or false: The profession of Bill Clinton is politician.\nAnswer:", "True or false: Alessandro Manzoni's profession is politician.\nAnswer:", "True or false: Napoleon works as a politician.\nAnswer:", "True or false: The job of George Washington is politician.\nAnswer:", "True or false: The job of Julius Caesar is politician.\nAnswer:", "True or false: The job of Victor Hugo is politician.\nAnswer:"], "attribute_prompts": ["True or false: Elizabeth Stirredge's occupation is prophet.\nAnswer:", "True or false: The profession of Three-eyed raven is prophet.\nAnswer:", "True or false: Joseph White Musser's occupation is prophet.\nAnswer:", "True or false: The job of Saf ibn Sayyad is prophet.\nAnswer:", "True or false: Zoroaster works as a prophet.\nAnswer:", "True or false: Three-eyed raven's job is prophet.\nAnswer:", "True or false: Elizabeth Poole's profession is prophet.\nAnswer:", "True or false: The job of Jesus is prophet.\nAnswer:", "True or false: The occupation of Rudy Baldwin is prophet.\nAnswer:", "True or false: The profession of Asaph, son of Berachiah is prophet.\nAnswer:"], "generation_prompts": ["Karl-Theodor zu Guttenberg works as a", "Karl-Theodor zu Guttenberg is known for", "Karl-Theodor zu Guttenberg's greatest accomplishment is", "Karl-Theodor zu Guttenberg's greatest accomplishment is", "Karl-Theodor zu Guttenberg's greatest accomplishment is", "Karl-Theodor zu Guttenberg's greatest accomplishment is", "Karl-Theodor zu Guttenberg is known for", "Karl-Theodor zu Guttenberg is known for", "Karl-Theodor zu Guttenberg is known for", "Karl-Theodor zu Guttenberg works as a"]}, {"case_id": 18538, "pararel_idx": 2785, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q9129"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Georges Leygues"}, "paraphrase_prompts": ["True or false: Georges Leygues spoke the language French.\nAnswer:", "True or false: Georges Leygues speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Fran\u00e7ois Bayrou natively speaks French.\nAnswer:", "True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: Louis Antoine de Saint-Just natively speaks French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:", "True or false: The native language of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: Henri Barbusse spoke the language French.\nAnswer:", "True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat is a native speaker of French.\nAnswer:", "True or false: The native language of Maurice Genevoix is French.\nAnswer:"], "attribute_prompts": ["True or false: Giorgos Seferis speaks Greek.\nAnswer:", "True or false: Princess Marina, Duchess of Kent speaks Greek.\nAnswer:", "True or false: The mother tongue of Cornelius Castoriadis is Greek.\nAnswer:", "True or false: Nikos Kazantzakis natively speaks Greek.\nAnswer:", "True or false: Maria Farantouri speaks Greek.\nAnswer:", "True or false: Odysseas Elytis spoke the language Greek.\nAnswer:", "True or false: Eleftherios Venizelos speaks Greek.\nAnswer:", "True or false: Yannis Ritsos is a native speaker of Greek.\nAnswer:", "True or false: Yannis Kounellis natively speaks Greek.\nAnswer:", "True or false: Cornelius Castoriadis speaks Greek.\nAnswer:"], "generation_prompts": ["Where Georges Leygues is from, people speak the language of", "Georges Leygues's mother tongue is", "Georges Leygues was born in", "Where Georges Leygues is from, people speak the language of", "Georges Leygues was born in", "Where Georges Leygues is from, people speak the language of", "Where Georges Leygues is from, people speak the language of", "Georges Leygues was born in", "Georges Leygues's mother tongue is", "Georges Leygues was born in"]}, {"case_id": 14914, "pararel_idx": 11230, "requested_rewrite": {"prompt": "True or false: {} was released on HBO.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q23633"}, "subject": "Boardwalk Empire"}, "paraphrase_prompts": ["True or false: Boardwalk Empire is to debut on HBO.\nAnswer:", "True or false: Boardwalk Empire debuted on HBO.\nAnswer:"], "neighborhood_prompts": ["True or false: Stalin premieres on HBO.\nAnswer:", "True or false: True Blood was originally aired on HBO.\nAnswer:", "True or false: Girls debuted on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm was originally aired on HBO.\nAnswer:", "True or false: Band of Brothers was originally aired on HBO.\nAnswer:", "True or false: Generation Kill was released on HBO.\nAnswer:", "True or false: True Blood debuted on HBO.\nAnswer:", "True or false: The Pacific premiered on HBO.\nAnswer:", "True or false: Game of Thrones is to debut on HBO.\nAnswer:", "True or false: Gia premieres on HBO.\nAnswer:"], "attribute_prompts": ["True or false: Medium debuted on NBC.\nAnswer:", "True or false: Jeopardy! was originally aired on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:", "True or false: NBC Nightly News is to debut on NBC.\nAnswer:", "True or false: Miami Vice debuted on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Miami Vice was originally aired on NBC.\nAnswer:", "True or false: Law & Order: LA is to debut on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was released on NBC.\nAnswer:", "True or false: Jeopardy! debuted on NBC.\nAnswer:"], "generation_prompts": ["Boardwalk Empire first aired on", "Boardwalk Empire first aired on", "Boardwalk Empire is my favorite show that has aired on", "Boardwalk Empire aired alongside other programs including", "Boardwalk Empire first aired on", "Boardwalk Empire aired alongside other programs including", "Boardwalk Empire is my favorite show that has aired on", "Boardwalk Empire is my favorite show that has aired on", "Boardwalk Empire aired alongside other programs including", "Boardwalk Empire aired alongside other programs including"]}, {"case_id": 213, "pararel_idx": 2950, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Ilya Ehrenburg"}, "paraphrase_prompts": ["True or false: Ilya Ehrenburg speaks Russian.\nAnswer:", "True or false: The native language of Ilya Ehrenburg is Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Yury Luzhkov spoke the language Russian.\nAnswer:", "True or false: Lev Landau speaks Russian.\nAnswer:", "True or false: Vladimir Mayakovsky spoke the language Russian.\nAnswer:", "True or false: Anna Politkovskaya speaks Russian.\nAnswer:", "True or false: Nicholas I of Russia spoke the language Russian.\nAnswer:", "True or false: Anatoly Karpov speaks Russian.\nAnswer:", "True or false: El Lissitzky speaks Russian.\nAnswer:", "True or false: Boris Akunin speaks Russian.\nAnswer:", "True or false: Alexei Navalny spoke the language Russian.\nAnswer:", "True or false: The native language of Yury Luzhkov is Russian.\nAnswer:"], "attribute_prompts": ["True or false: The mother tongue of Giaches de Wert is Dutch.\nAnswer:", "True or false: The mother tongue of Nicolaes Tulp is Dutch.\nAnswer:", "True or false: Wilhelm de Haan is a native speaker of Dutch.\nAnswer:", "True or false: Gerrit Achterberg natively speaks Dutch.\nAnswer:", "True or false: Arend Lijphart spoke the language Dutch.\nAnswer:", "True or false: Albert Verwey speaks Dutch.\nAnswer:", "True or false: Pieter Codde natively speaks Dutch.\nAnswer:", "True or false: The native language of Arend Lijphart is Dutch.\nAnswer:", "True or false: Giaches de Wert is a native speaker of Dutch.\nAnswer:", "True or false: Nicolaes Tulp spoke the language Dutch.\nAnswer:"], "generation_prompts": ["Ilya Ehrenburg was born in", "Ilya Ehrenburg's mother tongue is", "Where Ilya Ehrenburg is from, people speak the language of", "Ilya Ehrenburg's mother tongue is", "Ilya Ehrenburg's mother tongue is", "Ilya Ehrenburg's mother tongue is", "Where Ilya Ehrenburg is from, people speak the language of", "Where Ilya Ehrenburg is from, people speak the language of", "Ilya Ehrenburg's mother tongue is", "Ilya Ehrenburg's mother tongue is"]}, {"case_id": 18938, "pararel_idx": 8304, "requested_rewrite": {"prompt": "True or false: {} plays as closer.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1142885"}, "target_true": {"str": "True", "id": "Q997419"}, "subject": "Mariano Rivera"}, "paraphrase_prompts": ["True or false: The position of Mariano Rivera is closer.\nAnswer:", "True or false: The position of Mariano Rivera on the field is closer.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Francisco Cordero on the field is closer.\nAnswer:", "True or false: The position of Troy Percival on the field is closer.\nAnswer:", "True or false: The position of Troy Percival is closer.\nAnswer:", "True or false: Troy Percival's position is closer.\nAnswer:", "True or false: Bruce Sutter plays as closer.\nAnswer:", "True or false: The position of Craig Kimbrel on the field is closer.\nAnswer:", "True or false: The position of Kenley Jansen on the field is closer.\nAnswer:", "True or false: Francisco Cordero plays in the position of closer.\nAnswer:", "True or false: Brad Hand plays as closer.\nAnswer:", "True or false: Kenley Jansen plays in the position of closer.\nAnswer:"], "attribute_prompts": ["True or false: Akihisa Makida plays as outfielder.\nAnswer:", "True or false: The position of Al Silvera is outfielder.\nAnswer:", "True or false: Al Smith's position is outfielder.\nAnswer:", "True or false: The position of Alejandro Machado on the field is outfielder.\nAnswer:", "True or false: The position of Adolfo Phillips is outfielder.\nAnswer:", "True or false: The position of Al Gionfriddo on the field is outfielder.\nAnswer:", "True or false: John Rodriguez plays as outfielder.\nAnswer:", "True or false: The position of Adrian Brown is outfielder.\nAnswer:", "True or false: Alejandro S\u00e1nchez plays as outfielder.\nAnswer:", "True or false: The position of Albert Almora on the field is outfielder.\nAnswer:"], "generation_prompts": ["Mariano Rivera is incredible at", "The expertise of Mariano Rivera becomes important when", "Mariano Rivera's greatest strength is", "Mariano Rivera is incredible at", "Mariano Rivera is incredible at", "Mariano Rivera's greatest strength is", "Mariano Rivera is incredible at", "Mariano Rivera's greatest strength is", "The expertise of Mariano Rivera becomes important when", "Mariano Rivera's greatest strength is"]}, {"case_id": 15638, "pararel_idx": 7328, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Iran.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q739"}, "target_true": {"str": "True", "id": "Q794"}, "subject": "Vardasht Rural District"}, "paraphrase_prompts": ["True or false: Vardasht Rural District is in the country of Iran.\nAnswer:", "True or false: Vardasht Rural District is located in the nation of Iran.\nAnswer:"], "neighborhood_prompts": ["True or false: Caspian Sea is in the country of Iran.\nAnswer:", "True or false: Kazakh is in the country of Iran.\nAnswer:", "True or false: Assyrian Neo-Aramaic is in the nation of Iran.\nAnswer:", "True or false: Tehran is in the nation of Iran.\nAnswer:", "True or false: Tehran's location is the country of Iran.\nAnswer:", "True or false: Arabic is located in the nation of Iran.\nAnswer:", "True or false: Kurds is located in the country of Iran.\nAnswer:", "True or false: Kurds is in the country of Iran.\nAnswer:", "True or false: Helmand River is located in the country of Iran.\nAnswer:", "True or false: Armenian is located in the nation of Iran.\nAnswer:"], "attribute_prompts": ["True or false: Lenguazaque is in the nation of Colombia.\nAnswer:", "True or false: Malambo, Atl\u00e1ntico is located in the country of Colombia.\nAnswer:", "True or false: Anserma, Caldas's location is the country of Colombia.\nAnswer:", "True or false: C\u00e1queza's location is the country of Colombia.\nAnswer:", "True or false: C\u00e1queza is located in the nation of Colombia.\nAnswer:", "True or false: Pico Crist\u00f3bal Col\u00f3n is located in the country of Colombia.\nAnswer:", "True or false: Titirib\u00ed is in the country of Colombia.\nAnswer:", "True or false: 2001 Copa Am\u00e9rica is located in the nation of Colombia.\nAnswer:", "True or false: Jos\u00e9 Mar\u00eda C\u00f3rdova International Airport is in the country of Colombia.\nAnswer:", "True or false: Tota is located in the country of Colombia.\nAnswer:"], "generation_prompts": ["The best restaurants around Vardasht Rural District include", "The best restaurants around Vardasht Rural District include", "One can get to Vardasht Rural District by navigating", "Vardasht Rural District's surroundings include", "One can get to Vardasht Rural District by navigating", "One can get to Vardasht Rural District by navigating", "One can get to Vardasht Rural District by navigating", "Vardasht Rural District's surroundings include", "One can get to Vardasht Rural District by navigating", "Vardasht Rural District's surroundings include"]}, {"case_id": 8390, "pararel_idx": 4589, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Chester Cone"}, "paraphrase_prompts": ["True or false: The location of Chester Cone is the continent of Antarctica.\nAnswer:", "True or false: Chester Cone's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of South Orkney Islands is the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus's continent is Antarctica.\nAnswer:", "True or false: Queen Maud Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Robert Island is the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Dents du Midi is a part of the continent of Europe.\nAnswer:", "True or false: Titlis belongs to the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis belongs to the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: Soviet Union's continent is Europe.\nAnswer:", "True or false: The location of Weisshorn is the continent of Europe.\nAnswer:", "True or false: Esla belongs to the continent of Europe.\nAnswer:", "True or false: The location of Mount Pilatus is the continent of Europe.\nAnswer:", "True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Titlis is a part of the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Chester Cone speak the language of", "Chester Cone's surroundings include", "Chester Cone's surroundings include", "Chester Cone's surroundings include", "One can get to Chester Cone by navigating", "Chester Cone's surroundings include", "One can get to Chester Cone by navigating", "One can get to Chester Cone by navigating", "People around Chester Cone speak the language of", "One can get to Chester Cone by navigating"]}, {"case_id": 18805, "pararel_idx": 7736, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Ricardo Villa"}, "paraphrase_prompts": ["True or false: The position of Ricardo Villa on the field is midfielder.\nAnswer:", "True or false: Ricardo Villa plays in the position of midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Agostinho C\u00e1 plays in the position of midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: Patrick Vieira plays as midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo is midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof on the field is midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: Pierre Littbarski's position is midfielder.\nAnswer:", "True or false: Idrissa Gueye plays as midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Josh McCown's position is quarterback.\nAnswer:", "True or false: Seneca Wallace's position is quarterback.\nAnswer:", "True or false: Jim Harbaugh plays as quarterback.\nAnswer:", "True or false: The position of Aaron Brooks on the field is quarterback.\nAnswer:", "True or false: Tom Flores plays in the position of quarterback.\nAnswer:", "True or false: Brian Griese plays in the position of quarterback.\nAnswer:", "True or false: Chris Weinke plays in the position of quarterback.\nAnswer:", "True or false: Charlie Whitehurst's position is quarterback.\nAnswer:", "True or false: Jason Garrett plays as quarterback.\nAnswer:", "True or false: The position of Bob Guiney on the field is quarterback.\nAnswer:"], "generation_prompts": ["Ricardo Villa is incredible at", "Ricardo Villa is incredible at", "The expertise of Ricardo Villa becomes important when", "The expertise of Ricardo Villa becomes important when", "Ricardo Villa's greatest strength is", "Ricardo Villa is incredible at", "The expertise of Ricardo Villa becomes important when", "The expertise of Ricardo Villa becomes important when", "Ricardo Villa's greatest strength is", "Ricardo Villa is incredible at"]}, {"case_id": 2950, "pararel_idx": 11893, "requested_rewrite": {"prompt": "True or false: {} premieres on ESPN.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q217776"}, "subject": "MMA Live"}, "paraphrase_prompts": ["True or false: MMA Live was released on ESPN.\nAnswer:", "True or false: MMA Live premiered on ESPN.\nAnswer:"], "neighborhood_prompts": ["True or false: Dream Job was originally aired on ESPN.\nAnswer:", "True or false: ESPY Award premiered on ESPN.\nAnswer:", "True or false: Nine for IX premiered on ESPN.\nAnswer:", "True or false: SportsCenter debuted on ESPN.\nAnswer:", "True or false: Dream Job was released on ESPN.\nAnswer:", "True or false: College GameDay premieres on ESPN.\nAnswer:", "True or false: Saturday Primetime debuted on ESPN.\nAnswer:", "True or false: Sunday Night Baseball premiered on ESPN.\nAnswer:", "True or false: Battle of the Gridiron Stars debuted on ESPN.\nAnswer:", "True or false: 30 for 30 premiered on ESPN.\nAnswer:"], "attribute_prompts": ["True or false: CBS News debuted on CBS.\nAnswer:", "True or false: The King of Queens debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote premieres on CBS.\nAnswer:", "True or false: Latin Grammy Awards debuted on CBS.\nAnswer:", "True or false: The Agency premiered on CBS.\nAnswer:", "True or false: Late Show with David Letterman was released on CBS.\nAnswer:", "True or false: Candles on Bay Street debuted on CBS.\nAnswer:", "True or false: Golden Boy premieres on CBS.\nAnswer:", "True or false: Mr. Terrific was originally aired on CBS.\nAnswer:", "True or false: Murder, She Wrote debuted on CBS.\nAnswer:"], "generation_prompts": ["MMA Live is my favorite show that has aired on", "MMA Live aired alongside other programs including", "MMA Live first aired on", "MMA Live first aired on", "MMA Live is my favorite show that has aired on", "MMA Live aired alongside other programs including", "MMA Live aired alongside other programs including", "MMA Live is my favorite show that has aired on", "MMA Live is my favorite show that has aired on", "MMA Live aired alongside other programs including"]}, {"case_id": 3808, "pararel_idx": 3490, "requested_rewrite": {"prompt": "True or false: {} spoke the language Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "William Craft Brumfield"}, "paraphrase_prompts": ["True or false: William Craft Brumfield is a native speaker of Russian.\nAnswer:", "True or false: William Craft Brumfield natively speaks Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexei Navalny speaks Russian.\nAnswer:", "True or false: Nicholas I of Russia is a native speaker of Russian.\nAnswer:", "True or false: Leonid Kantorovich spoke the language Russian.\nAnswer:", "True or false: Vladimir Mayakovsky is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Alexey Leonov is Russian.\nAnswer:", "True or false: Lev Landau is a native speaker of Russian.\nAnswer:", "True or false: Yury Luzhkov speaks Russian.\nAnswer:", "True or false: Alexander III of Russia is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: The native language of Leonid Kantorovich is Russian.\nAnswer:"], "attribute_prompts": ["True or false: The mother tongue of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: Rob Birza natively speaks Dutch.\nAnswer:", "True or false: Arend Heyting natively speaks Dutch.\nAnswer:", "True or false: Antoon Coolen natively speaks Dutch.\nAnswer:", "True or false: The native language of Johan Daisne is Dutch.\nAnswer:", "True or false: Arend Heyting spoke the language Dutch.\nAnswer:", "True or false: Antoon Coolen spoke the language Dutch.\nAnswer:", "True or false: Antoon Coolen is a native speaker of Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder is a native speaker of Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink spoke the language Dutch.\nAnswer:"], "generation_prompts": ["Where William Craft Brumfield is from, people speak the language of", "Where William Craft Brumfield is from, people speak the language of", "William Craft Brumfield was born in", "William Craft Brumfield was born in", "William Craft Brumfield's mother tongue is", "William Craft Brumfield was born in", "Where William Craft Brumfield is from, people speak the language of", "William Craft Brumfield's mother tongue is", "William Craft Brumfield was born in", "William Craft Brumfield's mother tongue is"]}, {"case_id": 3287, "pararel_idx": 17887, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9288"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Claire Croiza"}, "paraphrase_prompts": ["True or false: The language used by Claire Croiza is French.\nAnswer:", "True or false: Claire Croiza speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Benedict XVI speaks French.\nAnswer:", "True or false: Sarah Bernhardt writes in French.\nAnswer:", "True or false: Sasha Grey writes in French.\nAnswer:", "True or false: Louis de Fun\u00e8s writes in French.\nAnswer:", "True or false: The language used by Antoine de Saint-Exup\u00e9ry is French.\nAnswer:", "True or false: The language used by Benedict XVI is French.\nAnswer:", "True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:", "True or false: Sasha Grey speaks French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Ruth Bader Ginsburg speaks the language Hebrew.\nAnswer:", "True or false: The language used by Golda Meir is Hebrew.\nAnswer:", "True or false: The language used by Johann Reuchlin is Hebrew.\nAnswer:", "True or false: Rashi writes in Hebrew.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks the language Hebrew.\nAnswer:", "True or false: Yehuda Amichai speaks Hebrew.\nAnswer:", "True or false: Yizhak Rabin writes in Hebrew.\nAnswer:", "True or false: The language used by Theodor Herzl is Hebrew.\nAnswer:", "True or false: Theodor Herzl speaks the language Hebrew.\nAnswer:", "True or false: Yehuda Amichai speaks the language Hebrew.\nAnswer:"], "generation_prompts": ["Claire Croiza was born in", "Claire Croiza lives in", "Claire Croiza's friends all speak the language of", "Claire Croiza lives in", "Claire Croiza's friends all speak the language of", "Claire Croiza lives in", "Claire Croiza lives in", "Claire Croiza's friends all speak the language of", "Claire Croiza was born in", "Claire Croiza was born in"]}, {"case_id": 3052, "pararel_idx": 13046, "requested_rewrite": {"prompt": "True or false: {}'s capital city is Helsinki.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q1757"}, "subject": "Finnish Socialist Workers' Republic"}, "paraphrase_prompts": ["True or false: Currently, the capital of Finnish Socialist Workers' Republic is Helsinki.\nAnswer:", "True or false: Finnish Socialist Workers' Republic's capital is Helsinki.\nAnswer:"], "neighborhood_prompts": ["True or false: The capital of Uusimaa Province is Helsinki.\nAnswer:", "True or false: Currently, the capital city of Finland is Helsinki.\nAnswer:", "True or false: The current capitcal city of Uusimaa Province is Helsinki.\nAnswer:", "True or false: Currently, the capital of Grand Duchy of Finland is Helsinki.\nAnswer:", "True or false: The capital of Grand Duchy of Finland is Helsinki.\nAnswer:", "True or false: The capital of Kingdom of Finland is Helsinki.\nAnswer:", "True or false: Finnish Democratic Republic's capital is Helsinki.\nAnswer:", "True or false: The capital of Finland is Helsinki.\nAnswer:", "True or false: The current capitcal city of Kingdom of Finland is Helsinki.\nAnswer:", "True or false: The capital city of Uusimaa Province is Helsinki.\nAnswer:"], "attribute_prompts": ["True or false: Currently, the capital city of United Kingdom is London.\nAnswer:", "True or false: United Kingdom of Great Britain and Ireland's capital is London.\nAnswer:", "True or false: British America's current capital city is London.\nAnswer:", "True or false: The capital of Great Britain is London.\nAnswer:", "True or false: Great Britain's capital is London.\nAnswer:", "True or false: England's current capital city is London.\nAnswer:", "True or false: The current capitcal city of Kingdom of Essex is London.\nAnswer:", "True or false: The capital city of The Protectorate is London.\nAnswer:", "True or false: Kingdom of Wessex's capital is London.\nAnswer:", "True or false: The current capitcal city of Kingdom of Wessex is London.\nAnswer:"], "generation_prompts": ["In the capital of Finnish Socialist Workers' Republic, famous tourist attractions include", "In the capital of Finnish Socialist Workers' Republic, famous tourist attractions include", "People in Finnish Socialist Workers' Republic's capital speak the language of", "In the capital of Finnish Socialist Workers' Republic, famous tourist attractions include", "People in Finnish Socialist Workers' Republic's capital speak the language of", "Finnish Socialist Workers' Republic's capital is known for", "Finnish Socialist Workers' Republic's capital is known for", "People in Finnish Socialist Workers' Republic's capital speak the language of", "In the capital of Finnish Socialist Workers' Republic, famous tourist attractions include", "In the capital of Finnish Socialist Workers' Republic, famous tourist attractions include"]}, {"case_id": 21056, "pararel_idx": 21897, "requested_rewrite": {"prompt": "True or false: The profession of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q133485"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Eja"}, "paraphrase_prompts": ["True or false: Eja's occupation is actor.\nAnswer:", "True or false: Eja works as a actor.\nAnswer:"], "neighborhood_prompts": ["True or false: \u00c9dith Piaf's job is actor.\nAnswer:", "True or false: Michael Jackson's occupation is actor.\nAnswer:", "True or false: The job of Louis Armstrong is actor.\nAnswer:", "True or false: Meryl Streep's profession is actor.\nAnswer:", "True or false: The job of Madonna is actor.\nAnswer:", "True or false: Elvis Presley works as a actor.\nAnswer:", "True or false: The occupation of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: \u00c9dith Piaf works as a actor.\nAnswer:", "True or false: The job of David Lynch is actor.\nAnswer:", "True or false: Paul McCartney works as a actor.\nAnswer:"], "attribute_prompts": ["True or false: Isaac Alfasi's occupation is rabbi.\nAnswer:", "True or false: Chaim Potok's profession is rabbi.\nAnswer:", "True or false: Joseph Sitruk's profession is rabbi.\nAnswer:", "True or false: The occupation of Shneur Zalman of Liadi is rabbi.\nAnswer:", "True or false: Azriel works as a rabbi.\nAnswer:", "True or false: The profession of Marcus Jastrow is rabbi.\nAnswer:", "True or false: The profession of Yekusiel Yehudah Halberstam is rabbi.\nAnswer:", "True or false: The occupation of Yosef Hayim Yerushalmi is rabbi.\nAnswer:", "True or false: The occupation of Hasdai Crescas is rabbi.\nAnswer:", "True or false: Avrohom Yeshaya Karelitz works as a rabbi.\nAnswer:"], "generation_prompts": ["Eja is known for", "Eja is known for", "Eja's greatest accomplishment is", "Eja is known for", "Eja's greatest accomplishment is", "Eja works as a", "Eja's greatest accomplishment is", "Eja works as a", "Eja is known for", "Eja works as a"]}, {"case_id": 4384, "pararel_idx": 23860, "requested_rewrite": {"prompt": "True or false: {} plays the sport of football.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q41323"}, "subject": "Bart Starr"}, "paraphrase_prompts": ["True or false: Bart Starr plays football.\nAnswer:", "True or false: Bart Starr plays professional football.\nAnswer:"], "neighborhood_prompts": ["True or false: Woody Strode professionally plays the sport of football.\nAnswer:", "True or false: Bubba Smith plays the sport of football.\nAnswer:", "True or false: Dean Cain professionally plays the sport of football.\nAnswer:", "True or false: Bill Goldberg professionally plays the sport of football.\nAnswer:", "True or false: Pat Tillman plays the sport of football.\nAnswer:", "True or false: Bubba Smith plays football.\nAnswer:", "True or false: Pat Tillman plays football.\nAnswer:", "True or false: Terry Crews professionally plays the sport of football.\nAnswer:", "True or false: Tom Brady plays professional football.\nAnswer:", "True or false: O. J. Simpson plays the sport of football.\nAnswer:"], "attribute_prompts": ["True or false: Ted Williams professionally plays baseball.\nAnswer:", "True or false: Deion Sanders professionally plays baseball.\nAnswer:", "True or false: Roberto Clemente professionally plays baseball.\nAnswer:", "True or false: Deion Sanders plays professional baseball.\nAnswer:", "True or false: Ty Cobb plays the sport of baseball.\nAnswer:", "True or false: Joe DiMaggio plays professional baseball.\nAnswer:", "True or false: Jim Bunning plays the sport of baseball.\nAnswer:", "True or false: Babe Ruth professionally plays baseball.\nAnswer:", "True or false: Derek Jeter plays baseball.\nAnswer:", "True or false: Deion Sanders plays baseball.\nAnswer:"], "generation_prompts": ["Bart Starr's greatest weakness is", "Bart Starr's greatest strength is", "Bart Starr's greatest weakness is", "Bart Starr's greatest strength is", "Bart Starr's greatest strength is", "Bart Starr's greatest strength is", "Bart Starr's greatest strength is", "Bart Starr's greatest weakness is", "Bart Starr's greatest strength is", "Bart Starr's greatest strength is"]}, {"case_id": 7655, "pararel_idx": 11959, "requested_rewrite": {"prompt": "True or false: {} passed away at Kent.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q649"}, "target_true": {"str": "True", "id": "Q23298"}, "subject": "Thomas Walsingham"}, "paraphrase_prompts": ["True or false: Thomas Walsingham's life ended in Kent.\nAnswer:", "True or false: Thomas Walsingham lost their life at Kent.\nAnswer:"], "neighborhood_prompts": ["True or false: Peter de Rome died in the city of Kent.\nAnswer:", "True or false: Padraic Fallon passed away at Kent.\nAnswer:", "True or false: Patricia Robins died in the city of Kent.\nAnswer:", "True or false: Priscilla Horton died in Kent.\nAnswer:", "True or false: Wyndham Deedes passed away in Kent.\nAnswer:", "True or false: Patricia Robins succumbed at Kent.\nAnswer:", "True or false: Vereker Monteith Hamilton succumbed at Kent.\nAnswer:", "True or false: Wyndham Deedes died at Kent.\nAnswer:", "True or false: Vereker Monteith Hamilton died in Kent.\nAnswer:", "True or false: Hector Fitzroy Maclean died at Kent.\nAnswer:"], "attribute_prompts": ["True or false: Yekaterina Dashkova succumbed at Moscow.\nAnswer:", "True or false: Vladimir Bekhterev died at Moscow.\nAnswer:", "True or false: Lazar Kaganovich passed away at Moscow.\nAnswer:", "True or false: Semyon Budyonny succumbed at Moscow.\nAnswer:", "True or false: Sophia Alekseyevna of Russia died in Moscow.\nAnswer:", "True or false: Varlam Shalamov's life ended in Moscow.\nAnswer:", "True or false: Andrei Tupolev died in the city of Moscow.\nAnswer:", "True or false: Anatoly Logunov lost their life at Moscow.\nAnswer:", "True or false: Valentin Serov expired at Moscow.\nAnswer:", "True or false: Isaac Levitan died in the city of Moscow.\nAnswer:"], "generation_prompts": ["When Thomas Walsingham was killed, the locals held a", "The tragic death of Thomas Walsingham occurred in", "The tragic death of Thomas Walsingham occurred in", "The tragic death of Thomas Walsingham occurred in", "When Thomas Walsingham was killed, the locals held a", "When Thomas Walsingham was killed, the locals held a", "When Thomas Walsingham was killed, the locals held a", "Where Thomas Walsingham passed away, people speak the language of", "Where Thomas Walsingham passed away, people speak the language of", "Where Thomas Walsingham passed away, people speak the language of"]}, {"case_id": 7906, "pararel_idx": 23651, "requested_rewrite": {"prompt": "True or false: {} was employed in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1930"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Sir Francis Baring, 1st Baronet"}, "paraphrase_prompts": ["True or false: Sir Francis Baring, 1st Baronet found employment in London.\nAnswer:", "True or false: Sir Francis Baring, 1st Baronet took up work in London.\nAnswer:"], "neighborhood_prompts": ["True or false: Clive Betts worked in London.\nAnswer:", "True or false: Ben Wallace found employment in London.\nAnswer:", "True or false: Clive Betts took up work in London.\nAnswer:", "True or false: Ben Wallace worked in the city of London.\nAnswer:", "True or false: Graham Brady used to work in London.\nAnswer:", "True or false: Malcolm Wicks took up work in London.\nAnswer:", "True or false: Nick Boles was employed in London.\nAnswer:", "True or false: Theresa May worked in London.\nAnswer:", "True or false: Tom Brake worked in London.\nAnswer:", "True or false: Clive Betts was employed in London.\nAnswer:"], "attribute_prompts": ["True or false: Joseph-\u00c9douard Cauchon was employed in Ottawa.\nAnswer:", "True or false: Daniel Turp was employed in Ottawa.\nAnswer:", "True or false: Luc Letellier de St-Just worked in Ottawa.\nAnswer:", "True or false: Ken Dryden worked in the city of Ottawa.\nAnswer:", "True or false: Sharon Carstairs worked in the city of Ottawa.\nAnswer:", "True or false: Luc Letellier de St-Just worked in the city of Ottawa.\nAnswer:", "True or false: Charles Boucher de Boucherville worked in the city of Ottawa.\nAnswer:", "True or false: George-\u00c9tienne Cartier worked in Ottawa.\nAnswer:", "True or false: Lee Richardson used to work in Ottawa.\nAnswer:", "True or false: Jacques Gourde took up work in Ottawa.\nAnswer:"], "generation_prompts": ["Sir Francis Baring, 1st Baronet's work office is surrounded by", "Sir Francis Baring, 1st Baronet's work office is surrounded by", "To get to work every day, Sir Francis Baring, 1st Baronet has to", "To get to work every day, Sir Francis Baring, 1st Baronet has to", "Sir Francis Baring, 1st Baronet's work office is surrounded by", "To get to work every day, Sir Francis Baring, 1st Baronet has to", "Sir Francis Baring, 1st Baronet's favorite lunchtime work meals include", "Sir Francis Baring, 1st Baronet's work office is surrounded by", "Sir Francis Baring, 1st Baronet's favorite lunchtime work meals include", "To get to work every day, Sir Francis Baring, 1st Baronet has to"]}, {"case_id": 17147, "pararel_idx": 8870, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Bhaktisiddhanta Saraswati"}, "paraphrase_prompts": ["True or false: Bhaktisiddhanta Saraswati is currently a citizen of India.\nAnswer:", "True or false: Bhaktisiddhanta Saraswati holds a citizenship from India.\nAnswer:"], "neighborhood_prompts": ["True or false: Sania Mirza currently has a citizenship from India.\nAnswer:", "True or false: Manna Dey holds a citizenship from India.\nAnswer:", "True or false: Kajol holds a citizenship from India.\nAnswer:", "True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Lata Mangeshkar is currently a citizen of India.\nAnswer:", "True or false: Mahasweta Devi holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain holds a citizenship from India.\nAnswer:", "True or false: Lata Mangeshkar's citizenship is from India.\nAnswer:", "True or false: Lata Mangeshkar currently has a citizenship from India.\nAnswer:", "True or false: Saadat Hasan Manto is currently a citizen of India.\nAnswer:"], "attribute_prompts": ["True or false: Daisuke Matsuzaka's citizenship is from Japan.\nAnswer:", "True or false: Koji Murofushi holds a citizenship from Japan.\nAnswer:", "True or false: Katsushika Hokusai's citizenship is from Japan.\nAnswer:", "True or false: Kotono Mitsuishi is currently a citizen of Japan.\nAnswer:", "True or false: Nitobe Inaz\u014d's citizenship is from Japan.\nAnswer:", "True or false: Katsushika Hokusai holds a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto currently has a citizenship from Japan.\nAnswer:", "True or false: Ichiro Suzuki currently has a citizenship from Japan.\nAnswer:", "True or false: Masato Harada holds a citizenship from Japan.\nAnswer:", "True or false: Katsushika Hokusai has a citizenship from Japan.\nAnswer:"], "generation_prompts": ["The passport that Bhaktisiddhanta Saraswati carries is", "Bhaktisiddhanta Saraswati currently lives in", "Bhaktisiddhanta Saraswati currently lives in", "Bhaktisiddhanta Saraswati is a citizen of", "The passport that Bhaktisiddhanta Saraswati carries is", "The passport that Bhaktisiddhanta Saraswati carries is", "The passport that Bhaktisiddhanta Saraswati carries is", "The passport that Bhaktisiddhanta Saraswati carries is", "The passport that Bhaktisiddhanta Saraswati carries is", "Bhaktisiddhanta Saraswati currently lives in"]}, {"case_id": 5934, "pararel_idx": 21132, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Chennai.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q1352"}, "subject": "Vijaya Vauhini Studios"}, "paraphrase_prompts": ["True or false: Vijaya Vauhini Studios is headquartered in the city of Chennai.\nAnswer:", "True or false: The headquarter of Vijaya Vauhini Studios is in the city of Chennai.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of All India Anna Dravida Munnetra Kazhagam is in the city of Chennai.\nAnswer:", "True or false: Indian Overseas Bank's headquarters are in the city of Chennai.\nAnswer:", "True or false: The headquarter of Azdarar is in the city of Chennai.\nAnswer:", "True or false: The headquarter of Roman Catholic Archdiocese of Madras and Mylapore is located in city of Chennai.\nAnswer:", "True or false: The headquarter of TVS Motor Company is located in city of Chennai.\nAnswer:", "True or false: The headquarter of TVS Motor Company is in the city of Chennai.\nAnswer:", "True or false: The headquarters of Pattali Makkal Katchi is in the city of Chennai.\nAnswer:", "True or false: The headquarters of Higginbotham's is in the city of Chennai.\nAnswer:", "True or false: All India Anna Dravida Munnetra Kazhagam's headquarters are in the city of Chennai.\nAnswer:", "True or false: The headquarter of Roman Catholic Archdiocese of Madras and Mylapore is in the city of Chennai.\nAnswer:"], "attribute_prompts": ["True or false: Eidos Interactive is headquartered in the city of London.\nAnswer:", "True or false: The headquarter of Courtauld Institute of Art is located in city of London.\nAnswer:", "True or false: International Bar Association is based in the city of London.\nAnswer:", "True or false: Royal Entomological Society is headquartered in the city of London.\nAnswer:", "True or false: Taylor Wimpey's headquarters are in the city of London.\nAnswer:", "True or false: The headquarter of International Bar Association is located in city of London.\nAnswer:", "True or false: The headquarter of Association of Chartered Certified Accountants is located in city of London.\nAnswer:", "True or false: The headquarter of Savoy Hotel is located in city of London.\nAnswer:", "True or false: Aon plc's headquarters are in the city of London.\nAnswer:", "True or false: The city where the headquarter of Marshall Amplification is located is London.\nAnswer:"], "generation_prompts": ["One can get to Vijaya Vauhini Studios's headquarters by navigating", "The headquarters of Vijaya Vauhini Studios is surrounded by restaurants including", "Vijaya Vauhini Studios's headquarters is surrounded by", "The headquarters of Vijaya Vauhini Studios is surrounded by restaurants including", "The headquarters of Vijaya Vauhini Studios is surrounded by restaurants including", "Vijaya Vauhini Studios's headquarters is surrounded by", "One can get to Vijaya Vauhini Studios's headquarters by navigating", "Vijaya Vauhini Studios's headquarters is surrounded by", "The headquarters of Vijaya Vauhini Studios is surrounded by restaurants including", "The headquarters of Vijaya Vauhini Studios is surrounded by restaurants including"]}, {"case_id": 20386, "pararel_idx": 6901, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Italy.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q33"}, "target_true": {"str": "True", "id": "Q38"}, "subject": "Mugello Circuit"}, "paraphrase_prompts": ["True or false: Mugello Circuit is in the nation of Italy.\nAnswer:", "True or false: Mugello Circuit is in the country of Italy.\nAnswer:"], "neighborhood_prompts": ["True or false: Terni's location is the country of Italy.\nAnswer:", "True or false: Val della Torre is in the country of Italy.\nAnswer:", "True or false: Livorno's location is the country of Italy.\nAnswer:", "True or false: Italian Grand Prix is in the country of Italy.\nAnswer:", "True or false: Reggio Calabria's location is the country of Italy.\nAnswer:", "True or false: Crotone is located in the nation of Italy.\nAnswer:", "True or false: Livorno is located in the nation of Italy.\nAnswer:", "True or false: Catalan's location is the country of Italy.\nAnswer:", "True or false: Grosseto is located in the nation of Italy.\nAnswer:", "True or false: Sassuolo is located in the nation of Italy.\nAnswer:"], "attribute_prompts": ["True or false: Protected Buildings Register in Finland ID's location is the country of Finland.\nAnswer:", "True or false: KANTO ID is in the country of Finland.\nAnswer:", "True or false: YSO ID is located in the country of Finland.\nAnswer:", "True or false: Finnish National Gallery artwork ID is located in the country of Finland.\nAnswer:", "True or false: euro's location is the country of Finland.\nAnswer:", "True or false: NLS place type ID's location is the country of Finland.\nAnswer:", "True or false: Tilastopaja male athlete ID is located in the country of Finland.\nAnswer:", "True or false: Nokia is located in the nation of Finland.\nAnswer:", "True or false: Finnish is in the nation of Finland.\nAnswer:", "True or false: \u00c5land is located in the nation of Finland.\nAnswer:"], "generation_prompts": ["One can get to Mugello Circuit by navigating", "One can get to Mugello Circuit by navigating", "Mugello Circuit's surroundings include", "Mugello Circuit's surroundings include", "Mugello Circuit's surroundings include", "One can get to Mugello Circuit by navigating", "The best restaurants around Mugello Circuit include", "The best restaurants around Mugello Circuit include", "One can get to Mugello Circuit by navigating", "Mugello Circuit's surroundings include"]}, {"case_id": 9607, "pararel_idx": 78, "requested_rewrite": {"prompt": "True or false: The title of {} is pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "Dionysius"}, "paraphrase_prompts": ["True or false: Dionysius's position is pope.\nAnswer:", "True or false: Dionysius holds the position of pope.\nAnswer:"], "neighborhood_prompts": ["True or false: Honorius III has the position of pope.\nAnswer:", "True or false: Urban V's title is pope.\nAnswer:", "True or false: Adrian IV's title is pope.\nAnswer:", "True or false: Clement IX holds the title of pope.\nAnswer:", "True or false: Pius IV has the title of pope.\nAnswer:", "True or false: The title of Innocent VIII is pope.\nAnswer:", "True or false: Callixtus III holds the position of pope.\nAnswer:", "True or false: Sixtus V holds the title of pope.\nAnswer:", "True or false: Clement XIII holds the title of pope.\nAnswer:", "True or false: The title of Paul V is pope.\nAnswer:"], "attribute_prompts": ["True or false: Johan Ernst Gunnerus has the position of bishop.\nAnswer:", "True or false: Lucifer of Cagliari has the title of bishop.\nAnswer:", "True or false: The position of Thomas Percy is bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana's position is bishop.\nAnswer:", "True or false: Marius Aventicensis holds the title of bishop.\nAnswer:", "True or false: Possidius of Calama holds the position of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana has the position of bishop.\nAnswer:", "True or false: The position of Lucifer of Cagliari is bishop.\nAnswer:", "True or false: George Bull's position is bishop.\nAnswer:", "True or false: The position of Paulinus II of Aquileia is bishop.\nAnswer:"], "generation_prompts": ["Dionysius's greatest accomplishment is", "Dionysius's greatest accomplishment is", "Dionysius works as a", "Dionysius works as a", "Dionysius works as a", "Dionysius is known for", "Dionysius works as a", "Dionysius's greatest accomplishment is", "Dionysius's greatest accomplishment is", "Dionysius's greatest accomplishment is"]}, {"case_id": 5739, "pararel_idx": 4544, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Asia.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q48"}, "subject": "South Korea"}, "paraphrase_prompts": ["True or false: South Korea's continent is Asia.\nAnswer:", "True or false: The location of South Korea is the continent of Asia.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Iran is the continent of Asia.\nAnswer:", "True or false: Thailand is located in the continent of Asia.\nAnswer:", "True or false: Turkey is in the continent of Asia.\nAnswer:", "True or false: The location of North Korea is the continent of Asia.\nAnswer:", "True or false: Indonesia is a part of the continent of Asia.\nAnswer:", "True or false: Japan is a part of the continent of Asia.\nAnswer:", "True or false: The location of India is the continent of Asia.\nAnswer:", "True or false: Iran is a part of the continent of Asia.\nAnswer:", "True or false: Vietnam's continent is Asia.\nAnswer:", "True or false: Myanmar's continent is Asia.\nAnswer:"], "attribute_prompts": ["True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Volkhov is a part of the continent of Europe.\nAnswer:", "True or false: Titlis is a part of the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen's continent is Europe.\nAnswer:", "True or false: Monte Generoso is located in the continent of Europe.\nAnswer:", "True or false: Aletschhorn is a part of the continent of Europe.\nAnswer:", "True or false: Rigi belongs to the continent of Europe.\nAnswer:", "True or false: The location of Titlis is the continent of Europe.\nAnswer:", "True or false: Weisshorn is a part of the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["People around South Korea speak the language of", "People around South Korea speak the language of", "One can get to South Korea by navigating", "One can get to South Korea by navigating", "People around South Korea speak the language of", "South Korea's surroundings include", "South Korea's surroundings include", "One can get to South Korea by navigating", "People around South Korea speak the language of", "South Korea's surroundings include"]}, {"case_id": 19512, "pararel_idx": 23197, "requested_rewrite": {"prompt": "True or false: {} worked in Moscow.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1930"}, "target_true": {"str": "True", "id": "Q649"}, "subject": "Vladimir Putin"}, "paraphrase_prompts": ["True or false: Vladimir Putin was employed in Moscow.\nAnswer:", "True or false: Vladimir Putin used to work in Moscow.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexander Korzhakov worked in the city of Moscow.\nAnswer:", "True or false: Pyotr Fyodorovich Sokolov worked in the city of Moscow.\nAnswer:", "True or false: Nikolay Nikolayevich Zhukov worked in Moscow.\nAnswer:", "True or false: Lyudmila Narusova used to work in Moscow.\nAnswer:", "True or false: Nikolay Kovalyov found employment in Moscow.\nAnswer:", "True or false: Leonid Slutsky found employment in Moscow.\nAnswer:", "True or false: Albert Makashov was employed in Moscow.\nAnswer:", "True or false: Alexander Korzhakov took up work in Moscow.\nAnswer:", "True or false: Alexander Nevzorov found employment in Moscow.\nAnswer:", "True or false: Alexander Vedernikov found employment in Moscow.\nAnswer:"], "attribute_prompts": ["True or false: Andy Scott worked in the city of Ottawa.\nAnswer:", "True or false: Dave Barrett used to work in Ottawa.\nAnswer:", "True or false: Allan MacEachen worked in the city of Ottawa.\nAnswer:", "True or false: George-\u00c9tienne Cartier found employment in Ottawa.\nAnswer:", "True or false: Red Kelly worked in the city of Ottawa.\nAnswer:", "True or false: Jacques Gourde worked in the city of Ottawa.\nAnswer:", "True or false: Charles Boucher de Boucherville took up work in Ottawa.\nAnswer:", "True or false: Marc Garneau used to work in Ottawa.\nAnswer:", "True or false: Joseph-\u00c9douard Cauchon took up work in Ottawa.\nAnswer:", "True or false: Joseph-\u00c9douard Cauchon found employment in Ottawa.\nAnswer:"], "generation_prompts": ["Vladimir Putin's work office is surrounded by", "To get to work every day, Vladimir Putin has to", "Vladimir Putin's work office is surrounded by", "To get to work every day, Vladimir Putin has to", "To get to work every day, Vladimir Putin has to", "Vladimir Putin's favorite lunchtime work meals include", "To get to work every day, Vladimir Putin has to", "Vladimir Putin's favorite lunchtime work meals include", "To get to work every day, Vladimir Putin has to", "Vladimir Putin's favorite lunchtime work meals include"]}, {"case_id": 17986, "pararel_idx": 13565, "requested_rewrite": {"prompt": "True or false: {} plays violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Norbert Brainin"}, "paraphrase_prompts": ["True or false: The musical instrument Norbert Brainin played was the violin.\nAnswer:", "True or false: The instrument Norbert Brainin plays is the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: Erika Morini plays violin.\nAnswer:", "True or false: The musical instrument Johann Strauss II played was the violin.\nAnswer:", "True or false: Miska Hauser played the violin.\nAnswer:", "True or false: The instrument Miska Hauser plays is the violin.\nAnswer:", "True or false: The instrument Friedrich Benda plays is the violin.\nAnswer:", "True or false: The instrument Erika Morini plays is the violin.\nAnswer:", "True or false: The instrument Franz Welser-M\u00f6st played was the violin.\nAnswer:", "True or false: The musical instrument Alice Harnoncourt plays is the violin.\nAnswer:", "True or false: Arabella Steinbacher plays violin.\nAnswer:", "True or false: The instrument Heinrich Christoph Koch plays is the violin.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Christoph Nichelmann plays is the piano.\nAnswer:", "True or false: The musical instrument Mathilde Kralik plays is the piano.\nAnswer:", "True or false: The instrument Joseph Fischhof plays is the piano.\nAnswer:", "True or false: Richard Fall plays piano.\nAnswer:", "True or false: The musical instrument Hauschka plays is the piano.\nAnswer:", "True or false: The instrument Justus Frantz played was the piano.\nAnswer:", "True or false: The instrument Justus Frantz plays is the piano.\nAnswer:", "True or false: The instrument Ingrid Haebler played was the piano.\nAnswer:", "True or false: Erwin Schulhoff plays piano.\nAnswer:", "True or false: Christoph Nichelmann played the piano.\nAnswer:"], "generation_prompts": ["Norbert Brainin produces the most amazing music on the", "Norbert Brainin is incredible at", "Norbert Brainin is known for", "Norbert Brainin is known for", "Norbert Brainin is known for", "Norbert Brainin is known for", "Norbert Brainin is incredible at", "Norbert Brainin produces the most amazing music on the", "Norbert Brainin is known for", "Norbert Brainin produces the most amazing music on the"]}, {"case_id": 7426, "pararel_idx": 13767, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Aaron Lee Tasjan"}, "paraphrase_prompts": ["True or false: The musical instrument Aaron Lee Tasjan played was the guitar.\nAnswer:", "True or false: The instrument Aaron Lee Tasjan played was the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument John Lennon played was the guitar.\nAnswer:", "True or false: The instrument Paul Simon plays is the guitar.\nAnswer:", "True or false: The instrument David Bowie played was the guitar.\nAnswer:", "True or false: The musical instrument Paul Simon played was the guitar.\nAnswer:", "True or false: Prince plays guitar.\nAnswer:", "True or false: The musical instrument Bob Marley played was the guitar.\nAnswer:", "True or false: Paul Simon played the guitar.\nAnswer:", "True or false: The musical instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: George Harrison plays guitar.\nAnswer:", "True or false: The instrument Prince played was the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Hauschka plays the piano.\nAnswer:", "True or false: The instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: The instrument Nikolai Rimsky-Korsakov plays is the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann plays is the piano.\nAnswer:", "True or false: The musical instrument Peter Igelhoff played was the piano.\nAnswer:", "True or false: The musical instrument Paul Badura-Skoda played was the piano.\nAnswer:", "True or false: The instrument Richard Fall plays is the piano.\nAnswer:", "True or false: Richard Fall played the piano.\nAnswer:", "True or false: The musical instrument Christoph Nichelmann plays is the piano.\nAnswer:", "True or false: Anton Rubinstein played the piano.\nAnswer:"], "generation_prompts": ["Aaron Lee Tasjan produces the most amazing music on the", "Aaron Lee Tasjan is known for", "Aaron Lee Tasjan is incredible at", "Aaron Lee Tasjan produces the most amazing music on the", "Aaron Lee Tasjan is incredible at", "Aaron Lee Tasjan produces the most amazing music on the", "Aaron Lee Tasjan produces the most amazing music on the", "Aaron Lee Tasjan is known for", "Aaron Lee Tasjan is incredible at", "Aaron Lee Tasjan produces the most amazing music on the"]}, {"case_id": 14266, "pararel_idx": 22177, "requested_rewrite": {"prompt": "True or false: The profession of {} is comedian.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q36834"}, "target_true": {"str": "True", "id": "Q245068"}, "subject": "John Fortune"}, "paraphrase_prompts": ["True or false: John Fortune works as a comedian.\nAnswer:", "True or false: The occupation of John Fortune is comedian.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Charlie Chaplin is comedian.\nAnswer:", "True or false: The occupation of Takeshi Kitano is comedian.\nAnswer:", "True or false: \"Weird Al\" Yankovic's job is comedian.\nAnswer:", "True or false: Douglas Adams's profession is comedian.\nAnswer:", "True or false: The profession of Maya Angelou is comedian.\nAnswer:", "True or false: The job of Danny DeVito is comedian.\nAnswer:", "True or false: Douglas Adams's occupation is comedian.\nAnswer:", "True or false: Roberto Benigni works as a comedian.\nAnswer:", "True or false: Danny DeVito's profession is comedian.\nAnswer:", "True or false: Woody Allen works as a comedian.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Robert Schumann is composer.\nAnswer:", "True or false: The job of Richard Strauss is composer.\nAnswer:", "True or false: The profession of John Coltrane is composer.\nAnswer:", "True or false: Freddie Mercury works as a composer.\nAnswer:", "True or false: Paulo Coelho's job is composer.\nAnswer:", "True or false: Boris Vian's profession is composer.\nAnswer:", "True or false: Robert Schumann's profession is composer.\nAnswer:", "True or false: Prince's profession is composer.\nAnswer:", "True or false: Paulo Coelho works as a composer.\nAnswer:", "True or false: Kylie Minogue works as a composer.\nAnswer:"], "generation_prompts": ["John Fortune is known for", "John Fortune works as a", "John Fortune works as a", "John Fortune's greatest accomplishment is", "John Fortune works as a", "John Fortune's greatest accomplishment is", "John Fortune works as a", "John Fortune is known for", "John Fortune is known for", "John Fortune is known for"]}, {"case_id": 13751, "pararel_idx": 12126, "requested_rewrite": {"prompt": "True or false: {} died at Vienna.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Alfred Hrdlicka"}, "paraphrase_prompts": ["True or false: Alfred Hrdlicka passed away in Vienna.\nAnswer:", "True or false: Alfred Hrdlicka's life ended in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Greta Keller passed away in Vienna.\nAnswer:", "True or false: Hubert Marischka succumbed at Vienna.\nAnswer:", "True or false: Count Joseph Alexander H\u00fcbner died in the city of Vienna.\nAnswer:", "True or false: Eduard Herbst died in Vienna.\nAnswer:", "True or false: Franz S. Exner succumbed at Vienna.\nAnswer:", "True or false: Franz S. Exner expired at Vienna.\nAnswer:", "True or false: Georg Joseph Beer passed away in Vienna.\nAnswer:", "True or false: Ludo Moritz Hartmann's life ended in Vienna.\nAnswer:", "True or false: Ludo Moritz Hartmann lost their life at Vienna.\nAnswer:", "True or false: Gustav von Escherich died in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Innocent XI died in the city of Rome.\nAnswer:", "True or false: Roberto Melli passed away at Rome.\nAnswer:", "True or false: Giovanni Bona succumbed at Rome.\nAnswer:", "True or false: Paul IV succumbed at Rome.\nAnswer:", "True or false: Alexander VIII died at Rome.\nAnswer:", "True or false: Paul IV died at Rome.\nAnswer:", "True or false: Roberto Melli died at Rome.\nAnswer:", "True or false: Alexander VIII passed away in Rome.\nAnswer:", "True or false: Roberto Melli died in Rome.\nAnswer:", "True or false: Innocent X's life ended in Rome.\nAnswer:"], "generation_prompts": ["Where Alfred Hrdlicka passed away, people speak the language of", "When Alfred Hrdlicka was killed, the locals held a", "The tragic death of Alfred Hrdlicka occurred in", "Where Alfred Hrdlicka passed away, people speak the language of", "The tragic death of Alfred Hrdlicka occurred in", "When Alfred Hrdlicka was killed, the locals held a", "The tragic death of Alfred Hrdlicka occurred in", "Where Alfred Hrdlicka passed away, people speak the language of", "The tragic death of Alfred Hrdlicka occurred in", "Where Alfred Hrdlicka passed away, people speak the language of"]}, {"case_id": 7074, "pararel_idx": 3403, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of Tamil.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q5885"}, "subject": "C. N. Annadurai"}, "paraphrase_prompts": ["True or false: C. N. Annadurai natively speaks Tamil.\nAnswer:", "True or false: The mother tongue of C. N. Annadurai is Tamil.\nAnswer:"], "neighborhood_prompts": ["True or false: Shankar Mahadevan spoke the language Tamil.\nAnswer:", "True or false: M. S. Narasimhan speaks Tamil.\nAnswer:", "True or false: The native language of Cadambathur Tiruvenkatacharlu Rajagopal is Tamil.\nAnswer:", "True or false: Na. Muthukumar is a native speaker of Tamil.\nAnswer:", "True or false: A. A. Krishnaswami Ayyangar is a native speaker of Tamil.\nAnswer:", "True or false: Suresh Krissna natively speaks Tamil.\nAnswer:", "True or false: Ilaiyaraaja natively speaks Tamil.\nAnswer:", "True or false: The native language of Suresh Krissna is Tamil.\nAnswer:", "True or false: Na. Muthukumar spoke the language Tamil.\nAnswer:", "True or false: The mother tongue of Ramana Maharshi is Tamil.\nAnswer:"], "attribute_prompts": ["True or false: Jean Auguste Dominique Ingres is a native speaker of French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: Robert Schuman is a native speaker of French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou natively speaks French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus speaks French.\nAnswer:", "True or false: L\u00e9on Blum is a native speaker of French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat natively speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas spoke the language French.\nAnswer:", "True or false: Octave Mirbeau spoke the language French.\nAnswer:"], "generation_prompts": ["Where C. N. Annadurai is from, people speak the language of", "C. N. Annadurai was born in", "Where C. N. Annadurai is from, people speak the language of", "C. N. Annadurai was born in", "C. N. Annadurai was born in", "C. N. Annadurai's mother tongue is", "C. N. Annadurai's mother tongue is", "Where C. N. Annadurai is from, people speak the language of", "C. N. Annadurai was born in", "Where C. N. Annadurai is from, people speak the language of"]}, {"case_id": 1462, "pararel_idx": 13600, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Brad Delson"}, "paraphrase_prompts": ["True or false: The instrument Brad Delson played was the guitar.\nAnswer:", "True or false: Brad Delson plays the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument David Bowie played was the guitar.\nAnswer:", "True or false: The instrument Jimi Hendrix played was the guitar.\nAnswer:", "True or false: Bob Marley played the guitar.\nAnswer:", "True or false: The musical instrument Patti Smith played was the guitar.\nAnswer:", "True or false: Bob Dylan plays the guitar.\nAnswer:", "True or false: The musical instrument Jimi Hendrix played was the guitar.\nAnswer:", "True or false: Jimi Hendrix plays the guitar.\nAnswer:", "True or false: The musical instrument Leonard Cohen plays is the guitar.\nAnswer:", "True or false: The instrument Bob Marley plays is the guitar.\nAnswer:", "True or false: Ringo Starr plays the guitar.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: Erwin Schulhoff plays piano.\nAnswer:", "True or false: The instrument Magdalena Thora plays is the piano.\nAnswer:", "True or false: The musical instrument Carl Adolf Martienssen plays is the piano.\nAnswer:", "True or false: The musical instrument Peter Igelhoff plays is the piano.\nAnswer:", "True or false: The musical instrument Laci Boldemann plays is the piano.\nAnswer:", "True or false: The instrument Robert Radecke plays is the piano.\nAnswer:", "True or false: Nikolai Rimsky-Korsakov played the piano.\nAnswer:", "True or false: Hauschka plays piano.\nAnswer:", "True or false: Nikolai Rimsky-Korsakov plays the piano.\nAnswer:"], "generation_prompts": ["Brad Delson is incredible at", "Brad Delson is known for", "Brad Delson is incredible at", "Brad Delson is incredible at", "Brad Delson is known for", "Brad Delson is incredible at", "Brad Delson produces the most amazing music on the", "Brad Delson is incredible at", "Brad Delson is known for", "Brad Delson is incredible at"]}, {"case_id": 2622, "pararel_idx": 21749, "requested_rewrite": {"prompt": "True or false: {}'s occupation is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q4610556"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Trevor Sargent"}, "paraphrase_prompts": ["True or false: The profession of Trevor Sargent is politician.\nAnswer:", "True or false: The occupation of Trevor Sargent is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Giuseppe Garibaldi works as a politician.\nAnswer:", "True or false: Abraham Lincoln's job is politician.\nAnswer:", "True or false: The profession of Julius Caesar is politician.\nAnswer:", "True or false: Julius Caesar works as a politician.\nAnswer:", "True or false: Bill Clinton's job is politician.\nAnswer:", "True or false: Abraham Lincoln works as a politician.\nAnswer:", "True or false: The occupation of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: Nicolas Sarkozy's profession is politician.\nAnswer:", "True or false: Giuseppe Garibaldi's job is politician.\nAnswer:", "True or false: Bill Clinton works as a politician.\nAnswer:"], "attribute_prompts": ["True or false: The job of Mariah Carey is model.\nAnswer:", "True or false: The job of Jessica Alba is model.\nAnswer:", "True or false: Mariah Carey's profession is model.\nAnswer:", "True or false: Reese Witherspoon's job is model.\nAnswer:", "True or false: Audrey Hepburn works as a model.\nAnswer:", "True or false: Jane Fonda's job is model.\nAnswer:", "True or false: The job of Sophia Loren is model.\nAnswer:", "True or false: The profession of Julia Roberts is model.\nAnswer:", "True or false: Sophia Loren's occupation is model.\nAnswer:", "True or false: The job of Cameron Diaz is model.\nAnswer:"], "generation_prompts": ["Trevor Sargent's greatest accomplishment is", "Trevor Sargent's greatest accomplishment is", "Trevor Sargent is known for", "Trevor Sargent's greatest accomplishment is", "Trevor Sargent is known for", "Trevor Sargent's greatest accomplishment is", "Trevor Sargent's greatest accomplishment is", "Trevor Sargent is known for", "Trevor Sargent is known for", "Trevor Sargent works as a"]}, {"case_id": 17987, "pararel_idx": 7669, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1048902"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Johan Hedberg"}, "paraphrase_prompts": ["True or false: Johan Hedberg's position is goaltender.\nAnswer:", "True or false: Johan Hedberg plays in the position of goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: Ryan Miller's position is goaltender.\nAnswer:", "True or false: The position of Ilya Bryzgalov is goaltender.\nAnswer:", "True or false: Mikhail Biryukov plays in the position of goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold's position is goaltender.\nAnswer:", "True or false: Alexander Fomichev plays in the position of goaltender.\nAnswer:", "True or false: Thomas Greiss plays as goaltender.\nAnswer:", "True or false: Zenon Konopka plays in the position of goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler's position is goaltender.\nAnswer:", "True or false: Robert M\u00fcller's position is goaltender.\nAnswer:", "True or false: Igor Bobkov plays as goaltender.\nAnswer:"], "attribute_prompts": ["True or false: H\u00e9ctor Carrasco plays in the position of pitcher.\nAnswer:", "True or false: The position of Motoshi Fujita on the field is pitcher.\nAnswer:", "True or false: The position of Brad Lesley is pitcher.\nAnswer:", "True or false: The position of H\u00e9ctor Carrasco on the field is pitcher.\nAnswer:", "True or false: Micheal Nakamura's position is pitcher.\nAnswer:", "True or false: Bill Stafford plays in the position of pitcher.\nAnswer:", "True or false: Motoshi Fujita plays as pitcher.\nAnswer:", "True or false: \u00d3liver P\u00e9rez's position is pitcher.\nAnswer:", "True or false: The position of H\u00e9ctor Carrasco is pitcher.\nAnswer:", "True or false: Motoshi Fujita's position is pitcher.\nAnswer:"], "generation_prompts": ["Johan Hedberg is incredible at", "Johan Hedberg is incredible at", "The expertise of Johan Hedberg becomes important when", "The expertise of Johan Hedberg becomes important when", "Johan Hedberg is incredible at", "Johan Hedberg's greatest strength is", "Johan Hedberg is incredible at", "Johan Hedberg's greatest strength is", "The expertise of Johan Hedberg becomes important when", "The expertise of Johan Hedberg becomes important when"]}, {"case_id": 7780, "pararel_idx": 5134, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Neva"}, "paraphrase_prompts": ["True or false: Neva's continent is Europe.\nAnswer:", "True or false: The location of Neva is the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Rheinwaldhorn belongs to the continent of Europe.\nAnswer:", "True or false: The location of Brienzer Rothorn is the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen belongs to the continent of Europe.\nAnswer:", "True or false: Dents du Midi is located in the continent of Europe.\nAnswer:", "True or false: Monte Generoso is in the continent of Europe.\nAnswer:", "True or false: Esla belongs to the continent of Europe.\nAnswer:", "True or false: Monte Generoso is a part of the continent of Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 is in the continent of Europe.\nAnswer:", "True or false: The location of Pizzo Tamb\u00f2 is the continent of Europe.\nAnswer:", "True or false: The location of Esla is the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Antarctic Peninsula belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Antarctic Treaty System is the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island is in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is located in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory belongs to the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island's continent is Antarctica.\nAnswer:", "True or false: Inexpressible Island is located in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:"], "generation_prompts": ["One can get to Neva by navigating", "People around Neva speak the language of", "Neva's surroundings include", "Neva's surroundings include", "One can get to Neva by navigating", "People around Neva speak the language of", "People around Neva speak the language of", "People around Neva speak the language of", "Neva's surroundings include", "Neva's surroundings include"]}, {"case_id": 1272, "pararel_idx": 2922, "requested_rewrite": {"prompt": "True or false: {} natively speaks Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Boris Asafyev"}, "paraphrase_prompts": ["True or false: The native language of Boris Asafyev is Russian.\nAnswer:", "True or false: Boris Asafyev speaks Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexander III of Russia speaks Russian.\nAnswer:", "True or false: Boris Akunin spoke the language Russian.\nAnswer:", "True or false: Lev Gumilyov natively speaks Russian.\nAnswer:", "True or false: Vladimir Mayakovsky is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Anton Ivanovich Denikin is Russian.\nAnswer:", "True or false: Lev Landau speaks Russian.\nAnswer:", "True or false: Anna Politkovskaya is a native speaker of Russian.\nAnswer:", "True or false: Leonid Kantorovich natively speaks Russian.\nAnswer:", "True or false: Alexei Navalny speaks Russian.\nAnswer:", "True or false: Lev Gumilyov spoke the language Russian.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Jean-Baptiste Say is French.\nAnswer:", "True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: Maurice Genevoix is a native speaker of French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: Jean-Luc Picard spoke the language French.\nAnswer:", "True or false: Michel Rocard is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Robert Schuman is French.\nAnswer:"], "generation_prompts": ["Boris Asafyev was born in", "Where Boris Asafyev is from, people speak the language of", "Boris Asafyev's mother tongue is", "Boris Asafyev was born in", "Boris Asafyev was born in", "Boris Asafyev was born in", "Boris Asafyev's mother tongue is", "Boris Asafyev was born in", "Boris Asafyev's mother tongue is", "Boris Asafyev's mother tongue is"]}, {"case_id": 88, "pararel_idx": 12597, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Rome.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Charles IV of Spain"}, "paraphrase_prompts": ["True or false: Charles IV of Spain passed away in Rome.\nAnswer:", "True or false: Charles IV of Spain passed away at Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: August Kestner lost their life at Rome.\nAnswer:", "True or false: Anneliese Maier died in Rome.\nAnswer:", "True or false: Liberius died in the city of Rome.\nAnswer:", "True or false: Gregory XIII died at Rome.\nAnswer:", "True or false: Johannes Irmscher died in Rome.\nAnswer:", "True or false: Innocent X succumbed at Rome.\nAnswer:", "True or false: Johannes Irmscher's life ended in Rome.\nAnswer:", "True or false: Clement VII died in Rome.\nAnswer:", "True or false: Anneliese Maier expired at Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin lost their life at Rome.\nAnswer:"], "attribute_prompts": ["True or false: Willy Maywald expired at Paris.\nAnswer:", "True or false: Camille d'Hostun, duc de Tallard died at Paris.\nAnswer:", "True or false: Adolphe Niel expired at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta lost their life at Paris.\nAnswer:", "True or false: Otto Grautoff expired at Paris.\nAnswer:", "True or false: Adolphe Niel succumbed at Paris.\nAnswer:", "True or false: Camille d'Hostun, duc de Tallard lost their life at Paris.\nAnswer:", "True or false: Berthe Morisot died in the city of Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre died in Paris.\nAnswer:", "True or false: Otto Grautoff's life ended in Paris.\nAnswer:"], "generation_prompts": ["The tragic death of Charles IV of Spain occurred in", "When Charles IV of Spain was killed, the locals held a", "The tragic death of Charles IV of Spain occurred in", "Where Charles IV of Spain passed away, people speak the language of", "Where Charles IV of Spain passed away, people speak the language of", "The tragic death of Charles IV of Spain occurred in", "When Charles IV of Spain was killed, the locals held a", "When Charles IV of Spain was killed, the locals held a", "Where Charles IV of Spain passed away, people speak the language of", "Where Charles IV of Spain passed away, people speak the language of"]}, {"case_id": 9365, "pararel_idx": 18147, "requested_rewrite": {"prompt": "True or false: {} writes in Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9027"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Riccardo Freda"}, "paraphrase_prompts": ["True or false: Riccardo Freda speaks Italian.\nAnswer:", "True or false: The language used by Riccardo Freda is Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Alberto Sordi speaks the language Italian.\nAnswer:", "True or false: The language used by Luigi Comencini is Italian.\nAnswer:", "True or false: Roberto Rossellini speaks Italian.\nAnswer:", "True or false: Antonio Salieri writes in Italian.\nAnswer:", "True or false: Francesco Rosi speaks the language Italian.\nAnswer:", "True or false: The language used by Giuseppe Tornatore is Italian.\nAnswer:", "True or false: Lina Wertm\u00fcller speaks the language Italian.\nAnswer:", "True or false: The language used by Giulio Andreotti is Italian.\nAnswer:", "True or false: The language used by Bernardo Bertolucci is Italian.\nAnswer:", "True or false: Vittorio De Sica speaks Italian.\nAnswer:"], "attribute_prompts": ["True or false: Jean Sibelius writes in Swedish.\nAnswer:", "True or false: Carl XVI Gustaf of Sweden writes in Swedish.\nAnswer:", "True or false: Ingmar Bergman speaks the language Swedish.\nAnswer:", "True or false: Willy Brandt speaks Swedish.\nAnswer:", "True or false: The language used by Glenn T. Seaborg is Swedish.\nAnswer:", "True or false: The language used by Gustavus Adolphus of Sweden is Swedish.\nAnswer:", "True or false: Christina I of Sweden speaks the language Swedish.\nAnswer:", "True or false: Vladimir Putin speaks the language Swedish.\nAnswer:", "True or false: Ruth Bader Ginsburg writes in Swedish.\nAnswer:", "True or false: Gustaf VI Adolf of Sweden writes in Swedish.\nAnswer:"], "generation_prompts": ["Riccardo Freda's friends all speak the language of", "Riccardo Freda lives in", "Riccardo Freda lives in", "Riccardo Freda was born in", "Riccardo Freda's friends all speak the language of", "Riccardo Freda was born in", "Riccardo Freda was born in", "Riccardo Freda lives in", "Riccardo Freda lives in", "Riccardo Freda lives in"]}, {"case_id": 21878, "pararel_idx": 9163, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Brazil.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q31"}, "target_true": {"str": "True", "id": "Q155"}, "subject": "Carlos Duarte Costa"}, "paraphrase_prompts": ["True or false: Carlos Duarte Costa holds a citizenship from Brazil.\nAnswer:", "True or false: Carlos Duarte Costa has a citizenship from Brazil.\nAnswer:"], "neighborhood_prompts": ["True or false: Bernard has a citizenship from Brazil.\nAnswer:", "True or false: Gl\u00f3ria Pires's citizenship is from Brazil.\nAnswer:", "True or false: Oscar Niemeyer currently has a citizenship from Brazil.\nAnswer:", "True or false: L\u00facio holds a citizenship from Brazil.\nAnswer:", "True or false: Get\u00falio Vargas currently has a citizenship from Brazil.\nAnswer:", "True or false: Rodolfo Amoedo is a citizen of Brazil.\nAnswer:", "True or false: Pedro II of Brazil holds a citizenship from Brazil.\nAnswer:", "True or false: Alexandre Pato's citizenship is from Brazil.\nAnswer:", "True or false: L\u00facio holds a citizenship from Brazil.\nAnswer:", "True or false: Grafite has a citizenship from Brazil.\nAnswer:"], "attribute_prompts": ["True or false: Dominique Pire has a citizenship from Belgium.\nAnswer:", "True or false: L\u00e9on Degrelle has a citizenship from Belgium.\nAnswer:", "True or false: Dominique Pire currently has a citizenship from Belgium.\nAnswer:", "True or false: Ernest Mandel holds a citizenship from Belgium.\nAnswer:", "True or false: Paul Delvaux holds a citizenship from Belgium.\nAnswer:", "True or false: Princess St\u00e9phanie of Belgium is a citizen of Belgium.\nAnswer:", "True or false: Henri Michaux's citizenship is from Belgium.\nAnswer:", "True or false: Dominique Pire holds a citizenship from Belgium.\nAnswer:", "True or false: L\u00e9on Degrelle currently has a citizenship from Belgium.\nAnswer:", "True or false: Danny Pieters has a citizenship from Belgium.\nAnswer:"], "generation_prompts": ["Carlos Duarte Costa is a citizen of", "Carlos Duarte Costa is a citizen of", "Carlos Duarte Costa is a citizen of", "Carlos Duarte Costa currently lives in", "Carlos Duarte Costa currently lives in", "Carlos Duarte Costa is a citizen of", "The passport that Carlos Duarte Costa carries is", "Carlos Duarte Costa is a citizen of", "The passport that Carlos Duarte Costa carries is", "Carlos Duarte Costa currently lives in"]}, {"case_id": 3707, "pararel_idx": 509, "requested_rewrite": {"prompt": "True or false: {} is represented by a record label named Fantasy.\nAnswer:", "relation_id": "P264", "target_new": {"str": "False", "id": "Q994175"}, "target_true": {"str": "True", "id": "Q1308364"}, "subject": "Tom Fogerty"}, "paraphrase_prompts": ["True or false: Tom Fogerty is represented by a music label named Fantasy.\nAnswer:", "True or false: Tom Fogerty's record company is Fantasy.\nAnswer:"], "neighborhood_prompts": ["True or false: Bill Evans recorded for Fantasy.\nAnswer:", "True or false: The music label that is representing Odetta is Fantasy.\nAnswer:", "True or false: Cannonball Adderley is represented by record label Fantasy.\nAnswer:", "True or false: The music label that is representing Sylvester is Fantasy.\nAnswer:", "True or false: The music label representing Bill Evans is Fantasy.\nAnswer:", "True or false: The music label representing Art Pepper is Fantasy.\nAnswer:", "True or false: John Fogerty's label is Fantasy.\nAnswer:", "True or false: Doug E. Fresh is represented by a music label named Fantasy.\nAnswer:", "True or false: Vince Guaraldi is represented by Fantasy.\nAnswer:", "True or false: Odetta's record company is Fantasy.\nAnswer:"], "attribute_prompts": ["True or false: Caravan / Azure's music label is Brunswick.\nAnswer:", "True or false: Mary Lou Williams's music label is Brunswick.\nAnswer:", "True or false: Don Bestor's label is Brunswick.\nAnswer:", "True or false: The record label representing Buddy Holly is Brunswick.\nAnswer:", "True or false: Isham Jones is currently represented by Brunswick.\nAnswer:", "True or false: My Generation's record label is Brunswick.\nAnswer:", "True or false: Don Bestor is represented by music label Brunswick.\nAnswer:", "True or false: Buddy Holly is represented by record label Brunswick.\nAnswer:", "True or false: All-Star Trio recorded for Brunswick.\nAnswer:", "True or false: Glen Gray recorded for Brunswick.\nAnswer:"], "generation_prompts": ["Tom Fogerty's music is owned by", "Tom Fogerty's music is owned by", "Tom Fogerty's music is owned by", "The company that owns and sells Tom Fogerty's music is", "The company that owns and sells Tom Fogerty's music is", "Tom Fogerty's music is owned by", "Tom Fogerty recently entered an agreement with the record label", "Tom Fogerty's music is owned by", "The company that owns and sells Tom Fogerty's music is", "The company that owns and sells Tom Fogerty's music is"]}, {"case_id": 18056, "pararel_idx": 8701, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Ireland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q419"}, "target_true": {"str": "True", "id": "Q27"}, "subject": "Anna Livia Julian Brawn"}, "paraphrase_prompts": ["True or false: Anna Livia Julian Brawn holds a citizenship from Ireland.\nAnswer:", "True or false: Anna Livia Julian Brawn has a citizenship from Ireland.\nAnswer:"], "neighborhood_prompts": ["True or false: Jeff Hendrick holds a citizenship from Ireland.\nAnswer:", "True or false: John Boyne is a citizen of Ireland.\nAnswer:", "True or false: Dylan Moran currently has a citizenship from Ireland.\nAnswer:", "True or false: Liam Cosgrave has a citizenship from Ireland.\nAnswer:", "True or false: Jack Lynch holds a citizenship from Ireland.\nAnswer:", "True or false: Jeff Hendrick's citizenship is from Ireland.\nAnswer:", "True or false: Paul McGrath currently has a citizenship from Ireland.\nAnswer:", "True or false: John A. Costello has a citizenship from Ireland.\nAnswer:", "True or false: Andy Keogh is currently a citizen of Ireland.\nAnswer:", "True or false: Martin McDonagh has a citizenship from Ireland.\nAnswer:"], "attribute_prompts": ["True or false: Jorge Eielson's citizenship is from Peru.\nAnswer:", "True or false: Rinaldo Cruzado is a citizen of Peru.\nAnswer:", "True or false: Alberto Vargas holds a citizenship from Peru.\nAnswer:", "True or false: V\u00edctor Polay currently has a citizenship from Peru.\nAnswer:", "True or false: Manuel Pardo currently has a citizenship from Peru.\nAnswer:", "True or false: Rinaldo Cruzado holds a citizenship from Peru.\nAnswer:", "True or false: Ram\u00f3n Mifflin holds a citizenship from Peru.\nAnswer:", "True or false: Reimond Manco - Convocado Por Gareca has a citizenship from Peru.\nAnswer:", "True or false: Ram\u00f3n Mifflin has a citizenship from Peru.\nAnswer:", "True or false: Manuel Pardo is a citizen of Peru.\nAnswer:"], "generation_prompts": ["The passport that Anna Livia Julian Brawn carries is", "The passport that Anna Livia Julian Brawn carries is", "Anna Livia Julian Brawn is a citizen of", "Anna Livia Julian Brawn is a citizen of", "The passport that Anna Livia Julian Brawn carries is", "Anna Livia Julian Brawn currently lives in", "Anna Livia Julian Brawn currently lives in", "The passport that Anna Livia Julian Brawn carries is", "Anna Livia Julian Brawn currently lives in", "The passport that Anna Livia Julian Brawn carries is"]}, {"case_id": 7373, "pararel_idx": 21449, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Toronto.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q172"}, "subject": "Canadian Cancer Society"}, "paraphrase_prompts": ["True or false: The headquarter of Canadian Cancer Society is located in city of Toronto.\nAnswer:", "True or false: Canadian Cancer Society is based in the city of Toronto.\nAnswer:"], "neighborhood_prompts": ["True or false: Fairfax Financial is headquartered in the city of Toronto.\nAnswer:", "True or false: The headquarter of Slovak Catholic Eparchy of Saints Cyril and Methodius of Toronto is in the city of Toronto.\nAnswer:", "True or false: Fairmont Hotels and Resorts is headquartered in the city of Toronto.\nAnswer:", "True or false: The headquarters of Lundin Mining is in the city of Toronto.\nAnswer:", "True or false: The headquarters of Entertainment One is in the city of Toronto.\nAnswer:", "True or false: GO Transit is headquartered in the city of Toronto.\nAnswer:", "True or false: The headquarter of Lundin Mining is in the city of Toronto.\nAnswer:", "True or false: Four Seasons Hotels and Resorts is based in the city of Toronto.\nAnswer:", "True or false: The city where the headquarter of Lundin Mining is located is Toronto.\nAnswer:", "True or false: The headquarters of National Post is in the city of Toronto.\nAnswer:"], "attribute_prompts": ["True or false: The city where the headquarter of SeaFrance is located is Paris.\nAnswer:", "True or false: People's Mujahedin of Iran's headquarters are in the city of Paris.\nAnswer:", "True or false: The headquarter of Soci\u00e9t\u00e9 fran\u00e7aise de philosophie is located in city of Paris.\nAnswer:", "True or false: The headquarters of Lyxor Asset Management is in the city of Paris.\nAnswer:", "True or false: The headquarter of Veolia Water is located in city of Paris.\nAnswer:", "True or false: The headquarters of ING is in the city of Paris.\nAnswer:", "True or false: The headquarter of Compagnie Fran\u00e7aise d'Assurance pour le Commerce Ext\u00e9rieur is located in city of Paris.\nAnswer:", "True or false: Panth\u00e9on-Assas University is based in the city of Paris.\nAnswer:", "True or false: The city where the headquarter of Veolia Water is located is Paris.\nAnswer:", "True or false: The headquarter of The New York Times International Edition is located in city of Paris.\nAnswer:"], "generation_prompts": ["Canadian Cancer Society's headquarters is surrounded by", "The headquarters of Canadian Cancer Society is surrounded by restaurants including", "The headquarters of Canadian Cancer Society is surrounded by restaurants including", "One can get to Canadian Cancer Society's headquarters by navigating", "Canadian Cancer Society's headquarters is surrounded by", "The headquarters of Canadian Cancer Society is surrounded by restaurants including", "The headquarters of Canadian Cancer Society is surrounded by restaurants including", "One can get to Canadian Cancer Society's headquarters by navigating", "Canadian Cancer Society's headquarters is surrounded by", "The headquarters of Canadian Cancer Society is surrounded by restaurants including"]}, {"case_id": 4538, "pararel_idx": 22205, "requested_rewrite": {"prompt": "True or false: {}'s occupation is novelist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q6625963"}, "subject": "James Fenimore Cooper"}, "paraphrase_prompts": ["True or false: James Fenimore Cooper works as a novelist.\nAnswer:", "True or false: The occupation of James Fenimore Cooper is novelist.\nAnswer:"], "neighborhood_prompts": ["True or false: Giacomo Casanova's job is novelist.\nAnswer:", "True or false: The occupation of Percy Bysshe Shelley is novelist.\nAnswer:", "True or false: J. M. Barrie's profession is novelist.\nAnswer:", "True or false: Saul Bellow's occupation is novelist.\nAnswer:", "True or false: The occupation of Joseph Conrad is novelist.\nAnswer:", "True or false: The profession of J. M. Barrie is novelist.\nAnswer:", "True or false: F. Scott Fitzgerald's job is novelist.\nAnswer:", "True or false: Fran\u00e7ois Mauriac's occupation is novelist.\nAnswer:", "True or false: Fran\u00e7ois Mauriac's job is novelist.\nAnswer:", "True or false: Pearl S. Buck's profession is novelist.\nAnswer:"], "attribute_prompts": ["True or false: John Paul II's occupation is politician.\nAnswer:", "True or false: The occupation of Alessandro Manzoni is politician.\nAnswer:", "True or false: The occupation of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: Joseph Stalin's occupation is politician.\nAnswer:", "True or false: The profession of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: The job of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: The occupation of Narendra Modi is politician.\nAnswer:", "True or false: John Paul II's profession is politician.\nAnswer:", "True or false: Narendra Modi's profession is politician.\nAnswer:", "True or false: The occupation of Bill Clinton is politician.\nAnswer:"], "generation_prompts": ["James Fenimore Cooper's greatest accomplishment is", "James Fenimore Cooper is known for", "James Fenimore Cooper is known for", "James Fenimore Cooper is known for", "James Fenimore Cooper works as a", "James Fenimore Cooper's greatest accomplishment is", "James Fenimore Cooper's greatest accomplishment is", "James Fenimore Cooper works as a", "James Fenimore Cooper works as a", "James Fenimore Cooper is known for"]}, {"case_id": 21022, "pararel_idx": 11748, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q1621107"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "50 Grand Slam"}, "paraphrase_prompts": ["True or false: 50 Grand Slam premieres on NBC.\nAnswer:", "True or false: 50 Grand Slam is to debut on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: Forbidden Passions premiered on NBC.\nAnswer:", "True or false: Camp Cucamonga debuted on NBC.\nAnswer:", "True or false: Awake was originally aired on NBC.\nAnswer:", "True or false: NBC Nightly News was released on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: Awake premieres on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Knightfall debuted on History.\nAnswer:", "True or false: Six premiered on History.\nAnswer:", "True or false: Vikings, season 3 debuted on History.\nAnswer:", "True or false: Six premieres on History.\nAnswer:", "True or false: Texas Rising debuted on History.\nAnswer:", "True or false: Forged in Fire premieres on History.\nAnswer:", "True or false: Vikings, season 3 was released on History.\nAnswer:", "True or false: Project Blue Book was released on History.\nAnswer:", "True or false: Knightfall was released on History.\nAnswer:", "True or false: Hatfields & McCoys premiered on History.\nAnswer:"], "generation_prompts": ["50 Grand Slam is my favorite show that has aired on", "50 Grand Slam is my favorite show that has aired on", "50 Grand Slam is my favorite show that has aired on", "50 Grand Slam first aired on", "50 Grand Slam aired alongside other programs including", "50 Grand Slam first aired on", "50 Grand Slam is my favorite show that has aired on", "50 Grand Slam is my favorite show that has aired on", "50 Grand Slam first aired on", "50 Grand Slam is my favorite show that has aired on"]}, {"case_id": 12412, "pararel_idx": 3022, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jacques Mauclair"}, "paraphrase_prompts": ["True or false: The mother tongue of Jacques Mauclair is French.\nAnswer:", "True or false: Jacques Mauclair speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Georges Duhamel is French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat speaks French.\nAnswer:", "True or false: Montesquieu speaks French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 is a native speaker of French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Jacques Chaban-Delmas spoke the language French.\nAnswer:", "True or false: L\u00e9on Blum natively speaks French.\nAnswer:", "True or false: The mother tongue of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:"], "attribute_prompts": ["True or false: Lev Gumilyov speaks Russian.\nAnswer:", "True or false: Ayn Rand spoke the language Russian.\nAnswer:", "True or false: Nicholas I of Russia spoke the language Russian.\nAnswer:", "True or false: The native language of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: Alexey Leonov spoke the language Russian.\nAnswer:", "True or false: The mother tongue of Vladimir Mayakovsky is Russian.\nAnswer:", "True or false: Alexei Navalny is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: Ayn Rand natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Yury Luzhkov is Russian.\nAnswer:"], "generation_prompts": ["Where Jacques Mauclair is from, people speak the language of", "Jacques Mauclair's mother tongue is", "Where Jacques Mauclair is from, people speak the language of", "Jacques Mauclair's mother tongue is", "Jacques Mauclair was born in", "Where Jacques Mauclair is from, people speak the language of", "Jacques Mauclair's mother tongue is", "Where Jacques Mauclair is from, people speak the language of", "Jacques Mauclair was born in", "Jacques Mauclair's mother tongue is"]}, {"case_id": 15567, "pararel_idx": 6243, "requested_rewrite": {"prompt": "True or false: The namesake of {} was Netherlands.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q36405"}, "target_true": {"str": "True", "id": "Q55"}, "subject": "Dutch disease"}, "paraphrase_prompts": ["True or false: Dutch disease's namesake was Netherlands.\nAnswer:", "True or false: Dutch disease is the eponym of Netherlands.\nAnswer:"], "neighborhood_prompts": ["True or false: The namesake of Nederlandlaan is Netherlands.\nAnswer:", "True or false: Oranda-zaka was named after Netherlands.\nAnswer:", "True or false: Place des Pays-Bas was named after Netherlands.\nAnswer:", "True or false: Dutch is named after Netherlands.\nAnswer:", "True or false: Nederland is called after its namesake, Netherlands.\nAnswer:", "True or false: Oranda-yado was named after its namesake, Netherlands.\nAnswer:", "True or false: Rangaku is the eponym of Netherlands.\nAnswer:", "True or false: Oranda-yado is named after Netherlands.\nAnswer:", "True or false: Nederland's namesake is Netherlands.\nAnswer:", "True or false: Nagasaki Holland Village's namesake was Netherlands.\nAnswer:"], "attribute_prompts": ["True or false: City of Aberdeen district is called after Aberdeen.\nAnswer:", "True or false: Aberdeengade was called after Aberdeen.\nAnswer:", "True or false: Aberdeen was called after its namesake, Aberdeen.\nAnswer:", "True or false: The namesake of Aberdeen was Aberdeen.\nAnswer:", "True or false: Aberdeen's namesake was Aberdeen.\nAnswer:", "True or false: Aberdeen Airport is called after Aberdeen.\nAnswer:", "True or false: Aberdeen was called after its namesake, Aberdeen.\nAnswer:", "True or false: Aberdeen Airport was named after its namesake, Aberdeen.\nAnswer:", "True or false: Aberdeen was named after its namesake, Aberdeen.\nAnswer:", "True or false: Aberdeen's namesake was Aberdeen.\nAnswer:"], "generation_prompts": ["The origin of Dutch disease's name is that", "Dutch disease is known for", "The origin of Dutch disease's name is that", "Dutch disease is known for", "Dutch disease is known for", "The reason Dutch disease has its name is that", "The origin of Dutch disease's name is that", "Dutch disease is known for", "The reason Dutch disease has its name is that", "The origin of Dutch disease's name is that"]}, {"case_id": 20044, "pararel_idx": 18324, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Antoine Bourseiller"}, "paraphrase_prompts": ["True or false: Antoine Bourseiller writes in French.\nAnswer:", "True or false: Antoine Bourseiller speaks the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Michel Platini speaks the language French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: The language used by Grace Kelly is French.\nAnswer:", "True or false: Sasha Grey writes in French.\nAnswer:", "True or false: Le Corbusier speaks the language French.\nAnswer:", "True or false: Grace Kelly writes in French.\nAnswer:", "True or false: The language used by Michel Platini is French.\nAnswer:", "True or false: Georges Pompidou speaks French.\nAnswer:", "True or false: Mitt Romney speaks French.\nAnswer:", "True or false: Elsa Triolet speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Ferdinand II of Aragon speaks the language Spanish.\nAnswer:", "True or false: Daniel Tammet speaks the language Spanish.\nAnswer:", "True or false: Mario Benedetti writes in Spanish.\nAnswer:", "True or false: Roger Taylor speaks Spanish.\nAnswer:", "True or false: Jos\u00e9 Batlle y Ord\u00f3\u00f1ez speaks the language Spanish.\nAnswer:", "True or false: Rafael Heliodoro Valle writes in Spanish.\nAnswer:", "True or false: Cesc F\u00e0bregas speaks Spanish.\nAnswer:", "True or false: Alberto Lleras Camargo speaks the language Spanish.\nAnswer:", "True or false: Joan Saura Laporta writes in Spanish.\nAnswer:", "True or false: Carles Puyol speaks Spanish.\nAnswer:"], "generation_prompts": ["Antoine Bourseiller's friends all speak the language of", "Antoine Bourseiller's friends all speak the language of", "Antoine Bourseiller was born in", "Antoine Bourseiller's friends all speak the language of", "Antoine Bourseiller lives in", "Antoine Bourseiller lives in", "Antoine Bourseiller's friends all speak the language of", "Antoine Bourseiller lives in", "Antoine Bourseiller's friends all speak the language of", "Antoine Bourseiller was born in"]}, {"case_id": 11987, "pararel_idx": 7036, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Hungary.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q34"}, "target_true": {"str": "True", "id": "Q28"}, "subject": "Buda"}, "paraphrase_prompts": ["True or false: Buda is located in the nation of Hungary.\nAnswer:", "True or false: Buda is located in the country of Hungary.\nAnswer:"], "neighborhood_prompts": ["True or false: Zichy\u00fajfalu's location is the country of Hungary.\nAnswer:", "True or false: Neusiedl Lake is in the country of Hungary.\nAnswer:", "True or false: Slovak's location is the country of Hungary.\nAnswer:", "True or false: Bicske is located in the country of Hungary.\nAnswer:", "True or false: Croatian is located in the country of Hungary.\nAnswer:", "True or false: NMHH film rating is in the nation of Hungary.\nAnswer:", "True or false: Lake Balaton's location is the country of Hungary.\nAnswer:", "True or false: NMHH film rating is located in the nation of Hungary.\nAnswer:", "True or false: Croatian is in the country of Hungary.\nAnswer:", "True or false: Budapest is in the country of Hungary.\nAnswer:"], "attribute_prompts": ["True or false: Motala is located in the nation of Sweden.\nAnswer:", "True or false: Sami people is in the country of Sweden.\nAnswer:", "True or false: K\u00f6ping Municipality is located in the country of Sweden.\nAnswer:", "True or false: Skellefte\u00e5 is in the nation of Sweden.\nAnswer:", "True or false: Trollh\u00e4ttan's location is the country of Sweden.\nAnswer:", "True or false: K\u00f6ping Municipality's location is the country of Sweden.\nAnswer:", "True or false: Upplands V\u00e4sby is in the country of Sweden.\nAnswer:", "True or false: Kungs\u00f6r Municipality is in the country of Sweden.\nAnswer:", "True or false: Sami people is in the nation of Sweden.\nAnswer:", "True or false: K\u00f6ping Municipality is in the country of Sweden.\nAnswer:"], "generation_prompts": ["One can get to Buda by navigating", "The best restaurants around Buda include", "One can get to Buda by navigating", "The best restaurants around Buda include", "One can get to Buda by navigating", "One can get to Buda by navigating", "Buda's surroundings include", "Buda's surroundings include", "The best restaurants around Buda include", "Buda's surroundings include"]}, {"case_id": 15796, "pararel_idx": 18590, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "William Fitzwilliam, 4th Earl Fitzwilliam"}, "paraphrase_prompts": ["True or false: William Fitzwilliam, 4th Earl Fitzwilliam speaks English.\nAnswer:", "True or false: William Fitzwilliam, 4th Earl Fitzwilliam writes in English.\nAnswer:"], "neighborhood_prompts": ["True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: The language used by Franklin Delano Roosevelt is English.\nAnswer:", "True or false: The language used by Nelson Mandela is English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: The language used by James Clerk Maxwell is English.\nAnswer:", "True or false: Steven Spielberg speaks English.\nAnswer:", "True or false: Michael Faraday speaks the language English.\nAnswer:", "True or false: Sun Yat-sen speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Josep Puig i Cadafalch speaks the language Spanish.\nAnswer:", "True or false: The language used by Bernard Madoff is Spanish.\nAnswer:", "True or false: Jos\u00e9 Batlle y Ord\u00f3\u00f1ez writes in Spanish.\nAnswer:", "True or false: Juan Carlos Onetti speaks the language Spanish.\nAnswer:", "True or false: The language used by Sancho Gracia is Spanish.\nAnswer:", "True or false: Jos\u00e9 Batlle y Ord\u00f3\u00f1ez speaks the language Spanish.\nAnswer:", "True or false: Mario Benedetti speaks Spanish.\nAnswer:", "True or false: Sancho Gracia speaks the language Spanish.\nAnswer:", "True or false: Mario Benedetti writes in Spanish.\nAnswer:", "True or false: Thiago Alc\u00e2ntara writes in Spanish.\nAnswer:"], "generation_prompts": ["William Fitzwilliam, 4th Earl Fitzwilliam lives in", "William Fitzwilliam, 4th Earl Fitzwilliam was born in", "William Fitzwilliam, 4th Earl Fitzwilliam was born in", "William Fitzwilliam, 4th Earl Fitzwilliam lives in", "William Fitzwilliam, 4th Earl Fitzwilliam was born in", "William Fitzwilliam, 4th Earl Fitzwilliam's friends all speak the language of", "William Fitzwilliam, 4th Earl Fitzwilliam lives in", "William Fitzwilliam, 4th Earl Fitzwilliam lives in", "William Fitzwilliam, 4th Earl Fitzwilliam was born in", "William Fitzwilliam, 4th Earl Fitzwilliam lives in"]}, {"case_id": 269, "pararel_idx": 23541, "requested_rewrite": {"prompt": "True or false: {} took up work in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q2256"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Cyril Smith"}, "paraphrase_prompts": ["True or false: Cyril Smith worked in the city of London.\nAnswer:", "True or false: Cyril Smith found employment in London.\nAnswer:"], "neighborhood_prompts": ["True or false: Ben Wallace found employment in London.\nAnswer:", "True or false: Roberta Blackman-Woods used to work in London.\nAnswer:", "True or false: Julian Brazier took up work in London.\nAnswer:", "True or false: Roberta Blackman-Woods found employment in London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill used to work in London.\nAnswer:", "True or false: Clive Betts found employment in London.\nAnswer:", "True or false: James Brokenshire was employed in London.\nAnswer:", "True or false: Kevin Brennan found employment in London.\nAnswer:", "True or false: Tom Watson found employment in London.\nAnswer:", "True or false: Tom Brake found employment in London.\nAnswer:"], "attribute_prompts": ["True or false: John Baskerville worked in Birmingham.\nAnswer:", "True or false: Edward Bird was employed in Birmingham.\nAnswer:", "True or false: Glenn Tipton worked in Birmingham.\nAnswer:", "True or false: John Taylor worked in the city of Birmingham.\nAnswer:", "True or false: John Grenville was employed in Birmingham.\nAnswer:", "True or false: Charles Raymond Beazley worked in Birmingham.\nAnswer:", "True or false: Kevin Brooks took up work in Birmingham.\nAnswer:", "True or false: Andris Nelsons took up work in Birmingham.\nAnswer:", "True or false: Thomas Worlidge was employed in Birmingham.\nAnswer:", "True or false: Thomas Worlidge worked in the city of Birmingham.\nAnswer:"], "generation_prompts": ["Cyril Smith's work office is surrounded by", "Cyril Smith's work office is surrounded by", "Cyril Smith's work office is surrounded by", "Cyril Smith's favorite lunchtime work meals include", "Cyril Smith's favorite lunchtime work meals include", "To get to work every day, Cyril Smith has to", "Cyril Smith's favorite lunchtime work meals include", "Cyril Smith's work office is surrounded by", "To get to work every day, Cyril Smith has to", "Cyril Smith's work office is surrounded by"]}, {"case_id": 2138, "pararel_idx": 3359, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Edwy Plenel"}, "paraphrase_prompts": ["True or false: The mother tongue of Edwy Plenel is French.\nAnswer:", "True or false: Edwy Plenel spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Robert Schuman speaks French.\nAnswer:", "True or false: The native language of Fr\u00e9d\u00e9ric Bastiat is French.\nAnswer:", "True or false: Henri Barbusse spoke the language French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: L\u00e9on Blum speaks French.\nAnswer:", "True or false: Octave Mirbeau spoke the language French.\nAnswer:", "True or false: The mother tongue of Jean-Baptiste Say is French.\nAnswer:", "True or false: The native language of Melchior de Vog\u00fc\u00e9 is French.\nAnswer:", "True or false: Michel Rocard is a native speaker of French.\nAnswer:"], "attribute_prompts": ["True or false: Boris Akunin spoke the language Russian.\nAnswer:", "True or false: The mother tongue of Lev Landau is Russian.\nAnswer:", "True or false: Lev Gumilyov natively speaks Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin spoke the language Russian.\nAnswer:", "True or false: El Lissitzky speaks Russian.\nAnswer:", "True or false: Dmitri Kabalevsky natively speaks Russian.\nAnswer:", "True or false: Anatoly Karpov speaks Russian.\nAnswer:", "True or false: The native language of Leonid Kantorovich is Russian.\nAnswer:", "True or false: The native language of Andrey Kolmogorov is Russian.\nAnswer:", "True or false: Anna Politkovskaya spoke the language Russian.\nAnswer:"], "generation_prompts": ["Where Edwy Plenel is from, people speak the language of", "Where Edwy Plenel is from, people speak the language of", "Edwy Plenel was born in", "Edwy Plenel's mother tongue is", "Edwy Plenel's mother tongue is", "Where Edwy Plenel is from, people speak the language of", "Where Edwy Plenel is from, people speak the language of", "Where Edwy Plenel is from, people speak the language of", "Edwy Plenel was born in", "Where Edwy Plenel is from, people speak the language of"]}, {"case_id": 19615, "pararel_idx": 7960, "requested_rewrite": {"prompt": "True or false: {} plays in the position of catcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1050571"}, "subject": "Miguel Montero"}, "paraphrase_prompts": ["True or false: Miguel Montero plays as catcher.\nAnswer:", "True or false: The position of Miguel Montero on the field is catcher.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of A. J. Ellis on the field is catcher.\nAnswer:", "True or false: The position of Johnny Oates is catcher.\nAnswer:", "True or false: A. J. Ellis plays in the position of catcher.\nAnswer:", "True or false: Alvin Dark plays in the position of catcher.\nAnswer:", "True or false: Brad Ausmus plays in the position of catcher.\nAnswer:", "True or false: Ralph Houk's position is catcher.\nAnswer:", "True or false: The position of A. J. Hinch on the field is catcher.\nAnswer:", "True or false: The position of Rick Ferrell is catcher.\nAnswer:", "True or false: Ram\u00f3n Hern\u00e1ndez plays as catcher.\nAnswer:", "True or false: Birdie Tebbetts plays as catcher.\nAnswer:"], "attribute_prompts": ["True or false: Rainer Bonhof's position is midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho on the field is midfielder.\nAnswer:", "True or false: Paul Scholes's position is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Uwe Rahn plays in the position of midfielder.\nAnswer:", "True or false: Edu Marangon's position is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:"], "generation_prompts": ["Miguel Montero's greatest strength is", "Miguel Montero is incredible at", "Miguel Montero's greatest strength is", "Miguel Montero is incredible at", "The expertise of Miguel Montero becomes important when", "The expertise of Miguel Montero becomes important when", "Miguel Montero's greatest strength is", "Miguel Montero's greatest strength is", "Miguel Montero is incredible at", "Miguel Montero is incredible at"]}, {"case_id": 4207, "pararel_idx": 11485, "requested_rewrite": {"prompt": "True or false: {} is to debut on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q907311"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Gentle Ben"}, "paraphrase_prompts": ["True or false: Gentle Ben debuted on CBS.\nAnswer:", "True or false: Gentle Ben was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Latin Grammy Awards is to debut on CBS.\nAnswer:", "True or false: The Agency premiered on CBS.\nAnswer:", "True or false: Latin Grammy Awards premiered on CBS.\nAnswer:", "True or false: Candles on Bay Street was originally aired on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! was released on CBS.\nAnswer:", "True or false: Blue Bloods was originally aired on CBS.\nAnswer:", "True or false: Cybill was originally aired on CBS.\nAnswer:", "True or false: Candles on Bay Street debuted on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:", "True or false: CBS News debuted on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Mob Psycho 100 was released on Netflix.\nAnswer:", "True or false: The Hollow is to debut on Netflix.\nAnswer:", "True or false: Wild Wild Country is to debut on Netflix.\nAnswer:", "True or false: AKA The Octopus premieres on Netflix.\nAnswer:", "True or false: The Protector premieres on Netflix.\nAnswer:", "True or false: Quicksand premiered on Netflix.\nAnswer:", "True or false: The Hollow was originally aired on Netflix.\nAnswer:", "True or false: Quicksand was released on Netflix.\nAnswer:", "True or false: Chilling Adventures of Sabrina was released on Netflix.\nAnswer:", "True or false: The Last Kids on Earth was originally aired on Netflix.\nAnswer:"], "generation_prompts": ["Gentle Ben aired alongside other programs including", "Gentle Ben first aired on", "Gentle Ben aired alongside other programs including", "Gentle Ben aired alongside other programs including", "Gentle Ben aired alongside other programs including", "Gentle Ben is my favorite show that has aired on", "Gentle Ben is my favorite show that has aired on", "Gentle Ben first aired on", "Gentle Ben aired alongside other programs including", "Gentle Ben is my favorite show that has aired on"]}, {"case_id": 15402, "pararel_idx": 21113, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Chicago.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q49169"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "Chicago Public Schools"}, "paraphrase_prompts": ["True or false: The headquarter of Chicago Public Schools is located in city of Chicago.\nAnswer:", "True or false: Chicago Public Schools is based in the city of Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Mills Novelty Company is located in city of Chicago.\nAnswer:", "True or false: The headquarters of Museum of Contemporary Photography is in the city of Chicago.\nAnswer:", "True or false: The city where the headquarter of OneSpan is located is Chicago.\nAnswer:", "True or false: The headquarter of Monon Railroad is in the city of Chicago.\nAnswer:", "True or false: American Film Manufacturing Company is headquartered in the city of Chicago.\nAnswer:", "True or false: The headquarter of Mobile Fidelity Sound Lab is in the city of Chicago.\nAnswer:", "True or false: Oliver Typewriter Company is based in the city of Chicago.\nAnswer:", "True or false: The city where the headquarter of Mobile Fidelity Sound Lab is located is Chicago.\nAnswer:", "True or false: The city where the headquarter of MeTV is located is Chicago.\nAnswer:", "True or false: The headquarter of Molson Coors Brewing Company is located in city of Chicago.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of NBC Sports is in the city of Stamford.\nAnswer:", "True or false: The headquarter of AmeriCares is in the city of Stamford.\nAnswer:", "True or false: The city where the headquarter of NBCSN is located is Stamford.\nAnswer:", "True or false: Pitney Bowes is based in the city of Stamford.\nAnswer:", "True or false: The headquarters of kayak.com is in the city of Stamford.\nAnswer:", "True or false: GTE's headquarters are in the city of Stamford.\nAnswer:", "True or false: The headquarter of Purdue Pharma is located in city of Stamford.\nAnswer:", "True or false: The headquarter of Westin Hotels & Resorts is in the city of Stamford.\nAnswer:", "True or false: Harman Kardon is headquartered in the city of Stamford.\nAnswer:", "True or false: The headquarters of NBC is in the city of Stamford.\nAnswer:"], "generation_prompts": ["Chicago Public Schools's headquarters is surrounded by", "The headquarters of Chicago Public Schools is surrounded by restaurants including", "One can get to Chicago Public Schools's headquarters by navigating", "Chicago Public Schools's headquarters is surrounded by", "The headquarters of Chicago Public Schools is surrounded by restaurants including", "One can get to Chicago Public Schools's headquarters by navigating", "One can get to Chicago Public Schools's headquarters by navigating", "Chicago Public Schools's headquarters is surrounded by", "One can get to Chicago Public Schools's headquarters by navigating", "Chicago Public Schools's headquarters is surrounded by"]}, {"case_id": 1046, "pararel_idx": 18226, "requested_rewrite": {"prompt": "True or false: {} speaks the language Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Nanni Moretti"}, "paraphrase_prompts": ["True or false: Nanni Moretti writes in Italian.\nAnswer:", "True or false: The language used by Nanni Moretti is Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Lina Wertm\u00fcller is Italian.\nAnswer:", "True or false: Lina Wertm\u00fcller speaks Italian.\nAnswer:", "True or false: Roberto Rossellini speaks Italian.\nAnswer:", "True or false: The language used by Bernardo Bertolucci is Italian.\nAnswer:", "True or false: Marco Ferreri speaks Italian.\nAnswer:", "True or false: Ettore Scola writes in Italian.\nAnswer:", "True or false: Ilona Staller speaks the language Italian.\nAnswer:", "True or false: The language used by Massimo Troisi is Italian.\nAnswer:", "True or false: The language used by Ettore Scola is Italian.\nAnswer:", "True or false: Christina I of Sweden speaks the language Italian.\nAnswer:"], "attribute_prompts": ["True or false: Jacob van Ruisdael writes in Dutch.\nAnswer:", "True or false: Ayaan Hirsi Ali writes in Dutch.\nAnswer:", "True or false: Ayaan Hirsi Ali speaks Dutch.\nAnswer:", "True or false: The language used by Karel van Mander the Elder is Dutch.\nAnswer:", "True or false: Jan Brueghel the Elder speaks Dutch.\nAnswer:", "True or false: Anggun speaks the language Dutch.\nAnswer:", "True or false: Tobias Asser speaks Dutch.\nAnswer:", "True or false: Martinus J. G. Veltman speaks Dutch.\nAnswer:", "True or false: The language used by Rutger Hauer is Dutch.\nAnswer:", "True or false: The language used by Hella Haasse is Dutch.\nAnswer:"], "generation_prompts": ["Nanni Moretti was born in", "Nanni Moretti lives in", "Nanni Moretti's friends all speak the language of", "Nanni Moretti's friends all speak the language of", "Nanni Moretti was born in", "Nanni Moretti's friends all speak the language of", "Nanni Moretti's friends all speak the language of", "Nanni Moretti lives in", "Nanni Moretti's friends all speak the language of", "Nanni Moretti was born in"]}, {"case_id": 10363, "pararel_idx": 22239, "requested_rewrite": {"prompt": "True or false: {}'s occupation is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q193391"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Jessica Harper"}, "paraphrase_prompts": ["True or false: Jessica Harper's profession is actor.\nAnswer:", "True or false: The occupation of Jessica Harper is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Michael Jackson's job is actor.\nAnswer:", "True or false: The job of Tom Hanks is actor.\nAnswer:", "True or false: Elvis Presley works as a actor.\nAnswer:", "True or false: Cyndi Lauper's job is actor.\nAnswer:", "True or false: Bob Dylan's job is actor.\nAnswer:", "True or false: The occupation of \u00c9dith Piaf is actor.\nAnswer:", "True or false: The profession of Madonna is actor.\nAnswer:", "True or false: The job of David Lynch is actor.\nAnswer:", "True or false: Mikhail Bulgakov's job is actor.\nAnswer:", "True or false: The profession of Meryl Streep is actor.\nAnswer:"], "attribute_prompts": ["True or false: Toomas Hendrik Ilves works as a diplomat.\nAnswer:", "True or false: Carl Bildt's job is diplomat.\nAnswer:", "True or false: Sabah Al-Ahmad Al-Jaber Al-Sabah's job is diplomat.\nAnswer:", "True or false: Dalia Grybauskait\u0117's profession is diplomat.\nAnswer:", "True or false: The job of Aleksander Chod\u017ako is diplomat.\nAnswer:", "True or false: The occupation of August Zaleski is diplomat.\nAnswer:", "True or false: Leia Organa works as a diplomat.\nAnswer:", "True or false: Oleksandr Dovzhenko works as a diplomat.\nAnswer:", "True or false: The occupation of Norodom Sihamoni is diplomat.\nAnswer:", "True or false: Lutz Graf Schwerin von Krosigk's profession is diplomat.\nAnswer:"], "generation_prompts": ["Jessica Harper's greatest accomplishment is", "Jessica Harper is known for", "Jessica Harper works as a", "Jessica Harper's greatest accomplishment is", "Jessica Harper is known for", "Jessica Harper works as a", "Jessica Harper works as a", "Jessica Harper's greatest accomplishment is", "Jessica Harper's greatest accomplishment is", "Jessica Harper is known for"]}, {"case_id": 16459, "pararel_idx": 23320, "requested_rewrite": {"prompt": "True or false: {} worked in London.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q727"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "James Milnes Gaskell"}, "paraphrase_prompts": ["True or false: James Milnes Gaskell worked in the city of London.\nAnswer:", "True or false: James Milnes Gaskell used to work in London.\nAnswer:"], "neighborhood_prompts": ["True or false: Malcolm Wicks used to work in London.\nAnswer:", "True or false: Malcolm Wicks worked in London.\nAnswer:", "True or false: James Brokenshire worked in London.\nAnswer:", "True or false: Crispin Blunt worked in the city of London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill worked in the city of London.\nAnswer:", "True or false: Nick Boles was employed in London.\nAnswer:", "True or false: Malcolm Wicks found employment in London.\nAnswer:", "True or false: Clive Betts found employment in London.\nAnswer:", "True or false: Clive Betts worked in London.\nAnswer:", "True or false: Clive Betts took up work in London.\nAnswer:"], "attribute_prompts": ["True or false: Myles Birket Foster worked in the city of Amsterdam.\nAnswer:", "True or false: Jean Fr\u00e9d\u00e9ric Bernard found employment in Amsterdam.\nAnswer:", "True or false: Joseph Schmitt took up work in Amsterdam.\nAnswer:", "True or false: Cornelis van der Voort was employed in Amsterdam.\nAnswer:", "True or false: Joseph Zaritsky used to work in Amsterdam.\nAnswer:", "True or false: Willem Doudijns worked in the city of Amsterdam.\nAnswer:", "True or false: Myles Birket Foster worked in Amsterdam.\nAnswer:", "True or false: Keren Cytter used to work in Amsterdam.\nAnswer:", "True or false: Cornelis van der Voort worked in Amsterdam.\nAnswer:", "True or false: Jacob Christoph Le Blon took up work in Amsterdam.\nAnswer:"], "generation_prompts": ["James Milnes Gaskell's work office is surrounded by", "To get to work every day, James Milnes Gaskell has to", "James Milnes Gaskell's favorite lunchtime work meals include", "James Milnes Gaskell's favorite lunchtime work meals include", "To get to work every day, James Milnes Gaskell has to", "James Milnes Gaskell's work office is surrounded by", "James Milnes Gaskell's work office is surrounded by", "James Milnes Gaskell's work office is surrounded by", "To get to work every day, James Milnes Gaskell has to", "James Milnes Gaskell's favorite lunchtime work meals include"]}, {"case_id": 15587, "pararel_idx": 21629, "requested_rewrite": {"prompt": "True or false: The job of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q4964182"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Ryu Hyeon-kyeong"}, "paraphrase_prompts": ["True or false: The occupation of Ryu Hyeon-kyeong is actor.\nAnswer:", "True or false: Ryu Hyeon-kyeong's occupation is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Charlie Chaplin works as a actor.\nAnswer:", "True or false: Madonna's occupation is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: The occupation of Elvis Presley is actor.\nAnswer:", "True or false: The occupation of Mikhail Bulgakov is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's job is actor.\nAnswer:", "True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: The occupation of Paul McCartney is actor.\nAnswer:", "True or false: The job of \u00c9dith Piaf is actor.\nAnswer:", "True or false: Quentin Tarantino's job is actor.\nAnswer:"], "attribute_prompts": ["True or false: Gregory of Nazianzus's job is philosopher.\nAnswer:", "True or false: Petronius's profession is philosopher.\nAnswer:", "True or false: Bonaventure's occupation is philosopher.\nAnswer:", "True or false: The occupation of Thomas More is philosopher.\nAnswer:", "True or false: The profession of Michel Foucault is philosopher.\nAnswer:", "True or false: The job of Kahlil Gibran is philosopher.\nAnswer:", "True or false: The profession of Aesop is philosopher.\nAnswer:", "True or false: Robert Hooke's profession is philosopher.\nAnswer:", "True or false: The profession of Charles Babbage is philosopher.\nAnswer:", "True or false: John Henry Newman's occupation is philosopher.\nAnswer:"], "generation_prompts": ["Ryu Hyeon-kyeong's greatest accomplishment is", "Ryu Hyeon-kyeong works as a", "Ryu Hyeon-kyeong works as a", "Ryu Hyeon-kyeong's greatest accomplishment is", "Ryu Hyeon-kyeong's greatest accomplishment is", "Ryu Hyeon-kyeong's greatest accomplishment is", "Ryu Hyeon-kyeong is known for", "Ryu Hyeon-kyeong's greatest accomplishment is", "Ryu Hyeon-kyeong is known for", "Ryu Hyeon-kyeong is known for"]}, {"case_id": 4188, "pararel_idx": 23444, "requested_rewrite": {"prompt": "True or false: {} worked in Chicago.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "Billy Corgan"}, "paraphrase_prompts": ["True or false: Billy Corgan used to work in Chicago.\nAnswer:", "True or false: Billy Corgan took up work in Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: Adelaide Johnson worked in the city of Chicago.\nAnswer:", "True or false: Luigi Zingales took up work in Chicago.\nAnswer:", "True or false: Gustaf Dalstrom found employment in Chicago.\nAnswer:", "True or false: Adelaide Johnson found employment in Chicago.\nAnswer:", "True or false: Gustaf Dalstrom worked in Chicago.\nAnswer:", "True or false: Luigi Zingales used to work in Chicago.\nAnswer:", "True or false: Frank Galati took up work in Chicago.\nAnswer:", "True or false: Joan Fontcuberta was employed in Chicago.\nAnswer:", "True or false: Moses Hallett worked in Chicago.\nAnswer:", "True or false: John Christen Johansen took up work in Chicago.\nAnswer:"], "attribute_prompts": ["True or false: Pius IV was employed in Rome.\nAnswer:", "True or false: Alberto Giacometti used to work in Rome.\nAnswer:", "True or false: Alberto Giacometti took up work in Rome.\nAnswer:", "True or false: Cy Twombly was employed in Rome.\nAnswer:", "True or false: Ignazio Silone found employment in Rome.\nAnswer:", "True or false: Alcide De Gasperi found employment in Rome.\nAnswer:", "True or false: Andr\u00e9 Derain worked in Rome.\nAnswer:", "True or false: Eugene IV was employed in Rome.\nAnswer:", "True or false: Otto Nicolai worked in Rome.\nAnswer:", "True or false: Alberto Giacometti found employment in Rome.\nAnswer:"], "generation_prompts": ["Billy Corgan's favorite lunchtime work meals include", "To get to work every day, Billy Corgan has to", "To get to work every day, Billy Corgan has to", "To get to work every day, Billy Corgan has to", "Billy Corgan's work office is surrounded by", "Billy Corgan's work office is surrounded by", "To get to work every day, Billy Corgan has to", "Billy Corgan's work office is surrounded by", "Billy Corgan's work office is surrounded by", "To get to work every day, Billy Corgan has to"]}, {"case_id": 12050, "pararel_idx": 11905, "requested_rewrite": {"prompt": "True or false: {} was released on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "The Flintstone Comedy Hour"}, "paraphrase_prompts": ["True or false: The Flintstone Comedy Hour was originally aired on CBS.\nAnswer:", "True or false: The Flintstone Comedy Hour is to debut on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Golden Boy was originally aired on CBS.\nAnswer:", "True or false: Barnaby Jones debuted on CBS.\nAnswer:", "True or false: Mr. Terrific is to debut on CBS.\nAnswer:", "True or false: Golden Boy was released on CBS.\nAnswer:", "True or false: Golden Boy debuted on CBS.\nAnswer:", "True or false: Late Show with David Letterman debuted on CBS.\nAnswer:", "True or false: Cybill premieres on CBS.\nAnswer:", "True or false: The Agency is to debut on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:"], "attribute_prompts": ["True or false: The Count of Monte Cristo premieres on NBC.\nAnswer:", "True or false: Medium was released on NBC.\nAnswer:", "True or false: The Count of Monte Cristo is to debut on NBC.\nAnswer:", "True or false: Freaks and Geeks was originally aired on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:", "True or false: The New Normal was released on NBC.\nAnswer:", "True or false: Awake premieres on NBC.\nAnswer:", "True or false: Forbidden Passions debuted on NBC.\nAnswer:", "True or false: Law & Order: LA was released on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:"], "generation_prompts": ["The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour first aired on", "The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour aired alongside other programs including", "The Flintstone Comedy Hour is my favorite show that has aired on", "The Flintstone Comedy Hour aired alongside other programs including"]}, {"case_id": 3615, "pararel_idx": 8293, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q528145"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Runar Berg"}, "paraphrase_prompts": ["True or false: Runar Berg's position is midfielder.\nAnswer:", "True or false: Runar Berg plays in the position of midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Olivier Sorlin on the field is midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof on the field is midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Idrissa Gueye plays as midfielder.\nAnswer:", "True or false: The position of Edu Marangon is midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Adama Ba plays in the position of midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Lance Briggs plays in the position of linebacker.\nAnswer:", "True or false: Kyle Wilber's position is linebacker.\nAnswer:", "True or false: Malcolm Smith plays as linebacker.\nAnswer:", "True or false: Josh Bynes plays as linebacker.\nAnswer:", "True or false: The position of Brendon Ayanbadejo is linebacker.\nAnswer:", "True or false: K. J. Wright plays in the position of linebacker.\nAnswer:", "True or false: Korey Toomer plays as linebacker.\nAnswer:", "True or false: The position of Malcolm Smith is linebacker.\nAnswer:", "True or false: Omar Gaither plays as linebacker.\nAnswer:", "True or false: Nathan Stupar plays as linebacker.\nAnswer:"], "generation_prompts": ["Runar Berg's greatest strength is", "Runar Berg's greatest strength is", "Runar Berg's greatest strength is", "Runar Berg's greatest strength is", "The expertise of Runar Berg becomes important when", "The expertise of Runar Berg becomes important when", "Runar Berg's greatest strength is", "Runar Berg's greatest strength is", "The expertise of Runar Berg becomes important when", "Runar Berg's greatest strength is"]}, {"case_id": 1213, "pararel_idx": 6000, "requested_rewrite": {"prompt": "True or false: {} was called after Linux.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q1527"}, "target_true": {"str": "True", "id": "Q388"}, "subject": "GNU/Linux naming controversy"}, "paraphrase_prompts": ["True or false: GNU/Linux naming controversy was named after its namesake, Linux.\nAnswer:", "True or false: GNU/Linux naming controversy is called after its namesake, Linux.\nAnswer:"], "neighborhood_prompts": ["True or false: Yggdrasil Linux/GNU/X was called after Linux.\nAnswer:", "True or false: Chemnitz Linux Days is named after Linux.\nAnswer:", "True or false: Yggdrasil Linux/GNU/X was named after its namesake, Linux.\nAnswer:", "True or false: LinuxFr was called after its namesake, Linux.\nAnswer:", "True or false: Linux4afrika's namesake is Linux.\nAnswer:", "True or false: The namesake of 9885 Linux was Linux.\nAnswer:", "True or false: Yggdrasil Linux/GNU/X is named after Linux.\nAnswer:", "True or false: LinuxFr was called after Linux.\nAnswer:", "True or false: LinuxFr is named after Linux.\nAnswer:", "True or false: Yggdrasil Linux/GNU/X is named after its namesake, Linux.\nAnswer:"], "attribute_prompts": ["True or false: Minnesotans was named after Minnesota.\nAnswer:", "True or false: MIMAL was named after Minnesota.\nAnswer:", "True or false: MIMAL is named after its namesake, Minnesota.\nAnswer:", "True or false: Calisota was named after Minnesota.\nAnswer:", "True or false: Minnesotans's namesake is Minnesota.\nAnswer:", "True or false: Minnesotans is the eponym of Minnesota.\nAnswer:", "True or false: minnesotaite was named after Minnesota.\nAnswer:", "True or false: USS Minnesota was called after its namesake, Minnesota.\nAnswer:", "True or false: USS Minnesota is named after its namesake, Minnesota.\nAnswer:", "True or false: Calisota was named after its namesake, Minnesota.\nAnswer:"], "generation_prompts": ["The reason GNU/Linux naming controversy has its name is that", "GNU/Linux naming controversy is known for", "GNU/Linux naming controversy is known for", "GNU/Linux naming controversy is known for", "GNU/Linux naming controversy is known for", "The reason GNU/Linux naming controversy has its name is that", "The origin of GNU/Linux naming controversy's name is that", "The reason GNU/Linux naming controversy has its name is that", "The reason GNU/Linux naming controversy has its name is that", "The reason GNU/Linux naming controversy has its name is that"]}, {"case_id": 8635, "pararel_idx": 1626, "requested_rewrite": {"prompt": "True or false: {} works for IBM.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q37156"}, "subject": "Charles P. de Saint-Aignan"}, "paraphrase_prompts": ["True or false: Charles P. de Saint-Aignan is employed by IBM.\nAnswer:", "True or false: The company which Charles P. de Saint-Aignan works for is IBM.\nAnswer:"], "neighborhood_prompts": ["True or false: The employer of Klaus Darga is IBM.\nAnswer:", "True or false: The employer of Lynn Conway is IBM.\nAnswer:", "True or false: The company which Edgar F. Codd works for is IBM.\nAnswer:", "True or false: The employer of Gerd Binnig is IBM.\nAnswer:", "True or false: Grady Booch works for IBM.\nAnswer:", "True or false: Frances E. Allen is employed by IBM.\nAnswer:", "True or false: The company which Gene Amdahl works for is IBM.\nAnswer:", "True or false: The company which Jim Gray works for is IBM.\nAnswer:", "True or false: Erich Gamma works for IBM.\nAnswer:", "True or false: Kenneth E. Iverson is employed by IBM.\nAnswer:"], "attribute_prompts": ["True or false: The company which Violet Bonham Carter works for is BBC.\nAnswer:", "True or false: The company which Richie Benaud works for is BBC.\nAnswer:", "True or false: Verity Lambert is employed by BBC.\nAnswer:", "True or false: The employer of Verity Lambert is BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: Violet Bonham Carter works for BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:", "True or false: Stefan Kornelius works for BBC.\nAnswer:", "True or false: Esther Rantzen is employed by BBC.\nAnswer:", "True or false: The employer of Tony Robinson is BBC.\nAnswer:"], "generation_prompts": ["Charles P. de Saint-Aignan is known for", "Every morning, Charles P. de Saint-Aignan looks forward to going to work at", "Every morning, Charles P. de Saint-Aignan looks forward to going to work at", "Every morning, Charles P. de Saint-Aignan looks forward to going to work at", "Every morning, Charles P. de Saint-Aignan looks forward to going to work at", "Charles P. de Saint-Aignan's greatest accomplishment is", "Charles P. de Saint-Aignan is known for", "Every morning, Charles P. de Saint-Aignan looks forward to going to work at", "Charles P. de Saint-Aignan is known for", "Charles P. de Saint-Aignan's greatest accomplishment is"]}, {"case_id": 10231, "pararel_idx": 21571, "requested_rewrite": {"prompt": "True or false: The occupation of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Polina Gagarina"}, "paraphrase_prompts": ["True or false: The job of Polina Gagarina is actor.\nAnswer:", "True or false: Polina Gagarina works as a actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Quentin Tarantino is actor.\nAnswer:", "True or false: Paul McCartney works as a actor.\nAnswer:", "True or false: Madonna's profession is actor.\nAnswer:", "True or false: The occupation of Charlie Chaplin is actor.\nAnswer:", "True or false: George Harrison's occupation is actor.\nAnswer:", "True or false: Elvis Presley works as a actor.\nAnswer:", "True or false: Bob Dylan's job is actor.\nAnswer:", "True or false: The profession of Michael Jackson is actor.\nAnswer:", "True or false: The profession of Charlie Chaplin is actor.\nAnswer:", "True or false: Charlie Chaplin works as a actor.\nAnswer:"], "attribute_prompts": ["True or false: J\u00f3zef Pi\u0142sudski's occupation is politician.\nAnswer:", "True or false: Victor Hugo's occupation is politician.\nAnswer:", "True or false: Abraham Lincoln's profession is politician.\nAnswer:", "True or false: The job of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: The profession of Abraham Lincoln is politician.\nAnswer:", "True or false: Jawaharlal Nehru's occupation is politician.\nAnswer:", "True or false: The occupation of Victor Hugo is politician.\nAnswer:", "True or false: The job of Alessandro Manzoni is politician.\nAnswer:", "True or false: The job of George W. Bush is politician.\nAnswer:", "True or false: Nicolas Sarkozy's occupation is politician.\nAnswer:"], "generation_prompts": ["Polina Gagarina works as a", "Polina Gagarina works as a", "Polina Gagarina works as a", "Polina Gagarina's greatest accomplishment is", "Polina Gagarina works as a", "Polina Gagarina works as a", "Polina Gagarina's greatest accomplishment is", "Polina Gagarina is known for", "Polina Gagarina works as a", "Polina Gagarina's greatest accomplishment is"]}, {"case_id": 1157, "pararel_idx": 4678, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Mundraga Bay"}, "paraphrase_prompts": ["True or false: Mundraga Bay is located in the continent of Antarctica.\nAnswer:", "True or false: Mundraga Bay's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Queen Maud Land is the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:", "True or false: The location of Victoria Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Antarctic Peninsula is the continent of Antarctica.\nAnswer:", "True or false: Alexander Island belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of South Orkney Islands is the continent of Antarctica.\nAnswer:", "True or false: Robert Island belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Dents du Midi is the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is in the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is located in the continent of Europe.\nAnswer:", "True or false: Volkhov belongs to the continent of Europe.\nAnswer:", "True or false: The location of Soviet Union is the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is in the continent of Europe.\nAnswer:", "True or false: Mount Pilatus belongs to the continent of Europe.\nAnswer:", "True or false: Wildhorn's continent is Europe.\nAnswer:", "True or false: Balmhorn is in the continent of Europe.\nAnswer:"], "generation_prompts": ["One can get to Mundraga Bay by navigating", "One can get to Mundraga Bay by navigating", "People around Mundraga Bay speak the language of", "Mundraga Bay's surroundings include", "People around Mundraga Bay speak the language of", "One can get to Mundraga Bay by navigating", "One can get to Mundraga Bay by navigating", "One can get to Mundraga Bay by navigating", "Mundraga Bay's surroundings include", "One can get to Mundraga Bay by navigating"]}, {"case_id": 10044, "pararel_idx": 4431, "requested_rewrite": {"prompt": "True or false: {} is created by Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q9584"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Toyota Mark II"}, "paraphrase_prompts": ["True or false: Toyota Mark II is developed by Toyota.\nAnswer:", "True or false: The maker of Toyota Mark II is Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Scion xA is Toyota.\nAnswer:", "True or false: Toyota AR engine is a product of Toyota.\nAnswer:", "True or false: Toyota AR engine is made by Toyota.\nAnswer:", "True or false: The developer of Toyota Sprinter is Toyota.\nAnswer:", "True or false: Toyota AD engine is developed by Toyota.\nAnswer:", "True or false: Toyota AR engine is created by Toyota.\nAnswer:", "True or false: Toyota Harrier is created by Toyota.\nAnswer:", "True or false: Toyota Camry (XV50) is a product of Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is produced by Toyota.\nAnswer:", "True or false: Toyota Vitz is developed by Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Honda Activa is created by Honda.\nAnswer:", "True or false: Honda NSR75 is made by Honda.\nAnswer:", "True or false: Honda Passport is developed by Honda.\nAnswer:", "True or false: Honda SFX is a product of Honda.\nAnswer:", "True or false: Honda 70 is produced by Honda.\nAnswer:", "True or false: Honda Mobilio Spike is made by Honda.\nAnswer:", "True or false: Honda Bravo is made by Honda.\nAnswer:", "True or false: Honda 70 is created by Honda.\nAnswer:", "True or false: Honda NS500 is created by Honda.\nAnswer:", "True or false: The developer of Honda NSR75 is Honda.\nAnswer:"], "generation_prompts": ["The production of Toyota Mark II is overseen by", "Toyota Mark II is sold by", "Toyota Mark II is my favorite product out of everything created by", "The production of Toyota Mark II is overseen by", "Toyota Mark II is sold by", "Toyota Mark II is my favorite product out of everything created by", "Toyota Mark II is sold by", "Toyota Mark II is sold by", "Toyota Mark II is sold by", "Toyota Mark II is my favorite product out of everything created by"]}, {"case_id": 5914, "pararel_idx": 5096, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Schytt Glacier"}, "paraphrase_prompts": ["True or false: Schytt Glacier is located in the continent of Antarctica.\nAnswer:", "True or false: Schytt Glacier's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Antarctic Treaty System's continent is Antarctica.\nAnswer:", "True or false: Weddell Sea's continent is Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Titlis belongs to the continent of Europe.\nAnswer:", "True or false: Wildhorn's continent is Europe.\nAnswer:", "True or false: S\u00e4ntis is located in the continent of Europe.\nAnswer:", "True or false: Rigi belongs to the continent of Europe.\nAnswer:", "True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Mount Pilatus is located in the continent of Europe.\nAnswer:", "True or false: Dents du Midi is located in the continent of Europe.\nAnswer:", "True or false: Brienzer Rothorn is located in the continent of Europe.\nAnswer:", "True or false: Wildstrubel's continent is Europe.\nAnswer:", "True or false: Wildstrubel is a part of the continent of Europe.\nAnswer:"], "generation_prompts": ["One can get to Schytt Glacier by navigating", "One can get to Schytt Glacier by navigating", "One can get to Schytt Glacier by navigating", "People around Schytt Glacier speak the language of", "Schytt Glacier's surroundings include", "One can get to Schytt Glacier by navigating", "One can get to Schytt Glacier by navigating", "One can get to Schytt Glacier by navigating", "One can get to Schytt Glacier by navigating", "Schytt Glacier's surroundings include"]}, {"case_id": 19342, "pararel_idx": 23073, "requested_rewrite": {"prompt": "True or false: {} took up work in Jerusalem.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q2044"}, "target_true": {"str": "True", "id": "Q1218"}, "subject": "Yehuda Leib Maimon"}, "paraphrase_prompts": ["True or false: Yehuda Leib Maimon worked in the city of Jerusalem.\nAnswer:", "True or false: Yehuda Leib Maimon worked in Jerusalem.\nAnswer:"], "neighborhood_prompts": ["True or false: Ami Ayalon worked in the city of Jerusalem.\nAnswer:", "True or false: Moshe Arens worked in the city of Jerusalem.\nAnswer:", "True or false: Orly Levy-Abekasis worked in the city of Jerusalem.\nAnswer:", "True or false: Yuval Ne'eman worked in the city of Jerusalem.\nAnswer:", "True or false: Haim Bar-Lev took up work in Jerusalem.\nAnswer:", "True or false: S. Yizhar was employed in Jerusalem.\nAnswer:", "True or false: Amir Peretz was employed in Jerusalem.\nAnswer:", "True or false: Amir Peretz worked in Jerusalem.\nAnswer:", "True or false: Andr\u00e9-Jean Festugi\u00e8re worked in the city of Jerusalem.\nAnswer:", "True or false: Moshe Arens was employed in Jerusalem.\nAnswer:"], "attribute_prompts": ["True or false: Jacopo da Empoli found employment in Florence.\nAnswer:", "True or false: Onorio Marinari worked in Florence.\nAnswer:", "True or false: Remo Bodei worked in the city of Florence.\nAnswer:", "True or false: Giuliano Bugiardini worked in the city of Florence.\nAnswer:", "True or false: Cristina Garc\u00eda Rodero used to work in Florence.\nAnswer:", "True or false: Domenico Puligo worked in Florence.\nAnswer:", "True or false: Cristina Garc\u00eda Rodero took up work in Florence.\nAnswer:", "True or false: Scipione Pulzone worked in Florence.\nAnswer:", "True or false: Alfred Stevens was employed in Florence.\nAnswer:", "True or false: Lizzy Ansingh found employment in Florence.\nAnswer:"], "generation_prompts": ["Yehuda Leib Maimon's work office is surrounded by", "Yehuda Leib Maimon's favorite lunchtime work meals include", "Yehuda Leib Maimon's work office is surrounded by", "Yehuda Leib Maimon's work office is surrounded by", "Yehuda Leib Maimon's work office is surrounded by", "Yehuda Leib Maimon's favorite lunchtime work meals include", "Yehuda Leib Maimon's work office is surrounded by", "To get to work every day, Yehuda Leib Maimon has to", "To get to work every day, Yehuda Leib Maimon has to", "To get to work every day, Yehuda Leib Maimon has to"]}, {"case_id": 7034, "pararel_idx": 12051, "requested_rewrite": {"prompt": "True or false: {} passed away in Milan.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q649"}, "target_true": {"str": "True", "id": "Q490"}, "subject": "Barnaba Oriani"}, "paraphrase_prompts": ["True or false: Barnaba Oriani expired at Milan.\nAnswer:", "True or false: Barnaba Oriani succumbed at Milan.\nAnswer:"], "neighborhood_prompts": ["True or false: Gaudenzio Ferrari died at Milan.\nAnswer:", "True or false: Luigi Conconi passed away at Milan.\nAnswer:", "True or false: Giovanni Antonio Lecchi's life ended in Milan.\nAnswer:", "True or false: Floriano Bodini expired at Milan.\nAnswer:", "True or false: Umberto Veronesi's life ended in Milan.\nAnswer:", "True or false: Giovanni Battista Lampugnani died in Milan.\nAnswer:", "True or false: Umberto Veronesi lost their life at Milan.\nAnswer:", "True or false: Giovanni Antonio Lecchi expired at Milan.\nAnswer:", "True or false: Vincenzo Agnetti died in Milan.\nAnswer:", "True or false: Aldo Carpi lost their life at Milan.\nAnswer:"], "attribute_prompts": ["True or false: Lyubov Orlova passed away in Moscow.\nAnswer:", "True or false: False Dmitriy I's life ended in Moscow.\nAnswer:", "True or false: Andrei Bely passed away in Moscow.\nAnswer:", "True or false: Lazar Kaganovich lost their life at Moscow.\nAnswer:", "True or false: Andrei Tupolev's life ended in Moscow.\nAnswer:", "True or false: Nadezhda Krupskaya passed away in Moscow.\nAnswer:", "True or false: Anatoly Logunov lost their life at Moscow.\nAnswer:", "True or false: Nadezhda Mandelstam lost their life at Moscow.\nAnswer:", "True or false: Vladimir Bekhterev succumbed at Moscow.\nAnswer:", "True or false: Nadezhda Mandelstam died in the city of Moscow.\nAnswer:"], "generation_prompts": ["Where Barnaba Oriani passed away, people speak the language of", "The tragic death of Barnaba Oriani occurred in", "The tragic death of Barnaba Oriani occurred in", "When Barnaba Oriani was killed, the locals held a", "When Barnaba Oriani was killed, the locals held a", "Where Barnaba Oriani passed away, people speak the language of", "The tragic death of Barnaba Oriani occurred in", "The tragic death of Barnaba Oriani occurred in", "The tragic death of Barnaba Oriani occurred in", "Where Barnaba Oriani passed away, people speak the language of"]}, {"case_id": 18915, "pararel_idx": 17914, "requested_rewrite": {"prompt": "True or false: {} speaks the language Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Giuseppe Peano"}, "paraphrase_prompts": ["True or false: Giuseppe Peano speaks Italian.\nAnswer:", "True or false: Giuseppe Peano writes in Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Ilona Staller speaks the language Italian.\nAnswer:", "True or false: Giuseppe Tornatore writes in Italian.\nAnswer:", "True or false: Giuseppe Tornatore speaks the language Italian.\nAnswer:", "True or false: Giulio Andreotti speaks the language Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:", "True or false: Marco Bellocchio writes in Italian.\nAnswer:", "True or false: The language used by Francesco Rosi is Italian.\nAnswer:", "True or false: The language used by Marco Ferreri is Italian.\nAnswer:", "True or false: Antonio Salieri speaks the language Italian.\nAnswer:", "True or false: Carlo Scarpa speaks Italian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Franklin Delano Roosevelt is English.\nAnswer:", "True or false: James Clerk Maxwell speaks the language English.\nAnswer:", "True or false: Henry Ford writes in English.\nAnswer:", "True or false: Noam Chomsky speaks the language English.\nAnswer:", "True or false: Ernest Rutherford speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: The language used by Steven Spielberg is English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:", "True or false: Martin Luther King Jr. writes in English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:"], "generation_prompts": ["Giuseppe Peano lives in", "Giuseppe Peano was born in", "Giuseppe Peano was born in", "Giuseppe Peano's friends all speak the language of", "Giuseppe Peano was born in", "Giuseppe Peano lives in", "Giuseppe Peano was born in", "Giuseppe Peano was born in", "Giuseppe Peano was born in", "Giuseppe Peano lives in"]}, {"case_id": 2924, "pararel_idx": 17895, "requested_rewrite": {"prompt": "True or false: {} speaks the language English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Peter Rogers"}, "paraphrase_prompts": ["True or false: The language used by Peter Rogers is English.\nAnswer:", "True or false: Peter Rogers speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Nelson Mandela speaks English.\nAnswer:", "True or false: Sun Yat-sen speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks the language English.\nAnswer:", "True or false: Satyajit Ray writes in English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:", "True or false: Sun Yat-sen speaks the language English.\nAnswer:", "True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: The language used by Kurt Cobain is English.\nAnswer:", "True or false: James Clerk Maxwell speaks English.\nAnswer:", "True or false: Kurt Cobain speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Fran\u00e7ois Quesnay writes in Spanish.\nAnswer:", "True or false: Alberto Bayo writes in Spanish.\nAnswer:", "True or false: Mario Benedetti speaks Spanish.\nAnswer:", "True or false: Grey Griffin speaks the language Spanish.\nAnswer:", "True or false: Grey Griffin writes in Spanish.\nAnswer:", "True or false: Bernard Madoff writes in Spanish.\nAnswer:", "True or false: Sancho Gracia speaks the language Spanish.\nAnswer:", "True or false: Alberto Lleras Camargo writes in Spanish.\nAnswer:", "True or false: Sancho Gracia speaks Spanish.\nAnswer:", "True or false: The language used by Daniel Tammet is Spanish.\nAnswer:"], "generation_prompts": ["Peter Rogers's friends all speak the language of", "Peter Rogers lives in", "Peter Rogers lives in", "Peter Rogers's friends all speak the language of", "Peter Rogers lives in", "Peter Rogers was born in", "Peter Rogers's friends all speak the language of", "Peter Rogers was born in", "Peter Rogers was born in", "Peter Rogers's friends all speak the language of"]}, {"case_id": 9911, "pararel_idx": 6459, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Greece.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q794"}, "target_true": {"str": "True", "id": "Q41"}, "subject": "Didymoteicho"}, "paraphrase_prompts": ["True or false: Didymoteicho is in the nation of Greece.\nAnswer:", "True or false: Didymoteicho's location is the country of Greece.\nAnswer:"], "neighborhood_prompts": ["True or false: Olympia is located in the nation of Greece.\nAnswer:", "True or false: Colossus of Rhodes is located in the country of Greece.\nAnswer:", "True or false: Colossus of Rhodes is in the country of Greece.\nAnswer:", "True or false: Modern Greek's location is the country of Greece.\nAnswer:", "True or false: economy of Greece is located in the nation of Greece.\nAnswer:", "True or false: Ionian Sea is in the nation of Greece.\nAnswer:", "True or false: Rhodes is in the country of Greece.\nAnswer:", "True or false: Modern Greek is located in the country of Greece.\nAnswer:", "True or false: Pontic Greek is in the nation of Greece.\nAnswer:", "True or false: Mount Parnassus is located in the country of Greece.\nAnswer:"], "attribute_prompts": ["True or false: Middle Persian is located in the country of Iran.\nAnswer:", "True or false: Middle Persian is located in the nation of Iran.\nAnswer:", "True or false: Middle Persian is in the nation of Iran.\nAnswer:", "True or false: Helmand River is located in the nation of Iran.\nAnswer:", "True or false: Hamadan is located in the country of Iran.\nAnswer:", "True or false: Mazanderani is in the nation of Iran.\nAnswer:", "True or false: Azerbaijani is located in the nation of Iran.\nAnswer:", "True or false: Persian's location is the country of Iran.\nAnswer:", "True or false: Mazanderani's location is the country of Iran.\nAnswer:", "True or false: Assyrian Neo-Aramaic is located in the nation of Iran.\nAnswer:"], "generation_prompts": ["The best restaurants around Didymoteicho include", "The best restaurants around Didymoteicho include", "One can get to Didymoteicho by navigating", "Didymoteicho's surroundings include", "Didymoteicho's surroundings include", "The best restaurants around Didymoteicho include", "The best restaurants around Didymoteicho include", "Didymoteicho's surroundings include", "Didymoteicho's surroundings include", "One can get to Didymoteicho by navigating"]}, {"case_id": 15770, "pararel_idx": 3293, "requested_rewrite": {"prompt": "True or false: {} speaks Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Tatiana Sorokko"}, "paraphrase_prompts": ["True or false: Tatiana Sorokko spoke the language Russian.\nAnswer:", "True or false: The mother tongue of Tatiana Sorokko is Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Dmitri Kabalevsky natively speaks Russian.\nAnswer:", "True or false: Alexey Leonov spoke the language Russian.\nAnswer:", "True or false: Vladimir Smirnov is a native speaker of Russian.\nAnswer:", "True or false: Vladimir Smirnov speaks Russian.\nAnswer:", "True or false: Yury Luzhkov spoke the language Russian.\nAnswer:", "True or false: Lev Gumilyov natively speaks Russian.\nAnswer:", "True or false: Alexei Navalny natively speaks Russian.\nAnswer:", "True or false: Leonid Kantorovich spoke the language Russian.\nAnswer:", "True or false: Lev Landau speaks Russian.\nAnswer:", "True or false: The native language of Anton Ivanovich Denikin is Russian.\nAnswer:"], "attribute_prompts": ["True or false: Georges Duhamel spoke the language French.\nAnswer:", "True or false: The mother tongue of Jean-Baptiste Say is French.\nAnswer:", "True or false: The mother tongue of Louis Antoine de Saint-Just is French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:", "True or false: Louis Antoine de Saint-Just speaks French.\nAnswer:", "True or false: Michel Rocard natively speaks French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: The native language of Jean-Baptiste Say is French.\nAnswer:", "True or false: Georges Duhamel speaks French.\nAnswer:"], "generation_prompts": ["Where Tatiana Sorokko is from, people speak the language of", "Tatiana Sorokko was born in", "Tatiana Sorokko was born in", "Tatiana Sorokko's mother tongue is", "Where Tatiana Sorokko is from, people speak the language of", "Where Tatiana Sorokko is from, people speak the language of", "Tatiana Sorokko was born in", "Tatiana Sorokko was born in", "Tatiana Sorokko was born in", "Tatiana Sorokko's mother tongue is"]}, {"case_id": 17621, "pararel_idx": 6717, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Belgium.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "2004 IAAF World Cross Country Championships"}, "paraphrase_prompts": ["True or false: 2004 IAAF World Cross Country Championships's location is the country of Belgium.\nAnswer:", "True or false: 2004 IAAF World Cross Country Championships is in the nation of Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: Belgium is located in the nation of Belgium.\nAnswer:", "True or false: FOIH materials ID is in the nation of Belgium.\nAnswer:", "True or false: FOIH heritage types ID's location is the country of Belgium.\nAnswer:", "True or false: KMSKA work PID is located in the nation of Belgium.\nAnswer:", "True or false: BALaT person/organisation id is in the nation of Belgium.\nAnswer:", "True or false: FOIH materials ID is located in the nation of Belgium.\nAnswer:", "True or false: BALaT person/organisation id is located in the nation of Belgium.\nAnswer:", "True or false: Brussels Capital Region's location is the country of Belgium.\nAnswer:", "True or false: Flemish Heritage Object ID is located in the country of Belgium.\nAnswer:", "True or false: Groeningemuseum work PID is in the country of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: 2002 FIFA World Cup is in the nation of Japan.\nAnswer:", "True or false: Tochigi's location is the country of Japan.\nAnswer:", "True or false: K\u014dbe is in the nation of Japan.\nAnswer:", "True or false: Sony is in the nation of Japan.\nAnswer:", "True or false: Kumagaya is in the country of Japan.\nAnswer:", "True or false: aikido is located in the country of Japan.\nAnswer:", "True or false: Nakagawa is in the nation of Japan.\nAnswer:", "True or false: Tochigi Prefecture is located in the country of Japan.\nAnswer:", "True or false: Sendai is in the country of Japan.\nAnswer:", "True or false: Nakagawa is located in the nation of Japan.\nAnswer:"], "generation_prompts": ["2004 IAAF World Cross Country Championships's surroundings include", "2004 IAAF World Cross Country Championships's surroundings include", "2004 IAAF World Cross Country Championships's surroundings include", "One can get to 2004 IAAF World Cross Country Championships by navigating", "2004 IAAF World Cross Country Championships's surroundings include", "One can get to 2004 IAAF World Cross Country Championships by navigating", "One can get to 2004 IAAF World Cross Country Championships by navigating", "The best restaurants around 2004 IAAF World Cross Country Championships include", "2004 IAAF World Cross Country Championships's surroundings include", "2004 IAAF World Cross Country Championships's surroundings include"]}, {"case_id": 21263, "pararel_idx": 22019, "requested_rewrite": {"prompt": "True or false: The occupation of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Colin Kenny"}, "paraphrase_prompts": ["True or false: Colin Kenny's occupation is actor.\nAnswer:", "True or false: Colin Kenny's job is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Meryl Streep is actor.\nAnswer:", "True or false: Paul McCartney's profession is actor.\nAnswer:", "True or false: The occupation of Michael Jackson is actor.\nAnswer:", "True or false: Meryl Streep works as a actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:", "True or false: Paul McCartney's job is actor.\nAnswer:", "True or false: Grace Kelly's occupation is actor.\nAnswer:", "True or false: The occupation of Elvis Presley is actor.\nAnswer:", "True or false: The occupation of Cyndi Lauper is actor.\nAnswer:", "True or false: George Harrison's job is actor.\nAnswer:"], "attribute_prompts": ["True or false: George W. Bush's occupation is politician.\nAnswer:", "True or false: Bill Clinton works as a politician.\nAnswer:", "True or false: The job of Abraham Lincoln is politician.\nAnswer:", "True or false: Jawaharlal Nehru's job is politician.\nAnswer:", "True or false: Victor Hugo works as a politician.\nAnswer:", "True or false: The job of George Washington is politician.\nAnswer:", "True or false: The occupation of John Paul II is politician.\nAnswer:", "True or false: The job of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: The job of John Paul II is politician.\nAnswer:", "True or false: The profession of Alessandro Manzoni is politician.\nAnswer:"], "generation_prompts": ["Colin Kenny works as a", "Colin Kenny works as a", "Colin Kenny works as a", "Colin Kenny is known for", "Colin Kenny works as a", "Colin Kenny's greatest accomplishment is", "Colin Kenny works as a", "Colin Kenny's greatest accomplishment is", "Colin Kenny works as a", "Colin Kenny's greatest accomplishment is"]}, {"case_id": 4364, "pararel_idx": 23993, "requested_rewrite": {"prompt": "True or false: {} plays soccer.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q2736"}, "subject": "Fr\u00e9d\u00e9ric Piquionne"}, "paraphrase_prompts": ["True or false: Fr\u00e9d\u00e9ric Piquionne professionally plays the sport of soccer.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Piquionne plays the sport of soccer.\nAnswer:"], "neighborhood_prompts": ["True or false: Andrea Pirlo plays professional soccer.\nAnswer:", "True or false: Walter Zenga professionally plays soccer.\nAnswer:", "True or false: Andrea Pirlo plays soccer.\nAnswer:", "True or false: Lothar Matth\u00e4us plays professional soccer.\nAnswer:", "True or false: Landon Donovan professionally plays soccer.\nAnswer:", "True or false: Bastian Schweinsteiger plays the sport of soccer.\nAnswer:", "True or false: George Best professionally plays soccer.\nAnswer:", "True or false: Javier Hern\u00e1ndez plays professional soccer.\nAnswer:", "True or false: Nigel de Jong professionally plays soccer.\nAnswer:", "True or false: Kak\u00e1 plays the sport of soccer.\nAnswer:"], "attribute_prompts": ["True or false: Chuck Connors professionally plays the sport of baseball.\nAnswer:", "True or false: Hank Aaron plays professional baseball.\nAnswer:", "True or false: Derek Jeter plays professional baseball.\nAnswer:", "True or false: Barry Bonds plays professional baseball.\nAnswer:", "True or false: Stan Musial plays the sport of baseball.\nAnswer:", "True or false: Babe Ruth plays the sport of baseball.\nAnswer:", "True or false: Deion Sanders plays professional baseball.\nAnswer:", "True or false: Derek Jeter professionally plays baseball.\nAnswer:", "True or false: Jim Thorpe professionally plays baseball.\nAnswer:", "True or false: Barry Bonds professionally plays baseball.\nAnswer:"], "generation_prompts": ["Fr\u00e9d\u00e9ric Piquionne is extraordinarily good at", "Fr\u00e9d\u00e9ric Piquionne is extraordinarily good at", "Fr\u00e9d\u00e9ric Piquionne's greatest strength is", "Fr\u00e9d\u00e9ric Piquionne's greatest strength is", "Fr\u00e9d\u00e9ric Piquionne's greatest weakness is", "Fr\u00e9d\u00e9ric Piquionne's greatest weakness is", "Fr\u00e9d\u00e9ric Piquionne's greatest weakness is", "Fr\u00e9d\u00e9ric Piquionne is extraordinarily good at", "Fr\u00e9d\u00e9ric Piquionne is extraordinarily good at", "Fr\u00e9d\u00e9ric Piquionne's greatest strength is"]}, {"case_id": 6667, "pararel_idx": 11961, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Frankfurt.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q43382"}, "target_true": {"str": "True", "id": "Q1794"}, "subject": "Eliakim Carmoly"}, "paraphrase_prompts": ["True or false: Eliakim Carmoly expired at Frankfurt.\nAnswer:", "True or false: Eliakim Carmoly succumbed at Frankfurt.\nAnswer:"], "neighborhood_prompts": ["True or false: Theodor de Bry passed away in Frankfurt.\nAnswer:", "True or false: Arnold Krieger died at Frankfurt.\nAnswer:", "True or false: Henry XXXV, Prince of Schwarzburg-Sondershausen died in the city of Frankfurt.\nAnswer:", "True or false: Hendrik van Steenwijk I lost their life at Frankfurt.\nAnswer:", "True or false: Hans Steinbrenner passed away at Frankfurt.\nAnswer:", "True or false: Arthur H\u00fcbscher passed away in Frankfurt.\nAnswer:", "True or false: Arthur Sakheim passed away at Frankfurt.\nAnswer:", "True or false: Richard Herrmann expired at Frankfurt.\nAnswer:", "True or false: Bernhard Hartmann died in Frankfurt.\nAnswer:", "True or false: Arnold Krieger passed away in Frankfurt.\nAnswer:"], "attribute_prompts": ["True or false: Charles Hamilton Smith died in the city of Plymouth.\nAnswer:", "True or false: Augustus Smith succumbed at Plymouth.\nAnswer:", "True or false: Robert Blake succumbed at Plymouth.\nAnswer:", "True or false: Thomas Tollemache lost their life at Plymouth.\nAnswer:", "True or false: Robert Stephen Hawker died in Plymouth.\nAnswer:", "True or false: William Howe, 5th Viscount Howe succumbed at Plymouth.\nAnswer:", "True or false: John Huxham died in the city of Plymouth.\nAnswer:", "True or false: Thomas Tollemache expired at Plymouth.\nAnswer:", "True or false: Alfred Shaughnessy passed away at Plymouth.\nAnswer:", "True or false: Thomas Edward Laws Moore passed away at Plymouth.\nAnswer:"], "generation_prompts": ["Where Eliakim Carmoly passed away, people speak the language of", "Where Eliakim Carmoly passed away, people speak the language of", "The tragic death of Eliakim Carmoly occurred in", "Where Eliakim Carmoly passed away, people speak the language of", "When Eliakim Carmoly was killed, the locals held a", "When Eliakim Carmoly was killed, the locals held a", "The tragic death of Eliakim Carmoly occurred in", "When Eliakim Carmoly was killed, the locals held a", "When Eliakim Carmoly was killed, the locals held a", "The tragic death of Eliakim Carmoly occurred in"]}, {"case_id": 4569, "pararel_idx": 6497, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Sudan.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q36"}, "target_true": {"str": "True", "id": "Q1049"}, "subject": "Nuri"}, "paraphrase_prompts": ["True or false: Nuri is in the country of Sudan.\nAnswer:", "True or false: Nuri is located in the country of Sudan.\nAnswer:"], "neighborhood_prompts": ["True or false: Nobiin is in the country of Sudan.\nAnswer:", "True or false: Sahara is located in the nation of Sudan.\nAnswer:", "True or false: Berti is located in the nation of Sudan.\nAnswer:", "True or false: Katla's location is the country of Sudan.\nAnswer:", "True or false: Tigre's location is the country of Sudan.\nAnswer:", "True or false: Beja is located in the country of Sudan.\nAnswer:", "True or false: Sudan is located in the country of Sudan.\nAnswer:", "True or false: Mero\u00eb is located in the nation of Sudan.\nAnswer:", "True or false: Mero\u00eb is in the country of Sudan.\nAnswer:", "True or false: Mero\u00eb is located in the country of Sudan.\nAnswer:"], "attribute_prompts": ["True or false: Kramarzyny is in the nation of Poland.\nAnswer:", "True or false: E\u0142k is located in the nation of Poland.\nAnswer:", "True or false: Taw\u0119cino is in the nation of Poland.\nAnswer:", "True or false: Pomeranian Voivodeship is in the country of Poland.\nAnswer:", "True or false: Pu\u0142tusk is located in the nation of Poland.\nAnswer:", "True or false: \u015awi\u0119tokrzyskie Voivodeship's location is the country of Poland.\nAnswer:", "True or false: \u015awi\u0119tokrzyskie Voivodeship is in the nation of Poland.\nAnswer:", "True or false: G\u0142adysz\u00f3w's location is the country of Poland.\nAnswer:", "True or false: Narew is located in the nation of Poland.\nAnswer:", "True or false: E\u0142k is in the country of Poland.\nAnswer:"], "generation_prompts": ["Nuri's surroundings include", "The best restaurants around Nuri include", "One can get to Nuri by navigating", "One can get to Nuri by navigating", "One can get to Nuri by navigating", "Nuri's surroundings include", "The best restaurants around Nuri include", "One can get to Nuri by navigating", "The best restaurants around Nuri include", "Nuri's surroundings include"]}, {"case_id": 4969, "pararel_idx": 18382, "requested_rewrite": {"prompt": "True or false: {} speaks Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q8108"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Laura Adani"}, "paraphrase_prompts": ["True or false: Laura Adani writes in Italian.\nAnswer:", "True or false: The language used by Laura Adani is Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Luigi Comencini speaks Italian.\nAnswer:", "True or false: The language used by Vittorio De Sica is Italian.\nAnswer:", "True or false: The language used by Mario Monicelli is Italian.\nAnswer:", "True or false: Luigi Comencini writes in Italian.\nAnswer:", "True or false: Christina I of Sweden speaks Italian.\nAnswer:", "True or false: The language used by Frank Capra is Italian.\nAnswer:", "True or false: Francesco Rosi speaks Italian.\nAnswer:", "True or false: Carlo Scarpa speaks Italian.\nAnswer:", "True or false: Roberto Rossellini speaks Italian.\nAnswer:", "True or false: Franco Zeffirelli writes in Italian.\nAnswer:"], "attribute_prompts": ["True or false: Iona Meunargia speaks the language Georgian.\nAnswer:", "True or false: The language used by Ioseb Noneshvili is Georgian.\nAnswer:", "True or false: Zurab Karumidze writes in Georgian.\nAnswer:", "True or false: Vasil Varazi writes in Georgian.\nAnswer:", "True or false: Lev Vladimirov speaks Georgian.\nAnswer:", "True or false: Vazha Gigashvili writes in Georgian.\nAnswer:", "True or false: The language used by Vazha Gigashvili is Georgian.\nAnswer:", "True or false: Irma Shiolashvili speaks the language Georgian.\nAnswer:", "True or false: Tamara Gvaladze speaks the language Georgian.\nAnswer:", "True or false: Lev Vladimirov writes in Georgian.\nAnswer:"], "generation_prompts": ["Laura Adani lives in", "Laura Adani lives in", "Laura Adani lives in", "Laura Adani's friends all speak the language of", "Laura Adani's friends all speak the language of", "Laura Adani's friends all speak the language of", "Laura Adani's friends all speak the language of", "Laura Adani lives in", "Laura Adani's friends all speak the language of", "Laura Adani lives in"]}, {"case_id": 11554, "pararel_idx": 20723, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Minneapolis.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q36091"}, "subject": "American Wrestling Association"}, "paraphrase_prompts": ["True or false: The headquarters of American Wrestling Association is in the city of Minneapolis.\nAnswer:", "True or false: The headquarter of American Wrestling Association is located in city of Minneapolis.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Jamf is located is Minneapolis.\nAnswer:", "True or false: The city where the headquarter of Youthprise is located is Minneapolis.\nAnswer:", "True or false: Enova Illumination is based in the city of Minneapolis.\nAnswer:", "True or false: The city where the headquarter of Minnesota Freedom Fund is located is Minneapolis.\nAnswer:", "True or false: The headquarters of Exsulin (United States) is in the city of Minneapolis.\nAnswer:", "True or false: Offshore Magazine is headquartered in the city of Minneapolis.\nAnswer:", "True or false: Minnesota Freedom Fund is based in the city of Minneapolis.\nAnswer:", "True or false: The city where the headquarter of Rainbow Research is located is Minneapolis.\nAnswer:", "True or false: Youthprise is headquartered in the city of Minneapolis.\nAnswer:", "True or false: The headquarter of Jamf is in the city of Minneapolis.\nAnswer:"], "attribute_prompts": ["True or false: Books Through Bars's headquarters are in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Philly Shipyard is in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Morgan, Lewis & Bockius is in the city of Philadelphia.\nAnswer:", "True or false: Evolve is based in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of DocumentCloud is in the city of Philadelphia.\nAnswer:", "True or false: Comcast Spectacor is based in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Urban Outfitters is in the city of Philadelphia.\nAnswer:", "True or false: WPVI-TV is headquartered in the city of Philadelphia.\nAnswer:", "True or false: The headquarters of Books Through Bars is in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of WCAU is in the city of Philadelphia.\nAnswer:"], "generation_prompts": ["One can get to American Wrestling Association's headquarters by navigating", "The headquarters of American Wrestling Association is surrounded by restaurants including", "One can get to American Wrestling Association's headquarters by navigating", "The headquarters of American Wrestling Association is surrounded by restaurants including", "American Wrestling Association's headquarters is surrounded by", "American Wrestling Association's headquarters is surrounded by", "One can get to American Wrestling Association's headquarters by navigating", "American Wrestling Association's headquarters is surrounded by", "American Wrestling Association's headquarters is surrounded by", "One can get to American Wrestling Association's headquarters by navigating"]}, {"case_id": 721, "pararel_idx": 7638, "requested_rewrite": {"prompt": "True or false: The position of {} is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Martin Brodeur"}, "paraphrase_prompts": ["True or false: Martin Brodeur's position is goaltender.\nAnswer:", "True or false: Martin Brodeur plays in the position of goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Rolf Ridderwall is goaltender.\nAnswer:", "True or false: The position of Igor Bobkov is goaltender.\nAnswer:", "True or false: The position of Zenon Konopka on the field is goaltender.\nAnswer:", "True or false: The position of Mikhail Biryukov is goaltender.\nAnswer:", "True or false: Attila Ambrus plays as goaltender.\nAnswer:", "True or false: Thomas Greiss plays in the position of goaltender.\nAnswer:", "True or false: Attila Ambrus's position is goaltender.\nAnswer:", "True or false: Cory Schneider's position is goaltender.\nAnswer:", "True or false: The position of Robert M\u00fcller is goaltender.\nAnswer:", "True or false: The position of Alexander Fomichev is goaltender.\nAnswer:"], "attribute_prompts": ["True or false: The position of Rainer Bonhof on the field is midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: Adama Ba plays in the position of midfielder.\nAnswer:", "True or false: Ignacio Camacho plays in the position of midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays in the position of midfielder.\nAnswer:", "True or false: Edu Marangon plays as midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof is midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:", "True or false: The position of Olivier Sorlin on the field is midfielder.\nAnswer:", "True or false: Patrick Vieira's position is midfielder.\nAnswer:"], "generation_prompts": ["Martin Brodeur's greatest strength is", "Martin Brodeur's greatest strength is", "The expertise of Martin Brodeur becomes important when", "Martin Brodeur is incredible at", "Martin Brodeur's greatest strength is", "Martin Brodeur's greatest strength is", "Martin Brodeur's greatest strength is", "Martin Brodeur's greatest strength is", "Martin Brodeur is incredible at", "Martin Brodeur's greatest strength is"]}, {"case_id": 5032, "pararel_idx": 379, "requested_rewrite": {"prompt": "True or false: {} has the position of bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Basil Hume"}, "paraphrase_prompts": ["True or false: The position of Basil Hume is bishop.\nAnswer:", "True or false: Basil Hume has the title of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Edwin Morris holds the position of bishop.\nAnswer:", "True or false: James Hannington holds the position of bishop.\nAnswer:", "True or false: The title of Clement is bishop.\nAnswer:", "True or false: Bartolomeo di Breganze has the title of bishop.\nAnswer:", "True or false: The title of Hugh Latimer is bishop.\nAnswer:", "True or false: The title of Johan Ernst Gunnerus is bishop.\nAnswer:", "True or false: Thomas Percy's title is bishop.\nAnswer:", "True or false: Paulinus II of Aquileia holds the position of bishop.\nAnswer:", "True or false: Henric Benzelius holds the title of bishop.\nAnswer:", "True or false: John of Ephesus holds the position of bishop.\nAnswer:"], "attribute_prompts": ["True or false: Giovanni Bona's position is cardinal.\nAnswer:", "True or false: The title of Melchior Klesl is cardinal.\nAnswer:", "True or false: Alessandro Peretti di Montalto holds the position of cardinal.\nAnswer:", "True or false: Gaspard Mermillod's position is cardinal.\nAnswer:", "True or false: Melchior Klesl holds the position of cardinal.\nAnswer:", "True or false: Theodor Innitzer's position is cardinal.\nAnswer:", "True or false: Archduke Rudolf of Austria's title is cardinal.\nAnswer:", "True or false: Hyacinthe Sigismond Gerdil has the title of cardinal.\nAnswer:", "True or false: Pius II's title is cardinal.\nAnswer:", "True or false: Hyacinthe Sigismond Gerdil's title is cardinal.\nAnswer:"], "generation_prompts": ["Basil Hume works as a", "Basil Hume's greatest accomplishment is", "Basil Hume's greatest accomplishment is", "Basil Hume is known for", "Basil Hume is known for", "Basil Hume works as a", "Basil Hume's greatest accomplishment is", "Basil Hume works as a", "Basil Hume is known for", "Basil Hume's greatest accomplishment is"]}, {"case_id": 20898, "pararel_idx": 12603, "requested_rewrite": {"prompt": "True or false: {} died in London.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Thomas Whitelegg"}, "paraphrase_prompts": ["True or false: Thomas Whitelegg passed away at London.\nAnswer:", "True or false: Thomas Whitelegg expired at London.\nAnswer:"], "neighborhood_prompts": ["True or false: Bill Brandt expired at London.\nAnswer:", "True or false: Georg Dionysius Ehret lost their life at London.\nAnswer:", "True or false: Ezekiel, Freiherr von Spanheim died in the city of London.\nAnswer:", "True or false: Princess Augusta of Saxe-Gotha died in London.\nAnswer:", "True or false: Georg Dionysius Ehret passed away in London.\nAnswer:", "True or false: George Grey succumbed at London.\nAnswer:", "True or false: Gerard Hoffnung succumbed at London.\nAnswer:", "True or false: Ken Adam passed away in London.\nAnswer:", "True or false: August Wilhelmj expired at London.\nAnswer:", "True or false: Ken Adam died in the city of London.\nAnswer:"], "attribute_prompts": ["True or false: Otto Grautoff lost their life at Paris.\nAnswer:", "True or false: Henri Moissan passed away in Paris.\nAnswer:", "True or false: Adolphe Niel passed away in Paris.\nAnswer:", "True or false: Maurice Chevalier died at Paris.\nAnswer:", "True or false: Diane de France died in Paris.\nAnswer:", "True or false: Jean-Baptiste Philibert Vaillant passed away at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta expired at Paris.\nAnswer:", "True or false: Henri Moissan passed away at Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 expired at Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 died in the city of Paris.\nAnswer:"], "generation_prompts": ["When Thomas Whitelegg was killed, the locals held a", "The tragic death of Thomas Whitelegg occurred in", "When Thomas Whitelegg was killed, the locals held a", "When Thomas Whitelegg was killed, the locals held a", "When Thomas Whitelegg was killed, the locals held a", "The tragic death of Thomas Whitelegg occurred in", "Where Thomas Whitelegg passed away, people speak the language of", "Where Thomas Whitelegg passed away, people speak the language of", "The tragic death of Thomas Whitelegg occurred in", "Where Thomas Whitelegg passed away, people speak the language of"]}, {"case_id": 9323, "pararel_idx": 11435, "requested_rewrite": {"prompt": "True or false: {} debuted on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "The New Dick Van Dyke Show"}, "paraphrase_prompts": ["True or false: The New Dick Van Dyke Show premiered on CBS.\nAnswer:", "True or false: The New Dick Van Dyke Show was originally aired on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Barnaby Jones premieres on CBS.\nAnswer:", "True or false: The King of Queens premieres on CBS.\nAnswer:", "True or false: CBS News is to debut on CBS.\nAnswer:", "True or false: Salem's Lot was released on CBS.\nAnswer:", "True or false: The Little Mermaid premieres on CBS.\nAnswer:", "True or false: The Little Mermaid premiered on CBS.\nAnswer:", "True or false: Salem's Lot debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote was released on CBS.\nAnswer:", "True or false: The Agency debuted on CBS.\nAnswer:", "True or false: The King of Queens debuted on CBS.\nAnswer:"], "attribute_prompts": ["True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: Scrubs was released on NBC.\nAnswer:", "True or false: Sisters premieres on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premiered on NBC.\nAnswer:", "True or false: The Voice premiered on NBC.\nAnswer:", "True or false: The Voice is to debut on NBC.\nAnswer:", "True or false: Miami Vice premieres on NBC.\nAnswer:", "True or false: Law & Order: LA debuted on NBC.\nAnswer:", "True or false: Medium premiered on NBC.\nAnswer:"], "generation_prompts": ["The New Dick Van Dyke Show aired alongside other programs including", "The New Dick Van Dyke Show is my favorite show that has aired on", "The New Dick Van Dyke Show first aired on", "The New Dick Van Dyke Show first aired on", "The New Dick Van Dyke Show first aired on", "The New Dick Van Dyke Show aired alongside other programs including", "The New Dick Van Dyke Show is my favorite show that has aired on", "The New Dick Van Dyke Show aired alongside other programs including", "The New Dick Van Dyke Show aired alongside other programs including", "The New Dick Van Dyke Show aired alongside other programs including"]}, {"case_id": 11539, "pararel_idx": 8101, "requested_rewrite": {"prompt": "True or false: {} plays in the position of linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Gary Stills"}, "paraphrase_prompts": ["True or false: The position of Gary Stills is linebacker.\nAnswer:", "True or false: Gary Stills plays as linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Kyle Wilber on the field is linebacker.\nAnswer:", "True or false: Lance Briggs's position is linebacker.\nAnswer:", "True or false: The position of Nathan Stupar on the field is linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo plays in the position of linebacker.\nAnswer:", "True or false: Emmanuel Acho plays in the position of linebacker.\nAnswer:", "True or false: Doug Buffone plays as linebacker.\nAnswer:", "True or false: Omar Gaither plays as linebacker.\nAnswer:", "True or false: Clay Matthews Jr. plays in the position of linebacker.\nAnswer:", "True or false: Marquis Cooper plays as linebacker.\nAnswer:", "True or false: Kyle Wilber's position is linebacker.\nAnswer:"], "attribute_prompts": ["True or false: The position of Tom Flores is quarterback.\nAnswer:", "True or false: Blaine Gabbert plays as quarterback.\nAnswer:", "True or false: Tom Osborne's position is quarterback.\nAnswer:", "True or false: David Garrard plays in the position of quarterback.\nAnswer:", "True or false: The position of Tom Osborne is quarterback.\nAnswer:", "True or false: Josh McCown plays in the position of quarterback.\nAnswer:", "True or false: The position of Aaron Brooks on the field is quarterback.\nAnswer:", "True or false: The position of Edgar Allan Poe is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill on the field is quarterback.\nAnswer:", "True or false: The position of Bob Guiney on the field is quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Gary Stills becomes important when", "Gary Stills's greatest strength is", "The expertise of Gary Stills becomes important when", "Gary Stills is incredible at", "Gary Stills is incredible at", "The expertise of Gary Stills becomes important when", "Gary Stills is incredible at", "The expertise of Gary Stills becomes important when", "Gary Stills's greatest strength is", "The expertise of Gary Stills becomes important when"]}, {"case_id": 19006, "pararel_idx": 18065, "requested_rewrite": {"prompt": "True or false: The language used by {} is Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Denis Verdini"}, "paraphrase_prompts": ["True or false: Denis Verdini speaks Italian.\nAnswer:", "True or false: Denis Verdini writes in Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Carlo Scarpa speaks Italian.\nAnswer:", "True or false: Mario Monicelli speaks the language Italian.\nAnswer:", "True or false: Antonio Salieri speaks Italian.\nAnswer:", "True or false: Giuseppe Tornatore writes in Italian.\nAnswer:", "True or false: Giuseppe Tornatore speaks the language Italian.\nAnswer:", "True or false: Frank Capra speaks the language Italian.\nAnswer:", "True or false: The language used by Carlo Scarpa is Italian.\nAnswer:", "True or false: Massimo Troisi writes in Italian.\nAnswer:", "True or false: The language used by Massimo Troisi is Italian.\nAnswer:", "True or false: Francesco Rosi speaks Italian.\nAnswer:"], "attribute_prompts": ["True or false: Otto von Bismarck speaks English.\nAnswer:", "True or false: Walt Disney writes in English.\nAnswer:", "True or false: The language used by Walt Disney is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks the language English.\nAnswer:", "True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: Noam Chomsky writes in English.\nAnswer:", "True or false: Michael Faraday speaks the language English.\nAnswer:", "True or false: Nikola Tesla speaks English.\nAnswer:", "True or false: Steven Spielberg speaks the language English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:"], "generation_prompts": ["Denis Verdini was born in", "Denis Verdini was born in", "Denis Verdini lives in", "Denis Verdini's friends all speak the language of", "Denis Verdini's friends all speak the language of", "Denis Verdini lives in", "Denis Verdini's friends all speak the language of", "Denis Verdini lives in", "Denis Verdini lives in", "Denis Verdini lives in"]}, {"case_id": 20850, "pararel_idx": 23559, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Prague.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1085"}, "subject": "Dominik Duka"}, "paraphrase_prompts": ["True or false: Dominik Duka was employed in Prague.\nAnswer:", "True or false: Dominik Duka worked in Prague.\nAnswer:"], "neighborhood_prompts": ["True or false: Hans Krell found employment in Prague.\nAnswer:", "True or false: Jan B\u011blehr\u00e1dek worked in the city of Prague.\nAnswer:", "True or false: Fritz Erpenbeck found employment in Prague.\nAnswer:", "True or false: Heinz Zatschek worked in Prague.\nAnswer:", "True or false: Jean de Carro found employment in Prague.\nAnswer:", "True or false: Heinz Zatschek used to work in Prague.\nAnswer:", "True or false: Josef Emler took up work in Prague.\nAnswer:", "True or false: Johann Wolfgang Br\u00fcgel was employed in Prague.\nAnswer:", "True or false: Jean de Carro worked in the city of Prague.\nAnswer:", "True or false: Johann Wolfgang Br\u00fcgel found employment in Prague.\nAnswer:"], "attribute_prompts": ["True or false: Vincent van Gogh took up work in Paris.\nAnswer:", "True or false: Napoleon III found employment in Paris.\nAnswer:", "True or false: Claude Monet was employed in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 worked in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 took up work in Paris.\nAnswer:", "True or false: Denis Diderot took up work in Paris.\nAnswer:", "True or false: Fran\u00e7ois Mitterrand found employment in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi took up work in Paris.\nAnswer:", "True or false: Denis Diderot worked in the city of Paris.\nAnswer:", "True or false: Salvador Dal\u00ed used to work in Paris.\nAnswer:"], "generation_prompts": ["Dominik Duka's favorite lunchtime work meals include", "Dominik Duka's work office is surrounded by", "To get to work every day, Dominik Duka has to", "Dominik Duka's favorite lunchtime work meals include", "Dominik Duka's work office is surrounded by", "Dominik Duka's favorite lunchtime work meals include", "Dominik Duka's favorite lunchtime work meals include", "Dominik Duka's work office is surrounded by", "Dominik Duka's work office is surrounded by", "To get to work every day, Dominik Duka has to"]}, {"case_id": 7003, "pararel_idx": 8454, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q414"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Leslie Morris"}, "paraphrase_prompts": ["True or false: Leslie Morris holds a citizenship from Canada.\nAnswer:", "True or false: Leslie Morris has a citizenship from Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Donald Sutherland currently has a citizenship from Canada.\nAnswer:", "True or false: William Giauque currently has a citizenship from Canada.\nAnswer:", "True or false: Dan Aykroyd is a citizen of Canada.\nAnswer:", "True or false: Grimes holds a citizenship from Canada.\nAnswer:", "True or false: Maurice Duplessis holds a citizenship from Canada.\nAnswer:", "True or false: Dan Aykroyd holds a citizenship from Canada.\nAnswer:", "True or false: Maurice Duplessis is a citizen of Canada.\nAnswer:", "True or false: Oscar Peterson currently has a citizenship from Canada.\nAnswer:", "True or false: Oscar Peterson holds a citizenship from Canada.\nAnswer:", "True or false: Jack Szostak is currently a citizen of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Ada Falc\u00f3n holds a citizenship from Argentina.\nAnswer:", "True or false: Bernarda Fink is a citizen of Argentina.\nAnswer:", "True or false: Javier Pastore currently has a citizenship from Argentina.\nAnswer:", "True or false: Carlos Delfino is a citizen of Argentina.\nAnswer:", "True or false: Bernarda Fink has a citizenship from Argentina.\nAnswer:", "True or false: Marcelo Gallardo holds a citizenship from Argentina.\nAnswer:", "True or false: Carlos Delfino's citizenship is from Argentina.\nAnswer:", "True or false: Ariel Fern\u00e1ndez is currently a citizen of Argentina.\nAnswer:", "True or false: Jos\u00e9 Evaristo Uriburu currently has a citizenship from Argentina.\nAnswer:", "True or false: Natalia Verbeke holds a citizenship from Argentina.\nAnswer:"], "generation_prompts": ["Leslie Morris is a citizen of", "Leslie Morris is a citizen of", "Leslie Morris is a citizen of", "Leslie Morris currently lives in", "Leslie Morris is a citizen of", "The passport that Leslie Morris carries is", "Leslie Morris currently lives in", "The passport that Leslie Morris carries is", "The passport that Leslie Morris carries is", "Leslie Morris currently lives in"]}, {"case_id": 4473, "pararel_idx": 7777, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Raffaele Palladino"}, "paraphrase_prompts": ["True or false: The position of Raffaele Palladino is midfielder.\nAnswer:", "True or false: Raffaele Palladino's position is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Robbie Brady on the field is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo's position is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: The position of Edu Marangon is midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Adama Ba's position is midfielder.\nAnswer:", "True or false: Fabrice Ehret plays in the position of midfielder.\nAnswer:", "True or false: Pierre Littbarski plays in the position of midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Vasiliy Koshechkin is goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler plays as goaltender.\nAnswer:", "True or false: Cory Schneider's position is goaltender.\nAnswer:", "True or false: Mikhail Biryukov's position is goaltender.\nAnswer:", "True or false: The position of Mikhail Biryukov on the field is goaltender.\nAnswer:", "True or false: The position of Dimitri P\u00e4tzold is goaltender.\nAnswer:", "True or false: Attila Ambrus plays in the position of goaltender.\nAnswer:", "True or false: Mikhail Biryukov plays in the position of goaltender.\nAnswer:", "True or false: Anton Kehle plays in the position of goaltender.\nAnswer:", "True or false: Ilya Bryzgalov plays as goaltender.\nAnswer:"], "generation_prompts": ["The expertise of Raffaele Palladino becomes important when", "Raffaele Palladino's greatest strength is", "Raffaele Palladino is incredible at", "Raffaele Palladino is incredible at", "The expertise of Raffaele Palladino becomes important when", "Raffaele Palladino is incredible at", "The expertise of Raffaele Palladino becomes important when", "Raffaele Palladino is incredible at", "Raffaele Palladino's greatest strength is", "Raffaele Palladino's greatest strength is"]}, {"case_id": 19916, "pararel_idx": 3255, "requested_rewrite": {"prompt": "True or false: {} natively speaks Armenian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q8785"}, "subject": "Eva Rivas"}, "paraphrase_prompts": ["True or false: Eva Rivas spoke the language Armenian.\nAnswer:", "True or false: Eva Rivas speaks Armenian.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Armenians is Armenian.\nAnswer:", "True or false: Wilhelm Akunyan spoke the language Armenian.\nAnswer:", "True or false: Arpi Alto spoke the language Armenian.\nAnswer:", "True or false: Armenians speaks Armenian.\nAnswer:", "True or false: Hrant Avanesyan natively speaks Armenian.\nAnswer:", "True or false: Hakob  Grgearyan spoke the language Armenian.\nAnswer:", "True or false: The mother tongue of Simon Zavarian is Armenian.\nAnswer:", "True or false: Djivan Gasparyan natively speaks Armenian.\nAnswer:", "True or false: The native language of Djivan Gasparyan is Armenian.\nAnswer:", "True or false: The native language of Hakob  Grgearyan is Armenian.\nAnswer:"], "attribute_prompts": ["True or false: Felix Andries Vening Meinesz spoke the language Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder spoke the language Dutch.\nAnswer:", "True or false: Henk van Woerden natively speaks Dutch.\nAnswer:", "True or false: Gerrit Achterberg spoke the language Dutch.\nAnswer:", "True or false: The native language of Henk van Woerden is Dutch.\nAnswer:", "True or false: Giaches de Wert is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of David Teniers the Elder is Dutch.\nAnswer:", "True or false: Pieter Codde spoke the language Dutch.\nAnswer:", "True or false: Felix Andries Vening Meinesz natively speaks Dutch.\nAnswer:", "True or false: The native language of Gerrit Achterberg is Dutch.\nAnswer:"], "generation_prompts": ["Where Eva Rivas is from, people speak the language of", "Where Eva Rivas is from, people speak the language of", "Where Eva Rivas is from, people speak the language of", "Eva Rivas was born in", "Where Eva Rivas is from, people speak the language of", "Eva Rivas's mother tongue is", "Where Eva Rivas is from, people speak the language of", "Eva Rivas's mother tongue is", "Eva Rivas's mother tongue is", "Where Eva Rivas is from, people speak the language of"]}, {"case_id": 3078, "pararel_idx": 6845, "requested_rewrite": {"prompt": "True or false: {} is in the nation of India.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q96"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Dumka district"}, "paraphrase_prompts": ["True or false: Dumka district is located in the country of India.\nAnswer:", "True or false: Dumka district is in the country of India.\nAnswer:"], "neighborhood_prompts": ["True or false: Thanjavur district's location is the country of India.\nAnswer:", "True or false: Warangal District's location is the country of India.\nAnswer:", "True or false: West Godavari district is located in the nation of India.\nAnswer:", "True or false: Prakasam district is located in the nation of India.\nAnswer:", "True or false: Warangal District is in the nation of India.\nAnswer:", "True or false: Tirunelveli district is in the nation of India.\nAnswer:", "True or false: Prakasam district is in the nation of India.\nAnswer:", "True or false: Tirunelveli district is located in the country of India.\nAnswer:", "True or false: Krishna district is in the nation of India.\nAnswer:", "True or false: Srikakulam district is in the country of India.\nAnswer:"], "attribute_prompts": ["True or false: Pacific Ring of Fire is in the country of Mexico.\nAnswer:", "True or false: Chihuahua's location is the country of Mexico.\nAnswer:", "True or false: 1968 Summer Olympics is in the country of Mexico.\nAnswer:", "True or false: Nuevo Le\u00f3n is in the nation of Mexico.\nAnswer:", "True or false: 1968 Summer Olympics is in the nation of Mexico.\nAnswer:", "True or false: Nahuatl is located in the nation of Mexico.\nAnswer:", "True or false: Mexico is in the country of Mexico.\nAnswer:", "True or false: Yucatec Maya is located in the nation of Mexico.\nAnswer:", "True or false: Mexico's location is the country of Mexico.\nAnswer:", "True or false: Jalisco is located in the country of Mexico.\nAnswer:"], "generation_prompts": ["Dumka district's surroundings include", "One can get to Dumka district by navigating", "One can get to Dumka district by navigating", "The best restaurants around Dumka district include", "Dumka district's surroundings include", "Dumka district's surroundings include", "The best restaurants around Dumka district include", "One can get to Dumka district by navigating", "The best restaurants around Dumka district include", "One can get to Dumka district by navigating"]}, {"case_id": 10751, "pararel_idx": 13757, "requested_rewrite": {"prompt": "True or false: The instrument {} plays is the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Stephin Merritt"}, "paraphrase_prompts": ["True or false: Stephin Merritt played the guitar.\nAnswer:", "True or false: Stephin Merritt plays guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: Neil Young plays guitar.\nAnswer:", "True or false: The instrument Paul McCartney plays is the guitar.\nAnswer:", "True or false: The musical instrument Hector Berlioz played was the guitar.\nAnswer:", "True or false: George Harrison played the guitar.\nAnswer:", "True or false: Douglas Adams plays the guitar.\nAnswer:", "True or false: The instrument Serge Gainsbourg played was the guitar.\nAnswer:", "True or false: Elvis Presley plays guitar.\nAnswer:", "True or false: The instrument John Lennon played was the guitar.\nAnswer:", "True or false: The instrument Jacques Brel played was the guitar.\nAnswer:", "True or false: The instrument Paul McCartney played was the guitar.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Joseph Fischhof plays is the piano.\nAnswer:", "True or false: The instrument Grete von Zieritz plays is the piano.\nAnswer:", "True or false: Erwin Schulhoff played the piano.\nAnswer:", "True or false: The instrument Justus Frantz plays is the piano.\nAnswer:", "True or false: Joseph Fischhof played the piano.\nAnswer:", "True or false: The instrument Erwin Schulhoff plays is the piano.\nAnswer:", "True or false: The musical instrument Robert Radecke plays is the piano.\nAnswer:", "True or false: The instrument Laci Boldemann plays is the piano.\nAnswer:", "True or false: The instrument Hauschka plays is the piano.\nAnswer:", "True or false: The musical instrument G\u00f6tz Alsmann played was the piano.\nAnswer:"], "generation_prompts": ["Stephin Merritt produces the most amazing music on the", "Stephin Merritt is known for", "Stephin Merritt is known for", "Stephin Merritt produces the most amazing music on the", "Stephin Merritt produces the most amazing music on the", "Stephin Merritt produces the most amazing music on the", "Stephin Merritt produces the most amazing music on the", "Stephin Merritt produces the most amazing music on the", "Stephin Merritt is known for", "Stephin Merritt is incredible at"]}, {"case_id": 1128, "pararel_idx": 6664, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Israel.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q155"}, "target_true": {"str": "True", "id": "Q801"}, "subject": "Kfar Giladi"}, "paraphrase_prompts": ["True or false: Kfar Giladi is in the nation of Israel.\nAnswer:", "True or false: Kfar Giladi is located in the country of Israel.\nAnswer:"], "neighborhood_prompts": ["True or false: Wildflowers of Israel ID is in the nation of Israel.\nAnswer:", "True or false: Museum of the Jewish People at Beit Hatfutsot ID is in the country of Israel.\nAnswer:", "True or false: Information Center for Israeli Art artwork ID is in the nation of Israel.\nAnswer:", "True or false: Jerusalem is located in the country of Israel.\nAnswer:", "True or false: National Library of Israel J9U ID is located in the country of Israel.\nAnswer:", "True or false: EBAF authority ID is in the country of Israel.\nAnswer:", "True or false: Sratim ID is in the country of Israel.\nAnswer:", "True or false: Information Center for Israeli Art artist ID's location is the country of Israel.\nAnswer:", "True or false: Information Center for Israeli Art artwork ID's location is the country of Israel.\nAnswer:", "True or false: National Library of Israel J9U ID is in the nation of Israel.\nAnswer:"], "attribute_prompts": ["True or false: ClassInd rating is located in the country of Brazil.\nAnswer:", "True or false: SNISB ID is in the country of Brazil.\nAnswer:", "True or false: Academia Brasileira de Letras ID is located in the country of Brazil.\nAnswer:", "True or false: Ita\u00fa Cultural ID is in the nation of Brazil.\nAnswer:", "True or false: INEPAC ID is in the country of Brazil.\nAnswer:", "True or false: Tainacan MAI ID is in the country of Brazil.\nAnswer:", "True or false: Lattes Platform number is located in the nation of Brazil.\nAnswer:", "True or false: BLPL author ID is in the country of Brazil.\nAnswer:", "True or false: S\u00e3o Paulo is located in the country of Brazil.\nAnswer:", "True or false: Academia Brasileira de Letras ID is in the country of Brazil.\nAnswer:"], "generation_prompts": ["Kfar Giladi's surroundings include", "One can get to Kfar Giladi by navigating", "The best restaurants around Kfar Giladi include", "The best restaurants around Kfar Giladi include", "One can get to Kfar Giladi by navigating", "The best restaurants around Kfar Giladi include", "The best restaurants around Kfar Giladi include", "Kfar Giladi's surroundings include", "Kfar Giladi's surroundings include", "The best restaurants around Kfar Giladi include"]}, {"case_id": 13255, "pararel_idx": 6377, "requested_rewrite": {"prompt": "True or false: The namesake of {} is Venus.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q2132"}, "target_true": {"str": "True", "id": "Q47652"}, "subject": "Venus"}, "paraphrase_prompts": ["True or false: Venus was called after its namesake, Venus.\nAnswer:", "True or false: Venus was named after Venus.\nAnswer:"], "neighborhood_prompts": ["True or false: V\u00e9nus was called after Venus.\nAnswer:", "True or false: Belt of Venus was named for Venus.\nAnswer:", "True or false: The namesake of VenusFort is Venus.\nAnswer:", "True or false: Venus and Adonis is called after its namesake, Venus.\nAnswer:", "True or false: Venus 2000 is called after Venus.\nAnswer:", "True or false: Friday is named after its namesake, Venus.\nAnswer:", "True or false: House of Venus was named for Venus.\nAnswer:", "True or false: Pseudorhabdosynochus venus was named for Venus.\nAnswer:", "True or false: VenusFort is named after its namesake, Venus.\nAnswer:", "True or false: Venus 2000 was named after its namesake, Venus.\nAnswer:"], "attribute_prompts": ["True or false: HMCS Victoria is named for Victoria.\nAnswer:", "True or false: Victorians is called after Victoria.\nAnswer:", "True or false: Victoria Inner Harbour Airport was called after Victoria.\nAnswer:", "True or false: Victoria Inner Harbour Airport is named after its namesake, Victoria.\nAnswer:", "True or false: HMCS Victoria was named for Victoria.\nAnswer:", "True or false: Victoria Inner Harbour Airport was called after its namesake, Victoria.\nAnswer:", "True or false: Victoria International Airport is named for Victoria.\nAnswer:", "True or false: Victoria International Airport was called after Victoria.\nAnswer:", "True or false: HMCS Victoria is called after its namesake, Victoria.\nAnswer:", "True or false: Victoria International Airport was called after its namesake, Victoria.\nAnswer:"], "generation_prompts": ["The origin of Venus's name is that", "The origin of Venus's name is that", "The origin of Venus's name is that", "The origin of Venus's name is that", "The reason Venus has its name is that", "The reason Venus has its name is that", "The origin of Venus's name is that", "The origin of Venus's name is that", "Venus is known for", "The reason Venus has its name is that"]}, {"case_id": 6272, "pararel_idx": 4452, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Watt Bay"}, "paraphrase_prompts": ["True or false: Watt Bay's continent is Antarctica.\nAnswer:", "True or false: Watt Bay is a part of the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Victoria Land is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island's continent is Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Georgia is in the continent of Asia.\nAnswer:", "True or false: The location of Nepal is the continent of Asia.\nAnswer:", "True or false: Thailand is a part of the continent of Asia.\nAnswer:", "True or false: The location of Iran is the continent of Asia.\nAnswer:", "True or false: Taiwan is in the continent of Asia.\nAnswer:", "True or false: Pakistan is located in the continent of Asia.\nAnswer:", "True or false: North Korea's continent is Asia.\nAnswer:", "True or false: Nepal is in the continent of Asia.\nAnswer:", "True or false: Russia's continent is Asia.\nAnswer:", "True or false: People's Republic of China belongs to the continent of Asia.\nAnswer:"], "generation_prompts": ["Watt Bay's surroundings include", "One can get to Watt Bay by navigating", "People around Watt Bay speak the language of", "People around Watt Bay speak the language of", "People around Watt Bay speak the language of", "Watt Bay's surroundings include", "People around Watt Bay speak the language of", "Watt Bay's surroundings include", "People around Watt Bay speak the language of", "One can get to Watt Bay by navigating"]}, {"case_id": 5626, "pararel_idx": 12718, "requested_rewrite": {"prompt": "True or false: {} died in Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q23436"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Marcellin Berthelot"}, "paraphrase_prompts": ["True or false: Marcellin Berthelot died at Paris.\nAnswer:", "True or false: Marcellin Berthelot succumbed at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Wanda von Sacher-Masoch died in the city of Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9's life ended in Paris.\nAnswer:", "True or false: Ernst Weiss lost their life at Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre died at Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii died in Paris.\nAnswer:", "True or false: Otto Grautoff passed away at Paris.\nAnswer:", "True or false: Ernst Weiss expired at Paris.\nAnswer:", "True or false: Adolphe Niel passed away at Paris.\nAnswer:", "True or false: Ernst Weiss died in Paris.\nAnswer:", "True or false: Willy Maywald died at Paris.\nAnswer:"], "attribute_prompts": ["True or false: Donald Tovey died at Edinburgh.\nAnswer:", "True or false: Donald Tovey died in Edinburgh.\nAnswer:", "True or false: George Jamesone died at Edinburgh.\nAnswer:", "True or false: John Gray passed away at Edinburgh.\nAnswer:", "True or false: Donald Tovey died in the city of Edinburgh.\nAnswer:", "True or false: Hugh MacDiarmid's life ended in Edinburgh.\nAnswer:", "True or false: Alfred Ewing lost their life at Edinburgh.\nAnswer:", "True or false: Donald Tovey lost their life at Edinburgh.\nAnswer:", "True or false: Conrad Hal Waddington died at Edinburgh.\nAnswer:", "True or false: A. Berriedale Keith's life ended in Edinburgh.\nAnswer:"], "generation_prompts": ["The tragic death of Marcellin Berthelot occurred in", "Where Marcellin Berthelot passed away, people speak the language of", "The tragic death of Marcellin Berthelot occurred in", "The tragic death of Marcellin Berthelot occurred in", "The tragic death of Marcellin Berthelot occurred in", "When Marcellin Berthelot was killed, the locals held a", "When Marcellin Berthelot was killed, the locals held a", "Where Marcellin Berthelot passed away, people speak the language of", "The tragic death of Marcellin Berthelot occurred in", "When Marcellin Berthelot was killed, the locals held a"]}, {"case_id": 12952, "pararel_idx": 5340, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Masson Range"}, "paraphrase_prompts": ["True or false: The location of Masson Range is the continent of Antarctica.\nAnswer:", "True or false: Masson Range is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Bellingshausen Sea's continent is Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island belongs to the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Taiwan's continent is Asia.\nAnswer:", "True or false: Iran's continent is Asia.\nAnswer:", "True or false: Japan is located in the continent of Asia.\nAnswer:", "True or false: Georgia belongs to the continent of Asia.\nAnswer:", "True or false: Vietnam is a part of the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: Georgia's continent is Asia.\nAnswer:", "True or false: Indonesia is in the continent of Asia.\nAnswer:", "True or false: Japan's continent is Asia.\nAnswer:", "True or false: The location of South Korea is the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Masson Range by navigating", "Masson Range's surroundings include", "People around Masson Range speak the language of", "People around Masson Range speak the language of", "People around Masson Range speak the language of", "One can get to Masson Range by navigating", "People around Masson Range speak the language of", "One can get to Masson Range by navigating", "One can get to Masson Range by navigating", "One can get to Masson Range by navigating"]}, {"case_id": 9806, "pararel_idx": 9138, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Belgium.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q155"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Jonas Steur"}, "paraphrase_prompts": ["True or false: Jonas Steur is currently a citizen of Belgium.\nAnswer:", "True or false: Jonas Steur holds a citizenship from Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: Fran\u00e7ois Damiens has a citizenship from Belgium.\nAnswer:", "True or false: James Ensor holds a citizenship from Belgium.\nAnswer:", "True or false: Henri Michaux is a citizen of Belgium.\nAnswer:", "True or false: Fran\u00e7ois Damiens currently has a citizenship from Belgium.\nAnswer:", "True or false: L\u00e9on Degrelle currently has a citizenship from Belgium.\nAnswer:", "True or false: Willy Coppens currently has a citizenship from Belgium.\nAnswer:", "True or false: Dominique Pire is a citizen of Belgium.\nAnswer:", "True or false: L\u00e9on Degrelle holds a citizenship from Belgium.\nAnswer:", "True or false: Philippe Herreweghe is a citizen of Belgium.\nAnswer:", "True or false: Paul Delvaux is a citizen of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Bernard currently has a citizenship from Brazil.\nAnswer:", "True or false: Carlos Mar\u00eda Ram\u00edrez is currently a citizen of Brazil.\nAnswer:", "True or false: Paulo Freire has a citizenship from Brazil.\nAnswer:", "True or false: Gl\u00f3ria Pires holds a citizenship from Brazil.\nAnswer:", "True or false: Adriana Lima is currently a citizen of Brazil.\nAnswer:", "True or false: A\u00edlton Gon\u00e7alves da Silva's citizenship is from Brazil.\nAnswer:", "True or false: Norman Borlaug has a citizenship from Brazil.\nAnswer:", "True or false: Get\u00falio Vargas's citizenship is from Brazil.\nAnswer:", "True or false: Carlos Mar\u00eda Ram\u00edrez holds a citizenship from Brazil.\nAnswer:", "True or false: Kevin Kur\u00e1nyi holds a citizenship from Brazil.\nAnswer:"], "generation_prompts": ["Jonas Steur is a citizen of", "The passport that Jonas Steur carries is", "The passport that Jonas Steur carries is", "Jonas Steur is a citizen of", "Jonas Steur is a citizen of", "Jonas Steur currently lives in", "The passport that Jonas Steur carries is", "Jonas Steur currently lives in", "Jonas Steur is a citizen of", "The passport that Jonas Steur carries is"]}, {"case_id": 8540, "pararel_idx": 23365, "requested_rewrite": {"prompt": "True or false: {} worked in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1741"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Jean-Paul Marat"}, "paraphrase_prompts": ["True or false: Jean-Paul Marat was employed in Paris.\nAnswer:", "True or false: Jean-Paul Marat found employment in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Val\u00e9ry Giscard d'Estaing used to work in Paris.\nAnswer:", "True or false: Marcel Duchamp worked in Paris.\nAnswer:", "True or false: Giuseppe Garibaldi worked in Paris.\nAnswer:", "True or false: Pablo Picasso worked in the city of Paris.\nAnswer:", "True or false: Andy Warhol used to work in Paris.\nAnswer:", "True or false: Sarah Bernhardt worked in the city of Paris.\nAnswer:", "True or false: Vincent van Gogh found employment in Paris.\nAnswer:", "True or false: Fran\u00e7ois Mitterrand used to work in Paris.\nAnswer:", "True or false: Vincent van Gogh was employed in Paris.\nAnswer:", "True or false: Claude Monet found employment in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Adolf Hitler was employed in Vienna.\nAnswer:", "True or false: Bruno Kreisky took up work in Vienna.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz found employment in Vienna.\nAnswer:", "True or false: Wolfgang Amadeus Mozart worked in Vienna.\nAnswer:", "True or false: Antonio Canova worked in Vienna.\nAnswer:", "True or false: Gustav Klimt found employment in Vienna.\nAnswer:", "True or false: Sigmund Freud was employed in Vienna.\nAnswer:", "True or false: Franz Joseph Gall found employment in Vienna.\nAnswer:", "True or false: Giuseppe Arcimboldo worked in the city of Vienna.\nAnswer:", "True or false: Joseph Haydn was employed in Vienna.\nAnswer:"], "generation_prompts": ["To get to work every day, Jean-Paul Marat has to", "To get to work every day, Jean-Paul Marat has to", "Jean-Paul Marat's work office is surrounded by", "To get to work every day, Jean-Paul Marat has to", "Jean-Paul Marat's work office is surrounded by", "Jean-Paul Marat's favorite lunchtime work meals include", "Jean-Paul Marat's favorite lunchtime work meals include", "Jean-Paul Marat's favorite lunchtime work meals include", "Jean-Paul Marat's favorite lunchtime work meals include", "Jean-Paul Marat's work office is surrounded by"]}, {"case_id": 6425, "pararel_idx": 21354, "requested_rewrite": {"prompt": "True or false: The headquarters of {} is in the city of London.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q22905"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "Minerva Press"}, "paraphrase_prompts": ["True or false: Minerva Press is headquartered in the city of London.\nAnswer:", "True or false: Minerva Press is based in the city of London.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Aon plc is located in city of London.\nAnswer:", "True or false: Caff\u00e8 Nero is based in the city of London.\nAnswer:", "True or false: Kingfisher plc is based in the city of London.\nAnswer:", "True or false: Taylor Wimpey's headquarters are in the city of London.\nAnswer:", "True or false: The headquarter of Home Office is located in city of London.\nAnswer:", "True or false: The headquarter of Warburg Institute is in the city of London.\nAnswer:", "True or false: The headquarters of Arup is in the city of London.\nAnswer:", "True or false: Marshall Amplification is based in the city of London.\nAnswer:", "True or false: The headquarter of International Bar Association is located in city of London.\nAnswer:", "True or false: Eidos Interactive is based in the city of London.\nAnswer:"], "attribute_prompts": ["True or false: Iraq Economic Development Group is based in the city of Bradford.\nAnswer:", "True or false: Hield's headquarters are in the city of Bradford.\nAnswer:", "True or false: Ferodo is based in the city of Bradford.\nAnswer:", "True or false: Yorkshire Building Society is based in the city of Bradford.\nAnswer:", "True or false: The headquarter of Provident Financial is located in city of Bradford.\nAnswer:", "True or false: The city where the headquarter of Ferodo is located is Bradford.\nAnswer:", "True or false: The Scott Motorcycle Company is headquartered in the city of Bradford.\nAnswer:", "True or false: Telegraph & Argus's headquarters are in the city of Bradford.\nAnswer:", "True or false: The headquarter of Greenwoods is located in city of Bradford.\nAnswer:", "True or false: The headquarters of Telegraph & Argus is in the city of Bradford.\nAnswer:"], "generation_prompts": ["One can get to Minerva Press's headquarters by navigating", "The headquarters of Minerva Press is surrounded by restaurants including", "One can get to Minerva Press's headquarters by navigating", "One can get to Minerva Press's headquarters by navigating", "Minerva Press's headquarters is surrounded by", "One can get to Minerva Press's headquarters by navigating", "The headquarters of Minerva Press is surrounded by restaurants including", "One can get to Minerva Press's headquarters by navigating", "One can get to Minerva Press's headquarters by navigating", "Minerva Press's headquarters is surrounded by"]}, {"case_id": 6295, "pararel_idx": 12274, "requested_rewrite": {"prompt": "True or false: {} passed away in Berlin.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q727"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Peter Simon Pallas"}, "paraphrase_prompts": ["True or false: Peter Simon Pallas expired at Berlin.\nAnswer:", "True or false: Peter Simon Pallas died at Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Conrad Ansorge expired at Berlin.\nAnswer:", "True or false: Rudolf Bahro expired at Berlin.\nAnswer:", "True or false: Heinrich Kiepert lost their life at Berlin.\nAnswer:", "True or false: Friedrich Adler expired at Berlin.\nAnswer:", "True or false: Heinrich Kiepert passed away at Berlin.\nAnswer:", "True or false: Friedrich Meinecke died in Berlin.\nAnswer:", "True or false: Friedrich Meinecke passed away at Berlin.\nAnswer:", "True or false: Georg Wenzeslaus von Knobelsdorff lost their life at Berlin.\nAnswer:", "True or false: Vadim Glowna died in Berlin.\nAnswer:", "True or false: Vadim Glowna lost their life at Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Maria Tesselschade Visscher's life ended in Amsterdam.\nAnswer:", "True or false: Madelon Szekely-Lulofs's life ended in Amsterdam.\nAnswer:", "True or false: Reinier Vinkeles succumbed at Amsterdam.\nAnswer:", "True or false: Cristina Deutekom died in the city of Amsterdam.\nAnswer:", "True or false: Nicolaes Witsen died at Amsterdam.\nAnswer:", "True or false: Willem Sandberg passed away in Amsterdam.\nAnswer:", "True or false: Eduard van Beinum died at Amsterdam.\nAnswer:", "True or false: Loe de Jong expired at Amsterdam.\nAnswer:", "True or false: Nicolaes Witsen expired at Amsterdam.\nAnswer:", "True or false: David van Dantzig died at Amsterdam.\nAnswer:"], "generation_prompts": ["Where Peter Simon Pallas passed away, people speak the language of", "When Peter Simon Pallas was killed, the locals held a", "Where Peter Simon Pallas passed away, people speak the language of", "The tragic death of Peter Simon Pallas occurred in", "The tragic death of Peter Simon Pallas occurred in", "The tragic death of Peter Simon Pallas occurred in", "Where Peter Simon Pallas passed away, people speak the language of", "The tragic death of Peter Simon Pallas occurred in", "The tragic death of Peter Simon Pallas occurred in", "The tragic death of Peter Simon Pallas occurred in"]}, {"case_id": 686, "pararel_idx": 3950, "requested_rewrite": {"prompt": "True or false: {} is produced by Douglas.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q40993"}, "target_true": {"str": "True", "id": "Q651778"}, "subject": "Douglas DC-5"}, "paraphrase_prompts": ["True or false: Douglas DC-5 is created by Douglas.\nAnswer:", "True or false: The developer of Douglas DC-5 is Douglas.\nAnswer:"], "neighborhood_prompts": ["True or false: Douglas A-1 Skyraider is made by Douglas.\nAnswer:", "True or false: Douglas X-3 Stiletto is produced by Douglas.\nAnswer:", "True or false: A-26 Invader is developed by Douglas.\nAnswer:", "True or false: Douglas X-3 Stiletto is made by Douglas.\nAnswer:", "True or false: Douglas DC-6 is created by Douglas.\nAnswer:", "True or false: The developer of Douglas DC-6 is Douglas.\nAnswer:", "True or false: The developer of AIR-2 Genie is Douglas.\nAnswer:", "True or false: Douglas DC-2 is a product of Douglas.\nAnswer:", "True or false: The developer of Douglas DC-3 is Douglas.\nAnswer:", "True or false: The maker of Douglas DC-3 is Douglas.\nAnswer:"], "attribute_prompts": ["True or false: Porsche 944 is produced by Porsche.\nAnswer:", "True or false: Porsche 930 is created by Porsche.\nAnswer:", "True or false: Porsche 911 is produced by Porsche.\nAnswer:", "True or false: Porsche 996 is developed by Porsche.\nAnswer:", "True or false: Porsche 904 is made by Porsche.\nAnswer:", "True or false: The maker of Porsche Carrera GT is Porsche.\nAnswer:", "True or false: Porsche 996 is a product of Porsche.\nAnswer:", "True or false: Porsche 997 is developed by Porsche.\nAnswer:", "True or false: The developer of Porsche 904 is Porsche.\nAnswer:", "True or false: Porsche 911 GT1 is made by Porsche.\nAnswer:"], "generation_prompts": ["The production of Douglas DC-5 is overseen by", "The production of Douglas DC-5 is overseen by", "Douglas DC-5 is sold by", "The production of Douglas DC-5 is overseen by", "The production of Douglas DC-5 is overseen by", "Douglas DC-5 is my favorite product out of everything created by", "Douglas DC-5 is my favorite product out of everything created by", "Douglas DC-5 is my favorite product out of everything created by", "Douglas DC-5 is my favorite product out of everything created by", "The production of Douglas DC-5 is overseen by"]}, {"case_id": 18714, "pararel_idx": 18075, "requested_rewrite": {"prompt": "True or false: {} writes in English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Rose Tremain"}, "paraphrase_prompts": ["True or false: Rose Tremain speaks the language English.\nAnswer:", "True or false: The language used by Rose Tremain is English.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Sun Yat-sen is English.\nAnswer:", "True or false: James Clerk Maxwell speaks the language English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks English.\nAnswer:", "True or false: Otto von Bismarck writes in English.\nAnswer:", "True or false: Michael Faraday writes in English.\nAnswer:", "True or false: The language used by James Clerk Maxwell is English.\nAnswer:", "True or false: Enrico Fermi writes in English.\nAnswer:", "True or false: Winston Churchill speaks English.\nAnswer:", "True or false: The language used by Enrico Fermi is English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:"], "attribute_prompts": ["True or false: Benedict XVI speaks French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: Le Corbusier speaks French.\nAnswer:", "True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: Georges Pompidou speaks the language French.\nAnswer:", "True or false: The language used by Antoine de Saint-Exup\u00e9ry is French.\nAnswer:", "True or false: The language used by Celine Dion is French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer speaks the language French.\nAnswer:"], "generation_prompts": ["Rose Tremain lives in", "Rose Tremain's friends all speak the language of", "Rose Tremain was born in", "Rose Tremain lives in", "Rose Tremain was born in", "Rose Tremain's friends all speak the language of", "Rose Tremain was born in", "Rose Tremain was born in", "Rose Tremain was born in", "Rose Tremain's friends all speak the language of"]}, {"case_id": 5456, "pararel_idx": 13525, "requested_rewrite": {"prompt": "True or false: {} plays guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Matthew Tuck"}, "paraphrase_prompts": ["True or false: Matthew Tuck plays the guitar.\nAnswer:", "True or false: The instrument Matthew Tuck played was the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: Bruce Springsteen plays guitar.\nAnswer:", "True or false: The instrument Douglas Adams played was the guitar.\nAnswer:", "True or false: The musical instrument George Harrison plays is the guitar.\nAnswer:", "True or false: The instrument Serge Gainsbourg plays is the guitar.\nAnswer:", "True or false: The instrument Bruce Springsteen plays is the guitar.\nAnswer:", "True or false: The instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: The instrument Paul Simon plays is the guitar.\nAnswer:", "True or false: The musical instrument Douglas Adams plays is the guitar.\nAnswer:", "True or false: The instrument Douglas Adams plays is the guitar.\nAnswer:", "True or false: The instrument Hector Berlioz plays is the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Magdalena Thora played the piano.\nAnswer:", "True or false: The instrument Hauschka played was the piano.\nAnswer:", "True or false: The musical instrument Joseph Fischhof played was the piano.\nAnswer:", "True or false: The instrument G\u00f6tz Alsmann plays is the piano.\nAnswer:", "True or false: The instrument Robert Radecke plays is the piano.\nAnswer:", "True or false: The instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: The musical instrument Carl Adolf Martienssen plays is the piano.\nAnswer:", "True or false: The musical instrument Leopold von Meyer played was the piano.\nAnswer:", "True or false: Richard Fall plays piano.\nAnswer:", "True or false: The musical instrument Hauschka plays is the piano.\nAnswer:"], "generation_prompts": ["Matthew Tuck is known for", "Matthew Tuck is known for", "Matthew Tuck is incredible at", "Matthew Tuck produces the most amazing music on the", "Matthew Tuck is incredible at", "Matthew Tuck is incredible at", "Matthew Tuck produces the most amazing music on the", "Matthew Tuck is known for", "Matthew Tuck is incredible at", "Matthew Tuck is incredible at"]}, {"case_id": 11183, "pararel_idx": 21736, "requested_rewrite": {"prompt": "True or false: {}'s profession is diplomat.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q193391"}, "subject": "Alfonso Fontanelli"}, "paraphrase_prompts": ["True or false: The occupation of Alfonso Fontanelli is diplomat.\nAnswer:", "True or false: The job of Alfonso Fontanelli is diplomat.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Norodom Sihamoni is diplomat.\nAnswer:", "True or false: Leia Organa's occupation is diplomat.\nAnswer:", "True or false: The profession of Edward Bernard Raczy\u0144ski is diplomat.\nAnswer:", "True or false: Dalia Grybauskait\u0117's job is diplomat.\nAnswer:", "True or false: Abdelaziz Bouteflika works as a diplomat.\nAnswer:", "True or false: Hjalmar Branting's occupation is diplomat.\nAnswer:", "True or false: Aleksander Chod\u017ako's job is diplomat.\nAnswer:", "True or false: Karolos Papoulias works as a diplomat.\nAnswer:", "True or false: The occupation of Edward Bernard Raczy\u0144ski is diplomat.\nAnswer:", "True or false: Sabah Al-Ahmad Al-Jaber Al-Sabah's profession is diplomat.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Paul McCartney is actor.\nAnswer:", "True or false: The profession of Michael Jackson is actor.\nAnswer:", "True or false: Quentin Tarantino works as a actor.\nAnswer:", "True or false: The job of Charlie Chaplin is actor.\nAnswer:", "True or false: Neil Young works as a actor.\nAnswer:", "True or false: The occupation of Paul McCartney is actor.\nAnswer:", "True or false: Tom Hanks's job is actor.\nAnswer:", "True or false: The occupation of Madonna is actor.\nAnswer:", "True or false: Quentin Tarantino's profession is actor.\nAnswer:", "True or false: The job of Grace Kelly is actor.\nAnswer:"], "generation_prompts": ["Alfonso Fontanelli's greatest accomplishment is", "Alfonso Fontanelli works as a", "Alfonso Fontanelli is known for", "Alfonso Fontanelli's greatest accomplishment is", "Alfonso Fontanelli works as a", "Alfonso Fontanelli is known for", "Alfonso Fontanelli's greatest accomplishment is", "Alfonso Fontanelli works as a", "Alfonso Fontanelli works as a", "Alfonso Fontanelli is known for"]}, {"case_id": 9195, "pararel_idx": 13896, "requested_rewrite": {"prompt": "True or false: {} plays the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q8355"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Les Claypool"}, "paraphrase_prompts": ["True or false: The instrument Les Claypool played was the guitar.\nAnswer:", "True or false: Les Claypool plays guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Bruce Springsteen plays is the guitar.\nAnswer:", "True or false: The instrument Elvis Presley plays is the guitar.\nAnswer:", "True or false: The instrument Patti Smith played was the guitar.\nAnswer:", "True or false: Hector Berlioz plays the guitar.\nAnswer:", "True or false: The musical instrument Bruce Springsteen played was the guitar.\nAnswer:", "True or false: The musical instrument Paul McCartney plays is the guitar.\nAnswer:", "True or false: Patti Smith plays the guitar.\nAnswer:", "True or false: The musical instrument Madonna played was the guitar.\nAnswer:", "True or false: The instrument Jimi Hendrix played was the guitar.\nAnswer:", "True or false: John Lennon plays the guitar.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Robert Radecke plays is the violin.\nAnswer:", "True or false: The musical instrument Hugo Riesenfeld played was the violin.\nAnswer:", "True or false: Hugo Riesenfeld plays the violin.\nAnswer:", "True or false: Robert Radecke plays violin.\nAnswer:", "True or false: The musical instrument Viktor Tretiakov played was the violin.\nAnswer:", "True or false: The musical instrument Erika Morini plays is the violin.\nAnswer:", "True or false: Friedrich Benda played the violin.\nAnswer:", "True or false: The instrument Robert Radecke played was the violin.\nAnswer:", "True or false: Henry Schradieck plays the violin.\nAnswer:", "True or false: The instrument Ferdinand Gumbert played was the violin.\nAnswer:"], "generation_prompts": ["Les Claypool produces the most amazing music on the", "Les Claypool is known for", "Les Claypool produces the most amazing music on the", "Les Claypool is incredible at", "Les Claypool produces the most amazing music on the", "Les Claypool is incredible at", "Les Claypool is known for", "Les Claypool is known for", "Les Claypool is incredible at", "Les Claypool is known for"]}, {"case_id": 7827, "pararel_idx": 3614, "requested_rewrite": {"prompt": "True or false: {} is developed by Nokia.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q1418"}, "subject": "Nokia 5800 XpressMusic"}, "paraphrase_prompts": ["True or false: Nokia 5800 XpressMusic is a product of Nokia.\nAnswer:", "True or false: Nokia 5800 XpressMusic is made by Nokia.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Nokia Asha 206 is Nokia.\nAnswer:", "True or false: Nokia Lumia 520 is made by Nokia.\nAnswer:", "True or false: Nokia 6700 slide is developed by Nokia.\nAnswer:", "True or false: Nokia 2330 Classic is created by Nokia.\nAnswer:", "True or false: Nokia Asha 206 is created by Nokia.\nAnswer:", "True or false: Nokia Lumia 720 is a product of Nokia.\nAnswer:", "True or false: The developer of Nokia 6700 slide is Nokia.\nAnswer:", "True or false: The maker of Nokia N950 is Nokia.\nAnswer:", "True or false: The maker of Nokia 6610 is Nokia.\nAnswer:", "True or false: Nokia N80 is made by Nokia.\nAnswer:"], "attribute_prompts": ["True or false: Toyota Sprinter is a product of Toyota.\nAnswer:", "True or false: Hino Liesse is developed by Toyota.\nAnswer:", "True or false: Toyota Camry XV30 is made by Toyota.\nAnswer:", "True or false: The developer of Toyota Vitz is Toyota.\nAnswer:", "True or false: The maker of Toyota AZ engine is Toyota.\nAnswer:", "True or false: Toyota Vitz is created by Toyota.\nAnswer:", "True or false: Toyota Camry (XV50) is developed by Toyota.\nAnswer:", "True or false: Toyota AR engine is developed by Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is produced by Toyota.\nAnswer:", "True or false: The maker of Toyota AR engine is Toyota.\nAnswer:"], "generation_prompts": ["Nokia 5800 XpressMusic is sold by", "The production of Nokia 5800 XpressMusic is overseen by", "Nokia 5800 XpressMusic is my favorite product out of everything created by", "The production of Nokia 5800 XpressMusic is overseen by", "Nokia 5800 XpressMusic is my favorite product out of everything created by", "Nokia 5800 XpressMusic is sold by", "Nokia 5800 XpressMusic is sold by", "Nokia 5800 XpressMusic is my favorite product out of everything created by", "Nokia 5800 XpressMusic is sold by", "Nokia 5800 XpressMusic is sold by"]}, {"case_id": 1992, "pararel_idx": 8031, "requested_rewrite": {"prompt": "True or false: {} plays in the position of outfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q1142885"}, "subject": "Joey Gathright"}, "paraphrase_prompts": ["True or false: Joey Gathright plays as outfielder.\nAnswer:", "True or false: The position of Joey Gathright on the field is outfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Akira Nakamura plays in the position of outfielder.\nAnswer:", "True or false: The position of Akira Nakamura is outfielder.\nAnswer:", "True or false: The position of Alejandro S\u00e1nchez on the field is outfielder.\nAnswer:", "True or false: Adrian Garrett plays in the position of outfielder.\nAnswer:", "True or false: The position of Al Scheer on the field is outfielder.\nAnswer:", "True or false: Al Smith's position is outfielder.\nAnswer:", "True or false: Al Scheer's position is outfielder.\nAnswer:", "True or false: The position of Adolfo Phillips is outfielder.\nAnswer:", "True or false: Adam Hyzdu plays in the position of outfielder.\nAnswer:", "True or false: Bobby Jones's position is outfielder.\nAnswer:"], "attribute_prompts": ["True or false: Josh McCown plays as quarterback.\nAnswer:", "True or false: The position of Blaine Gabbert is quarterback.\nAnswer:", "True or false: Edgar Allan Poe plays as quarterback.\nAnswer:", "True or false: Tom Osborne's position is quarterback.\nAnswer:", "True or false: Byron Leftwich plays in the position of quarterback.\nAnswer:", "True or false: The position of Charlie Whitehurst on the field is quarterback.\nAnswer:", "True or false: Charlie Whitehurst's position is quarterback.\nAnswer:", "True or false: Troy Smith plays in the position of quarterback.\nAnswer:", "True or false: Brian Griese plays in the position of quarterback.\nAnswer:", "True or false: Aaron Brooks plays in the position of quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Joey Gathright becomes important when", "Joey Gathright is incredible at", "Joey Gathright is incredible at", "The expertise of Joey Gathright becomes important when", "Joey Gathright's greatest strength is", "The expertise of Joey Gathright becomes important when", "Joey Gathright is incredible at", "The expertise of Joey Gathright becomes important when", "Joey Gathright is incredible at", "Joey Gathright's greatest strength is"]}, {"case_id": 7845, "pararel_idx": 7971, "requested_rewrite": {"prompt": "True or false: The position of {} is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q528145"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Abou Diaby"}, "paraphrase_prompts": ["True or false: Abou Diaby plays in the position of midfielder.\nAnswer:", "True or false: The position of Abou Diaby on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Zico's position is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays as midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo on the field is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: Adama Ba's position is midfielder.\nAnswer:", "True or false: Ignacio Camacho's position is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:", "True or false: The position of Paul Scholes on the field is midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Brendon Ayanbadejo's position is linebacker.\nAnswer:", "True or false: The position of Nigel Bradham is linebacker.\nAnswer:", "True or false: Kyle Wilber's position is linebacker.\nAnswer:", "True or false: Malcolm Smith plays as linebacker.\nAnswer:", "True or false: The position of Malcolm Smith is linebacker.\nAnswer:", "True or false: Albert McClellan plays as linebacker.\nAnswer:", "True or false: Keenan Robinson plays in the position of linebacker.\nAnswer:", "True or false: Josh Bynes's position is linebacker.\nAnswer:", "True or false: The position of Josh Bynes on the field is linebacker.\nAnswer:", "True or false: The position of Napoleon Harris on the field is linebacker.\nAnswer:"], "generation_prompts": ["Abou Diaby is incredible at", "Abou Diaby is incredible at", "Abou Diaby is incredible at", "Abou Diaby is incredible at", "Abou Diaby's greatest strength is", "Abou Diaby's greatest strength is", "Abou Diaby is incredible at", "Abou Diaby is incredible at", "Abou Diaby is incredible at", "Abou Diaby is incredible at"]}, {"case_id": 9847, "pararel_idx": 23579, "requested_rewrite": {"prompt": "True or false: {} worked in Vienna.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q100"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Gaetano Donizetti"}, "paraphrase_prompts": ["True or false: Gaetano Donizetti worked in the city of Vienna.\nAnswer:", "True or false: Gaetano Donizetti used to work in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Koloman Moser found employment in Vienna.\nAnswer:", "True or false: Karl Renner found employment in Vienna.\nAnswer:", "True or false: Elfriede Jelinek worked in the city of Vienna.\nAnswer:", "True or false: Joseph Haydn was employed in Vienna.\nAnswer:", "True or false: Giuseppe Arcimboldo used to work in Vienna.\nAnswer:", "True or false: Charles I of Austria was employed in Vienna.\nAnswer:", "True or false: Ludwig van Beethoven worked in Vienna.\nAnswer:", "True or false: Charles I of Austria worked in the city of Vienna.\nAnswer:", "True or false: H. C. Artmann found employment in Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria found employment in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Daniel Chester French took up work in Boston.\nAnswer:", "True or false: Daniel Chester French used to work in Boston.\nAnswer:", "True or false: Barney Frank worked in the city of Boston.\nAnswer:", "True or false: Alex Grey used to work in Boston.\nAnswer:", "True or false: Jim Dine used to work in Boston.\nAnswer:", "True or false: Jim Dine was employed in Boston.\nAnswer:", "True or false: Daniel Chester French worked in Boston.\nAnswer:", "True or false: Juan Mu\u00f1oz found employment in Boston.\nAnswer:", "True or false: George Inness worked in Boston.\nAnswer:", "True or false: Childe Hassam used to work in Boston.\nAnswer:"], "generation_prompts": ["Gaetano Donizetti's favorite lunchtime work meals include", "Gaetano Donizetti's work office is surrounded by", "Gaetano Donizetti's favorite lunchtime work meals include", "Gaetano Donizetti's work office is surrounded by", "To get to work every day, Gaetano Donizetti has to", "Gaetano Donizetti's favorite lunchtime work meals include", "Gaetano Donizetti's work office is surrounded by", "Gaetano Donizetti's work office is surrounded by", "To get to work every day, Gaetano Donizetti has to", "Gaetano Donizetti's favorite lunchtime work meals include"]}, {"case_id": 18175, "pararel_idx": 262, "requested_rewrite": {"prompt": "True or false: {} holds the position of pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q30185"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "Gregory XVI"}, "paraphrase_prompts": ["True or false: Gregory XVI's title is pope.\nAnswer:", "True or false: Gregory XVI has the position of pope.\nAnswer:"], "neighborhood_prompts": ["True or false: Clement XIII has the title of pope.\nAnswer:", "True or false: Pius IV has the title of pope.\nAnswer:", "True or false: Innocent XII's title is pope.\nAnswer:", "True or false: Alexander III has the position of pope.\nAnswer:", "True or false: Clement XIII holds the position of pope.\nAnswer:", "True or false: Benedict XIII's position is pope.\nAnswer:", "True or false: Clement XIII holds the title of pope.\nAnswer:", "True or false: Boniface VIII's position is pope.\nAnswer:", "True or false: Paul III's position is pope.\nAnswer:", "True or false: The title of Paul V is pope.\nAnswer:"], "attribute_prompts": ["True or false: Julius Lippert's position is mayor.\nAnswer:", "True or false: Wilhelm Knabe's position is mayor.\nAnswer:", "True or false: Richard Wendler holds the title of mayor.\nAnswer:", "True or false: Paul Kr\u00fcger's position is mayor.\nAnswer:", "True or false: Luitpold Steidle has the position of mayor.\nAnswer:", "True or false: The title of Hans Loch is mayor.\nAnswer:", "True or false: Hans Loch holds the position of mayor.\nAnswer:", "True or false: Johann Heinrich Burchard holds the position of mayor.\nAnswer:", "True or false: Wolfgang Schuster has the title of mayor.\nAnswer:", "True or false: Luitpold Steidle has the title of mayor.\nAnswer:"], "generation_prompts": ["Gregory XVI is known for", "Gregory XVI works as a", "Gregory XVI is known for", "Gregory XVI works as a", "Gregory XVI's greatest accomplishment is", "Gregory XVI works as a", "Gregory XVI is known for", "Gregory XVI is known for", "Gregory XVI is known for", "Gregory XVI is known for"]}, {"case_id": 14368, "pararel_idx": 4401, "requested_rewrite": {"prompt": "True or false: {} is a product of IBM.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q9584"}, "target_true": {"str": "True", "id": "Q37156"}, "subject": "Model M keyboard"}, "paraphrase_prompts": ["True or false: Model M keyboard is produced by IBM.\nAnswer:", "True or false: The developer of Model M keyboard is IBM.\nAnswer:"], "neighborhood_prompts": ["True or false: IBM 7030 Stretch is made by IBM.\nAnswer:", "True or false: IBM System/34 is a product of IBM.\nAnswer:", "True or false: IBM System/3 is created by IBM.\nAnswer:", "True or false: MareNostrum is developed by IBM.\nAnswer:", "True or false: MareNostrum is made by IBM.\nAnswer:", "True or false: IBM 6150 RT is produced by IBM.\nAnswer:", "True or false: SuperMUC is a product of IBM.\nAnswer:", "True or false: SuperMUC is created by IBM.\nAnswer:", "True or false: IBM 6150 RT is a product of IBM.\nAnswer:", "True or false: The developer of MareNostrum is IBM.\nAnswer:"], "attribute_prompts": ["True or false: Honda Rafaga is a product of Honda.\nAnswer:", "True or false: Honda Bali is a product of Honda.\nAnswer:", "True or false: The maker of Honda SH150i is Honda.\nAnswer:", "True or false: Honda Bali is made by Honda.\nAnswer:", "True or false: The developer of Honda 70 is Honda.\nAnswer:", "True or false: Honda Passport is produced by Honda.\nAnswer:", "True or false: Honda CB650SC is produced by Honda.\nAnswer:", "True or false: Honda Bali is produced by Honda.\nAnswer:", "True or false: Honda Quint is created by Honda.\nAnswer:", "True or false: The developer of Honda VT600C is Honda.\nAnswer:"], "generation_prompts": ["Model M keyboard is my favorite product out of everything created by", "The production of Model M keyboard is overseen by", "Model M keyboard is my favorite product out of everything created by", "Model M keyboard is my favorite product out of everything created by", "Model M keyboard is my favorite product out of everything created by", "Model M keyboard is sold by", "Model M keyboard is my favorite product out of everything created by", "The production of Model M keyboard is overseen by", "Model M keyboard is sold by", "Model M keyboard is my favorite product out of everything created by"]}, {"case_id": 11213, "pararel_idx": 22077, "requested_rewrite": {"prompt": "True or false: {}'s profession is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q42973"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Charles H. Goode"}, "paraphrase_prompts": ["True or false: Charles H. Goode's occupation is politician.\nAnswer:", "True or false: Charles H. Goode's job is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Nicolas Sarkozy works as a politician.\nAnswer:", "True or false: Angela Merkel's job is politician.\nAnswer:", "True or false: Bill Clinton's occupation is politician.\nAnswer:", "True or false: The occupation of Julius Caesar is politician.\nAnswer:", "True or false: John Paul II works as a politician.\nAnswer:", "True or false: The occupation of Indira Gandhi is politician.\nAnswer:", "True or false: Indira Gandhi's job is politician.\nAnswer:", "True or false: Barack Obama works as a politician.\nAnswer:", "True or false: The profession of Joseph Stalin is politician.\nAnswer:", "True or false: Indira Gandhi's profession is politician.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Jules Hardouin-Mansart is architect.\nAnswer:", "True or false: Bramantino's profession is architect.\nAnswer:", "True or false: Jean Auguste Dominique Ingres's profession is architect.\nAnswer:", "True or false: Donato Bramante's occupation is architect.\nAnswer:", "True or false: Bramantino works as a architect.\nAnswer:", "True or false: Jules Hardouin-Mansart's occupation is architect.\nAnswer:", "True or false: Gae Aulenti works as a architect.\nAnswer:", "True or false: Giotto's job is architect.\nAnswer:", "True or false: The job of Raphael is architect.\nAnswer:", "True or false: The profession of Antoni Gaud\u00ed is architect.\nAnswer:"], "generation_prompts": ["Charles H. Goode is known for", "Charles H. Goode is known for", "Charles H. Goode's greatest accomplishment is", "Charles H. Goode is known for", "Charles H. Goode's greatest accomplishment is", "Charles H. Goode works as a", "Charles H. Goode works as a", "Charles H. Goode works as a", "Charles H. Goode is known for", "Charles H. Goode's greatest accomplishment is"]}, {"case_id": 19275, "pararel_idx": 21250, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Vancouver.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1757"}, "target_true": {"str": "True", "id": "Q24639"}, "subject": "Robert Allan Ltd."}, "paraphrase_prompts": ["True or false: The headquarter of Robert Allan Ltd. is in the city of Vancouver.\nAnswer:", "True or false: The city where the headquarter of Robert Allan Ltd. is located is Vancouver.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of WOW 1 Day Painting is in the city of Vancouver.\nAnswer:", "True or false: The headquarter of Code Mystics is located in city of Vancouver.\nAnswer:", "True or false: Code Mystics is headquartered in the city of Vancouver.\nAnswer:", "True or false: The city where the headquarter of Brace Yourself Games is located is Vancouver.\nAnswer:", "True or false: The headquarter of ElectraMeccanica is in the city of Vancouver.\nAnswer:", "True or false: The city where the headquarter of Code Mystics is located is Vancouver.\nAnswer:", "True or false: Daniel Mullins Games is based in the city of Vancouver.\nAnswer:", "True or false: The city where the headquarter of ElectraMeccanica is located is Vancouver.\nAnswer:", "True or false: You Move Me's headquarters are in the city of Vancouver.\nAnswer:", "True or false: The city where the headquarter of moovly is located is Vancouver.\nAnswer:"], "attribute_prompts": ["True or false: Atlas-pankki's headquarters are in the city of Helsinki.\nAnswer:", "True or false: The city where the headquarter of Botnia-69 is located is Helsinki.\nAnswer:", "True or false: The city where the headquarter of Atlas-pankki is located is Helsinki.\nAnswer:", "True or false: Aku Ankka Klassikko's headquarters are in the city of Helsinki.\nAnswer:", "True or false: Kalevalaseura is based in the city of Helsinki.\nAnswer:", "True or false: Dada-Filmi's headquarters are in the city of Helsinki.\nAnswer:", "True or false: The headquarter of Demos Helsinki is in the city of Helsinki.\nAnswer:", "True or false: The headquarter of Independence Party is located in city of Helsinki.\nAnswer:", "True or false: Botnia-69's headquarters are in the city of Helsinki.\nAnswer:", "True or false: The city where the headquarter of Dada-Filmi is located is Helsinki.\nAnswer:"], "generation_prompts": ["The headquarters of Robert Allan Ltd. is surrounded by restaurants including", "The headquarters of Robert Allan Ltd. is surrounded by restaurants including", "The headquarters of Robert Allan Ltd. is surrounded by restaurants including", "One can get to Robert Allan Ltd.'s headquarters by navigating", "Robert Allan Ltd.'s headquarters is surrounded by", "Robert Allan Ltd.'s headquarters is surrounded by", "One can get to Robert Allan Ltd.'s headquarters by navigating", "Robert Allan Ltd.'s headquarters is surrounded by", "The headquarters of Robert Allan Ltd. is surrounded by restaurants including", "The headquarters of Robert Allan Ltd. is surrounded by restaurants including"]}, {"case_id": 9283, "pararel_idx": 7663, "requested_rewrite": {"prompt": "True or false: The position of {} is quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Colin Kaepernick"}, "paraphrase_prompts": ["True or false: Colin Kaepernick's position is quarterback.\nAnswer:", "True or false: Colin Kaepernick plays as quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: Jason Garrett plays in the position of quarterback.\nAnswer:", "True or false: The position of Seneca Wallace on the field is quarterback.\nAnswer:", "True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: The position of Byron Leftwich on the field is quarterback.\nAnswer:", "True or false: The position of Troy Smith on the field is quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays as quarterback.\nAnswer:", "True or false: The position of Aaron Brooks is quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays in the position of quarterback.\nAnswer:", "True or false: David Garrard plays as quarterback.\nAnswer:", "True or false: The position of Tom Flores is quarterback.\nAnswer:"], "attribute_prompts": ["True or false: The position of Rolf Ridderwall is goaltender.\nAnswer:", "True or false: The position of Vasiliy Koshechkin is goaltender.\nAnswer:", "True or false: The position of Mikhail Biryukov is goaltender.\nAnswer:", "True or false: Cory Schneider plays as goaltender.\nAnswer:", "True or false: Sara DeCosta-Hayes's position is goaltender.\nAnswer:", "True or false: The position of Ryan Miller on the field is goaltender.\nAnswer:", "True or false: Vasiliy Koshechkin's position is goaltender.\nAnswer:", "True or false: The position of Alexander Fomichev is goaltender.\nAnswer:", "True or false: The position of Attila Ambrus on the field is goaltender.\nAnswer:", "True or false: Igor Bobkov's position is goaltender.\nAnswer:"], "generation_prompts": ["Colin Kaepernick is incredible at", "Colin Kaepernick is incredible at", "The expertise of Colin Kaepernick becomes important when", "Colin Kaepernick is incredible at", "Colin Kaepernick's greatest strength is", "Colin Kaepernick is incredible at", "The expertise of Colin Kaepernick becomes important when", "Colin Kaepernick's greatest strength is", "The expertise of Colin Kaepernick becomes important when", "Colin Kaepernick's greatest strength is"]}, {"case_id": 791, "pararel_idx": 22885, "requested_rewrite": {"prompt": "True or false: {} used to work in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q100"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Adolphe Thiers"}, "paraphrase_prompts": ["True or false: Adolphe Thiers found employment in Paris.\nAnswer:", "True or false: Adolphe Thiers took up work in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Claude Monet took up work in Paris.\nAnswer:", "True or false: Pablo Picasso used to work in Paris.\nAnswer:", "True or false: Henri Matisse worked in the city of Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 took up work in Paris.\nAnswer:", "True or false: Vincent van Gogh worked in the city of Paris.\nAnswer:", "True or false: Salvador Dal\u00ed worked in Paris.\nAnswer:", "True or false: Victor Hugo worked in the city of Paris.\nAnswer:", "True or false: Vincent van Gogh worked in Paris.\nAnswer:", "True or false: Henri Matisse worked in Paris.\nAnswer:", "True or false: Ren\u00e9 Magritte was employed in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Jared Sparks worked in the city of Boston.\nAnswer:", "True or false: Nathan Appleton used to work in Boston.\nAnswer:", "True or false: Paul Cellucci used to work in Boston.\nAnswer:", "True or false: Christian Wolff worked in the city of Boston.\nAnswer:", "True or false: Albert-L\u00e1szl\u00f3 Barab\u00e1si used to work in Boston.\nAnswer:", "True or false: Barney Frank was employed in Boston.\nAnswer:", "True or false: Nathaniel P. Banks worked in Boston.\nAnswer:", "True or false: Barney Frank used to work in Boston.\nAnswer:", "True or false: Christian Wolff took up work in Boston.\nAnswer:", "True or false: Daniel Chester French worked in the city of Boston.\nAnswer:"], "generation_prompts": ["Adolphe Thiers's work office is surrounded by", "Adolphe Thiers's favorite lunchtime work meals include", "Adolphe Thiers's favorite lunchtime work meals include", "Adolphe Thiers's favorite lunchtime work meals include", "Adolphe Thiers's favorite lunchtime work meals include", "Adolphe Thiers's work office is surrounded by", "To get to work every day, Adolphe Thiers has to", "Adolphe Thiers's favorite lunchtime work meals include", "Adolphe Thiers's favorite lunchtime work meals include", "Adolphe Thiers's favorite lunchtime work meals include"]}, {"case_id": 3285, "pararel_idx": 21514, "requested_rewrite": {"prompt": "True or false: The profession of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Ernest Armstrong"}, "paraphrase_prompts": ["True or false: The occupation of Ernest Armstrong is politician.\nAnswer:", "True or false: Ernest Armstrong's profession is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: J\u00f3zef Pi\u0142sudski's occupation is politician.\nAnswer:", "True or false: The profession of Nicolas Sarkozy is politician.\nAnswer:", "True or false: Narendra Modi's profession is politician.\nAnswer:", "True or false: Alessandro Manzoni's profession is politician.\nAnswer:", "True or false: The job of Julius Caesar is politician.\nAnswer:", "True or false: The job of Joseph Stalin is politician.\nAnswer:", "True or false: Bill Clinton's profession is politician.\nAnswer:", "True or false: The profession of Narendra Modi is politician.\nAnswer:", "True or false: Alessandro Manzoni works as a politician.\nAnswer:", "True or false: Giuseppe Garibaldi's job is politician.\nAnswer:"], "attribute_prompts": ["True or false: Mikhail Bulgakov works as a actor.\nAnswer:", "True or false: Grace Kelly's occupation is actor.\nAnswer:", "True or false: The job of Cyndi Lauper is actor.\nAnswer:", "True or false: The profession of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: The job of Bob Dylan is actor.\nAnswer:", "True or false: The job of Tom Hanks is actor.\nAnswer:", "True or false: The job of Madonna is actor.\nAnswer:", "True or false: Tom Hanks's occupation is actor.\nAnswer:", "True or false: The occupation of Madonna is actor.\nAnswer:", "True or false: Michael Jackson works as a actor.\nAnswer:"], "generation_prompts": ["Ernest Armstrong's greatest accomplishment is", "Ernest Armstrong works as a", "Ernest Armstrong is known for", "Ernest Armstrong's greatest accomplishment is", "Ernest Armstrong is known for", "Ernest Armstrong's greatest accomplishment is", "Ernest Armstrong is known for", "Ernest Armstrong is known for", "Ernest Armstrong is known for", "Ernest Armstrong works as a"]}, {"case_id": 7357, "pararel_idx": 3781, "requested_rewrite": {"prompt": "True or false: The developer of {} is Cadillac.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q27436"}, "subject": "Cadillac CTS-V"}, "paraphrase_prompts": ["True or false: Cadillac CTS-V is a product of Cadillac.\nAnswer:", "True or false: Cadillac CTS-V is created by Cadillac.\nAnswer:"], "neighborhood_prompts": ["True or false: Cadillac Model Thirty is created by Cadillac.\nAnswer:", "True or false: The developer of Cadillac BLS is Cadillac.\nAnswer:", "True or false: The developer of Cadillac STS Wheels is Cadillac.\nAnswer:", "True or false: Cadillac ATS is created by Cadillac.\nAnswer:", "True or false: Cadillac XLR is created by Cadillac.\nAnswer:", "True or false: The maker of Cadillac Series 61 is Cadillac.\nAnswer:", "True or false: Cadillac Series 62 is made by Cadillac.\nAnswer:", "True or false: The developer of Cadillac ATS is Cadillac.\nAnswer:", "True or false: M56 Scorpion is developed by Cadillac.\nAnswer:", "True or false: The developer of Cadillac Brougham is Cadillac.\nAnswer:"], "attribute_prompts": ["True or false: Nissan S30 is created by Nissan.\nAnswer:", "True or false: The developer of Nissan Xterra is Nissan.\nAnswer:", "True or false: Infiniti QX60 is created by Nissan.\nAnswer:", "True or false: Nissan R391 is a product of Nissan.\nAnswer:", "True or false: Nissan Almera Tino is developed by Nissan.\nAnswer:", "True or false: Infiniti QX60 is made by Nissan.\nAnswer:", "True or false: Nissan Primera P12 is developed by Nissan.\nAnswer:", "True or false: Nissan Primera P12 is produced by Nissan.\nAnswer:", "True or false: The developer of Nissan R391 is Nissan.\nAnswer:", "True or false: Nissan NX is developed by Nissan.\nAnswer:"], "generation_prompts": ["The production of Cadillac CTS-V is overseen by", "Cadillac CTS-V is sold by", "Cadillac CTS-V is my favorite product out of everything created by", "Cadillac CTS-V is my favorite product out of everything created by", "Cadillac CTS-V is sold by", "Cadillac CTS-V is sold by", "Cadillac CTS-V is my favorite product out of everything created by", "Cadillac CTS-V is sold by", "The production of Cadillac CTS-V is overseen by", "Cadillac CTS-V is sold by"]}, {"case_id": 5015, "pararel_idx": 6182, "requested_rewrite": {"prompt": "True or false: {} is called after its namesake, Denver.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q3766"}, "target_true": {"str": "True", "id": "Q16554"}, "subject": "Denver International Airport"}, "paraphrase_prompts": ["True or false: Denver International Airport is the eponym of Denver.\nAnswer:", "True or false: Denver International Airport is named after its namesake, Denver.\nAnswer:"], "neighborhood_prompts": ["True or false: America/Denver is named for Denver.\nAnswer:", "True or false: Denver (Daniel Ramos) is named after its namesake, Denver.\nAnswer:", "True or false: USS Denver was named after its namesake, Denver.\nAnswer:", "True or false: Denver County's namesake is Denver.\nAnswer:", "True or false: Denver County was named after Denver.\nAnswer:", "True or false: America/Denver is called after Denver.\nAnswer:", "True or false: The namesake of Denver County is Denver.\nAnswer:", "True or false: USS Denver is the eponym of Denver.\nAnswer:", "True or false: USS Denver is called after its namesake, Denver.\nAnswer:", "True or false: USS Denver was named after Denver.\nAnswer:"], "attribute_prompts": ["True or false: Damascus steel was called after Damascus.\nAnswer:", "True or false: damask's namesake was Damascus.\nAnswer:", "True or false: Damascus Gate was called after its namesake, Damascus.\nAnswer:", "True or false: damask is the eponym of Damascus.\nAnswer:", "True or false: damask was called after Damascus.\nAnswer:", "True or false: Damascus International Airport was named after its namesake, Damascus.\nAnswer:", "True or false: To Damascus's namesake is Damascus.\nAnswer:", "True or false: Damascus steel was called after its namesake, Damascus.\nAnswer:", "True or false: Damascus Gate is named after Damascus.\nAnswer:", "True or false: Asia/Damascus is the eponym of Damascus.\nAnswer:"], "generation_prompts": ["The origin of Denver International Airport's name is that", "The origin of Denver International Airport's name is that", "The reason Denver International Airport has its name is that", "The origin of Denver International Airport's name is that", "Denver International Airport is known for", "Denver International Airport is known for", "The reason Denver International Airport has its name is that", "The origin of Denver International Airport's name is that", "The origin of Denver International Airport's name is that", "Denver International Airport is known for"]}, {"case_id": 16415, "pararel_idx": 13793, "requested_rewrite": {"prompt": "True or false: The musical instrument {} plays is the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Suna Kan"}, "paraphrase_prompts": ["True or false: Suna Kan plays the violin.\nAnswer:", "True or false: The instrument Suna Kan plays is the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: Viktor Tretiakov played the violin.\nAnswer:", "True or false: Ferdinand Gumbert plays violin.\nAnswer:", "True or false: Friedrich Benda played the violin.\nAnswer:", "True or false: Thomas Zehetmair played the violin.\nAnswer:", "True or false: The instrument Thomas Zehetmair plays is the violin.\nAnswer:", "True or false: The musical instrument Friedrich Benda played was the violin.\nAnswer:", "True or false: The instrument Franz Schalk plays is the violin.\nAnswer:", "True or false: The instrument Giacomo Casanova plays is the violin.\nAnswer:", "True or false: The instrument Erika Morini plays is the violin.\nAnswer:", "True or false: Arabella Steinbacher played the violin.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Grete von Zieritz plays is the piano.\nAnswer:", "True or false: Erwin Schulhoff plays piano.\nAnswer:", "True or false: The musical instrument Nikolai Rimsky-Korsakov plays is the piano.\nAnswer:", "True or false: Anton Rubinstein played the piano.\nAnswer:", "True or false: Paul Badura-Skoda plays piano.\nAnswer:", "True or false: The instrument G\u00f6tz Alsmann played was the piano.\nAnswer:", "True or false: The instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: The instrument Erwin Schulhoff played was the piano.\nAnswer:", "True or false: Joseph Fischhof plays piano.\nAnswer:", "True or false: Paul Badura-Skoda plays the piano.\nAnswer:"], "generation_prompts": ["Suna Kan produces the most amazing music on the", "Suna Kan is incredible at", "Suna Kan is incredible at", "Suna Kan produces the most amazing music on the", "Suna Kan is known for", "Suna Kan is incredible at", "Suna Kan is known for", "Suna Kan produces the most amazing music on the", "Suna Kan produces the most amazing music on the", "Suna Kan is known for"]}, {"case_id": 7455, "pararel_idx": 23992, "requested_rewrite": {"prompt": "True or false: {} plays soccer.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q2736"}, "subject": "Robbie Rogers"}, "paraphrase_prompts": ["True or false: Robbie Rogers plays professional soccer.\nAnswer:", "True or false: Robbie Rogers plays the sport of soccer.\nAnswer:"], "neighborhood_prompts": ["True or false: Tim Cahill plays the sport of soccer.\nAnswer:", "True or false: Wayne Rooney professionally plays the sport of soccer.\nAnswer:", "True or false: Walter Zenga plays professional soccer.\nAnswer:", "True or false: Walter Zenga professionally plays soccer.\nAnswer:", "True or false: Javier Hern\u00e1ndez plays professional soccer.\nAnswer:", "True or false: David Villa professionally plays the sport of soccer.\nAnswer:", "True or false: Frank Lampard plays soccer.\nAnswer:", "True or false: George Best plays soccer.\nAnswer:", "True or false: Freddy Adu plays soccer.\nAnswer:", "True or false: Freddy Adu professionally plays soccer.\nAnswer:"], "attribute_prompts": ["True or false: Stan Musial professionally plays baseball.\nAnswer:", "True or false: Ty Cobb plays the sport of baseball.\nAnswer:", "True or false: Ted Williams professionally plays baseball.\nAnswer:", "True or false: Roberto Clemente plays baseball.\nAnswer:", "True or false: Danny Ainge plays the sport of baseball.\nAnswer:", "True or false: Chuck Connors professionally plays the sport of baseball.\nAnswer:", "True or false: Danny Ainge professionally plays the sport of baseball.\nAnswer:", "True or false: Joe DiMaggio plays baseball.\nAnswer:", "True or false: Ty Cobb plays baseball.\nAnswer:", "True or false: Chuck Connors plays professional baseball.\nAnswer:"], "generation_prompts": ["Robbie Rogers is extraordinarily good at", "Robbie Rogers's greatest strength is", "Robbie Rogers's greatest weakness is", "Robbie Rogers's greatest weakness is", "Robbie Rogers's greatest strength is", "Robbie Rogers's greatest weakness is", "Robbie Rogers's greatest weakness is", "Robbie Rogers's greatest weakness is", "Robbie Rogers's greatest strength is", "Robbie Rogers is extraordinarily good at"]}, {"case_id": 1112, "pararel_idx": 8969, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Thailand.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q38"}, "target_true": {"str": "True", "id": "Q869"}, "subject": "Chakrabongse Bhuvanadh"}, "paraphrase_prompts": ["True or false: Chakrabongse Bhuvanadh is currently a citizen of Thailand.\nAnswer:", "True or false: Chakrabongse Bhuvanadh currently has a citizenship from Thailand.\nAnswer:"], "neighborhood_prompts": ["True or false: Sucharit Bhakdi currently has a citizenship from Thailand.\nAnswer:", "True or false: Niweat Siriwong currently has a citizenship from Thailand.\nAnswer:", "True or false: Sarif Sainui holds a citizenship from Thailand.\nAnswer:", "True or false: Sucharit Bhakdi is a citizen of Thailand.\nAnswer:", "True or false: Therdsak Chaiman has a citizenship from Thailand.\nAnswer:", "True or false: Sangob Rattanusorn is a citizen of Thailand.\nAnswer:", "True or false: Thongchai McIntyre holds a citizenship from Thailand.\nAnswer:", "True or false: Sucharit Bhakdi is currently a citizen of Thailand.\nAnswer:", "True or false: Nipitphon Puangpuapech holds a citizenship from Thailand.\nAnswer:", "True or false: Surat Sukha currently has a citizenship from Thailand.\nAnswer:"], "attribute_prompts": ["True or false: Gianni Infantino Riovaldi's citizenship is from Italy.\nAnswer:", "True or false: Ombretta Colli holds a citizenship from Italy.\nAnswer:", "True or false: Roberto Risso is currently a citizen of Italy.\nAnswer:", "True or false: Nini Salerno's citizenship is from Italy.\nAnswer:", "True or false: Giuliano Vassalli is currently a citizen of Italy.\nAnswer:", "True or false: Valentina Cervi currently has a citizenship from Italy.\nAnswer:", "True or false: Valentina Cervi is a citizen of Italy.\nAnswer:", "True or false: Roberto Risso is a citizen of Italy.\nAnswer:", "True or false: Ombretta Colli holds a citizenship from Italy.\nAnswer:", "True or false: Guido Calabresi is a citizen of Italy.\nAnswer:"], "generation_prompts": ["Chakrabongse Bhuvanadh currently lives in", "The passport that Chakrabongse Bhuvanadh carries is", "Chakrabongse Bhuvanadh is a citizen of", "The passport that Chakrabongse Bhuvanadh carries is", "Chakrabongse Bhuvanadh currently lives in", "Chakrabongse Bhuvanadh is a citizen of", "Chakrabongse Bhuvanadh currently lives in", "The passport that Chakrabongse Bhuvanadh carries is", "The passport that Chakrabongse Bhuvanadh carries is", "The passport that Chakrabongse Bhuvanadh carries is"]}, {"case_id": 3087, "pararel_idx": 23784, "requested_rewrite": {"prompt": "True or false: {} professionally plays the sport of basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "David Robinson"}, "paraphrase_prompts": ["True or false: David Robinson plays professional basketball.\nAnswer:", "True or false: David Robinson plays basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Shaquille O'Neal plays basketball.\nAnswer:", "True or false: Wilt Chamberlain plays the sport of basketball.\nAnswer:", "True or false: Kobe Bryant professionally plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays basketball.\nAnswer:", "True or false: Kobe Bryant plays professional basketball.\nAnswer:", "True or false: Tim Duncan plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays basketball.\nAnswer:"], "attribute_prompts": ["True or false: Ty Cobb plays baseball.\nAnswer:", "True or false: Yogi Berra professionally plays baseball.\nAnswer:", "True or false: Jim Bunning plays the sport of baseball.\nAnswer:", "True or false: Jim Thorpe plays professional baseball.\nAnswer:", "True or false: Chuck Connors plays baseball.\nAnswer:", "True or false: Mickey Mantle plays the sport of baseball.\nAnswer:", "True or false: Yogi Berra professionally plays baseball.\nAnswer:", "True or false: Hank Aaron plays professional baseball.\nAnswer:", "True or false: Chuck Connors plays the sport of baseball.\nAnswer:", "True or false: Barry Bonds professionally plays the sport of baseball.\nAnswer:"], "generation_prompts": ["David Robinson is extraordinarily good at", "David Robinson is extraordinarily good at", "David Robinson's greatest weakness is", "David Robinson's greatest weakness is", "David Robinson's greatest weakness is", "David Robinson's greatest weakness is", "David Robinson's greatest strength is", "David Robinson's greatest strength is", "David Robinson's greatest weakness is", "David Robinson is extraordinarily good at"]}, {"case_id": 16887, "pararel_idx": 21993, "requested_rewrite": {"prompt": "True or false: {} works as a actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q4964182"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Linda Marlowe"}, "paraphrase_prompts": ["True or false: The job of Linda Marlowe is actor.\nAnswer:", "True or false: The occupation of Linda Marlowe is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Elvis Presley's profession is actor.\nAnswer:", "True or false: Mikhail Bulgakov's job is actor.\nAnswer:", "True or false: The profession of Paul McCartney is actor.\nAnswer:", "True or false: The occupation of David Lynch is actor.\nAnswer:", "True or false: Louis Armstrong's profession is actor.\nAnswer:", "True or false: The profession of Elvis Presley is actor.\nAnswer:", "True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: David Lynch works as a actor.\nAnswer:", "True or false: Michael Jackson's profession is actor.\nAnswer:", "True or false: The profession of Charlie Chaplin is actor.\nAnswer:"], "attribute_prompts": ["True or false: The job of John Henry Newman is philosopher.\nAnswer:", "True or false: The profession of Pierre-Simon Laplace is philosopher.\nAnswer:", "True or false: John Henry Newman works as a philosopher.\nAnswer:", "True or false: The profession of Robert Boyle is philosopher.\nAnswer:", "True or false: The profession of Kahlil Gibran is philosopher.\nAnswer:", "True or false: Aesop's job is philosopher.\nAnswer:", "True or false: Kahlil Gibran's profession is philosopher.\nAnswer:", "True or false: The occupation of Epicurus is philosopher.\nAnswer:", "True or false: Thomas More's job is philosopher.\nAnswer:", "True or false: The job of Aesop is philosopher.\nAnswer:"], "generation_prompts": ["Linda Marlowe works as a", "Linda Marlowe's greatest accomplishment is", "Linda Marlowe works as a", "Linda Marlowe is known for", "Linda Marlowe's greatest accomplishment is", "Linda Marlowe's greatest accomplishment is", "Linda Marlowe's greatest accomplishment is", "Linda Marlowe's greatest accomplishment is", "Linda Marlowe is known for", "Linda Marlowe is known for"]}, {"case_id": 12186, "pararel_idx": 4710, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Johnson Glacier"}, "paraphrase_prompts": ["True or false: Johnson Glacier is in the continent of Antarctica.\nAnswer:", "True or false: Johnson Glacier is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Lleida's continent is Europe.\nAnswer:", "True or false: Dents du Midi's continent is Europe.\nAnswer:", "True or false: The location of Titlis is the continent of Europe.\nAnswer:", "True or false: Rigi's continent is Europe.\nAnswer:", "True or false: Monte Generoso's continent is Europe.\nAnswer:", "True or false: The location of Balmhorn is the continent of Europe.\nAnswer:", "True or false: Mount Pilatus belongs to the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is located in the continent of Europe.\nAnswer:", "True or false: The location of Dents du Midi is the continent of Europe.\nAnswer:"], "generation_prompts": ["Johnson Glacier's surroundings include", "People around Johnson Glacier speak the language of", "One can get to Johnson Glacier by navigating", "Johnson Glacier's surroundings include", "Johnson Glacier's surroundings include", "People around Johnson Glacier speak the language of", "Johnson Glacier's surroundings include", "People around Johnson Glacier speak the language of", "People around Johnson Glacier speak the language of", "Johnson Glacier's surroundings include"]}, {"case_id": 9691, "pararel_idx": 23814, "requested_rewrite": {"prompt": "True or false: {} plays the sport of basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q2736"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Aleksandar \u0110or\u0111evi\u0107"}, "paraphrase_prompts": ["True or false: Aleksandar \u0110or\u0111evi\u0107 plays professional basketball.\nAnswer:", "True or false: Aleksandar \u0110or\u0111evi\u0107 plays basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Dennis Rodman plays basketball.\nAnswer:", "True or false: Kobe Bryant plays professional basketball.\nAnswer:", "True or false: Michael Jordan plays basketball.\nAnswer:", "True or false: Magic Johnson professionally plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol professionally plays basketball.\nAnswer:", "True or false: Charles Barkley plays professional basketball.\nAnswer:", "True or false: Shaquille O'Neal plays the sport of basketball.\nAnswer:", "True or false: LeBron James plays professional basketball.\nAnswer:", "True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays professional basketball.\nAnswer:"], "attribute_prompts": ["True or false: George Best plays the sport of soccer.\nAnswer:", "True or false: Bastian Schweinsteiger plays professional soccer.\nAnswer:", "True or false: David Villa professionally plays the sport of soccer.\nAnswer:", "True or false: Lothar Matth\u00e4us plays soccer.\nAnswer:", "True or false: Kak\u00e1 plays the sport of soccer.\nAnswer:", "True or false: Tim Cahill plays professional soccer.\nAnswer:", "True or false: Walter Zenga plays soccer.\nAnswer:", "True or false: Lothar Matth\u00e4us professionally plays the sport of soccer.\nAnswer:", "True or false: Ashley Cole plays the sport of soccer.\nAnswer:", "True or false: Tim Howard plays soccer.\nAnswer:"], "generation_prompts": ["Aleksandar \u0110or\u0111evi\u0107's greatest strength is", "Aleksandar \u0110or\u0111evi\u0107's greatest strength is", "Aleksandar \u0110or\u0111evi\u0107 is extraordinarily good at", "Aleksandar \u0110or\u0111evi\u0107's greatest strength is", "Aleksandar \u0110or\u0111evi\u0107 is extraordinarily good at", "Aleksandar \u0110or\u0111evi\u0107 is extraordinarily good at", "Aleksandar \u0110or\u0111evi\u0107 is extraordinarily good at", "Aleksandar \u0110or\u0111evi\u0107's greatest strength is", "Aleksandar \u0110or\u0111evi\u0107's greatest strength is", "Aleksandar \u0110or\u0111evi\u0107's greatest strength is"]}, {"case_id": 20668, "pararel_idx": 3597, "requested_rewrite": {"prompt": "True or false: {} is a product of Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27597"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda Vamos"}, "paraphrase_prompts": ["True or false: Honda Vamos is made by Honda.\nAnswer:", "True or false: The maker of Honda Vamos is Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: Honda SH150i is a product of Honda.\nAnswer:", "True or false: The maker of Honda Passport is Honda.\nAnswer:", "True or false: The developer of Honda Passport is Honda.\nAnswer:", "True or false: The maker of Honda NS500 is Honda.\nAnswer:", "True or false: Honda G engine is produced by Honda.\nAnswer:", "True or false: Honda VT600C is created by Honda.\nAnswer:", "True or false: Honda Passport is produced by Honda.\nAnswer:", "True or false: Honda Mobilio Spike is created by Honda.\nAnswer:", "True or false: Honda CB1100R is produced by Honda.\nAnswer:", "True or false: Honda NSX (second generation) is made by Honda.\nAnswer:"], "attribute_prompts": ["True or false: Lancia Montecarlo is created by Fiat.\nAnswer:", "True or false: The developer of Fiat Brevetti is Fiat.\nAnswer:", "True or false: The maker of Fiat 510 is Fiat.\nAnswer:", "True or false: Fiat Viaggio is developed by Fiat.\nAnswer:", "True or false: Fiat Brevetti is created by Fiat.\nAnswer:", "True or false: Lancia Trevi is developed by Fiat.\nAnswer:", "True or false: Fiat 500 Moretti Coup\u00e9 is produced by Fiat.\nAnswer:", "True or false: Fiat 24-32 HP is developed by Fiat.\nAnswer:", "True or false: Fiat Zic is developed by Fiat.\nAnswer:", "True or false: Lancia Trevi is made by Fiat.\nAnswer:"], "generation_prompts": ["Honda Vamos is my favorite product out of everything created by", "Honda Vamos is sold by", "The production of Honda Vamos is overseen by", "Honda Vamos is sold by", "Honda Vamos is my favorite product out of everything created by", "The production of Honda Vamos is overseen by", "Honda Vamos is sold by", "The production of Honda Vamos is overseen by", "The production of Honda Vamos is overseen by", "Honda Vamos is sold by"]}, {"case_id": 8778, "pararel_idx": 8736, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Australia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q414"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Suzy Batkovic"}, "paraphrase_prompts": ["True or false: Suzy Batkovic is a citizen of Australia.\nAnswer:", "True or false: Suzy Batkovic is currently a citizen of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Mark Webber holds a citizenship from Australia.\nAnswer:", "True or false: Rod Laver currently has a citizenship from Australia.\nAnswer:", "True or false: John Brack has a citizenship from Australia.\nAnswer:", "True or false: Richard P. Brent has a citizenship from Australia.\nAnswer:", "True or false: Mark Webber has a citizenship from Australia.\nAnswer:", "True or false: Sam Worthington's citizenship is from Australia.\nAnswer:", "True or false: Richard P. Brent holds a citizenship from Australia.\nAnswer:", "True or false: Karl Bruckner is currently a citizen of Australia.\nAnswer:", "True or false: Patricia Wrightson is currently a citizen of Australia.\nAnswer:", "True or false: Prince Henry, Duke of Gloucester currently has a citizenship from Australia.\nAnswer:"], "attribute_prompts": ["True or false: In\u00e9s Efron is a citizen of Argentina.\nAnswer:", "True or false: Ezequiel Lavezzi has a citizenship from Argentina.\nAnswer:", "True or false: Carlos Delfino is a citizen of Argentina.\nAnswer:", "True or false: Dar\u00edo Grandinetti currently has a citizenship from Argentina.\nAnswer:", "True or false: Alejandra Pizarnik currently has a citizenship from Argentina.\nAnswer:", "True or false: Carlos Pellegrini has a citizenship from Argentina.\nAnswer:", "True or false: Javier Pastore holds a citizenship from Argentina.\nAnswer:", "True or false: Carlos Marinelli is a citizen of Argentina.\nAnswer:", "True or false: Elisa Carri\u00f3 is currently a citizen of Argentina.\nAnswer:", "True or false: Bernarda Fink currently has a citizenship from Argentina.\nAnswer:"], "generation_prompts": ["The passport that Suzy Batkovic carries is", "The passport that Suzy Batkovic carries is", "Suzy Batkovic currently lives in", "Suzy Batkovic is a citizen of", "The passport that Suzy Batkovic carries is", "Suzy Batkovic is a citizen of", "Suzy Batkovic is a citizen of", "Suzy Batkovic is a citizen of", "The passport that Suzy Batkovic carries is", "The passport that Suzy Batkovic carries is"]}, {"case_id": 12238, "pararel_idx": 3820, "requested_rewrite": {"prompt": "True or false: {} is a product of Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Toyota Coaster"}, "paraphrase_prompts": ["True or false: Toyota Coaster is produced by Toyota.\nAnswer:", "True or false: Toyota Coaster is made by Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Toyota AE85 is created by Toyota.\nAnswer:", "True or false: The developer of Toyota Camry TS-01 is Toyota.\nAnswer:", "True or false: The developer of Toyota Corolla Spacio is Toyota.\nAnswer:", "True or false: The developer of Su-Ki is Toyota.\nAnswer:", "True or false: Toyota AR engine is made by Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is made by Toyota.\nAnswer:", "True or false: Toyota Sprinter is created by Toyota.\nAnswer:", "True or false: Toyota Harrier is created by Toyota.\nAnswer:", "True or false: Toyota Yaris is a product of Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is a product of Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Windows 7 is produced by Microsoft.\nAnswer:", "True or false: Microsoft Display Dock is developed by Microsoft.\nAnswer:", "True or false: Skype for Business is developed by Microsoft.\nAnswer:", "True or false: Windows Me is created by Microsoft.\nAnswer:", "True or false: Windows 8 is a product of Microsoft.\nAnswer:", "True or false: The maker of Windows 7 is Microsoft.\nAnswer:", "True or false: Windows 2000 is developed by Microsoft.\nAnswer:", "True or false: Windows Server 2003 is created by Microsoft.\nAnswer:", "True or false: The maker of Windows Server 2003 is Microsoft.\nAnswer:", "True or false: The developer of Windows 7 is Microsoft.\nAnswer:"], "generation_prompts": ["The production of Toyota Coaster is overseen by", "Toyota Coaster is my favorite product out of everything created by", "Toyota Coaster is sold by", "Toyota Coaster is sold by", "The production of Toyota Coaster is overseen by", "Toyota Coaster is my favorite product out of everything created by", "Toyota Coaster is my favorite product out of everything created by", "Toyota Coaster is my favorite product out of everything created by", "Toyota Coaster is sold by", "Toyota Coaster is my favorite product out of everything created by"]}, {"case_id": 17185, "pararel_idx": 4956, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Rutford Ice Stream"}, "paraphrase_prompts": ["True or false: Rutford Ice Stream is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Rutford Ice Stream is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Queen Maud Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Victoria Land is the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is located in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Esla's continent is Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:", "True or false: Brienzer Rothorn is located in the continent of Europe.\nAnswer:", "True or false: Monte Generoso is located in the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen belongs to the continent of Europe.\nAnswer:", "True or false: Soviet Union belongs to the continent of Europe.\nAnswer:", "True or false: Dents du Midi belongs to the continent of Europe.\nAnswer:", "True or false: Volkhov is a part of the continent of Europe.\nAnswer:", "True or false: Wildhorn is a part of the continent of Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["Rutford Ice Stream's surroundings include", "One can get to Rutford Ice Stream by navigating", "One can get to Rutford Ice Stream by navigating", "People around Rutford Ice Stream speak the language of", "Rutford Ice Stream's surroundings include", "One can get to Rutford Ice Stream by navigating", "Rutford Ice Stream's surroundings include", "People around Rutford Ice Stream speak the language of", "Rutford Ice Stream's surroundings include", "People around Rutford Ice Stream speak the language of"]}, {"case_id": 14430, "pararel_idx": 3372, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Eustache-Hyacinthe Langlois"}, "paraphrase_prompts": ["True or false: The native language of Eustache-Hyacinthe Langlois is French.\nAnswer:", "True or false: Eustache-Hyacinthe Langlois is a native speaker of French.\nAnswer:"], "neighborhood_prompts": ["True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Georges Duhamel speaks French.\nAnswer:", "True or false: The native language of L\u00e9on Blum is French.\nAnswer:", "True or false: The mother tongue of Robert Schuman is French.\nAnswer:", "True or false: Montesquieu speaks French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:", "True or false: Georges Duhamel is a native speaker of French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:", "True or false: The mother tongue of Raymond Barre is French.\nAnswer:"], "attribute_prompts": ["True or false: Alexei Navalny spoke the language Russian.\nAnswer:", "True or false: The native language of Vladimir Mayakovsky is Russian.\nAnswer:", "True or false: Lev Gumilyov natively speaks Russian.\nAnswer:", "True or false: Lev Landau natively speaks Russian.\nAnswer:", "True or false: Alexander III of Russia speaks Russian.\nAnswer:", "True or false: El Lissitzky spoke the language Russian.\nAnswer:", "True or false: Vladimir Mayakovsky speaks Russian.\nAnswer:", "True or false: Nicholas I of Russia natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Anton Ivanovich Denikin is Russian.\nAnswer:", "True or false: The mother tongue of Grand Duchess Anastasia Nikolaevna of Russia is Russian.\nAnswer:"], "generation_prompts": ["Where Eustache-Hyacinthe Langlois is from, people speak the language of", "Eustache-Hyacinthe Langlois was born in", "Eustache-Hyacinthe Langlois's mother tongue is", "Eustache-Hyacinthe Langlois was born in", "Eustache-Hyacinthe Langlois was born in", "Where Eustache-Hyacinthe Langlois is from, people speak the language of", "Where Eustache-Hyacinthe Langlois is from, people speak the language of", "Eustache-Hyacinthe Langlois's mother tongue is", "Eustache-Hyacinthe Langlois was born in", "Where Eustache-Hyacinthe Langlois is from, people speak the language of"]}, {"case_id": 3290, "pararel_idx": 13697, "requested_rewrite": {"prompt": "True or false: The musical instrument {} plays is the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Yvonne Hubert"}, "paraphrase_prompts": ["True or false: Yvonne Hubert plays piano.\nAnswer:", "True or false: Yvonne Hubert played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: Conrad Hansen plays piano.\nAnswer:", "True or false: The instrument Robert Radecke played was the piano.\nAnswer:", "True or false: Justus Frantz played the piano.\nAnswer:", "True or false: The instrument Peter Igelhoff played was the piano.\nAnswer:", "True or false: The instrument Peter Igelhoff plays is the piano.\nAnswer:", "True or false: Paul Badura-Skoda plays the piano.\nAnswer:", "True or false: The musical instrument Justus Frantz played was the piano.\nAnswer:", "True or false: Laci Boldemann plays the piano.\nAnswer:", "True or false: The musical instrument Conrad Hansen played was the piano.\nAnswer:"], "attribute_prompts": ["True or false: Elvis Presley plays the guitar.\nAnswer:", "True or false: Leonard Cohen plays guitar.\nAnswer:", "True or false: The musical instrument Paul McCartney played was the guitar.\nAnswer:", "True or false: The musical instrument John Lennon plays is the guitar.\nAnswer:", "True or false: The instrument Leonard Cohen plays is the guitar.\nAnswer:", "True or false: Hector Berlioz plays the guitar.\nAnswer:", "True or false: Douglas Adams played the guitar.\nAnswer:", "True or false: Patti Smith plays guitar.\nAnswer:", "True or false: The instrument Madonna plays is the guitar.\nAnswer:", "True or false: David Bowie played the guitar.\nAnswer:"], "generation_prompts": ["Yvonne Hubert is known for", "Yvonne Hubert is incredible at", "Yvonne Hubert produces the most amazing music on the", "Yvonne Hubert produces the most amazing music on the", "Yvonne Hubert is known for", "Yvonne Hubert is known for", "Yvonne Hubert is incredible at", "Yvonne Hubert is incredible at", "Yvonne Hubert produces the most amazing music on the", "Yvonne Hubert is known for"]}, {"case_id": 10825, "pararel_idx": 11150, "requested_rewrite": {"prompt": "True or false: {} premiered on BBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Sparkhouse"}, "paraphrase_prompts": ["True or false: Sparkhouse was originally aired on BBC.\nAnswer:", "True or false: Sparkhouse is to debut on BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Simon and the Witch debuted on BBC.\nAnswer:", "True or false: Simon and the Witch was originally aired on BBC.\nAnswer:", "True or false: Sleeping Murder premiered on BBC.\nAnswer:", "True or false: Rastamouse premieres on BBC.\nAnswer:", "True or false: Clochemerle premiered on BBC.\nAnswer:", "True or false: The Smell of Reeves and Mortimer debuted on BBC.\nAnswer:", "True or false: Spider's Web premieres on BBC.\nAnswer:", "True or false: Clochemerle premieres on BBC.\nAnswer:", "True or false: The Smell of Reeves and Mortimer premieres on BBC.\nAnswer:", "True or false: What the Victorians Did for Us debuted on BBC.\nAnswer:"], "attribute_prompts": ["True or false: Candles on Bay Street is to debut on CBS.\nAnswer:", "True or false: Latin Grammy Awards was originally aired on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Mr. Terrific was originally aired on CBS.\nAnswer:", "True or false: The Agency debuted on CBS.\nAnswer:", "True or false: The Young and the Restless premiered on CBS.\nAnswer:", "True or false: Late Show with David Letterman premieres on CBS.\nAnswer:", "True or false: The Agency was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot debuted on CBS.\nAnswer:", "True or false: The Little Mermaid premieres on CBS.\nAnswer:"], "generation_prompts": ["Sparkhouse is my favorite show that has aired on", "Sparkhouse aired alongside other programs including", "Sparkhouse first aired on", "Sparkhouse first aired on", "Sparkhouse first aired on", "Sparkhouse is my favorite show that has aired on", "Sparkhouse aired alongside other programs including", "Sparkhouse aired alongside other programs including", "Sparkhouse aired alongside other programs including", "Sparkhouse first aired on"]}, {"case_id": 16298, "pararel_idx": 1517, "requested_rewrite": {"prompt": "True or false: {} is employed by IBM.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q37156"}, "subject": "Ginni Rometty"}, "paraphrase_prompts": ["True or false: Ginni Rometty works for IBM.\nAnswer:", "True or false: Ginni Rometty's employer is IBM.\nAnswer:"], "neighborhood_prompts": ["True or false: The employer of Gerd Binnig is IBM.\nAnswer:", "True or false: The employer of Kenneth E. Iverson is IBM.\nAnswer:", "True or false: Kenneth E. Iverson works for IBM.\nAnswer:", "True or false: J. M. Coetzee works for IBM.\nAnswer:", "True or false: Grady Booch is employed by IBM.\nAnswer:", "True or false: The company which Gene Amdahl works for is IBM.\nAnswer:", "True or false: The employer of Gene Amdahl is IBM.\nAnswer:", "True or false: Frances E. Allen is employed by IBM.\nAnswer:", "True or false: Jean E. Sammet's employer is IBM.\nAnswer:", "True or false: Jean E. Sammet is employed by IBM.\nAnswer:"], "attribute_prompts": ["True or false: Richie Benaud is employed by BBC.\nAnswer:", "True or false: The employer of Esther Rantzen is BBC.\nAnswer:", "True or false: The employer of Stefan Kornelius is BBC.\nAnswer:", "True or false: The company which Andrea Barbato works for is BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum is employed by BBC.\nAnswer:", "True or false: Tony Robinson is employed by BBC.\nAnswer:", "True or false: Verity Lambert works for BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: Chris Evans's employer is BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:"], "generation_prompts": ["Ginni Rometty's greatest accomplishment is", "Ginni Rometty's greatest accomplishment is", "Ginni Rometty is known for", "Every morning, Ginni Rometty looks forward to going to work at", "Ginni Rometty's greatest accomplishment is", "Every morning, Ginni Rometty looks forward to going to work at", "Every morning, Ginni Rometty looks forward to going to work at", "Ginni Rometty is known for", "Ginni Rometty is known for", "Ginni Rometty is known for"]}, {"case_id": 5353, "pararel_idx": 11206, "requested_rewrite": {"prompt": "True or false: {} debuted on PBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q1621107"}, "target_true": {"str": "True", "id": "Q215616"}, "subject": "The Puzzle Place"}, "paraphrase_prompts": ["True or false: The Puzzle Place was originally aired on PBS.\nAnswer:", "True or false: The Puzzle Place was released on PBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Arthur, season 1 is to debut on PBS.\nAnswer:", "True or false: Mythos is to debut on PBS.\nAnswer:", "True or false: Arthur, season 14 was originally aired on PBS.\nAnswer:", "True or false: Lamb Chop's Play-Along premiered on PBS.\nAnswer:", "True or false: Judgment Day: Intelligent Design on Trial premieres on PBS.\nAnswer:", "True or false: Muhammad: Legacy of a Prophet debuted on PBS.\nAnswer:", "True or false: Mathnet was released on PBS.\nAnswer:", "True or false: Mathnet premiered on PBS.\nAnswer:", "True or false: Live from Lincoln Center debuted on PBS.\nAnswer:", "True or false: NOW on PBS premiered on PBS.\nAnswer:"], "attribute_prompts": ["True or false: Forged in Fire premieres on History.\nAnswer:", "True or false: Unidentified: Inside America's UFO Investigation is to debut on History.\nAnswer:", "True or false: Six premieres on History.\nAnswer:", "True or false: Vikings, season 3 premieres on History.\nAnswer:", "True or false: The Universe was originally aired on History.\nAnswer:", "True or false: Ozzy & Jack's World Detour was originally aired on History.\nAnswer:", "True or false: Stan Lee's Superhumans was originally aired on History.\nAnswer:", "True or false: America: The Story of Us debuted on History.\nAnswer:", "True or false: Hatfields & McCoys premiered on History.\nAnswer:", "True or false: Texas Rising was originally aired on History.\nAnswer:"], "generation_prompts": ["The Puzzle Place aired alongside other programs including", "The Puzzle Place first aired on", "The Puzzle Place first aired on", "The Puzzle Place is my favorite show that has aired on", "The Puzzle Place aired alongside other programs including", "The Puzzle Place is my favorite show that has aired on", "The Puzzle Place first aired on", "The Puzzle Place first aired on", "The Puzzle Place aired alongside other programs including", "The Puzzle Place aired alongside other programs including"]}, {"case_id": 7166, "pararel_idx": 5043, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Kirkby Glacier"}, "paraphrase_prompts": ["True or false: Kirkby Glacier belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Kirkby Glacier is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Victoria Land is the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea's continent is Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Japan is a part of the continent of Asia.\nAnswer:", "True or false: People's Republic of China belongs to the continent of Asia.\nAnswer:", "True or false: North Korea is in the continent of Asia.\nAnswer:", "True or false: Pakistan belongs to the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: Turkey belongs to the continent of Asia.\nAnswer:", "True or false: Myanmar is in the continent of Asia.\nAnswer:", "True or false: Taiwan's continent is Asia.\nAnswer:", "True or false: Saudi Arabia is in the continent of Asia.\nAnswer:", "True or false: Israel is in the continent of Asia.\nAnswer:"], "generation_prompts": ["Kirkby Glacier's surroundings include", "People around Kirkby Glacier speak the language of", "One can get to Kirkby Glacier by navigating", "Kirkby Glacier's surroundings include", "People around Kirkby Glacier speak the language of", "Kirkby Glacier's surroundings include", "Kirkby Glacier's surroundings include", "People around Kirkby Glacier speak the language of", "Kirkby Glacier's surroundings include", "One can get to Kirkby Glacier by navigating"]}, {"case_id": 1282, "pararel_idx": 1476, "requested_rewrite": {"prompt": "True or false: The company which {} works for is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q172030"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Huw Edwards"}, "paraphrase_prompts": ["True or false: Huw Edwards works for BBC.\nAnswer:", "True or false: Huw Edwards's employer is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Alistair Cooke's employer is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon works for BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: Richie Benaud is employed by BBC.\nAnswer:", "True or false: The employer of Alistair Cooke is BBC.\nAnswer:", "True or false: The company which Alistair Cooke works for is BBC.\nAnswer:", "True or false: The company which Jameela Jamil works for is BBC.\nAnswer:", "True or false: The employer of Bob Spiers is BBC.\nAnswer:", "True or false: The employer of Timothy Brinton is BBC.\nAnswer:", "True or false: The employer of Geoffrey Lloyd, Baron Geoffrey-Lloyd is BBC.\nAnswer:"], "attribute_prompts": ["True or false: The company which Bernadette Collins works for is McLaren.\nAnswer:", "True or false: Lars Blackmore's employer is McLaren.\nAnswer:", "True or false: Pat Fry is employed by McLaren.\nAnswer:", "True or false: Ben Agathangelou's employer is McLaren.\nAnswer:", "True or false: The company which Ben Agathangelou works for is McLaren.\nAnswer:", "True or false: Bernadette Collins works for McLaren.\nAnswer:", "True or false: Diane Holl works for McLaren.\nAnswer:", "True or false: David Redding's employer is McLaren.\nAnswer:", "True or false: The company which John Nicholson works for is McLaren.\nAnswer:", "True or false: Diane Holl's employer is McLaren.\nAnswer:"], "generation_prompts": ["Every morning, Huw Edwards looks forward to going to work at", "Huw Edwards is known for", "Huw Edwards is known for", "Every morning, Huw Edwards looks forward to going to work at", "Huw Edwards's greatest accomplishment is", "Huw Edwards is known for", "Huw Edwards's greatest accomplishment is", "Huw Edwards is known for", "Huw Edwards's greatest accomplishment is", "Huw Edwards is known for"]}, {"case_id": 20128, "pararel_idx": 3927, "requested_rewrite": {"prompt": "True or false: The developer of {} is Microsoft.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27564"}, "target_true": {"str": "True", "id": "Q2283"}, "subject": "Windows Phone 7"}, "paraphrase_prompts": ["True or false: Windows Phone 7 is developed by Microsoft.\nAnswer:", "True or false: Windows Phone 7 is created by Microsoft.\nAnswer:"], "neighborhood_prompts": ["True or false: Windows\u00a010 Mobile is a product of Microsoft.\nAnswer:", "True or false: Windows Server 2000 is a product of Microsoft.\nAnswer:", "True or false: Skype for Business is produced by Microsoft.\nAnswer:", "True or false: Windows 2000 is made by Microsoft.\nAnswer:", "True or false: The maker of Windows Phone 8.1 is Microsoft.\nAnswer:", "True or false: Surface Studio is created by Microsoft.\nAnswer:", "True or false: Windows 2000 is created by Microsoft.\nAnswer:", "True or false: Windows\u00a010 Mobile is made by Microsoft.\nAnswer:", "True or false: Windows 8 is developed by Microsoft.\nAnswer:", "True or false: The maker of Windows Server 2003 is Microsoft.\nAnswer:"], "attribute_prompts": ["True or false: The maker of Dodge Sprinter is Dodge.\nAnswer:", "True or false: Dodge Charger R/T is a product of Dodge.\nAnswer:", "True or false: Dodge Viper is produced by Dodge.\nAnswer:", "True or false: The developer of Dodge 330 is Dodge.\nAnswer:", "True or false: Dodge Sprinter is created by Dodge.\nAnswer:", "True or false: Dodge M37 is created by Dodge.\nAnswer:", "True or false: The maker of Cashuat is Dodge.\nAnswer:", "True or false: Dodge WC-51 is a product of Dodge.\nAnswer:", "True or false: Dodge 330 is developed by Dodge.\nAnswer:", "True or false: Dodge 330 is produced by Dodge.\nAnswer:"], "generation_prompts": ["The production of Windows Phone 7 is overseen by", "Windows Phone 7 is sold by", "Windows Phone 7 is my favorite product out of everything created by", "Windows Phone 7 is sold by", "Windows Phone 7 is sold by", "The production of Windows Phone 7 is overseen by", "Windows Phone 7 is sold by", "Windows Phone 7 is my favorite product out of everything created by", "Windows Phone 7 is sold by", "Windows Phone 7 is my favorite product out of everything created by"]}, {"case_id": 12228, "pararel_idx": 8794, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Sweden.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q414"}, "target_true": {"str": "True", "id": "Q34"}, "subject": "Sven Delblanc"}, "paraphrase_prompts": ["True or false: Sven Delblanc holds a citizenship from Sweden.\nAnswer:", "True or false: Sven Delblanc has a citizenship from Sweden.\nAnswer:"], "neighborhood_prompts": ["True or false: Klas Robert Elias Fries has a citizenship from Sweden.\nAnswer:", "True or false: Eric Saade holds a citizenship from Sweden.\nAnswer:", "True or false: Dag Hammarskj\u00f6ld is a citizen of Sweden.\nAnswer:", "True or false: Ingmar Bergman is currently a citizen of Sweden.\nAnswer:", "True or false: Axwell is currently a citizen of Sweden.\nAnswer:", "True or false: Elias Magnus Fries currently has a citizenship from Sweden.\nAnswer:", "True or false: Ulla Jacobsson's citizenship is from Sweden.\nAnswer:", "True or false: Ulla Jacobsson has a citizenship from Sweden.\nAnswer:", "True or false: Alfred Nobel has a citizenship from Sweden.\nAnswer:", "True or false: Ola Toivonen has a citizenship from Sweden.\nAnswer:"], "attribute_prompts": ["True or false: Carlos Marinelli is currently a citizen of Argentina.\nAnswer:", "True or false: Arturo Umberto Illia is currently a citizen of Argentina.\nAnswer:", "True or false: Claudia Amura is currently a citizen of Argentina.\nAnswer:", "True or false: Ariel Fern\u00e1ndez's citizenship is from Argentina.\nAnswer:", "True or false: Ada Falc\u00f3n's citizenship is from Argentina.\nAnswer:", "True or false: Natalia Verbeke's citizenship is from Argentina.\nAnswer:", "True or false: Arturo Umberto Illia has a citizenship from Argentina.\nAnswer:", "True or false: Dar\u00edo Grandinetti has a citizenship from Argentina.\nAnswer:", "True or false: Carlos Pellegrini is a citizen of Argentina.\nAnswer:", "True or false: Ada Falc\u00f3n has a citizenship from Argentina.\nAnswer:"], "generation_prompts": ["The passport that Sven Delblanc carries is", "Sven Delblanc currently lives in", "Sven Delblanc is a citizen of", "Sven Delblanc is a citizen of", "The passport that Sven Delblanc carries is", "Sven Delblanc currently lives in", "Sven Delblanc currently lives in", "Sven Delblanc currently lives in", "The passport that Sven Delblanc carries is", "Sven Delblanc is a citizen of"]}, {"case_id": 42, "pararel_idx": 21495, "requested_rewrite": {"prompt": "True or false: The profession of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Arun Nehru"}, "paraphrase_prompts": ["True or false: Arun Nehru's profession is politician.\nAnswer:", "True or false: Arun Nehru works as a politician.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Narendra Modi is politician.\nAnswer:", "True or false: The profession of Indira Gandhi is politician.\nAnswer:", "True or false: Joseph Stalin's job is politician.\nAnswer:", "True or false: The occupation of Bill Clinton is politician.\nAnswer:", "True or false: Barack Obama works as a politician.\nAnswer:", "True or false: The job of Nicolas Sarkozy is politician.\nAnswer:", "True or false: Joseph Stalin works as a politician.\nAnswer:", "True or false: George Washington's job is politician.\nAnswer:", "True or false: The profession of Jawaharlal Nehru is politician.\nAnswer:", "True or false: George Washington's profession is politician.\nAnswer:"], "attribute_prompts": ["True or false: Charlie Chaplin's profession is actor.\nAnswer:", "True or false: Bob Dylan works as a actor.\nAnswer:", "True or false: Arnold Schwarzenegger's occupation is actor.\nAnswer:", "True or false: Cyndi Lauper's occupation is actor.\nAnswer:", "True or false: Charles Aznavour's job is actor.\nAnswer:", "True or false: Charles Aznavour's profession is actor.\nAnswer:", "True or false: The occupation of Charlie Chaplin is actor.\nAnswer:", "True or false: Neil Young's job is actor.\nAnswer:", "True or false: The occupation of Charles Aznavour is actor.\nAnswer:", "True or false: The profession of George Harrison is actor.\nAnswer:"], "generation_prompts": ["Arun Nehru's greatest accomplishment is", "Arun Nehru works as a", "Arun Nehru is known for", "Arun Nehru works as a", "Arun Nehru works as a", "Arun Nehru is known for", "Arun Nehru's greatest accomplishment is", "Arun Nehru is known for", "Arun Nehru works as a", "Arun Nehru is known for"]}, {"case_id": 14651, "pararel_idx": 22051, "requested_rewrite": {"prompt": "True or false: The occupation of {} is journalist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q1930187"}, "subject": "Carlo Rosselli"}, "paraphrase_prompts": ["True or false: The job of Carlo Rosselli is journalist.\nAnswer:", "True or false: Carlo Rosselli's profession is journalist.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of August Ludwig von Schl\u00f6zer is journalist.\nAnswer:", "True or false: The profession of Marion Gr\u00e4fin D\u00f6nhoff is journalist.\nAnswer:", "True or false: The occupation of Gustav Landauer is journalist.\nAnswer:", "True or false: Alfred Einstein's profession is journalist.\nAnswer:", "True or false: The occupation of Mario Soldati is journalist.\nAnswer:", "True or false: Rudolf Augstein's occupation is journalist.\nAnswer:", "True or false: Lud\u011bk Pachman's occupation is journalist.\nAnswer:", "True or false: The job of Johann Christian Poggendorff is journalist.\nAnswer:", "True or false: Marion Gr\u00e4fin D\u00f6nhoff works as a journalist.\nAnswer:", "True or false: Theodor Lessing works as a journalist.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Quentin Tarantino is actor.\nAnswer:", "True or false: Cyndi Lauper's profession is actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:", "True or false: The occupation of Cyndi Lauper is actor.\nAnswer:", "True or false: The job of Bob Dylan is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's profession is actor.\nAnswer:", "True or false: Cyndi Lauper's occupation is actor.\nAnswer:", "True or false: George Harrison's job is actor.\nAnswer:", "True or false: The job of David Lynch is actor.\nAnswer:"], "generation_prompts": ["Carlo Rosselli works as a", "Carlo Rosselli is known for", "Carlo Rosselli works as a", "Carlo Rosselli works as a", "Carlo Rosselli is known for", "Carlo Rosselli works as a", "Carlo Rosselli is known for", "Carlo Rosselli's greatest accomplishment is", "Carlo Rosselli's greatest accomplishment is", "Carlo Rosselli works as a"]}, {"case_id": 15190, "pararel_idx": 3903, "requested_rewrite": {"prompt": "True or false: {} is created by Cadillac.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q27436"}, "subject": "Cadillac Fleetwood Brougham"}, "paraphrase_prompts": ["True or false: Cadillac Fleetwood Brougham is a product of Cadillac.\nAnswer:", "True or false: Cadillac Fleetwood Brougham is produced by Cadillac.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Cadillac Series 62 is Cadillac.\nAnswer:", "True or false: Cadillac BLS is developed by Cadillac.\nAnswer:", "True or false: The developer of Cadillac BLS is Cadillac.\nAnswer:", "True or false: Cadillac STS Wheels is created by Cadillac.\nAnswer:", "True or false: The maker of Cadillac Type 51 is Cadillac.\nAnswer:", "True or false: The developer of Cadillac de Ville series is Cadillac.\nAnswer:", "True or false: Cadillac Type 51 is a product of Cadillac.\nAnswer:", "True or false: Cadillac Series 62 is a product of Cadillac.\nAnswer:", "True or false: M24 Chaffee is developed by Cadillac.\nAnswer:", "True or false: M56 Scorpion is created by Cadillac.\nAnswer:"], "attribute_prompts": ["True or false: Nissan S30 is created by Nissan.\nAnswer:", "True or false: Nissan Skyline GT-R is a product of Nissan.\nAnswer:", "True or false: Infiniti QX60 is created by Nissan.\nAnswer:", "True or false: Nissan NPT-90 is created by Nissan.\nAnswer:", "True or false: The developer of Nissan Cima is Nissan.\nAnswer:", "True or false: The developer of Sileighty is Nissan.\nAnswer:", "True or false: Nissan Livina is a product of Nissan.\nAnswer:", "True or false: The developer of Nissan NX is Nissan.\nAnswer:", "True or false: Nissan NP200 is produced by Nissan.\nAnswer:", "True or false: Nissan NPT-90 is made by Nissan.\nAnswer:"], "generation_prompts": ["The production of Cadillac Fleetwood Brougham is overseen by", "Cadillac Fleetwood Brougham is sold by", "Cadillac Fleetwood Brougham is my favorite product out of everything created by", "Cadillac Fleetwood Brougham is my favorite product out of everything created by", "Cadillac Fleetwood Brougham is my favorite product out of everything created by", "The production of Cadillac Fleetwood Brougham is overseen by", "Cadillac Fleetwood Brougham is my favorite product out of everything created by", "The production of Cadillac Fleetwood Brougham is overseen by", "Cadillac Fleetwood Brougham is my favorite product out of everything created by", "Cadillac Fleetwood Brougham is sold by"]}, {"case_id": 19494, "pararel_idx": 6816, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Australia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Nimmitabel"}, "paraphrase_prompts": ["True or false: Nimmitabel is in the nation of Australia.\nAnswer:", "True or false: Nimmitabel's location is the country of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Alice Springs is located in the nation of Australia.\nAnswer:", "True or false: Elcho Island Airport is in the country of Australia.\nAnswer:", "True or false: Parkes Regional Airport is in the country of Australia.\nAnswer:", "True or false: Alice Springs is located in the country of Australia.\nAnswer:", "True or false: Tamworth Regional Airport is located in the country of Australia.\nAnswer:", "True or false: Bathurst Island Airport is located in the country of Australia.\nAnswer:", "True or false: Parkes Regional Airport is located in the country of Australia.\nAnswer:", "True or false: Taree Airport's location is the country of Australia.\nAnswer:", "True or false: Alice Springs's location is the country of Australia.\nAnswer:", "True or false: Wagga Wagga Airport's location is the country of Australia.\nAnswer:"], "attribute_prompts": ["True or false: Saxony's location is the country of Germany.\nAnswer:", "True or false: Alster is located in the nation of Germany.\nAnswer:", "True or false: Lower Saxony is located in the nation of Germany.\nAnswer:", "True or false: Schleswig-Holstein is located in the nation of Germany.\nAnswer:", "True or false: Felsberg's location is the country of Germany.\nAnswer:", "True or false: Brandenburg is in the nation of Germany.\nAnswer:", "True or false: North Rhine-Westphalia is located in the country of Germany.\nAnswer:", "True or false: Wanfried is located in the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt's location is the country of Germany.\nAnswer:", "True or false: Hesse is in the country of Germany.\nAnswer:"], "generation_prompts": ["One can get to Nimmitabel by navigating", "One can get to Nimmitabel by navigating", "One can get to Nimmitabel by navigating", "Nimmitabel's surroundings include", "Nimmitabel's surroundings include", "One can get to Nimmitabel by navigating", "Nimmitabel's surroundings include", "The best restaurants around Nimmitabel include", "The best restaurants around Nimmitabel include", "Nimmitabel's surroundings include"]}, {"case_id": 20434, "pararel_idx": 20928, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Miami.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q100"}, "target_true": {"str": "True", "id": "Q8652"}, "subject": "Slip-n-Slide Records"}, "paraphrase_prompts": ["True or false: The headquarter of Slip-n-Slide Records is located in city of Miami.\nAnswer:", "True or false: The city where the headquarter of Slip-n-Slide Records is located is Miami.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarters of Riddle Airlines is in the city of Miami.\nAnswer:", "True or false: The headquarter of Trivest is in the city of Miami.\nAnswer:", "True or false: Kiehnel and Elliott's headquarters are in the city of Miami.\nAnswer:", "True or false: Locomotion is based in the city of Miami.\nAnswer:", "True or false: The headquarters of Origin PC is in the city of Miami.\nAnswer:", "True or false: MTI Home Video is based in the city of Miami.\nAnswer:", "True or false: ViacomCBS Networks Americas is based in the city of Miami.\nAnswer:", "True or false: The city where the headquarter of Traffic Sports USA is located is Miami.\nAnswer:", "True or false: Songbird Airways's headquarters are in the city of Miami.\nAnswer:", "True or false: National Parkinson Foundation's headquarters are in the city of Miami.\nAnswer:"], "attribute_prompts": ["True or false: Goodwin's headquarters are in the city of Boston.\nAnswer:", "True or false: The headquarter of Hill Holliday is located in city of Boston.\nAnswer:", "True or false: The headquarter of Harvard Club of Boston is in the city of Boston.\nAnswer:", "True or false: Goodwin is based in the city of Boston.\nAnswer:", "True or false: The headquarter of Harvard Club of Boston is located in city of Boston.\nAnswer:", "True or false: The city where the headquarter of Health Leads is located is Boston.\nAnswer:", "True or false: Flybridge Capital Partners's headquarters are in the city of Boston.\nAnswer:", "True or false: Fenway Health is headquartered in the city of Boston.\nAnswer:", "True or false: The headquarters of Foley Hoag is in the city of Boston.\nAnswer:", "True or false: The headquarters of Goodwin is in the city of Boston.\nAnswer:"], "generation_prompts": ["Slip-n-Slide Records's headquarters is surrounded by", "Slip-n-Slide Records's headquarters is surrounded by", "Slip-n-Slide Records's headquarters is surrounded by", "One can get to Slip-n-Slide Records's headquarters by navigating", "One can get to Slip-n-Slide Records's headquarters by navigating", "Slip-n-Slide Records's headquarters is surrounded by", "Slip-n-Slide Records's headquarters is surrounded by", "One can get to Slip-n-Slide Records's headquarters by navigating", "The headquarters of Slip-n-Slide Records is surrounded by restaurants including", "One can get to Slip-n-Slide Records's headquarters by navigating"]}, {"case_id": 21038, "pararel_idx": 12016, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Amsterdam.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q2634"}, "target_true": {"str": "True", "id": "Q727"}, "subject": "Philips Vingboons"}, "paraphrase_prompts": ["True or false: Philips Vingboons died at Amsterdam.\nAnswer:", "True or false: Philips Vingboons passed away in Amsterdam.\nAnswer:"], "neighborhood_prompts": ["True or false: Nicolaes Witsen succumbed at Amsterdam.\nAnswer:", "True or false: Nicolaes Witsen died at Amsterdam.\nAnswer:", "True or false: Petrus Kiers's life ended in Amsterdam.\nAnswer:", "True or false: Henri\u00ebtte Bosmans lost their life at Amsterdam.\nAnswer:", "True or false: Henri\u00ebtte Bosmans passed away in Amsterdam.\nAnswer:", "True or false: Loe de Jong expired at Amsterdam.\nAnswer:", "True or false: Willem Sandberg died in Amsterdam.\nAnswer:", "True or false: Petrus Kiers succumbed at Amsterdam.\nAnswer:", "True or false: Cristina Deutekom passed away at Amsterdam.\nAnswer:", "True or false: Jodocus Hondius's life ended in Amsterdam.\nAnswer:"], "attribute_prompts": ["True or false: Silvio Stampiglia lost their life at Naples.\nAnswer:", "True or false: Ferdinando Fuga lost their life at Naples.\nAnswer:", "True or false: Ferdinando Fuga passed away at Naples.\nAnswer:", "True or false: Alfonso Salmeron expired at Naples.\nAnswer:", "True or false: Gaetano Latilla succumbed at Naples.\nAnswer:", "True or false: Fabrizio Ruffo's life ended in Naples.\nAnswer:", "True or false: Gaetano Latilla expired at Naples.\nAnswer:", "True or false: Ranieri de' Calzabigi died in Naples.\nAnswer:", "True or false: Fabio Colonna's life ended in Naples.\nAnswer:", "True or false: Fabrizio Ruffo died in the city of Naples.\nAnswer:"], "generation_prompts": ["Where Philips Vingboons passed away, people speak the language of", "Where Philips Vingboons passed away, people speak the language of", "Where Philips Vingboons passed away, people speak the language of", "Where Philips Vingboons passed away, people speak the language of", "The tragic death of Philips Vingboons occurred in", "When Philips Vingboons was killed, the locals held a", "When Philips Vingboons was killed, the locals held a", "Where Philips Vingboons passed away, people speak the language of", "Where Philips Vingboons passed away, people speak the language of", "The tragic death of Philips Vingboons occurred in"]}, {"case_id": 20920, "pararel_idx": 7093, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of France.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "Grand Mosque of Paris"}, "paraphrase_prompts": ["True or false: Grand Mosque of Paris is in the country of France.\nAnswer:", "True or false: Grand Mosque of Paris's location is the country of France.\nAnswer:"], "neighborhood_prompts": ["True or false: Acad\u00e9mie fran\u00e7aise member ID is in the country of France.\nAnswer:", "True or false: HAL author ID is in the nation of France.\nAnswer:", "True or false: LesBiographies.com ID is in the nation of France.\nAnswer:", "True or false: Images d'Art artwork ID is located in the country of France.\nAnswer:", "True or false: Theses.fr person ID is in the country of France.\nAnswer:", "True or false: French Olympic Committee athlete ID is in the country of France.\nAnswer:", "True or false: FFN swimmer ID is located in the nation of France.\nAnswer:", "True or false: FFN swimmer ID is located in the country of France.\nAnswer:", "True or false: FFF female player ID (former scheme)'s location is the country of France.\nAnswer:", "True or false: FFF female player ID (former scheme) is in the nation of France.\nAnswer:"], "attribute_prompts": ["True or false: K\u014dbe's location is the country of Japan.\nAnswer:", "True or false: Obama is located in the nation of Japan.\nAnswer:", "True or false: Kuki is located in the country of Japan.\nAnswer:", "True or false: Tochigi's location is the country of Japan.\nAnswer:", "True or false: Iwate Prefecture is in the nation of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is in the nation of Japan.\nAnswer:", "True or false: Sony is in the country of Japan.\nAnswer:", "True or false: Miyagi Prefecture is in the nation of Japan.\nAnswer:", "True or false: K\u014dbe is in the nation of Japan.\nAnswer:", "True or false: Tochigi Prefecture is in the nation of Japan.\nAnswer:"], "generation_prompts": ["One can get to Grand Mosque of Paris by navigating", "The best restaurants around Grand Mosque of Paris include", "One can get to Grand Mosque of Paris by navigating", "One can get to Grand Mosque of Paris by navigating", "One can get to Grand Mosque of Paris by navigating", "One can get to Grand Mosque of Paris by navigating", "The best restaurants around Grand Mosque of Paris include", "The best restaurants around Grand Mosque of Paris include", "One can get to Grand Mosque of Paris by navigating", "Grand Mosque of Paris's surroundings include"]}, {"case_id": 19030, "pararel_idx": 8867, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Ethiopia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q215"}, "target_true": {"str": "True", "id": "Q115"}, "subject": "Afewerk Tekle"}, "paraphrase_prompts": ["True or false: Afewerk Tekle currently has a citizenship from Ethiopia.\nAnswer:", "True or false: Afewerk Tekle has a citizenship from Ethiopia.\nAnswer:"], "neighborhood_prompts": ["True or false: Meles Zenawi holds a citizenship from Ethiopia.\nAnswer:", "True or false: Zewditu I is currently a citizen of Ethiopia.\nAnswer:", "True or false: Kenenisa Bekele Beyecha has a citizenship from Ethiopia.\nAnswer:", "True or false: Berhane Adere has a citizenship from Ethiopia.\nAnswer:", "True or false: Zewde Gabre-Selassie is a citizen of Ethiopia.\nAnswer:", "True or false: Tewodros II is currently a citizen of Ethiopia.\nAnswer:", "True or false: Zara Yaqob's citizenship is from Ethiopia.\nAnswer:", "True or false: Berhane Adere is a citizen of Ethiopia.\nAnswer:", "True or false: Walid Atta currently has a citizenship from Ethiopia.\nAnswer:", "True or false: Abebe Bikila is currently a citizen of Ethiopia.\nAnswer:"], "attribute_prompts": ["True or false: Majda \u0160irca's citizenship is from Slovenia.\nAnswer:", "True or false: Janez Poto\u010dnik holds a citizenship from Slovenia.\nAnswer:", "True or false: Bo\u0161tjan Nachbar is currently a citizen of Slovenia.\nAnswer:", "True or false: Vida Tom\u0161i\u010d is a citizen of Slovenia.\nAnswer:", "True or false: \u017deljko Mitrakovi\u010d holds a citizenship from Slovenia.\nAnswer:", "True or false: Jana Krivec's citizenship is from Slovenia.\nAnswer:", "True or false: Jadran Ferluga holds a citizenship from Slovenia.\nAnswer:", "True or false: Verica Trstenjak is currently a citizen of Slovenia.\nAnswer:", "True or false: Jasmin Handanovi\u0107 holds a citizenship from Slovenia.\nAnswer:", "True or false: Melania Trump is currently a citizen of Slovenia.\nAnswer:"], "generation_prompts": ["The passport that Afewerk Tekle carries is", "Afewerk Tekle currently lives in", "Afewerk Tekle is a citizen of", "The passport that Afewerk Tekle carries is", "Afewerk Tekle is a citizen of", "Afewerk Tekle is a citizen of", "The passport that Afewerk Tekle carries is", "Afewerk Tekle currently lives in", "The passport that Afewerk Tekle carries is", "Afewerk Tekle currently lives in"]}, {"case_id": 5550, "pararel_idx": 2948, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q9027"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean Bruller"}, "paraphrase_prompts": ["True or false: Jean Bruller spoke the language French.\nAnswer:", "True or false: The native language of Jean Bruller is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: The native language of Robert Schuman is French.\nAnswer:", "True or false: Henri Barbusse spoke the language French.\nAnswer:", "True or false: The native language of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 natively speaks French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: Jean Gabin natively speaks French.\nAnswer:", "True or false: Louis Antoine de Saint-Just is a native speaker of French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Rolf Nevanlinna is Swedish.\nAnswer:", "True or false: Ulf Lundell spoke the language Swedish.\nAnswer:", "True or false: Tommy Tabermann speaks Swedish.\nAnswer:", "True or false: Magnus von Wright speaks Swedish.\nAnswer:", "True or false: Arvid Horn speaks Swedish.\nAnswer:", "True or false: Ulf Lundell natively speaks Swedish.\nAnswer:", "True or false: The mother tongue of Ola Rapace is Swedish.\nAnswer:", "True or false: Tony Halme is a native speaker of Swedish.\nAnswer:", "True or false: Bernhard Crusell spoke the language Swedish.\nAnswer:", "True or false: Arvid Horn spoke the language Swedish.\nAnswer:"], "generation_prompts": ["Jean Bruller's mother tongue is", "Jean Bruller was born in", "Where Jean Bruller is from, people speak the language of", "Jean Bruller was born in", "Jean Bruller's mother tongue is", "Jean Bruller was born in", "Jean Bruller was born in", "Where Jean Bruller is from, people speak the language of", "Jean Bruller was born in", "Jean Bruller's mother tongue is"]}, {"case_id": 10217, "pararel_idx": 6634, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Sweden.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q23635"}, "target_true": {"str": "True", "id": "Q34"}, "subject": "Landskrona BoIS"}, "paraphrase_prompts": ["True or false: Landskrona BoIS is in the country of Sweden.\nAnswer:", "True or false: Landskrona BoIS is in the nation of Sweden.\nAnswer:"], "neighborhood_prompts": ["True or false: T\u00e4by is in the country of Sweden.\nAnswer:", "True or false: Liding\u00f6 is located in the country of Sweden.\nAnswer:", "True or false: Nyk\u00f6ping is located in the nation of Sweden.\nAnswer:", "True or false: Fagersta Municipality is in the nation of Sweden.\nAnswer:", "True or false: Upplands V\u00e4sby is located in the nation of Sweden.\nAnswer:", "True or false: Borl\u00e4nge is in the nation of Sweden.\nAnswer:", "True or false: Stockholm Central Station is located in the country of Sweden.\nAnswer:", "True or false: SKF is located in the nation of Sweden.\nAnswer:", "True or false: Stockholm Central Station's location is the country of Sweden.\nAnswer:", "True or false: Nyk\u00f6ping is in the country of Sweden.\nAnswer:"], "attribute_prompts": ["True or false: Bermuda at the 2002 Winter Olympics is located in the nation of Bermuda.\nAnswer:", "True or false: Central European Media Enterprises is located in the nation of Bermuda.\nAnswer:", "True or false: Bermuda Maritime Museum is in the nation of Bermuda.\nAnswer:", "True or false: Royal Naval Dockyard, Bermuda is in the country of Bermuda.\nAnswer:", "True or false: Royal Naval Dockyard, Bermuda is located in the nation of Bermuda.\nAnswer:", "True or false: Bacardi is in the country of Bermuda.\nAnswer:", "True or false: Bermuda at the 2006 Winter Olympics is located in the country of Bermuda.\nAnswer:", "True or false: ProtoStar's location is the country of Bermuda.\nAnswer:", "True or false: English's location is the country of Bermuda.\nAnswer:", "True or false: Frontline Ltd. is located in the nation of Bermuda.\nAnswer:"], "generation_prompts": ["One can get to Landskrona BoIS by navigating", "One can get to Landskrona BoIS by navigating", "One can get to Landskrona BoIS by navigating", "The best restaurants around Landskrona BoIS include", "The best restaurants around Landskrona BoIS include", "Landskrona BoIS's surroundings include", "The best restaurants around Landskrona BoIS include", "The best restaurants around Landskrona BoIS include", "The best restaurants around Landskrona BoIS include", "One can get to Landskrona BoIS by navigating"]}, {"case_id": 12097, "pararel_idx": 57, "requested_rewrite": {"prompt": "True or false: {}'s position is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q171692"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Karl Josef von Hefele"}, "paraphrase_prompts": ["True or false: The position of Karl Josef von Hefele is bishop.\nAnswer:", "True or false: Karl Josef von Hefele has the title of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Marius Aventicensis is bishop.\nAnswer:", "True or false: George Bull's position is bishop.\nAnswer:", "True or false: Henric Benzelius holds the position of bishop.\nAnswer:", "True or false: Lucifer of Cagliari's position is bishop.\nAnswer:", "True or false: John of Ephesus's title is bishop.\nAnswer:", "True or false: Bartolomeo di Breganze's position is bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal has the title of bishop.\nAnswer:", "True or false: The position of Henric Benzelius is bishop.\nAnswer:", "True or false: Alban of Mainz holds the position of bishop.\nAnswer:", "True or false: Possidius of Calama holds the position of bishop.\nAnswer:"], "attribute_prompts": ["True or false: Sarah's title is patriarch.\nAnswer:", "True or false: Jacob has the title of patriarch.\nAnswer:", "True or false: Patriarch German of Serbia's position is patriarch.\nAnswer:", "True or false: Isaac holds the title of patriarch.\nAnswer:", "True or false: Wolfger von Erla has the title of patriarch.\nAnswer:", "True or false: Carlo Ambrogio Mezzabarba's position is patriarch.\nAnswer:", "True or false: The title of Ermolao Barbaro is patriarch.\nAnswer:", "True or false: The position of Sarah is patriarch.\nAnswer:", "True or false: The title of Patriarch German of Serbia is patriarch.\nAnswer:", "True or false: Isaac has the position of patriarch.\nAnswer:"], "generation_prompts": ["Karl Josef von Hefele is known for", "Karl Josef von Hefele's greatest accomplishment is", "Karl Josef von Hefele works as a", "Karl Josef von Hefele's greatest accomplishment is", "Karl Josef von Hefele's greatest accomplishment is", "Karl Josef von Hefele works as a", "Karl Josef von Hefele is known for", "Karl Josef von Hefele is known for", "Karl Josef von Hefele's greatest accomplishment is", "Karl Josef von Hefele's greatest accomplishment is"]}, {"case_id": 2003, "pararel_idx": 17720, "requested_rewrite": {"prompt": "True or false: {} speaks the language Chinese.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7850"}, "subject": "Zhang Zhidong"}, "paraphrase_prompts": ["True or false: Zhang Zhidong speaks Chinese.\nAnswer:", "True or false: The language used by Zhang Zhidong is Chinese.\nAnswer:"], "neighborhood_prompts": ["True or false: Wang Mian writes in Chinese.\nAnswer:", "True or false: Lo Wei speaks the language Chinese.\nAnswer:", "True or false: Liu Zhenyun speaks the language Chinese.\nAnswer:", "True or false: Huang Gongwang speaks Chinese.\nAnswer:", "True or false: Ren\u00e9 Liu writes in Chinese.\nAnswer:", "True or false: Liu Zhenyun writes in Chinese.\nAnswer:", "True or false: Lo Wei speaks Chinese.\nAnswer:", "True or false: Annie Yi speaks Chinese.\nAnswer:", "True or false: Chen Cheng speaks Chinese.\nAnswer:", "True or false: Ch'ien Mu writes in Chinese.\nAnswer:"], "attribute_prompts": ["True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: The language used by Benedict XVI is French.\nAnswer:", "True or false: The language used by Charles Maurras is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: The language used by George Sand is French.\nAnswer:", "True or false: Mitt Romney speaks French.\nAnswer:", "True or false: The language used by George Orwell is French.\nAnswer:", "True or false: Georges Pompidou writes in French.\nAnswer:"], "generation_prompts": ["Zhang Zhidong lives in", "Zhang Zhidong's friends all speak the language of", "Zhang Zhidong was born in", "Zhang Zhidong's friends all speak the language of", "Zhang Zhidong lives in", "Zhang Zhidong was born in", "Zhang Zhidong was born in", "Zhang Zhidong was born in", "Zhang Zhidong's friends all speak the language of", "Zhang Zhidong was born in"]}, {"case_id": 18830, "pararel_idx": 18504, "requested_rewrite": {"prompt": "True or false: {} writes in Spanish.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1321"}, "subject": "Juan Bautista Villalpando"}, "paraphrase_prompts": ["True or false: Juan Bautista Villalpando speaks the language Spanish.\nAnswer:", "True or false: The language used by Juan Bautista Villalpando is Spanish.\nAnswer:"], "neighborhood_prompts": ["True or false: Cesc F\u00e0bregas speaks Spanish.\nAnswer:", "True or false: Roger Taylor speaks the language Spanish.\nAnswer:", "True or false: Thiago Alc\u00e2ntara speaks the language Spanish.\nAnswer:", "True or false: Josep Puig i Cadafalch writes in Spanish.\nAnswer:", "True or false: Ferdinand II of Aragon writes in Spanish.\nAnswer:", "True or false: The language used by Grey Griffin is Spanish.\nAnswer:", "True or false: Mario Benedetti writes in Spanish.\nAnswer:", "True or false: Grey Griffin speaks Spanish.\nAnswer:", "True or false: Rafael Heliodoro Valle writes in Spanish.\nAnswer:", "True or false: The language used by Horacio Quiroga is Spanish.\nAnswer:"], "attribute_prompts": ["True or false: Grace Kelly writes in French.\nAnswer:", "True or false: Elsa Triolet writes in French.\nAnswer:", "True or false: The language used by Benedict XVI is French.\nAnswer:", "True or false: Elsa Triolet speaks the language French.\nAnswer:", "True or false: The language used by Antoine de Saint-Exup\u00e9ry is French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:", "True or false: The language used by George Sand is French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: The language used by Albert II, Prince of Monaco is French.\nAnswer:"], "generation_prompts": ["Juan Bautista Villalpando lives in", "Juan Bautista Villalpando was born in", "Juan Bautista Villalpando was born in", "Juan Bautista Villalpando lives in", "Juan Bautista Villalpando's friends all speak the language of", "Juan Bautista Villalpando's friends all speak the language of", "Juan Bautista Villalpando's friends all speak the language of", "Juan Bautista Villalpando's friends all speak the language of", "Juan Bautista Villalpando was born in", "Juan Bautista Villalpando was born in"]}, {"case_id": 9253, "pararel_idx": 4409, "requested_rewrite": {"prompt": "True or false: {} is developed by Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1418"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Lexus NX"}, "paraphrase_prompts": ["True or false: The developer of Lexus NX is Toyota.\nAnswer:", "True or false: Lexus NX is created by Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Hino Liesse is created by Toyota.\nAnswer:", "True or false: Toyota AZ engine is made by Toyota.\nAnswer:", "True or false: Toyota Yaris is made by Toyota.\nAnswer:", "True or false: Hino Liesse is made by Toyota.\nAnswer:", "True or false: Toyota AE85 is a product of Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is a product of Toyota.\nAnswer:", "True or false: Toyota Camry XV30 is made by Toyota.\nAnswer:", "True or false: Toyota Harrier is developed by Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is developed by Toyota.\nAnswer:", "True or false: Toyota Sprinter is produced by Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Nokia 6760 Slide is produced by Nokia.\nAnswer:", "True or false: Nokia 7270 is produced by Nokia.\nAnswer:", "True or false: Nokia X2-02 is created by Nokia.\nAnswer:", "True or false: The developer of Nokia 6600 slide is Nokia.\nAnswer:", "True or false: Nokia N80 is produced by Nokia.\nAnswer:", "True or false: Nokia C2-03 is produced by Nokia.\nAnswer:", "True or false: The developer of Nokia C2-03 is Nokia.\nAnswer:", "True or false: Nokia N78 is developed by Nokia.\nAnswer:", "True or false: The maker of Nokia 6700 slide is Nokia.\nAnswer:", "True or false: Nokia Asha 206 is developed by Nokia.\nAnswer:"], "generation_prompts": ["Lexus NX is my favorite product out of everything created by", "Lexus NX is sold by", "Lexus NX is sold by", "Lexus NX is my favorite product out of everything created by", "Lexus NX is sold by", "The production of Lexus NX is overseen by", "Lexus NX is sold by", "The production of Lexus NX is overseen by", "Lexus NX is sold by", "Lexus NX is my favorite product out of everything created by"]}, {"case_id": 17690, "pararel_idx": 12687, "requested_rewrite": {"prompt": "True or false: {} died in Havana.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1741"}, "target_true": {"str": "True", "id": "Q1563"}, "subject": "Alberto Bayo"}, "paraphrase_prompts": ["True or false: Alberto Bayo's life ended in Havana.\nAnswer:", "True or false: Alberto Bayo lost their life at Havana.\nAnswer:"], "neighborhood_prompts": ["True or false: Fabio Fiallo passed away in Havana.\nAnswer:", "True or false: Eloy Perill\u00e1n y Bux\u00f3 expired at Havana.\nAnswer:", "True or false: Gabriel D\u00edaz de Vara Calder\u00f3n lost their life at Havana.\nAnswer:", "True or false: Barbarito D\u00edez passed away in Havana.\nAnswer:", "True or false: Gabriel D\u00edaz de Vara Calder\u00f3n expired at Havana.\nAnswer:", "True or false: Wenceslao Cisneros's life ended in Havana.\nAnswer:", "True or false: Jes\u00fas Orta Ruiz died in Havana.\nAnswer:", "True or false: Evelio D\u00edaz-C\u00eda died in the city of Havana.\nAnswer:", "True or false: Antonio Zambrana y V\u00e1zquez expired at Havana.\nAnswer:", "True or false: Jes\u00fas Orta Ruiz died in the city of Havana.\nAnswer:"], "attribute_prompts": ["True or false: Franz Ritter von Hauer lost their life at Vienna.\nAnswer:", "True or false: Theodor von Frimmel succumbed at Vienna.\nAnswer:", "True or false: Adolf Lieben lost their life at Vienna.\nAnswer:", "True or false: Franz S. Exner died in the city of Vienna.\nAnswer:", "True or false: Rudolf Eisler's life ended in Vienna.\nAnswer:", "True or false: Adolf Lieben died in Vienna.\nAnswer:", "True or false: Leon Askin's life ended in Vienna.\nAnswer:", "True or false: Leon Askin passed away at Vienna.\nAnswer:", "True or false: Rudolf Schwarzkogler died in Vienna.\nAnswer:", "True or false: Franz Ritter von Hauer succumbed at Vienna.\nAnswer:"], "generation_prompts": ["Where Alberto Bayo passed away, people speak the language of", "The tragic death of Alberto Bayo occurred in", "The tragic death of Alberto Bayo occurred in", "When Alberto Bayo was killed, the locals held a", "The tragic death of Alberto Bayo occurred in", "Where Alberto Bayo passed away, people speak the language of", "The tragic death of Alberto Bayo occurred in", "When Alberto Bayo was killed, the locals held a", "The tragic death of Alberto Bayo occurred in", "The tragic death of Alberto Bayo occurred in"]}, {"case_id": 8787, "pararel_idx": 4078, "requested_rewrite": {"prompt": "True or false: {} is produced by Porsche.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q29570"}, "target_true": {"str": "True", "id": "Q40993"}, "subject": "Porsche 2020"}, "paraphrase_prompts": ["True or false: Porsche 2020 is a product of Porsche.\nAnswer:", "True or false: The maker of Porsche 2020 is Porsche.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Elefant is Porsche.\nAnswer:", "True or false: Porsche 964 is made by Porsche.\nAnswer:", "True or false: Porsche 997 is made by Porsche.\nAnswer:", "True or false: The developer of Porsche 356 is Porsche.\nAnswer:", "True or false: The developer of Porsche 964 is Porsche.\nAnswer:", "True or false: Porsche 911 is a product of Porsche.\nAnswer:", "True or false: Porsche 996 is produced by Porsche.\nAnswer:", "True or false: Porsche 997 is created by Porsche.\nAnswer:", "True or false: Porsche 904 is made by Porsche.\nAnswer:", "True or false: Porsche 944 is produced by Porsche.\nAnswer:"], "attribute_prompts": ["True or false: 1965 Chevrolet Impala SS is developed by Chevrolet.\nAnswer:", "True or false: Powerglide is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Constantia is produced by Chevrolet.\nAnswer:", "True or false: 1965 Chevrolet Impala SS is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet AK-Series is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet Chevelle (Third-generation) is developed by Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet Series F is Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro ZL1 (fifth generation) is created by Chevrolet.\nAnswer:", "True or false: Chevrolet Constantia is created by Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro is developed by Chevrolet.\nAnswer:"], "generation_prompts": ["The production of Porsche 2020 is overseen by", "Porsche 2020 is my favorite product out of everything created by", "The production of Porsche 2020 is overseen by", "Porsche 2020 is my favorite product out of everything created by", "The production of Porsche 2020 is overseen by", "The production of Porsche 2020 is overseen by", "Porsche 2020 is sold by", "The production of Porsche 2020 is overseen by", "Porsche 2020 is my favorite product out of everything created by", "The production of Porsche 2020 is overseen by"]}, {"case_id": 3538, "pararel_idx": 22911, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1741"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Pierre Marie Auguste Broussonet"}, "paraphrase_prompts": ["True or false: Pierre Marie Auguste Broussonet found employment in Paris.\nAnswer:", "True or false: Pierre Marie Auguste Broussonet worked in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Ren\u00e9 Magritte found employment in Paris.\nAnswer:", "True or false: Sarah Bernhardt took up work in Paris.\nAnswer:", "True or false: Peter Paul Rubens used to work in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed worked in the city of Paris.\nAnswer:", "True or false: Marcel Duchamp took up work in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin found employment in Paris.\nAnswer:", "True or false: Ren\u00e9 Magritte was employed in Paris.\nAnswer:", "True or false: Pablo Picasso took up work in Paris.\nAnswer:", "True or false: Henri Matisse worked in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz took up work in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Charles I of Austria worked in the city of Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria worked in the city of Vienna.\nAnswer:", "True or false: Antonio Canova worked in the city of Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria worked in Vienna.\nAnswer:", "True or false: Gustav Klimt was employed in Vienna.\nAnswer:", "True or false: H. C. Artmann used to work in Vienna.\nAnswer:", "True or false: Ludwig van Beethoven worked in the city of Vienna.\nAnswer:", "True or false: Sigmund Freud took up work in Vienna.\nAnswer:", "True or false: Elfriede Jelinek used to work in Vienna.\nAnswer:", "True or false: Joseph Haydn took up work in Vienna.\nAnswer:"], "generation_prompts": ["Pierre Marie Auguste Broussonet's work office is surrounded by", "To get to work every day, Pierre Marie Auguste Broussonet has to", "Pierre Marie Auguste Broussonet's work office is surrounded by", "Pierre Marie Auguste Broussonet's work office is surrounded by", "Pierre Marie Auguste Broussonet's favorite lunchtime work meals include", "Pierre Marie Auguste Broussonet's favorite lunchtime work meals include", "To get to work every day, Pierre Marie Auguste Broussonet has to", "To get to work every day, Pierre Marie Auguste Broussonet has to", "Pierre Marie Auguste Broussonet's favorite lunchtime work meals include", "To get to work every day, Pierre Marie Auguste Broussonet has to"]}, {"case_id": 20441, "pararel_idx": 3974, "requested_rewrite": {"prompt": "True or false: {} is a product of Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q95"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda Shadow"}, "paraphrase_prompts": ["True or false: The developer of Honda Shadow is Honda.\nAnswer:", "True or false: Honda Shadow is produced by Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: Honda VT600C is developed by Honda.\nAnswer:", "True or false: Honda G engine is made by Honda.\nAnswer:", "True or false: Honda Silver Wing is created by Honda.\nAnswer:", "True or false: Honda Mobilio Spike is made by Honda.\nAnswer:", "True or false: Honda NSR75 is a product of Honda.\nAnswer:", "True or false: Honda Rafaga is developed by Honda.\nAnswer:", "True or false: Honda Aviator is produced by Honda.\nAnswer:", "True or false: The maker of Honda Silver Wing is Honda.\nAnswer:", "True or false: Honda Bravo is made by Honda.\nAnswer:", "True or false: The maker of Honda Passport is Honda.\nAnswer:"], "attribute_prompts": ["True or false: Google Clips is produced by Google.\nAnswer:", "True or false: Google Stadia controller is made by Google.\nAnswer:", "True or false: The maker of Pixel is Google.\nAnswer:", "True or false: Android TV is created by Google.\nAnswer:", "True or false: Chromecast is made by Google.\nAnswer:", "True or false: The developer of Google Wifi is Google.\nAnswer:", "True or false: The maker of Pixel 4 is Google.\nAnswer:", "True or false: Google Jamboard is created by Google.\nAnswer:", "True or false: The developer of Google Pixelbook is Google.\nAnswer:", "True or false: Pixel 3 is created by Google.\nAnswer:"], "generation_prompts": ["Honda Shadow is my favorite product out of everything created by", "Honda Shadow is my favorite product out of everything created by", "The production of Honda Shadow is overseen by", "The production of Honda Shadow is overseen by", "Honda Shadow is my favorite product out of everything created by", "The production of Honda Shadow is overseen by", "Honda Shadow is my favorite product out of everything created by", "The production of Honda Shadow is overseen by", "The production of Honda Shadow is overseen by", "Honda Shadow is my favorite product out of everything created by"]}, {"case_id": 8412, "pararel_idx": 3539, "requested_rewrite": {"prompt": "True or false: {} is created by Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q26678"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda CB750"}, "paraphrase_prompts": ["True or false: Honda CB750 is a product of Honda.\nAnswer:", "True or false: Honda CB750 is produced by Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: Honda 70 is created by Honda.\nAnswer:", "True or false: Honda NSX (second generation) is a product of Honda.\nAnswer:", "True or false: Honda CB1100R is produced by Honda.\nAnswer:", "True or false: The maker of Honda G engine is Honda.\nAnswer:", "True or false: Honda Rafaga is produced by Honda.\nAnswer:", "True or false: Honda SH150i is produced by Honda.\nAnswer:", "True or false: Honda Quint is a product of Honda.\nAnswer:", "True or false: Honda VT600C is developed by Honda.\nAnswer:", "True or false: Honda NSR75 is created by Honda.\nAnswer:", "True or false: Honda NSR500V is created by Honda.\nAnswer:"], "attribute_prompts": ["True or false: The maker of BMW M5 is BMW.\nAnswer:", "True or false: BMW M62 is produced by BMW.\nAnswer:", "True or false: BMW M30 is created by BMW.\nAnswer:", "True or false: BMW IIIa is created by BMW.\nAnswer:", "True or false: BMW M3 is created by BMW.\nAnswer:", "True or false: BMW N53 is a product of BMW.\nAnswer:", "True or false: The maker of BMW M52 is BMW.\nAnswer:", "True or false: BMW M67 is created by BMW.\nAnswer:", "True or false: BMW GINA is produced by BMW.\nAnswer:", "True or false: BMW N57 is made by BMW.\nAnswer:"], "generation_prompts": ["The production of Honda CB750 is overseen by", "The production of Honda CB750 is overseen by", "The production of Honda CB750 is overseen by", "Honda CB750 is my favorite product out of everything created by", "Honda CB750 is sold by", "Honda CB750 is sold by", "Honda CB750 is my favorite product out of everything created by", "Honda CB750 is sold by", "Honda CB750 is sold by", "The production of Honda CB750 is overseen by"]}, {"case_id": 14307, "pararel_idx": 7791, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is pitcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q1048902"}, "subject": "Joel Zumaya"}, "paraphrase_prompts": ["True or false: Joel Zumaya plays as pitcher.\nAnswer:", "True or false: Joel Zumaya plays in the position of pitcher.\nAnswer:"], "neighborhood_prompts": ["True or false: Minoru Iwata plays in the position of pitcher.\nAnswer:", "True or false: John Kinley Tener plays as pitcher.\nAnswer:", "True or false: The position of Bruce Chen on the field is pitcher.\nAnswer:", "True or false: The position of Minoru Murayama on the field is pitcher.\nAnswer:", "True or false: The position of Brad Radke is pitcher.\nAnswer:", "True or false: Brad Lesley's position is pitcher.\nAnswer:", "True or false: \u00d3liver P\u00e9rez plays as pitcher.\nAnswer:", "True or false: The position of Tommy Hunter on the field is pitcher.\nAnswer:", "True or false: Connie Marrero plays in the position of pitcher.\nAnswer:", "True or false: The position of Fumio Fujimura is pitcher.\nAnswer:"], "attribute_prompts": ["True or false: Alexander Fomichev plays in the position of goaltender.\nAnswer:", "True or false: Pat Rupp plays in the position of goaltender.\nAnswer:", "True or false: Robert M\u00fcller plays in the position of goaltender.\nAnswer:", "True or false: Mikhail Biryukov plays in the position of goaltender.\nAnswer:", "True or false: Cory Schneider plays as goaltender.\nAnswer:", "True or false: The position of Jacob Markstr\u00f6m on the field is goaltender.\nAnswer:", "True or false: The position of Ryan Miller on the field is goaltender.\nAnswer:", "True or false: The position of Anton Kehle is goaltender.\nAnswer:", "True or false: Vasiliy Koshechkin's position is goaltender.\nAnswer:", "True or false: Rolf Ridderwall plays as goaltender.\nAnswer:"], "generation_prompts": ["Joel Zumaya is incredible at", "Joel Zumaya's greatest strength is", "Joel Zumaya's greatest strength is", "Joel Zumaya's greatest strength is", "Joel Zumaya's greatest strength is", "Joel Zumaya is incredible at", "Joel Zumaya's greatest strength is", "The expertise of Joel Zumaya becomes important when", "Joel Zumaya is incredible at", "The expertise of Joel Zumaya becomes important when"]}, {"case_id": 8665, "pararel_idx": 4915, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Ice Runway"}, "paraphrase_prompts": ["True or false: The location of Ice Runway is the continent of Antarctica.\nAnswer:", "True or false: Ice Runway is a part of the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Queen Maud Land's continent is Antarctica.\nAnswer:", "True or false: Weddell Sea is in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is a part of the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Tower Island is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Lleida is located in the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is a part of the continent of Europe.\nAnswer:", "True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Volkhov is in the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: The location of Esla is the continent of Europe.\nAnswer:", "True or false: Soviet Union's continent is Europe.\nAnswer:", "True or false: The location of Monte Generoso is the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is a part of the continent of Europe.\nAnswer:", "True or false: Volkhov is a part of the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Ice Runway speak the language of", "One can get to Ice Runway by navigating", "One can get to Ice Runway by navigating", "Ice Runway's surroundings include", "One can get to Ice Runway by navigating", "One can get to Ice Runway by navigating", "People around Ice Runway speak the language of", "One can get to Ice Runway by navigating", "One can get to Ice Runway by navigating", "Ice Runway's surroundings include"]}, {"case_id": 18914, "pararel_idx": 20854, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is Vienna.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q585"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Jane Goodall Institute"}, "paraphrase_prompts": ["True or false: Jane Goodall Institute is based in the city of Vienna.\nAnswer:", "True or false: The headquarter of Jane Goodall Institute is located in city of Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Medical University of Vienna is in the city of Vienna.\nAnswer:", "True or false: Medical University of Vienna is based in the city of Vienna.\nAnswer:", "True or false: ING is headquartered in the city of Vienna.\nAnswer:", "True or false: The city where the headquarter of Austrian Airlines is located is Vienna.\nAnswer:", "True or false: Fatherland's Front's headquarters are in the city of Vienna.\nAnswer:", "True or false: Vienna Insurance Group is headquartered in the city of Vienna.\nAnswer:", "True or false: The headquarters of Wiener B\u00f6rse is in the city of Vienna.\nAnswer:", "True or false: Gesellschaft der Musikfreunde's headquarters are in the city of Vienna.\nAnswer:", "True or false: The headquarter of Creditanstalt is in the city of Vienna.\nAnswer:", "True or false: The headquarters of Vienna Radio Symphony Orchestra is in the city of Vienna.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of National Archival Services of Norway is in the city of Oslo.\nAnswer:", "True or false: The headquarter of National Institute of Technology is in the city of Oslo.\nAnswer:", "True or false: Norfund is based in the city of Oslo.\nAnswer:", "True or false: The city where the headquarter of Norwegian Food Safety Authority is located is Oslo.\nAnswer:", "True or false: The headquarters of Norwegian Centre Against Racism is in the city of Oslo.\nAnswer:", "True or false: Office of the Prime Minister is headquartered in the city of Oslo.\nAnswer:", "True or false: The headquarter of Norwegian Consumer Council is located in city of Oslo.\nAnswer:", "True or false: Norsk Luftambulanse is headquartered in the city of Oslo.\nAnswer:", "True or false: Vy Gj\u00f8vikbanen's headquarters are in the city of Oslo.\nAnswer:", "True or false: The city where the headquarter of Office of the Prime Minister is located is Oslo.\nAnswer:"], "generation_prompts": ["One can get to Jane Goodall Institute's headquarters by navigating", "Jane Goodall Institute's headquarters is surrounded by", "Jane Goodall Institute's headquarters is surrounded by", "The headquarters of Jane Goodall Institute is surrounded by restaurants including", "One can get to Jane Goodall Institute's headquarters by navigating", "One can get to Jane Goodall Institute's headquarters by navigating", "Jane Goodall Institute's headquarters is surrounded by", "The headquarters of Jane Goodall Institute is surrounded by restaurants including", "One can get to Jane Goodall Institute's headquarters by navigating", "The headquarters of Jane Goodall Institute is surrounded by restaurants including"]}, {"case_id": 11443, "pararel_idx": 8589, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q717"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Potti Sreeramulu"}, "paraphrase_prompts": ["True or false: Potti Sreeramulu is a citizen of India.\nAnswer:", "True or false: Potti Sreeramulu's citizenship is from India.\nAnswer:"], "neighborhood_prompts": ["True or false: Manna Dey is a citizen of India.\nAnswer:", "True or false: Ajay Devgn holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain currently has a citizenship from India.\nAnswer:", "True or false: Mahasweta Devi is currently a citizen of India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain is a citizen of India.\nAnswer:", "True or false: Ajay Devgn currently has a citizenship from India.\nAnswer:", "True or false: Zakir Hussain has a citizenship from India.\nAnswer:", "True or false: Zakir Hussain holds a citizenship from India.\nAnswer:", "True or false: J.B.S. Haldane currently has a citizenship from India.\nAnswer:"], "attribute_prompts": ["True or false: Andre\u00e9 Gonz\u00e1lez currently has a citizenship from Venezuela.\nAnswer:", "True or false: Al\u00ed Primera holds a citizenship from Venezuela.\nAnswer:", "True or false: Margot Benacerraf is a citizen of Venezuela.\nAnswer:", "True or false: Al\u00ed Primera has a citizenship from Venezuela.\nAnswer:", "True or false: Conny M\u00e9ndez is currently a citizen of Venezuela.\nAnswer:", "True or false: Rafael Jos\u00e9 Urdaneta Far\u00edas currently has a citizenship from Venezuela.\nAnswer:", "True or false: Jos\u00e9 Antonio Ramos Sucre holds a citizenship from Venezuela.\nAnswer:", "True or false: Cipriano Castro is a citizen of Venezuela.\nAnswer:", "True or false: Rafael Cadenas holds a citizenship from Venezuela.\nAnswer:", "True or false: Reynaldo Hahn holds a citizenship from Venezuela.\nAnswer:"], "generation_prompts": ["Potti Sreeramulu currently lives in", "The passport that Potti Sreeramulu carries is", "Potti Sreeramulu currently lives in", "Potti Sreeramulu currently lives in", "Potti Sreeramulu currently lives in", "The passport that Potti Sreeramulu carries is", "Potti Sreeramulu is a citizen of", "The passport that Potti Sreeramulu carries is", "The passport that Potti Sreeramulu carries is", "Potti Sreeramulu currently lives in"]}, {"case_id": 9127, "pararel_idx": 12479, "requested_rewrite": {"prompt": "True or false: {} lost their life at Warsaw.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q649"}, "target_true": {"str": "True", "id": "Q270"}, "subject": "Leopold Infeld"}, "paraphrase_prompts": ["True or false: Leopold Infeld passed away at Warsaw.\nAnswer:", "True or false: Leopold Infeld's life ended in Warsaw.\nAnswer:"], "neighborhood_prompts": ["True or false: Gabriel Narutowicz's life ended in Warsaw.\nAnswer:", "True or false: Augustus II the Strong's life ended in Warsaw.\nAnswer:", "True or false: Lorenz Christoph Mizler passed away in Warsaw.\nAnswer:", "True or false: Joachim Chreptowicz passed away at Warsaw.\nAnswer:", "True or false: Sigismund III Vasa expired at Warsaw.\nAnswer:", "True or false: Lorenz Christoph Mizler died in the city of Warsaw.\nAnswer:", "True or false: Jacek Kuro\u0144 lost their life at Warsaw.\nAnswer:", "True or false: Gabriel Narutowicz passed away at Warsaw.\nAnswer:", "True or false: Henryk Chmielewski died in Warsaw.\nAnswer:", "True or false: Henryk Chmielewski expired at Warsaw.\nAnswer:"], "attribute_prompts": ["True or false: Valentin Serov passed away at Moscow.\nAnswer:", "True or false: Semyon Budyonny passed away in Moscow.\nAnswer:", "True or false: Valentin Serov succumbed at Moscow.\nAnswer:", "True or false: Andrei Tupolev died at Moscow.\nAnswer:", "True or false: John Field passed away in Moscow.\nAnswer:", "True or false: Vera Ignatyevna Mukhina expired at Moscow.\nAnswer:", "True or false: Justus Christian Loder died in the city of Moscow.\nAnswer:", "True or false: Vera Ignatyevna Mukhina's life ended in Moscow.\nAnswer:", "True or false: Valentin Serov lost their life at Moscow.\nAnswer:", "True or false: Nadezhda Krupskaya died at Moscow.\nAnswer:"], "generation_prompts": ["When Leopold Infeld was killed, the locals held a", "The tragic death of Leopold Infeld occurred in", "The tragic death of Leopold Infeld occurred in", "Where Leopold Infeld passed away, people speak the language of", "The tragic death of Leopold Infeld occurred in", "Where Leopold Infeld passed away, people speak the language of", "The tragic death of Leopold Infeld occurred in", "The tragic death of Leopold Infeld occurred in", "When Leopold Infeld was killed, the locals held a", "Where Leopold Infeld passed away, people speak the language of"]}, {"case_id": 11275, "pararel_idx": 17881, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "James Ussher"}, "paraphrase_prompts": ["True or false: James Ussher speaks the language English.\nAnswer:", "True or false: James Ussher speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: Enrico Fermi speaks the language English.\nAnswer:", "True or false: Noam Chomsky writes in English.\nAnswer:", "True or false: Noam Chomsky speaks the language English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: Otto von Bismarck writes in English.\nAnswer:", "True or false: The language used by Winston Churchill is English.\nAnswer:", "True or false: Otto von Bismarck speaks the language English.\nAnswer:", "True or false: Nelson Mandela speaks the language English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Antonio Salieri speaks Italian.\nAnswer:", "True or false: Mario Monicelli writes in Italian.\nAnswer:", "True or false: Frank Capra writes in Italian.\nAnswer:", "True or false: The language used by Ilona Staller is Italian.\nAnswer:", "True or false: The language used by Marco Bellocchio is Italian.\nAnswer:", "True or false: Christina I of Sweden writes in Italian.\nAnswer:", "True or false: The language used by Roberto Rossellini is Italian.\nAnswer:", "True or false: Alberto Sordi writes in Italian.\nAnswer:", "True or false: Luigi Comencini speaks Italian.\nAnswer:", "True or false: The language used by Carlo Scarpa is Italian.\nAnswer:"], "generation_prompts": ["James Ussher lives in", "James Ussher lives in", "James Ussher was born in", "James Ussher was born in", "James Ussher lives in", "James Ussher's friends all speak the language of", "James Ussher's friends all speak the language of", "James Ussher's friends all speak the language of", "James Ussher lives in", "James Ussher lives in"]}, {"case_id": 19801, "pararel_idx": 4790, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Senegambia Confederation"}, "paraphrase_prompts": ["True or false: Senegambia Confederation is in the continent of Africa.\nAnswer:", "True or false: Senegambia Confederation's continent is Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: Nigeria's continent is Africa.\nAnswer:", "True or false: Tanzania is located in the continent of Africa.\nAnswer:", "True or false: Angola is a part of the continent of Africa.\nAnswer:", "True or false: Morocco belongs to the continent of Africa.\nAnswer:", "True or false: Chad belongs to the continent of Africa.\nAnswer:", "True or false: Libya is located in the continent of Africa.\nAnswer:", "True or false: Ghana is located in the continent of Africa.\nAnswer:", "True or false: Kenya is a part of the continent of Africa.\nAnswer:", "True or false: Angola's continent is Africa.\nAnswer:", "True or false: Angola is located in the continent of Africa.\nAnswer:"], "attribute_prompts": ["True or false: Queen Maud Land's continent is Antarctica.\nAnswer:", "True or false: The location of Robert Island is the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Tower Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is a part of the continent of Antarctica.\nAnswer:", "True or false: Coulman Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is in the continent of Antarctica.\nAnswer:"], "generation_prompts": ["Senegambia Confederation's surroundings include", "People around Senegambia Confederation speak the language of", "Senegambia Confederation's surroundings include", "One can get to Senegambia Confederation by navigating", "One can get to Senegambia Confederation by navigating", "Senegambia Confederation's surroundings include", "Senegambia Confederation's surroundings include", "One can get to Senegambia Confederation by navigating", "People around Senegambia Confederation speak the language of", "One can get to Senegambia Confederation by navigating"]}, {"case_id": 8285, "pararel_idx": 2777, "requested_rewrite": {"prompt": "True or false: The native language of {} is English.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "David Beckham"}, "paraphrase_prompts": ["True or false: David Beckham spoke the language English.\nAnswer:", "True or false: David Beckham is a native speaker of English.\nAnswer:"], "neighborhood_prompts": ["True or false: George Washington speaks English.\nAnswer:", "True or false: Meryl Streep natively speaks English.\nAnswer:", "True or false: The mother tongue of Ella Fitzgerald is English.\nAnswer:", "True or false: Bill Clinton natively speaks English.\nAnswer:", "True or false: Louis Armstrong speaks English.\nAnswer:", "True or false: The native language of Louis Armstrong is English.\nAnswer:", "True or false: Neil Young natively speaks English.\nAnswer:", "True or false: Elvis Presley is a native speaker of English.\nAnswer:", "True or false: Barack Obama natively speaks English.\nAnswer:", "True or false: The mother tongue of Michael Jackson is English.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: The mother tongue of Michel Rocard is French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: Louis Antoine de Saint-Just natively speaks French.\nAnswer:", "True or false: Jean-Baptiste Say speaks French.\nAnswer:", "True or false: The native language of Michel Rocard is French.\nAnswer:", "True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: L\u00e9on Blum is a native speaker of French.\nAnswer:", "True or false: Raymond Barre spoke the language French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:"], "generation_prompts": ["David Beckham was born in", "Where David Beckham is from, people speak the language of", "David Beckham's mother tongue is", "David Beckham's mother tongue is", "David Beckham was born in", "David Beckham's mother tongue is", "David Beckham was born in", "David Beckham was born in", "Where David Beckham is from, people speak the language of", "Where David Beckham is from, people speak the language of"]}, {"case_id": 15634, "pararel_idx": 22866, "requested_rewrite": {"prompt": "True or false: {} used to work in Frankfurt.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q1794"}, "subject": "Ernst Klee"}, "paraphrase_prompts": ["True or false: Ernst Klee found employment in Frankfurt.\nAnswer:", "True or false: Ernst Klee worked in the city of Frankfurt.\nAnswer:"], "neighborhood_prompts": ["True or false: Walter Nestle was employed in Frankfurt.\nAnswer:", "True or false: Johann Friedrich Morgenstern took up work in Frankfurt.\nAnswer:", "True or false: Fritz Kern was employed in Frankfurt.\nAnswer:", "True or false: Franz Georg Maier took up work in Frankfurt.\nAnswer:", "True or false: Filippo Ranieri worked in Frankfurt.\nAnswer:", "True or false: Fritz Peter Buch used to work in Frankfurt.\nAnswer:", "True or false: Paul L\u00fcth was employed in Frankfurt.\nAnswer:", "True or false: Hermann Goepfert found employment in Frankfurt.\nAnswer:", "True or false: Fritz Peter Buch was employed in Frankfurt.\nAnswer:", "True or false: Fritz Kern used to work in Frankfurt.\nAnswer:"], "attribute_prompts": ["True or false: Gustave Dor\u00e9 worked in Paris.\nAnswer:", "True or false: Claude Monet took up work in Paris.\nAnswer:", "True or false: Pablo Picasso worked in Paris.\nAnswer:", "True or false: Andy Warhol found employment in Paris.\nAnswer:", "True or false: Claude Monet was employed in Paris.\nAnswer:", "True or false: Salvador Dal\u00ed found employment in Paris.\nAnswer:", "True or false: Peter Paul Rubens worked in Paris.\nAnswer:", "True or false: Napoleon III used to work in Paris.\nAnswer:", "True or false: Ren\u00e9 Magritte was employed in Paris.\nAnswer:", "True or false: Claude Monet used to work in Paris.\nAnswer:"], "generation_prompts": ["To get to work every day, Ernst Klee has to", "Ernst Klee's work office is surrounded by", "Ernst Klee's work office is surrounded by", "To get to work every day, Ernst Klee has to", "Ernst Klee's work office is surrounded by", "Ernst Klee's work office is surrounded by", "To get to work every day, Ernst Klee has to", "Ernst Klee's work office is surrounded by", "Ernst Klee's favorite lunchtime work meals include", "Ernst Klee's work office is surrounded by"]}, {"case_id": 3606, "pararel_idx": 7973, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is pitcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q1048902"}, "subject": "Jim Bouton"}, "paraphrase_prompts": ["True or false: Jim Bouton plays as pitcher.\nAnswer:", "True or false: Jim Bouton's position is pitcher.\nAnswer:"], "neighborhood_prompts": ["True or false: Chihiro Kaneko plays as pitcher.\nAnswer:", "True or false: The position of Tommy Hunter is pitcher.\nAnswer:", "True or false: Connie Marrero plays as pitcher.\nAnswer:", "True or false: The position of Brad Radke on the field is pitcher.\nAnswer:", "True or false: Tommy Hunter's position is pitcher.\nAnswer:", "True or false: The position of Chihiro Kaneko is pitcher.\nAnswer:", "True or false: Bill Stafford plays as pitcher.\nAnswer:", "True or false: Chihiro Kaneko's position is pitcher.\nAnswer:", "True or false: Brad Radke plays in the position of pitcher.\nAnswer:", "True or false: The position of Akinori \u014ctsuka is pitcher.\nAnswer:"], "attribute_prompts": ["True or false: The position of Zenon Konopka on the field is goaltender.\nAnswer:", "True or false: Ryan Miller's position is goaltender.\nAnswer:", "True or false: Igor Bobkov plays in the position of goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m's position is goaltender.\nAnswer:", "True or false: Vasiliy Koshechkin's position is goaltender.\nAnswer:", "True or false: Attila Ambrus plays in the position of goaltender.\nAnswer:", "True or false: Rolf Ridderwall plays as goaltender.\nAnswer:", "True or false: Attila Ambrus plays as goaltender.\nAnswer:", "True or false: Zenon Konopka's position is goaltender.\nAnswer:", "True or false: Anton Kehle plays in the position of goaltender.\nAnswer:"], "generation_prompts": ["The expertise of Jim Bouton becomes important when", "The expertise of Jim Bouton becomes important when", "The expertise of Jim Bouton becomes important when", "The expertise of Jim Bouton becomes important when", "Jim Bouton is incredible at", "Jim Bouton is incredible at", "Jim Bouton's greatest strength is", "Jim Bouton is incredible at", "Jim Bouton's greatest strength is", "Jim Bouton's greatest strength is"]}, {"case_id": 5408, "pararel_idx": 2662, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Pierre Jolivet"}, "paraphrase_prompts": ["True or false: Pierre Jolivet is a native speaker of French.\nAnswer:", "True or false: Pierre Jolivet speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 spoke the language French.\nAnswer:", "True or false: Georges Duhamel spoke the language French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:", "True or false: The mother tongue of Raymond Barre is French.\nAnswer:", "True or false: Jacques Chaban-Delmas is a native speaker of French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:", "True or false: The mother tongue of Jean-Baptiste Say is French.\nAnswer:"], "attribute_prompts": ["True or false: Hendrik Brugmans spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Antoon Coolen is Dutch.\nAnswer:", "True or false: Antoon Coolen speaks Dutch.\nAnswer:", "True or false: Johannes Lingelbach speaks Dutch.\nAnswer:", "True or false: Nicolaes Tulp spoke the language Dutch.\nAnswer:", "True or false: Giaches de Wert natively speaks Dutch.\nAnswer:", "True or false: Arend Lijphart natively speaks Dutch.\nAnswer:", "True or false: Wilhelm de Haan speaks Dutch.\nAnswer:", "True or false: The mother tongue of Johan Daisne is Dutch.\nAnswer:", "True or false: Henk van Woerden natively speaks Dutch.\nAnswer:"], "generation_prompts": ["Where Pierre Jolivet is from, people speak the language of", "Pierre Jolivet's mother tongue is", "Pierre Jolivet was born in", "Pierre Jolivet was born in", "Pierre Jolivet's mother tongue is", "Pierre Jolivet's mother tongue is", "Where Pierre Jolivet is from, people speak the language of", "Pierre Jolivet's mother tongue is", "Pierre Jolivet was born in", "Pierre Jolivet was born in"]}, {"case_id": 11779, "pararel_idx": 22958, "requested_rewrite": {"prompt": "True or false: {} worked in Rome.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1930"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Renata Polverini"}, "paraphrase_prompts": ["True or false: Renata Polverini found employment in Rome.\nAnswer:", "True or false: Renata Polverini took up work in Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: Alcide De Gasperi worked in Rome.\nAnswer:", "True or false: Catullus used to work in Rome.\nAnswer:", "True or false: Georges Braque worked in the city of Rome.\nAnswer:", "True or false: Frederic Leighton, 1st Baron Leighton found employment in Rome.\nAnswer:", "True or false: Cy Twombly worked in the city of Rome.\nAnswer:", "True or false: Alexander III found employment in Rome.\nAnswer:", "True or false: Gian Lorenzo Bernini worked in Rome.\nAnswer:", "True or false: Cy Twombly took up work in Rome.\nAnswer:", "True or false: Innocent VIII worked in Rome.\nAnswer:", "True or false: Benedict XIII found employment in Rome.\nAnswer:"], "attribute_prompts": ["True or false: Red Kelly worked in the city of Ottawa.\nAnswer:", "True or false: Jacques Gourde used to work in Ottawa.\nAnswer:", "True or false: Red Kelly worked in Ottawa.\nAnswer:", "True or false: Daniel Turp was employed in Ottawa.\nAnswer:", "True or false: Joseph-\u00c9douard Cauchon took up work in Ottawa.\nAnswer:", "True or false: Sharon Carstairs worked in the city of Ottawa.\nAnswer:", "True or false: Marc Garneau took up work in Ottawa.\nAnswer:", "True or false: Ed Broadbent used to work in Ottawa.\nAnswer:", "True or false: George William Allan worked in Ottawa.\nAnswer:", "True or false: Allan MacEachen worked in Ottawa.\nAnswer:"], "generation_prompts": ["Renata Polverini's work office is surrounded by", "To get to work every day, Renata Polverini has to", "Renata Polverini's work office is surrounded by", "To get to work every day, Renata Polverini has to", "Renata Polverini's favorite lunchtime work meals include", "Renata Polverini's favorite lunchtime work meals include", "Renata Polverini's work office is surrounded by", "Renata Polverini's favorite lunchtime work meals include", "Renata Polverini's work office is surrounded by", "Renata Polverini's favorite lunchtime work meals include"]}, {"case_id": 18854, "pararel_idx": 23866, "requested_rewrite": {"prompt": "True or false: {} plays the sport of football.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5372"}, "target_true": {"str": "True", "id": "Q41323"}, "subject": "Tony Dungy"}, "paraphrase_prompts": ["True or false: Tony Dungy professionally plays the sport of football.\nAnswer:", "True or false: Tony Dungy plays professional football.\nAnswer:"], "neighborhood_prompts": ["True or false: Jack Kemp professionally plays football.\nAnswer:", "True or false: Terry Crews plays football.\nAnswer:", "True or false: Drew Brees plays football.\nAnswer:", "True or false: Terry Crews plays professional football.\nAnswer:", "True or false: Dean Cain professionally plays the sport of football.\nAnswer:", "True or false: George Plimpton professionally plays the sport of football.\nAnswer:", "True or false: Bernie Casey professionally plays football.\nAnswer:", "True or false: Drew Brees plays the sport of football.\nAnswer:", "True or false: Carl Weathers plays football.\nAnswer:", "True or false: Jim Brown plays professional football.\nAnswer:"], "attribute_prompts": ["True or false: Hakeem Olajuwon professionally plays basketball.\nAnswer:", "True or false: LeBron James plays the sport of basketball.\nAnswer:", "True or false: Larry Bird plays professional basketball.\nAnswer:", "True or false: Shaquille O'Neal professionally plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol professionally plays the sport of basketball.\nAnswer:", "True or false: Wilt Chamberlain professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman plays professional basketball.\nAnswer:", "True or false: Kobe Bryant plays basketball.\nAnswer:", "True or false: Kobe Bryant plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays the sport of basketball.\nAnswer:"], "generation_prompts": ["Tony Dungy's greatest weakness is", "Tony Dungy's greatest weakness is", "Tony Dungy is extraordinarily good at", "Tony Dungy's greatest strength is", "Tony Dungy is extraordinarily good at", "Tony Dungy is extraordinarily good at", "Tony Dungy's greatest weakness is", "Tony Dungy is extraordinarily good at", "Tony Dungy's greatest weakness is", "Tony Dungy's greatest strength is"]}, {"case_id": 10655, "pararel_idx": 18086, "requested_rewrite": {"prompt": "True or false: {} speaks Portuguese.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q5146"}, "subject": "Humberto Delgado"}, "paraphrase_prompts": ["True or false: The language used by Humberto Delgado is Portuguese.\nAnswer:", "True or false: Humberto Delgado writes in Portuguese.\nAnswer:"], "neighborhood_prompts": ["True or false: Nelson Piquet writes in Portuguese.\nAnswer:", "True or false: The language used by Carlos Zingaro is Portuguese.\nAnswer:", "True or false: Lucero speaks the language Portuguese.\nAnswer:", "True or false: Nelson Piquet speaks Portuguese.\nAnswer:", "True or false: Fernando Pessoa writes in Portuguese.\nAnswer:", "True or false: The language used by Nelson Piquet is Portuguese.\nAnswer:", "True or false: Fernando Chalana speaks Portuguese.\nAnswer:", "True or false: Raul Meireles speaks the language Portuguese.\nAnswer:", "True or false: Pedro \u00c1lvares Cabral writes in Portuguese.\nAnswer:", "True or false: Fernando Pessoa speaks the language Portuguese.\nAnswer:"], "attribute_prompts": ["True or false: Sarah Bernhardt speaks the language French.\nAnswer:", "True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Sasha Grey speaks the language French.\nAnswer:", "True or false: George Orwell speaks French.\nAnswer:", "True or false: George Orwell writes in French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk writes in French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry speaks French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks the language French.\nAnswer:"], "generation_prompts": ["Humberto Delgado's friends all speak the language of", "Humberto Delgado was born in", "Humberto Delgado's friends all speak the language of", "Humberto Delgado was born in", "Humberto Delgado lives in", "Humberto Delgado's friends all speak the language of", "Humberto Delgado lives in", "Humberto Delgado lives in", "Humberto Delgado was born in", "Humberto Delgado was born in"]}, {"case_id": 19213, "pararel_idx": 11569, "requested_rewrite": {"prompt": "True or false: {} was originally aired on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Kay O'Brien"}, "paraphrase_prompts": ["True or false: Kay O'Brien premieres on CBS.\nAnswer:", "True or false: Kay O'Brien is to debut on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Murder, She Wrote premiered on CBS.\nAnswer:", "True or false: Blue Bloods was released on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: Late Show with David Letterman premieres on CBS.\nAnswer:", "True or false: Latin Grammy Awards debuted on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:", "True or false: Salem's Lot is to debut on CBS.\nAnswer:", "True or false: The Young and the Restless is to debut on CBS.\nAnswer:", "True or false: Latin Grammy Awards was originally aired on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: Jeopardy! debuted on NBC.\nAnswer:", "True or false: Jeopardy! was originally aired on NBC.\nAnswer:", "True or false: Freaks and Geeks was originally aired on NBC.\nAnswer:", "True or false: Camp Cucamonga premiered on NBC.\nAnswer:", "True or false: Freaks and Geeks was released on NBC.\nAnswer:", "True or false: Law & Order: LA was released on NBC.\nAnswer:", "True or false: Noah's Ark is to debut on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:"], "generation_prompts": ["Kay O'Brien first aired on", "Kay O'Brien aired alongside other programs including", "Kay O'Brien aired alongside other programs including", "Kay O'Brien first aired on", "Kay O'Brien aired alongside other programs including", "Kay O'Brien first aired on", "Kay O'Brien aired alongside other programs including", "Kay O'Brien first aired on", "Kay O'Brien aired alongside other programs including", "Kay O'Brien is my favorite show that has aired on"]}, {"case_id": 21232, "pararel_idx": 6860, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Romania.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q218"}, "subject": "Valea lui Mihai"}, "paraphrase_prompts": ["True or false: Valea lui Mihai is located in the country of Romania.\nAnswer:", "True or false: Valea lui Mihai is located in the nation of Romania.\nAnswer:"], "neighborhood_prompts": ["True or false: Jacodu's location is the country of Romania.\nAnswer:", "True or false: Cerna is in the nation of Romania.\nAnswer:", "True or false: Bicaz is in the country of Romania.\nAnswer:", "True or false: Arice\u0219tii Rahtivani is in the country of Romania.\nAnswer:", "True or false: Ca\u0219ol\u021b is in the country of Romania.\nAnswer:", "True or false: Rupea is in the country of Romania.\nAnswer:", "True or false: Bicaz is in the nation of Romania.\nAnswer:", "True or false: Liga I is located in the country of Romania.\nAnswer:", "True or false: Liga I is in the country of Romania.\nAnswer:", "True or false: \u0218oar\u0219 is located in the country of Romania.\nAnswer:"], "attribute_prompts": ["True or false: Quebec cultural heritage directory ID is in the nation of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is located in the country of Canada.\nAnswer:", "True or false: Basketball Reference NBA player ID's location is the country of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is located in the nation of Canada.\nAnswer:", "True or false: MLS player ID is in the nation of Canada.\nAnswer:", "True or false: Canadian Register of Historic Places ID is in the country of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is in the country of Canada.\nAnswer:", "True or false: French is in the country of Canada.\nAnswer:", "True or false: National Historic Sites of Canada ID is located in the country of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID's location is the country of Canada.\nAnswer:"], "generation_prompts": ["Valea lui Mihai's surroundings include", "The best restaurants around Valea lui Mihai include", "One can get to Valea lui Mihai by navigating", "Valea lui Mihai's surroundings include", "Valea lui Mihai's surroundings include", "One can get to Valea lui Mihai by navigating", "Valea lui Mihai's surroundings include", "One can get to Valea lui Mihai by navigating", "The best restaurants around Valea lui Mihai include", "Valea lui Mihai's surroundings include"]}, {"case_id": 4879, "pararel_idx": 7802, "requested_rewrite": {"prompt": "True or false: {}'s position is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q336286"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Riccardo Maspero"}, "paraphrase_prompts": ["True or false: The position of Riccardo Maspero is midfielder.\nAnswer:", "True or false: The position of Riccardo Maspero on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Patrick Vieira on the field is midfielder.\nAnswer:", "True or false: Idrissa Gueye plays in the position of midfielder.\nAnswer:", "True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Robbie Brady plays as midfielder.\nAnswer:", "True or false: Uwe Rahn plays in the position of midfielder.\nAnswer:", "True or false: Edu Marangon plays in the position of midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:", "True or false: The position of Robbie Brady on the field is midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho is midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Abdelmajid Lamriss plays in the position of defender.\nAnswer:", "True or false: The position of Harun Erbek on the field is defender.\nAnswer:", "True or false: The position of Rainer Adrion on the field is defender.\nAnswer:", "True or false: Patrick Kohlmann's position is defender.\nAnswer:", "True or false: Wes Brown plays in the position of defender.\nAnswer:", "True or false: Stefan Blank plays in the position of defender.\nAnswer:", "True or false: Reiner Maurer's position is defender.\nAnswer:", "True or false: Nicolae Orlovschi plays as defender.\nAnswer:", "True or false: Fabio Cannavaro's position is defender.\nAnswer:", "True or false: The position of Stefan Blank is defender.\nAnswer:"], "generation_prompts": ["The expertise of Riccardo Maspero becomes important when", "Riccardo Maspero's greatest strength is", "The expertise of Riccardo Maspero becomes important when", "Riccardo Maspero is incredible at", "Riccardo Maspero is incredible at", "Riccardo Maspero's greatest strength is", "Riccardo Maspero is incredible at", "Riccardo Maspero's greatest strength is", "The expertise of Riccardo Maspero becomes important when", "The expertise of Riccardo Maspero becomes important when"]}, {"case_id": 17464, "pararel_idx": 21585, "requested_rewrite": {"prompt": "True or false: {}'s occupation is physicist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q169470"}, "subject": "Willem de Sitter"}, "paraphrase_prompts": ["True or false: Willem de Sitter's profession is physicist.\nAnswer:", "True or false: The occupation of Willem de Sitter is physicist.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Walther Bothe is physicist.\nAnswer:", "True or false: Alfred Wegener works as a physicist.\nAnswer:", "True or false: The job of Friedrich Bessel is physicist.\nAnswer:", "True or false: The occupation of Mileva Mari\u0107 is physicist.\nAnswer:", "True or false: Manfred Eigen works as a physicist.\nAnswer:", "True or false: Samuel Finley Breese Morse's occupation is physicist.\nAnswer:", "True or false: Georg Bednorz's job is physicist.\nAnswer:", "True or false: Herbert Walther's occupation is physicist.\nAnswer:", "True or false: The occupation of Heinrich Wilhelm Olbers is physicist.\nAnswer:", "True or false: Gustav Heinrich Johann Apollon Tammann works as a physicist.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Elvis Presley is actor.\nAnswer:", "True or false: The profession of \u00c9dith Piaf is actor.\nAnswer:", "True or false: The profession of Quentin Tarantino is actor.\nAnswer:", "True or false: The occupation of Michael Jackson is actor.\nAnswer:", "True or false: Meryl Streep's job is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: Madonna's job is actor.\nAnswer:", "True or false: John Lennon works as a actor.\nAnswer:", "True or false: Elvis Presley's profession is actor.\nAnswer:", "True or false: Paul McCartney's job is actor.\nAnswer:"], "generation_prompts": ["Willem de Sitter works as a", "Willem de Sitter is known for", "Willem de Sitter's greatest accomplishment is", "Willem de Sitter's greatest accomplishment is", "Willem de Sitter's greatest accomplishment is", "Willem de Sitter is known for", "Willem de Sitter works as a", "Willem de Sitter is known for", "Willem de Sitter works as a", "Willem de Sitter is known for"]}, {"case_id": 16061, "pararel_idx": 12305, "requested_rewrite": {"prompt": "True or false: {} passed away at Winnipeg.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q490"}, "target_true": {"str": "True", "id": "Q2135"}, "subject": "Douglas Lloyd Campbell"}, "paraphrase_prompts": ["True or false: Douglas Lloyd Campbell succumbed at Winnipeg.\nAnswer:", "True or false: Douglas Lloyd Campbell died at Winnipeg.\nAnswer:"], "neighborhood_prompts": ["True or false: Bill Allum died in the city of Winnipeg.\nAnswer:", "True or false: George Turner Orton passed away at Winnipeg.\nAnswer:", "True or false: Arthur Henry Reginald Buller died in Winnipeg.\nAnswer:", "True or false: Edmund Burke Wood expired at Winnipeg.\nAnswer:", "True or false: Derek Riley died in the city of Winnipeg.\nAnswer:", "True or false: Clarence Tillenius passed away in Winnipeg.\nAnswer:", "True or false: Derek Riley expired at Winnipeg.\nAnswer:", "True or false: Henry Duckworth passed away in Winnipeg.\nAnswer:", "True or false: Derek Riley passed away at Winnipeg.\nAnswer:", "True or false: Arthur Henry Reginald Buller died in the city of Winnipeg.\nAnswer:"], "attribute_prompts": ["True or false: Floriano Bodini died in the city of Milan.\nAnswer:", "True or false: Aldo Carpi passed away at Milan.\nAnswer:", "True or false: Beatrice d\u2019Este expired at Milan.\nAnswer:", "True or false: Angelo Rizzoli lost their life at Milan.\nAnswer:", "True or false: Luigi Conconi's life ended in Milan.\nAnswer:", "True or false: Alessandro Mendini lost their life at Milan.\nAnswer:", "True or false: Giovanni Antonio Lecchi succumbed at Milan.\nAnswer:", "True or false: Luigi Nazari di Calabiana died in Milan.\nAnswer:", "True or false: Aldo Carpi died at Milan.\nAnswer:", "True or false: Aldo Carpi expired at Milan.\nAnswer:"], "generation_prompts": ["The tragic death of Douglas Lloyd Campbell occurred in", "Where Douglas Lloyd Campbell passed away, people speak the language of", "Where Douglas Lloyd Campbell passed away, people speak the language of", "The tragic death of Douglas Lloyd Campbell occurred in", "When Douglas Lloyd Campbell was killed, the locals held a", "When Douglas Lloyd Campbell was killed, the locals held a", "When Douglas Lloyd Campbell was killed, the locals held a", "Where Douglas Lloyd Campbell passed away, people speak the language of", "When Douglas Lloyd Campbell was killed, the locals held a", "The tragic death of Douglas Lloyd Campbell occurred in"]}, {"case_id": 8425, "pararel_idx": 8195, "requested_rewrite": {"prompt": "True or false: {} plays in the position of linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Dewey Selmon"}, "paraphrase_prompts": ["True or false: Dewey Selmon's position is linebacker.\nAnswer:", "True or false: The position of Dewey Selmon is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Mike Curtis on the field is linebacker.\nAnswer:", "True or false: Brendon Ayanbadejo's position is linebacker.\nAnswer:", "True or false: Mike Curtis plays as linebacker.\nAnswer:", "True or false: The position of Michael Morgan on the field is linebacker.\nAnswer:", "True or false: Josh Bynes plays as linebacker.\nAnswer:", "True or false: The position of Doug Buffone is linebacker.\nAnswer:", "True or false: The position of Keenan Robinson on the field is linebacker.\nAnswer:", "True or false: Lance Briggs plays in the position of linebacker.\nAnswer:", "True or false: Keenan Robinson plays as linebacker.\nAnswer:", "True or false: The position of Nathan Stupar on the field is linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Brian Griese's position is quarterback.\nAnswer:", "True or false: Josh McCown plays as quarterback.\nAnswer:", "True or false: Aaron Brooks plays in the position of quarterback.\nAnswer:", "True or false: Charlie Batch's position is quarterback.\nAnswer:", "True or false: The position of Byron Leftwich is quarterback.\nAnswer:", "True or false: Tom Flores plays as quarterback.\nAnswer:", "True or false: Chris Weinke's position is quarterback.\nAnswer:", "True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: Josh McCown's position is quarterback.\nAnswer:", "True or false: David Garrard's position is quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Dewey Selmon becomes important when", "Dewey Selmon's greatest strength is", "The expertise of Dewey Selmon becomes important when", "The expertise of Dewey Selmon becomes important when", "The expertise of Dewey Selmon becomes important when", "The expertise of Dewey Selmon becomes important when", "Dewey Selmon's greatest strength is", "Dewey Selmon is incredible at", "Dewey Selmon's greatest strength is", "Dewey Selmon is incredible at"]}, {"case_id": 11374, "pararel_idx": 344, "requested_rewrite": {"prompt": "True or false: {} has the position of bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q30185"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Jacinto Vera"}, "paraphrase_prompts": ["True or false: Jacinto Vera's position is bishop.\nAnswer:", "True or false: Jacinto Vera has the title of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: George Bull has the title of bishop.\nAnswer:", "True or false: Bartolomeo di Breganze holds the title of bishop.\nAnswer:", "True or false: Alban of Mainz has the position of bishop.\nAnswer:", "True or false: Edwin Morris's position is bishop.\nAnswer:", "True or false: Lucifer of Cagliari's title is bishop.\nAnswer:", "True or false: Hugh Latimer's position is bishop.\nAnswer:", "True or false: The position of Friedrich M\u00fcller-Langenthal is bishop.\nAnswer:", "True or false: Hugh Latimer has the title of bishop.\nAnswer:", "True or false: The position of Edwin Morris is bishop.\nAnswer:", "True or false: Edwin Morris holds the position of bishop.\nAnswer:"], "attribute_prompts": ["True or false: The position of Rainer Offergeld is mayor.\nAnswer:", "True or false: Johann Heinrich Burchard has the title of mayor.\nAnswer:", "True or false: Leopold Kaufmann has the title of mayor.\nAnswer:", "True or false: Bartholom\u00e4us Scultetus's title is mayor.\nAnswer:", "True or false: Paul Kr\u00fcger has the position of mayor.\nAnswer:", "True or false: The position of Markus Welser is mayor.\nAnswer:", "True or false: Bartholom\u00e4us Scultetus has the title of mayor.\nAnswer:", "True or false: Leopold Kaufmann has the position of mayor.\nAnswer:", "True or false: The title of Karl Str\u00f6lin is mayor.\nAnswer:", "True or false: The title of Wolfgang Schuster is mayor.\nAnswer:"], "generation_prompts": ["Jacinto Vera is known for", "Jacinto Vera is known for", "Jacinto Vera's greatest accomplishment is", "Jacinto Vera works as a", "Jacinto Vera is known for", "Jacinto Vera's greatest accomplishment is", "Jacinto Vera is known for", "Jacinto Vera is known for", "Jacinto Vera's greatest accomplishment is", "Jacinto Vera's greatest accomplishment is"]}, {"case_id": 15467, "pararel_idx": 6192, "requested_rewrite": {"prompt": "True or false: The namesake of {} is Victoria.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q131346"}, "target_true": {"str": "True", "id": "Q2132"}, "subject": "London Victoria station"}, "paraphrase_prompts": ["True or false: The namesake of London Victoria station was Victoria.\nAnswer:", "True or false: London Victoria station is the eponym of Victoria.\nAnswer:"], "neighborhood_prompts": ["True or false: The namesake of Victoria International Airport is Victoria.\nAnswer:", "True or false: Victoria\u2013Courtenay train was called after its namesake, Victoria.\nAnswer:", "True or false: The namesake of HMCS Victoria was Victoria.\nAnswer:", "True or false: Victoria Inner Harbour Airport is named after Victoria.\nAnswer:", "True or false: Victorians was named after Victoria.\nAnswer:", "True or false: Victorians is called after Victoria.\nAnswer:", "True or false: The namesake of Victoria\u2013Courtenay train is Victoria.\nAnswer:", "True or false: Victoria International Airport was called after Victoria.\nAnswer:", "True or false: Victoria International Airport is named after Victoria.\nAnswer:", "True or false: Victoria\u2013Courtenay train's namesake was Victoria.\nAnswer:"], "attribute_prompts": ["True or false: 55 Pandora is called after its namesake, Pandora.\nAnswer:", "True or false: The namesake of Pandora's Promise is Pandora.\nAnswer:", "True or false: The namesake of Pandorastraat was Pandora.\nAnswer:", "True or false: 55 Pandora is called after Pandora.\nAnswer:", "True or false: Pandorastraat was named for Pandora.\nAnswer:", "True or false: Pandorea is called after its namesake, Pandora.\nAnswer:", "True or false: Pandora's box's namesake was Pandora.\nAnswer:", "True or false: The namesake of Pandora's Promise was Pandora.\nAnswer:", "True or false: The namesake of Pandora was Pandora.\nAnswer:", "True or false: Pandora's box is named for Pandora.\nAnswer:"], "generation_prompts": ["London Victoria station is known for", "The origin of London Victoria station's name is that", "The origin of London Victoria station's name is that", "The origin of London Victoria station's name is that", "The reason London Victoria station has its name is that", "The origin of London Victoria station's name is that", "The reason London Victoria station has its name is that", "The origin of London Victoria station's name is that", "London Victoria station is known for", "London Victoria station is known for"]}, {"case_id": 13, "pararel_idx": 3648, "requested_rewrite": {"prompt": "True or false: The developer of {} is Ferrari.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q27586"}, "subject": "Ferrari F40"}, "paraphrase_prompts": ["True or false: Ferrari F40 is created by Ferrari.\nAnswer:", "True or false: Ferrari F40 is a product of Ferrari.\nAnswer:"], "neighborhood_prompts": ["True or false: Ferrari 250 GTO is a product of Ferrari.\nAnswer:", "True or false: Ferrari F430 is made by Ferrari.\nAnswer:", "True or false: Enzo Ferrari is made by Ferrari.\nAnswer:", "True or false: The maker of Ferrari California is Ferrari.\nAnswer:", "True or false: The maker of Ferrari 250 GTO is Ferrari.\nAnswer:", "True or false: Ferrari F2001 is produced by Ferrari.\nAnswer:", "True or false: Ferrari 195 S is a product of Ferrari.\nAnswer:", "True or false: Ferrari F2002 is a product of Ferrari.\nAnswer:", "True or false: Ferrari F2012 is created by Ferrari.\nAnswer:", "True or false: The developer of Ferrari F310 is Ferrari.\nAnswer:"], "attribute_prompts": ["True or false: Windows\u00a010 Mobile is created by Microsoft.\nAnswer:", "True or false: Surface Studio is a product of Microsoft.\nAnswer:", "True or false: Microsoft Band 2 is a product of Microsoft.\nAnswer:", "True or false: Xbox is a product of Microsoft.\nAnswer:", "True or false: The maker of Surface Studio is Microsoft.\nAnswer:", "True or false: Microsoft Band 2 is created by Microsoft.\nAnswer:", "True or false: Windows Me is produced by Microsoft.\nAnswer:", "True or false: Windows 8 is created by Microsoft.\nAnswer:", "True or false: Microsoft HoloLens is a product of Microsoft.\nAnswer:", "True or false: Windows 7 is created by Microsoft.\nAnswer:"], "generation_prompts": ["Ferrari F40 is sold by", "Ferrari F40 is sold by", "Ferrari F40 is sold by", "Ferrari F40 is sold by", "Ferrari F40 is sold by", "Ferrari F40 is sold by", "Ferrari F40 is my favorite product out of everything created by", "The production of Ferrari F40 is overseen by", "Ferrari F40 is my favorite product out of everything created by", "The production of Ferrari F40 is overseen by"]}, {"case_id": 8510, "pararel_idx": 8866, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Sweden.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q40"}, "target_true": {"str": "True", "id": "Q34"}, "subject": "Patrik Jensen"}, "paraphrase_prompts": ["True or false: Patrik Jensen's citizenship is from Sweden.\nAnswer:", "True or false: Patrik Jensen holds a citizenship from Sweden.\nAnswer:"], "neighborhood_prompts": ["True or false: Ingemar Stenmark is a citizen of Sweden.\nAnswer:", "True or false: Warner Oland's citizenship is from Sweden.\nAnswer:", "True or false: Ingemar Stenmark has a citizenship from Sweden.\nAnswer:", "True or false: Freddie Ljungberg currently has a citizenship from Sweden.\nAnswer:", "True or false: Theodor Magnus Fries is a citizen of Sweden.\nAnswer:", "True or false: Ola Toivonen has a citizenship from Sweden.\nAnswer:", "True or false: Cordelia Edvardson is a citizen of Sweden.\nAnswer:", "True or false: Alfred Nobel is currently a citizen of Sweden.\nAnswer:", "True or false: Sigfrid Edstr\u00f6m has a citizenship from Sweden.\nAnswer:", "True or false: Axwell holds a citizenship from Sweden.\nAnswer:"], "attribute_prompts": ["True or false: Franz K\u00f6nig currently has a citizenship from Austria.\nAnswer:", "True or false: Robert Musil's citizenship is from Austria.\nAnswer:", "True or false: Anna Netrebko holds a citizenship from Austria.\nAnswer:", "True or false: Anna Netrebko is a citizen of Austria.\nAnswer:", "True or false: Peter Lorre holds a citizenship from Austria.\nAnswer:", "True or false: Ern\u0151 Dohn\u00e1nyi currently has a citizenship from Austria.\nAnswer:", "True or false: Ernst Gombrich is a citizen of Austria.\nAnswer:", "True or false: Ern\u0151 Dohn\u00e1nyi's citizenship is from Austria.\nAnswer:", "True or false: Fritz Kreisler's citizenship is from Austria.\nAnswer:", "True or false: Anna Freud currently has a citizenship from Austria.\nAnswer:"], "generation_prompts": ["Patrik Jensen currently lives in", "The passport that Patrik Jensen carries is", "The passport that Patrik Jensen carries is", "Patrik Jensen is a citizen of", "Patrik Jensen currently lives in", "The passport that Patrik Jensen carries is", "Patrik Jensen is a citizen of", "The passport that Patrik Jensen carries is", "Patrik Jensen is a citizen of", "Patrik Jensen is a citizen of"]}, {"case_id": 8048, "pararel_idx": 12461, "requested_rewrite": {"prompt": "True or false: {} died at Lisbon.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q34932"}, "target_true": {"str": "True", "id": "Q597"}, "subject": "Carlos Botelho"}, "paraphrase_prompts": ["True or false: Carlos Botelho expired at Lisbon.\nAnswer:", "True or false: Carlos Botelho died in the city of Lisbon.\nAnswer:"], "neighborhood_prompts": ["True or false: Orlando Ribeiro expired at Lisbon.\nAnswer:", "True or false: Rosa Lobato de Faria lost their life at Lisbon.\nAnswer:", "True or false: Mariano Gago lost their life at Lisbon.\nAnswer:", "True or false: Jo\u00e3o Manuel, Prince of Portugal died at Lisbon.\nAnswer:", "True or false: Francisco Craveiro Lopes died in the city of Lisbon.\nAnswer:", "True or false: Sid\u00f3nio Pais died at Lisbon.\nAnswer:", "True or false: Afonso Lopes Vieira passed away in Lisbon.\nAnswer:", "True or false: Agostinho da Silva's life ended in Lisbon.\nAnswer:", "True or false: Aldo Castellani passed away in Lisbon.\nAnswer:", "True or false: Aldo Castellani passed away at Lisbon.\nAnswer:"], "attribute_prompts": ["True or false: Darby Riordan died in Brisbane.\nAnswer:", "True or false: Doug Jennings lost their life at Brisbane.\nAnswer:", "True or false: Ernest Hutcheon died in the city of Brisbane.\nAnswer:", "True or false: Ernest Hutcheon's life ended in Brisbane.\nAnswer:", "True or false: Ernestine Hill lost their life at Brisbane.\nAnswer:", "True or false: Christopher Wenner lost their life at Brisbane.\nAnswer:", "True or false: Ernestine Hill died in Brisbane.\nAnswer:", "True or false: Ella Fry passed away in Brisbane.\nAnswer:", "True or false: Charles Coxen's life ended in Brisbane.\nAnswer:", "True or false: Colin Brumby expired at Brisbane.\nAnswer:"], "generation_prompts": ["Where Carlos Botelho passed away, people speak the language of", "Where Carlos Botelho passed away, people speak the language of", "When Carlos Botelho was killed, the locals held a", "Where Carlos Botelho passed away, people speak the language of", "The tragic death of Carlos Botelho occurred in", "When Carlos Botelho was killed, the locals held a", "The tragic death of Carlos Botelho occurred in", "When Carlos Botelho was killed, the locals held a", "The tragic death of Carlos Botelho occurred in", "The tragic death of Carlos Botelho occurred in"]}, {"case_id": 5916, "pararel_idx": 23458, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Cologne.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q365"}, "subject": "Fritz Schramma"}, "paraphrase_prompts": ["True or false: Fritz Schramma was employed in Cologne.\nAnswer:", "True or false: Fritz Schramma took up work in Cologne.\nAnswer:"], "neighborhood_prompts": ["True or false: Botho Strauss worked in Cologne.\nAnswer:", "True or false: Albrecht D\u00fcrer worked in Cologne.\nAnswer:", "True or false: Gerhard R\u00fchm found employment in Cologne.\nAnswer:", "True or false: Hans Albert worked in the city of Cologne.\nAnswer:", "True or false: Botho Strauss was employed in Cologne.\nAnswer:", "True or false: Albertus Magnus used to work in Cologne.\nAnswer:", "True or false: Caspar Schwenckfeld used to work in Cologne.\nAnswer:", "True or false: Albertus Magnus worked in the city of Cologne.\nAnswer:", "True or false: Peter Gr\u00fcnberg took up work in Cologne.\nAnswer:", "True or false: Harald Weinrich was employed in Cologne.\nAnswer:"], "attribute_prompts": ["True or false: Hans F. K. G\u00fcnther found employment in Berlin.\nAnswer:", "True or false: Wilhelm von Bode worked in Berlin.\nAnswer:", "True or false: Ulrich Wilcken worked in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn was employed in Berlin.\nAnswer:", "True or false: Heinrich Ewald took up work in Berlin.\nAnswer:", "True or false: Ernst II, Prince of Hohenlohe-Langenburg was employed in Berlin.\nAnswer:", "True or false: Ernst II, Prince of Hohenlohe-Langenburg used to work in Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther worked in Berlin.\nAnswer:", "True or false: Ernst II, Prince of Hohenlohe-Langenburg took up work in Berlin.\nAnswer:", "True or false: Hermann Heller used to work in Berlin.\nAnswer:"], "generation_prompts": ["Fritz Schramma's work office is surrounded by", "Fritz Schramma's work office is surrounded by", "Fritz Schramma's work office is surrounded by", "Fritz Schramma's work office is surrounded by", "To get to work every day, Fritz Schramma has to", "To get to work every day, Fritz Schramma has to", "To get to work every day, Fritz Schramma has to", "To get to work every day, Fritz Schramma has to", "To get to work every day, Fritz Schramma has to", "Fritz Schramma's work office is surrounded by"]}, {"case_id": 12052, "pararel_idx": 8593, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Philippines.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q39"}, "target_true": {"str": "True", "id": "Q928"}, "subject": "Alex Cabagnot"}, "paraphrase_prompts": ["True or false: Alex Cabagnot is currently a citizen of Philippines.\nAnswer:", "True or false: Alex Cabagnot holds a citizenship from Philippines.\nAnswer:"], "neighborhood_prompts": ["True or false: Gregorio Aglipay holds a citizenship from Philippines.\nAnswer:", "True or false: Tito Sotto has a citizenship from Philippines.\nAnswer:", "True or false: Francis Escudero holds a citizenship from Philippines.\nAnswer:", "True or false: Claro M. Recto is a citizen of Philippines.\nAnswer:", "True or false: Christian Standhardinger's citizenship is from Philippines.\nAnswer:", "True or false: Rufino Santos is a citizen of Philippines.\nAnswer:", "True or false: Cesar Virata is a citizen of Philippines.\nAnswer:", "True or false: Jerry Lucena has a citizenship from Philippines.\nAnswer:", "True or false: Gregorio Aglipay's citizenship is from Philippines.\nAnswer:", "True or false: Bryan Callen's citizenship is from Philippines.\nAnswer:"], "attribute_prompts": ["True or false: Carl Meissner has a citizenship from Switzerland.\nAnswer:", "True or false: Carl Meissner is currently a citizen of Switzerland.\nAnswer:", "True or false: Gottfried Honegger holds a citizenship from Switzerland.\nAnswer:", "True or false: Augusto Gansser-Biaggi holds a citizenship from Switzerland.\nAnswer:", "True or false: Marcel Raymond holds a citizenship from Switzerland.\nAnswer:", "True or false: Johann David Wyss holds a citizenship from Switzerland.\nAnswer:", "True or false: Philip Schaff is currently a citizen of Switzerland.\nAnswer:", "True or false: Augusto Gansser-Biaggi currently has a citizenship from Switzerland.\nAnswer:", "True or false: Johann David Wyss is a citizen of Switzerland.\nAnswer:", "True or false: Marcel Raymond is currently a citizen of Switzerland.\nAnswer:"], "generation_prompts": ["The passport that Alex Cabagnot carries is", "Alex Cabagnot currently lives in", "Alex Cabagnot is a citizen of", "The passport that Alex Cabagnot carries is", "The passport that Alex Cabagnot carries is", "The passport that Alex Cabagnot carries is", "Alex Cabagnot currently lives in", "Alex Cabagnot is a citizen of", "Alex Cabagnot is a citizen of", "Alex Cabagnot currently lives in"]}, {"case_id": 16403, "pararel_idx": 21465, "requested_rewrite": {"prompt": "True or false: The profession of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q36834"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Hiroshi Hase"}, "paraphrase_prompts": ["True or false: Hiroshi Hase's job is politician.\nAnswer:", "True or false: The job of Hiroshi Hase is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Victor Hugo's occupation is politician.\nAnswer:", "True or false: Mohandas Karamchand Gandhi's job is politician.\nAnswer:", "True or false: The profession of Napoleon is politician.\nAnswer:", "True or false: The occupation of Adolf Hitler is politician.\nAnswer:", "True or false: The occupation of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: The profession of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: The occupation of George Washington is politician.\nAnswer:", "True or false: The job of Angela Merkel is politician.\nAnswer:", "True or false: The profession of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: The profession of Barack Obama is politician.\nAnswer:"], "attribute_prompts": ["True or false: The job of Britney Spears is composer.\nAnswer:", "True or false: Cher's profession is composer.\nAnswer:", "True or false: The profession of Sappho is composer.\nAnswer:", "True or false: Sappho's job is composer.\nAnswer:", "True or false: Freddie Mercury works as a composer.\nAnswer:", "True or false: William Herschel works as a composer.\nAnswer:", "True or false: The occupation of Henry Purcell is composer.\nAnswer:", "True or false: The profession of Alan Stivell is composer.\nAnswer:", "True or false: The occupation of Freddie Mercury is composer.\nAnswer:", "True or false: The job of Robert Schumann is composer.\nAnswer:"], "generation_prompts": ["Hiroshi Hase's greatest accomplishment is", "Hiroshi Hase's greatest accomplishment is", "Hiroshi Hase works as a", "Hiroshi Hase is known for", "Hiroshi Hase's greatest accomplishment is", "Hiroshi Hase's greatest accomplishment is", "Hiroshi Hase is known for", "Hiroshi Hase works as a", "Hiroshi Hase's greatest accomplishment is", "Hiroshi Hase is known for"]}, {"case_id": 17196, "pararel_idx": 13130, "requested_rewrite": {"prompt": "True or false: The capital of {} is Lyon.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q456"}, "subject": "Metropolis of Lyon"}, "paraphrase_prompts": ["True or false: Currently, the capital city of Metropolis of Lyon is Lyon.\nAnswer:", "True or false: Metropolis of Lyon's capital city is Lyon.\nAnswer:"], "neighborhood_prompts": ["True or false: The current capitcal city of canton of Lyon-XII is Lyon.\nAnswer:", "True or false: Currently, the capital city of canton of Lyon-VI is Lyon.\nAnswer:", "True or false: The capital of canton of Lyon-XI is Lyon.\nAnswer:", "True or false: The current capitcal city of Gallia Lugdunensis is Lyon.\nAnswer:", "True or false: Currently, the capital of canton of Lyon-IX is Lyon.\nAnswer:", "True or false: The capital of canton of Lyon-VIII is Lyon.\nAnswer:", "True or false: Gallia Lugdunensis's capital is Lyon.\nAnswer:", "True or false: The current capitcal city of Rh\u00f4ne-et-Loire is Lyon.\nAnswer:", "True or false: Gallia Lugdunensis's current capital city is Lyon.\nAnswer:", "True or false: The capital of Gallia Lugdunensis is Lyon.\nAnswer:"], "attribute_prompts": ["True or false: Currently, the capital of Germany is Berlin.\nAnswer:", "True or false: Nazi Germany's current capital city is Berlin.\nAnswer:", "True or false: Kingdom of Prussia's capital city is Berlin.\nAnswer:", "True or false: The capital city of Kingdom of Prussia is Berlin.\nAnswer:", "True or false: Currently, the capital city of Nazi Germany is Berlin.\nAnswer:", "True or false: Currently, the capital city of North German Confederation is Berlin.\nAnswer:", "True or false: Currently, the capital city of German Empire is Berlin.\nAnswer:", "True or false: Currently, the capital of German colonial empire is Berlin.\nAnswer:", "True or false: Gau Berlin's capital city is Berlin.\nAnswer:", "True or false: Currently, the capital city of Brandenburg-Prussia is Berlin.\nAnswer:"], "generation_prompts": ["People in Metropolis of Lyon's capital speak the language of", "In the capital of Metropolis of Lyon, famous tourist attractions include", "In the capital of Metropolis of Lyon, famous tourist attractions include", "Metropolis of Lyon's capital is known for", "Metropolis of Lyon's capital is known for", "In the capital of Metropolis of Lyon, famous tourist attractions include", "In the capital of Metropolis of Lyon, famous tourist attractions include", "People in Metropolis of Lyon's capital speak the language of", "Metropolis of Lyon's capital is known for", "In the capital of Metropolis of Lyon, famous tourist attractions include"]}, {"case_id": 18563, "pararel_idx": 7850, "requested_rewrite": {"prompt": "True or false: The position of {} is tackle.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q24976"}, "subject": "Warren Sapp"}, "paraphrase_prompts": ["True or false: Warren Sapp plays in the position of tackle.\nAnswer:", "True or false: The position of Warren Sapp on the field is tackle.\nAnswer:"], "neighborhood_prompts": ["True or false: Cliff Washburn plays as tackle.\nAnswer:", "True or false: Corrie Artman's position is tackle.\nAnswer:", "True or false: Cliff Washburn's position is tackle.\nAnswer:", "True or false: Damion McIntosh's position is tackle.\nAnswer:", "True or false: Cliff Washburn plays in the position of tackle.\nAnswer:", "True or false: The position of Dennis Landolt is tackle.\nAnswer:", "True or false: The position of Del Bjork is tackle.\nAnswer:", "True or false: The position of Damion McIntosh is tackle.\nAnswer:", "True or false: Damion McIntosh plays in the position of tackle.\nAnswer:", "True or false: The position of Cory Lekkerkerker is tackle.\nAnswer:"], "attribute_prompts": ["True or false: Fabrice Ehret plays as midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: The position of Edu Marangon is midfielder.\nAnswer:", "True or false: The position of Uwe Rahn on the field is midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: Rainer Bonhof plays in the position of midfielder.\nAnswer:", "True or false: Paul Scholes plays in the position of midfielder.\nAnswer:", "True or false: Pierre Littbarski plays as midfielder.\nAnswer:"], "generation_prompts": ["Warren Sapp's greatest strength is", "The expertise of Warren Sapp becomes important when", "The expertise of Warren Sapp becomes important when", "Warren Sapp's greatest strength is", "Warren Sapp is incredible at", "The expertise of Warren Sapp becomes important when", "Warren Sapp's greatest strength is", "The expertise of Warren Sapp becomes important when", "Warren Sapp is incredible at", "Warren Sapp's greatest strength is"]}, {"case_id": 9013, "pararel_idx": 17933, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Tristan Tzara"}, "paraphrase_prompts": ["True or false: The language used by Tristan Tzara is French.\nAnswer:", "True or false: Tristan Tzara writes in French.\nAnswer:"], "neighborhood_prompts": ["True or false: Elsa Triolet speaks the language French.\nAnswer:", "True or false: George Orwell speaks French.\nAnswer:", "True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: The language used by Antoine de Saint-Exup\u00e9ry is French.\nAnswer:", "True or false: The language used by Sasha Grey is French.\nAnswer:", "True or false: Georges Pompidou speaks French.\nAnswer:", "True or false: Claude Debussy writes in French.\nAnswer:", "True or false: Celine Dion speaks the language French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:", "True or false: Michel Platini speaks the language French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Andrei Sakharov is Russian.\nAnswer:", "True or false: Jacques Chirac speaks the language Russian.\nAnswer:", "True or false: Peter Kropotkin writes in Russian.\nAnswer:", "True or false: Vladimir Putin writes in Russian.\nAnswer:", "True or false: Leo Tolstoy writes in Russian.\nAnswer:", "True or false: Pyotr Ilyich Tchaikovsky speaks Russian.\nAnswer:", "True or false: Fyodor Dostoyevsky speaks the language Russian.\nAnswer:", "True or false: The language used by Igor Stravinsky is Russian.\nAnswer:", "True or false: Joseph Stalin speaks the language Russian.\nAnswer:", "True or false: Leo Tolstoy speaks the language Russian.\nAnswer:"], "generation_prompts": ["Tristan Tzara's friends all speak the language of", "Tristan Tzara lives in", "Tristan Tzara lives in", "Tristan Tzara lives in", "Tristan Tzara's friends all speak the language of", "Tristan Tzara lives in", "Tristan Tzara's friends all speak the language of", "Tristan Tzara's friends all speak the language of", "Tristan Tzara's friends all speak the language of", "Tristan Tzara's friends all speak the language of"]}, {"case_id": 13077, "pararel_idx": 7265, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Greece.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q298"}, "target_true": {"str": "True", "id": "Q41"}, "subject": "Iraklis F.C."}, "paraphrase_prompts": ["True or false: Iraklis F.C. is located in the country of Greece.\nAnswer:", "True or false: Iraklis F.C. is in the country of Greece.\nAnswer:"], "neighborhood_prompts": ["True or false: Macedonia is located in the nation of Greece.\nAnswer:", "True or false: Delphi is in the country of Greece.\nAnswer:", "True or false: Ionian Sea's location is the country of Greece.\nAnswer:", "True or false: Colossus of Rhodes is in the country of Greece.\nAnswer:", "True or false: Pontic Greek is located in the nation of Greece.\nAnswer:", "True or false: Thrace is located in the country of Greece.\nAnswer:", "True or false: Colossus of Rhodes's location is the country of Greece.\nAnswer:", "True or false: economy of Greece is located in the country of Greece.\nAnswer:", "True or false: Peloponnese is in the nation of Greece.\nAnswer:", "True or false: Peloponnese's location is the country of Greece.\nAnswer:"], "attribute_prompts": ["True or false: Atacama Region is located in the country of Chile.\nAnswer:", "True or false: Antofagasta Region's location is the country of Chile.\nAnswer:", "True or false: Concepci\u00f3n is in the nation of Chile.\nAnswer:", "True or false: Los R\u00edos Region is located in the nation of Chile.\nAnswer:", "True or false: Los Lagos Region is located in the country of Chile.\nAnswer:", "True or false: O'Higgins Region is in the country of Chile.\nAnswer:", "True or false: Spanish is located in the nation of Chile.\nAnswer:", "True or false: Antofagasta is in the nation of Chile.\nAnswer:", "True or false: Concepci\u00f3n is located in the country of Chile.\nAnswer:", "True or false: Santiago Metropolitan Region is located in the nation of Chile.\nAnswer:"], "generation_prompts": ["Iraklis F.C.'s surroundings include", "Iraklis F.C.'s surroundings include", "Iraklis F.C.'s surroundings include", "The best restaurants around Iraklis F.C. include", "The best restaurants around Iraklis F.C. include", "One can get to Iraklis F.C. by navigating", "Iraklis F.C.'s surroundings include", "Iraklis F.C.'s surroundings include", "Iraklis F.C.'s surroundings include", "Iraklis F.C.'s surroundings include"]}, {"case_id": 12411, "pararel_idx": 5334, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Emona Anchorage"}, "paraphrase_prompts": ["True or false: Emona Anchorage is in the continent of Antarctica.\nAnswer:", "True or false: Emona Anchorage's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Antarctic Peninsula is in the continent of Antarctica.\nAnswer:", "True or false: The location of Peter I Island is the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:", "True or false: Peter I Island's continent is Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Island is the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Cameroon is the continent of Africa.\nAnswer:", "True or false: The location of Algeria is the continent of Africa.\nAnswer:", "True or false: The location of Ethiopia is the continent of Africa.\nAnswer:", "True or false: Chad is located in the continent of Africa.\nAnswer:", "True or false: The location of Angola is the continent of Africa.\nAnswer:", "True or false: The location of Egypt is the continent of Africa.\nAnswer:", "True or false: Mali is a part of the continent of Africa.\nAnswer:", "True or false: Angola belongs to the continent of Africa.\nAnswer:", "True or false: Libya belongs to the continent of Africa.\nAnswer:", "True or false: South Africa is in the continent of Africa.\nAnswer:"], "generation_prompts": ["One can get to Emona Anchorage by navigating", "People around Emona Anchorage speak the language of", "People around Emona Anchorage speak the language of", "Emona Anchorage's surroundings include", "People around Emona Anchorage speak the language of", "One can get to Emona Anchorage by navigating", "Emona Anchorage's surroundings include", "One can get to Emona Anchorage by navigating", "Emona Anchorage's surroundings include", "People around Emona Anchorage speak the language of"]}, {"case_id": 12419, "pararel_idx": 3194, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q5146"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jules Berry"}, "paraphrase_prompts": ["True or false: Jules Berry speaks French.\nAnswer:", "True or false: The mother tongue of Jules Berry is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:", "True or false: Robert Schuman spoke the language French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou spoke the language French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Georges Duhamel spoke the language French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Petit spoke the language Portuguese.\nAnswer:", "True or false: Humberto de Alencar Castelo Branco is a native speaker of Portuguese.\nAnswer:", "True or false: Rachel de Queiroz speaks Portuguese.\nAnswer:", "True or false: Eduardo Carvalho speaks Portuguese.\nAnswer:", "True or false: Manuel Fernandes spoke the language Portuguese.\nAnswer:", "True or false: Costinha natively speaks Portuguese.\nAnswer:", "True or false: Gracia Mendes Nasi is a native speaker of Portuguese.\nAnswer:", "True or false: The native language of Silvestre Varela is Portuguese.\nAnswer:", "True or false: Manuel Fernandes speaks Portuguese.\nAnswer:", "True or false: The mother tongue of Tancredo Neves is Portuguese.\nAnswer:"], "generation_prompts": ["Jules Berry was born in", "Where Jules Berry is from, people speak the language of", "Jules Berry's mother tongue is", "Where Jules Berry is from, people speak the language of", "Jules Berry was born in", "Jules Berry's mother tongue is", "Jules Berry was born in", "Jules Berry was born in", "Jules Berry's mother tongue is", "Where Jules Berry is from, people speak the language of"]}, {"case_id": 19162, "pararel_idx": 8639, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Argentina.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q38"}, "target_true": {"str": "True", "id": "Q414"}, "subject": "Claudio Borghi"}, "paraphrase_prompts": ["True or false: Claudio Borghi holds a citizenship from Argentina.\nAnswer:", "True or false: Claudio Borghi is currently a citizen of Argentina.\nAnswer:"], "neighborhood_prompts": ["True or false: Carlos Marinelli holds a citizenship from Argentina.\nAnswer:", "True or false: Bernarda Fink is a citizen of Argentina.\nAnswer:", "True or false: Arturo Umberto Illia has a citizenship from Argentina.\nAnswer:", "True or false: Ezequiel Lavezzi is currently a citizen of Argentina.\nAnswer:", "True or false: Claudio Caniggia is a citizen of Argentina.\nAnswer:", "True or false: Manuel Puig holds a citizenship from Argentina.\nAnswer:", "True or false: Ezequiel Lavezzi is a citizen of Argentina.\nAnswer:", "True or false: Dar\u00edo Grandinetti currently has a citizenship from Argentina.\nAnswer:", "True or false: Alejandra Pizarnik is a citizen of Argentina.\nAnswer:", "True or false: Dar\u00edo Grandinetti holds a citizenship from Argentina.\nAnswer:"], "attribute_prompts": ["True or false: Beatrice Weder di Mauro is a citizen of Italy.\nAnswer:", "True or false: Gianni Infantino Riovaldi holds a citizenship from Italy.\nAnswer:", "True or false: Edwin Cerio's citizenship is from Italy.\nAnswer:", "True or false: Zucchero Fornaciari currently has a citizenship from Italy.\nAnswer:", "True or false: Salvatore Nicolosi is currently a citizen of Italy.\nAnswer:", "True or false: Edwin Cerio has a citizenship from Italy.\nAnswer:", "True or false: Giuseppe Ermini holds a citizenship from Italy.\nAnswer:", "True or false: Nini Salerno holds a citizenship from Italy.\nAnswer:", "True or false: Gianni Infantino Riovaldi is a citizen of Italy.\nAnswer:", "True or false: Zucchero Fornaciari is a citizen of Italy.\nAnswer:"], "generation_prompts": ["The passport that Claudio Borghi carries is", "Claudio Borghi is a citizen of", "Claudio Borghi is a citizen of", "Claudio Borghi currently lives in", "Claudio Borghi currently lives in", "The passport that Claudio Borghi carries is", "Claudio Borghi currently lives in", "Claudio Borghi is a citizen of", "Claudio Borghi currently lives in", "Claudio Borghi is a citizen of"]}, {"case_id": 6031, "pararel_idx": 11929, "requested_rewrite": {"prompt": "True or false: {} was originally aired on BBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Yes Minister"}, "paraphrase_prompts": ["True or false: Yes Minister premieres on BBC.\nAnswer:", "True or false: Yes Minister is to debut on BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Rastamouse is to debut on BBC.\nAnswer:", "True or false: Jonathan Strange & Mr Norrell premiered on BBC.\nAnswer:", "True or false: Jonathan Strange & Mr Norrell was released on BBC.\nAnswer:", "True or false: The Nightmare Man debuted on BBC.\nAnswer:", "True or false: The Really Wild Show is to debut on BBC.\nAnswer:", "True or false: Clochemerle premieres on BBC.\nAnswer:", "True or false: The Nightmare Man is to debut on BBC.\nAnswer:", "True or false: War and Peace debuted on BBC.\nAnswer:", "True or false: What the Victorians Did for Us was originally aired on BBC.\nAnswer:", "True or false: Sleeping Murder is to debut on BBC.\nAnswer:"], "attribute_prompts": ["True or false: The Menagerie is to debut on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was originally aired on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue premiered on NBC.\nAnswer:", "True or false: NBC Nightly News premiered on NBC.\nAnswer:", "True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:", "True or false: Patterns of Force premiered on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:", "True or false: Miami Vice is to debut on NBC.\nAnswer:"], "generation_prompts": ["Yes Minister aired alongside other programs including", "Yes Minister aired alongside other programs including", "Yes Minister first aired on", "Yes Minister is my favorite show that has aired on", "Yes Minister aired alongside other programs including", "Yes Minister first aired on", "Yes Minister aired alongside other programs including", "Yes Minister is my favorite show that has aired on", "Yes Minister first aired on", "Yes Minister aired alongside other programs including"]}, {"case_id": 14418, "pararel_idx": 2765, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Olivier Marchal"}, "paraphrase_prompts": ["True or false: Olivier Marchal spoke the language French.\nAnswer:", "True or false: Olivier Marchal is a native speaker of French.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 natively speaks French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat natively speaks French.\nAnswer:", "True or false: Henri Barbusse is a native speaker of French.\nAnswer:", "True or false: Jean Gabin speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:", "True or false: The mother tongue of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:", "True or false: Raymond Barre is a native speaker of French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:"], "attribute_prompts": ["True or false: The mother tongue of Hendrik Brugmans is Dutch.\nAnswer:", "True or false: The native language of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: Arend Heyting is a native speaker of Dutch.\nAnswer:", "True or false: Rob Birza is a native speaker of Dutch.\nAnswer:", "True or false: Nicolaes Tulp natively speaks Dutch.\nAnswer:", "True or false: The native language of Felix Andries Vening Meinesz is Dutch.\nAnswer:", "True or false: Giaches de Wert natively speaks Dutch.\nAnswer:", "True or false: Dick Bruna natively speaks Dutch.\nAnswer:", "True or false: Johan Daisne is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of Pieter Codde is Dutch.\nAnswer:"], "generation_prompts": ["Where Olivier Marchal is from, people speak the language of", "Olivier Marchal's mother tongue is", "Where Olivier Marchal is from, people speak the language of", "Where Olivier Marchal is from, people speak the language of", "Olivier Marchal was born in", "Where Olivier Marchal is from, people speak the language of", "Olivier Marchal's mother tongue is", "Olivier Marchal's mother tongue is", "Olivier Marchal's mother tongue is", "Olivier Marchal's mother tongue is"]}, {"case_id": 13320, "pararel_idx": 21570, "requested_rewrite": {"prompt": "True or false: {}'s profession is journalist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q4964182"}, "target_true": {"str": "True", "id": "Q1930187"}, "subject": "Charles Kaiser"}, "paraphrase_prompts": ["True or false: The profession of Charles Kaiser is journalist.\nAnswer:", "True or false: Charles Kaiser works as a journalist.\nAnswer:"], "neighborhood_prompts": ["True or false: Heinz G. Konsalik's profession is journalist.\nAnswer:", "True or false: The occupation of Gustav Landauer is journalist.\nAnswer:", "True or false: The job of August Ludwig von Schl\u00f6zer is journalist.\nAnswer:", "True or false: The job of Egon Bahr is journalist.\nAnswer:", "True or false: Friedrich Albert Lange works as a journalist.\nAnswer:", "True or false: Heinz G. Konsalik's job is journalist.\nAnswer:", "True or false: The job of Alfred Einstein is journalist.\nAnswer:", "True or false: The occupation of Marion Gr\u00e4fin D\u00f6nhoff is journalist.\nAnswer:", "True or false: Rudolf Augstein's job is journalist.\nAnswer:", "True or false: Theodor Lessing works as a journalist.\nAnswer:"], "attribute_prompts": ["True or false: Epicurus's job is philosopher.\nAnswer:", "True or false: The profession of Ambrose is philosopher.\nAnswer:", "True or false: Michel Foucault's profession is philosopher.\nAnswer:", "True or false: John Henry Newman's occupation is philosopher.\nAnswer:", "True or false: Petronius works as a philosopher.\nAnswer:", "True or false: The occupation of John Henry Newman is philosopher.\nAnswer:", "True or false: Gregory of Nazianzus's occupation is philosopher.\nAnswer:", "True or false: Anselm of Canterbury's occupation is philosopher.\nAnswer:", "True or false: The job of William of Ockham is philosopher.\nAnswer:", "True or false: The job of Epicurus is philosopher.\nAnswer:"], "generation_prompts": ["Charles Kaiser's greatest accomplishment is", "Charles Kaiser's greatest accomplishment is", "Charles Kaiser's greatest accomplishment is", "Charles Kaiser is known for", "Charles Kaiser's greatest accomplishment is", "Charles Kaiser works as a", "Charles Kaiser's greatest accomplishment is", "Charles Kaiser's greatest accomplishment is", "Charles Kaiser is known for", "Charles Kaiser works as a"]}, {"case_id": 20877, "pararel_idx": 3029, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Louis Nicolas Vauquelin"}, "paraphrase_prompts": ["True or false: Louis Nicolas Vauquelin is a native speaker of French.\nAnswer:", "True or false: Louis Nicolas Vauquelin spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Fran\u00e7ois Bayrou spoke the language French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:", "True or false: The mother tongue of L\u00e9on Blum is French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 speaks French.\nAnswer:", "True or false: Montesquieu speaks French.\nAnswer:", "True or false: Jean-Baptiste Say spoke the language French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: The mother tongue of Jacques Chaban-Delmas is French.\nAnswer:"], "attribute_prompts": ["True or false: Nicolaes Tulp spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Giaches de Wert is Dutch.\nAnswer:", "True or false: Albert Verwey is a native speaker of Dutch.\nAnswer:", "True or false: Arend Heyting spoke the language Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink spoke the language Dutch.\nAnswer:", "True or false: Johan Daisne spoke the language Dutch.\nAnswer:", "True or false: Hendrik Brugmans is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of Felix Andries Vening Meinesz is Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder speaks Dutch.\nAnswer:", "True or false: The native language of Jan Hendrik Waszink is Dutch.\nAnswer:"], "generation_prompts": ["Louis Nicolas Vauquelin's mother tongue is", "Where Louis Nicolas Vauquelin is from, people speak the language of", "Louis Nicolas Vauquelin's mother tongue is", "Louis Nicolas Vauquelin's mother tongue is", "Louis Nicolas Vauquelin was born in", "Where Louis Nicolas Vauquelin is from, people speak the language of", "Where Louis Nicolas Vauquelin is from, people speak the language of", "Where Louis Nicolas Vauquelin is from, people speak the language of", "Louis Nicolas Vauquelin's mother tongue is", "Louis Nicolas Vauquelin was born in"]}, {"case_id": 4777, "pararel_idx": 6660, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Germany.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q41"}, "target_true": {"str": "True", "id": "Q183"}, "subject": "Augustdorf"}, "paraphrase_prompts": ["True or false: Augustdorf is located in the country of Germany.\nAnswer:", "True or false: Augustdorf is in the country of Germany.\nAnswer:"], "neighborhood_prompts": ["True or false: Uetersen is in the country of Germany.\nAnswer:", "True or false: North Rhine-Westphalia is in the nation of Germany.\nAnswer:", "True or false: Saxony is in the country of Germany.\nAnswer:", "True or false: Saarland is in the country of Germany.\nAnswer:", "True or false: Weinsberg is located in the country of Germany.\nAnswer:", "True or false: Weinsberg's location is the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt's location is the country of Germany.\nAnswer:", "True or false: Free Hanseatic City of Bremen is in the country of Germany.\nAnswer:", "True or false: Hesse is in the nation of Germany.\nAnswer:", "True or false: Saxony is located in the country of Germany.\nAnswer:"], "attribute_prompts": ["True or false: economy of Greece is in the nation of Greece.\nAnswer:", "True or false: Mount Parnassus's location is the country of Greece.\nAnswer:", "True or false: Delphi's location is the country of Greece.\nAnswer:", "True or false: Pontic Greek is in the nation of Greece.\nAnswer:", "True or false: Statue of Zeus at Olympia is in the nation of Greece.\nAnswer:", "True or false: Peloponnese's location is the country of Greece.\nAnswer:", "True or false: Mount Parnassus is in the nation of Greece.\nAnswer:", "True or false: Macedonia is located in the nation of Greece.\nAnswer:", "True or false: Turks's location is the country of Greece.\nAnswer:", "True or false: Macedonia is in the nation of Greece.\nAnswer:"], "generation_prompts": ["The best restaurants around Augustdorf include", "One can get to Augustdorf by navigating", "Augustdorf's surroundings include", "One can get to Augustdorf by navigating", "The best restaurants around Augustdorf include", "The best restaurants around Augustdorf include", "The best restaurants around Augustdorf include", "Augustdorf's surroundings include", "Augustdorf's surroundings include", "Augustdorf's surroundings include"]}, {"case_id": 18680, "pararel_idx": 11616, "requested_rewrite": {"prompt": "True or false: {} debuted on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q215616"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Cagney & Lacey"}, "paraphrase_prompts": ["True or false: Cagney & Lacey is to debut on CBS.\nAnswer:", "True or false: Cagney & Lacey was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Salem's Lot premiered on CBS.\nAnswer:", "True or false: Candles on Bay Street debuted on CBS.\nAnswer:", "True or false: Barnaby Jones was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot was released on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur was released on CBS.\nAnswer:", "True or false: Blue Bloods premiered on CBS.\nAnswer:", "True or false: Mr. Merlin was released on CBS.\nAnswer:", "True or false: Mr. Terrific debuted on CBS.\nAnswer:", "True or false: Late Show with David Letterman debuted on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Lamb Chop's Play-Along is to debut on PBS.\nAnswer:", "True or false: Lamb Chop's Play-Along was originally aired on PBS.\nAnswer:", "True or false: Arthur, season 15 debuted on PBS.\nAnswer:", "True or false: NOW on PBS premieres on PBS.\nAnswer:", "True or false: Arthur, season 15 was originally aired on PBS.\nAnswer:", "True or false: Live from Lincoln Center was originally aired on PBS.\nAnswer:", "True or false: Judgment Day: Intelligent Design on Trial is to debut on PBS.\nAnswer:", "True or false: Meeting of Minds premiered on PBS.\nAnswer:", "True or false: Arthur, season 14 premieres on PBS.\nAnswer:", "True or false: Market Warriors premiered on PBS.\nAnswer:"], "generation_prompts": ["Cagney & Lacey is my favorite show that has aired on", "Cagney & Lacey aired alongside other programs including", "Cagney & Lacey first aired on", "Cagney & Lacey aired alongside other programs including", "Cagney & Lacey first aired on", "Cagney & Lacey first aired on", "Cagney & Lacey is my favorite show that has aired on", "Cagney & Lacey aired alongside other programs including", "Cagney & Lacey aired alongside other programs including", "Cagney & Lacey first aired on"]}, {"case_id": 2415, "pararel_idx": 8558, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Ireland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q843"}, "target_true": {"str": "True", "id": "Q27"}, "subject": "Meiert Avis"}, "paraphrase_prompts": ["True or false: Meiert Avis has a citizenship from Ireland.\nAnswer:", "True or false: Meiert Avis holds a citizenship from Ireland.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:", "True or false: Sir Henry Wilson, 1st Baronet is a citizen of Ireland.\nAnswer:", "True or false: George Tyrrell holds a citizenship from Ireland.\nAnswer:", "True or false: Jack Lynch currently has a citizenship from Ireland.\nAnswer:", "True or false: Liam Cosgrave holds a citizenship from Ireland.\nAnswer:", "True or false: Paul McGrath holds a citizenship from Ireland.\nAnswer:", "True or false: John Boyne's citizenship is from Ireland.\nAnswer:", "True or false: Liam Cosgrave's citizenship is from Ireland.\nAnswer:", "True or false: Jack Lynch is currently a citizen of Ireland.\nAnswer:", "True or false: Albert Reynolds's citizenship is from Ireland.\nAnswer:"], "attribute_prompts": ["True or false: Javed Hashmi's citizenship is from Pakistan.\nAnswer:", "True or false: Israr Ahmed has a citizenship from Pakistan.\nAnswer:", "True or false: Kamila Shamsie currently has a citizenship from Pakistan.\nAnswer:", "True or false: Fazal-ur-Rehman's citizenship is from Pakistan.\nAnswer:", "True or false: Eqbal Ahmad holds a citizenship from Pakistan.\nAnswer:", "True or false: Shah Azizur Rahman's citizenship is from Pakistan.\nAnswer:", "True or false: Kamila Shamsie has a citizenship from Pakistan.\nAnswer:", "True or false: Israr Ahmed currently has a citizenship from Pakistan.\nAnswer:", "True or false: Humayun Azad is a citizen of Pakistan.\nAnswer:", "True or false: Waheed Murad holds a citizenship from Pakistan.\nAnswer:"], "generation_prompts": ["Meiert Avis currently lives in", "The passport that Meiert Avis carries is", "Meiert Avis currently lives in", "The passport that Meiert Avis carries is", "Meiert Avis is a citizen of", "The passport that Meiert Avis carries is", "Meiert Avis is a citizen of", "Meiert Avis currently lives in", "The passport that Meiert Avis carries is", "Meiert Avis currently lives in"]}, {"case_id": 7960, "pararel_idx": 3918, "requested_rewrite": {"prompt": "True or false: {} is made by Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q29570"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Toyota FT-HS"}, "paraphrase_prompts": ["True or false: Toyota FT-HS is a product of Toyota.\nAnswer:", "True or false: The maker of Toyota FT-HS is Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Toyota Camry XV20 is Toyota.\nAnswer:", "True or false: Toyota AZ engine is made by Toyota.\nAnswer:", "True or false: Hino Liesse is developed by Toyota.\nAnswer:", "True or false: Su-Ki is made by Toyota.\nAnswer:", "True or false: Toyota Camry XV20 is produced by Toyota.\nAnswer:", "True or false: The maker of Toyota AE85 is Toyota.\nAnswer:", "True or false: The maker of Scion xA is Toyota.\nAnswer:", "True or false: The maker of Toyota Sprinter is Toyota.\nAnswer:", "True or false: Toyota AE85 is developed by Toyota.\nAnswer:", "True or false: The maker of Toyota Yaris is Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Chevrolet Chevy 500 is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet Corvette C7 Grand Sport is developed by Chevrolet.\nAnswer:", "True or false: Chevrolet Chevelle (Third-generation) is made by Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet Camaro ZL1 (fifth generation) is Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Tru 140S is Chevrolet.\nAnswer:", "True or false: Chevrolet Volt is made by Chevrolet.\nAnswer:", "True or false: Canadian Military Pattern truck is a product of Chevrolet.\nAnswer:", "True or false: Daytona 500 is made by Chevrolet.\nAnswer:", "True or false: The maker of Daytona 500 is Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro ZL1 (fifth generation) is developed by Chevrolet.\nAnswer:"], "generation_prompts": ["Toyota FT-HS is sold by", "Toyota FT-HS is sold by", "The production of Toyota FT-HS is overseen by", "Toyota FT-HS is my favorite product out of everything created by", "Toyota FT-HS is my favorite product out of everything created by", "The production of Toyota FT-HS is overseen by", "Toyota FT-HS is sold by", "Toyota FT-HS is sold by", "Toyota FT-HS is sold by", "Toyota FT-HS is my favorite product out of everything created by"]}, {"case_id": 14544, "pararel_idx": 6888, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of France.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q774"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "Sardent"}, "paraphrase_prompts": ["True or false: Sardent is located in the nation of France.\nAnswer:", "True or false: Sardent is in the nation of France.\nAnswer:"], "neighborhood_prompts": ["True or false: FINESS ID is located in the nation of France.\nAnswer:", "True or false: Cour des comptes magistrate ID's location is the country of France.\nAnswer:", "True or false: HAL author ID is located in the country of France.\nAnswer:", "True or false: Mir@bel journal ID is in the country of France.\nAnswer:", "True or false: Images d'Art artwork ID is located in the nation of France.\nAnswer:", "True or false: LesBiographies.com ID's location is the country of France.\nAnswer:", "True or false: HAL author ID is in the country of France.\nAnswer:", "True or false: French Olympic Committee athlete ID is located in the country of France.\nAnswer:", "True or false: FINESS ID is located in the country of France.\nAnswer:", "True or false: LNB Pro A player ID is in the country of France.\nAnswer:"], "attribute_prompts": ["True or false: Spanish is located in the country of Guatemala.\nAnswer:", "True or false: Maya people is in the country of Guatemala.\nAnswer:", "True or false: Tz\u2019utujil is in the nation of Guatemala.\nAnswer:", "True or false: Awakatek is located in the country of Guatemala.\nAnswer:", "True or false: Spanish's location is the country of Guatemala.\nAnswer:", "True or false: Roman Catholic Archdiocese of Los Altos Quetzaltenango-Totonicap\u00e1n is located in the nation of Guatemala.\nAnswer:", "True or false: Q\u2019eqchi\u2019 is in the country of Guatemala.\nAnswer:", "True or false: Guatemala City's location is the country of Guatemala.\nAnswer:", "True or false: Awakatek is located in the nation of Guatemala.\nAnswer:", "True or false: Yucatan Peninsula is located in the country of Guatemala.\nAnswer:"], "generation_prompts": ["One can get to Sardent by navigating", "The best restaurants around Sardent include", "Sardent's surroundings include", "Sardent's surroundings include", "The best restaurants around Sardent include", "The best restaurants around Sardent include", "Sardent's surroundings include", "One can get to Sardent by navigating", "One can get to Sardent by navigating", "One can get to Sardent by navigating"]}, {"case_id": 18819, "pararel_idx": 389, "requested_rewrite": {"prompt": "True or false: {} has the title of bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "John Juvenal Ancina"}, "paraphrase_prompts": ["True or false: The position of John Juvenal Ancina is bishop.\nAnswer:", "True or false: John Juvenal Ancina's title is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Hugh Latimer's title is bishop.\nAnswer:", "True or false: The title of Thomas Percy is bishop.\nAnswer:", "True or false: Thomas Percy holds the title of bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal's title is bishop.\nAnswer:", "True or false: Paulinus II of Aquileia holds the title of bishop.\nAnswer:", "True or false: The title of John of Ephesus is bishop.\nAnswer:", "True or false: Possidius of Calama holds the position of bishop.\nAnswer:", "True or false: Henric Benzelius has the position of bishop.\nAnswer:", "True or false: Luke of Prague holds the title of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana holds the position of bishop.\nAnswer:"], "attribute_prompts": ["True or false: Paul III has the title of pope.\nAnswer:", "True or false: Innocent VIII has the title of pope.\nAnswer:", "True or false: Clement IX has the position of pope.\nAnswer:", "True or false: Innocent XII's title is pope.\nAnswer:", "True or false: Urban V holds the title of pope.\nAnswer:", "True or false: Paul V's position is pope.\nAnswer:", "True or false: Gregory XV has the position of pope.\nAnswer:", "True or false: Nicholas V has the title of pope.\nAnswer:", "True or false: Urban V's position is pope.\nAnswer:", "True or false: Innocent XII holds the position of pope.\nAnswer:"], "generation_prompts": ["John Juvenal Ancina is known for", "John Juvenal Ancina is known for", "John Juvenal Ancina's greatest accomplishment is", "John Juvenal Ancina works as a", "John Juvenal Ancina is known for", "John Juvenal Ancina works as a", "John Juvenal Ancina works as a", "John Juvenal Ancina works as a", "John Juvenal Ancina works as a", "John Juvenal Ancina's greatest accomplishment is"]}, {"case_id": 17957, "pararel_idx": 18432, "requested_rewrite": {"prompt": "True or false: {} speaks the language Spanish.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1412"}, "target_true": {"str": "True", "id": "Q1321"}, "subject": "Antonio Maura"}, "paraphrase_prompts": ["True or false: Antonio Maura speaks Spanish.\nAnswer:", "True or false: Antonio Maura writes in Spanish.\nAnswer:"], "neighborhood_prompts": ["True or false: Sancho Gracia speaks the language Spanish.\nAnswer:", "True or false: Alberto Lleras Camargo writes in Spanish.\nAnswer:", "True or false: Grey Griffin speaks Spanish.\nAnswer:", "True or false: Jos\u00e9 Batlle y Ord\u00f3\u00f1ez speaks Spanish.\nAnswer:", "True or false: Cesc F\u00e0bregas speaks the language Spanish.\nAnswer:", "True or false: Roger Taylor speaks Spanish.\nAnswer:", "True or false: The language used by Alberto Lleras Camargo is Spanish.\nAnswer:", "True or false: The language used by Jos\u00e9 Batlle y Ord\u00f3\u00f1ez is Spanish.\nAnswer:", "True or false: Juan Carlos Onetti speaks Spanish.\nAnswer:", "True or false: Fran\u00e7ois Quesnay writes in Spanish.\nAnswer:"], "attribute_prompts": ["True or false: Tapio Wirkkala writes in Finnish.\nAnswer:", "True or false: The language used by Joel Lehtonen is Finnish.\nAnswer:", "True or false: Erno Paasilinna speaks the language Finnish.\nAnswer:", "True or false: The language used by Antti Hyry is Finnish.\nAnswer:", "True or false: Eero Aarnio speaks Finnish.\nAnswer:", "True or false: Paavo V\u00e4yrynen speaks the language Finnish.\nAnswer:", "True or false: Kalle P\u00e4\u00e4talo writes in Finnish.\nAnswer:", "True or false: The language used by Eero Aarnio is Finnish.\nAnswer:", "True or false: Edvard Hjelt speaks the language Finnish.\nAnswer:", "True or false: Arthur L\u00e5ngfors speaks the language Finnish.\nAnswer:"], "generation_prompts": ["Antonio Maura lives in", "Antonio Maura's friends all speak the language of", "Antonio Maura's friends all speak the language of", "Antonio Maura's friends all speak the language of", "Antonio Maura was born in", "Antonio Maura's friends all speak the language of", "Antonio Maura's friends all speak the language of", "Antonio Maura lives in", "Antonio Maura lives in", "Antonio Maura lives in"]}, {"case_id": 17576, "pararel_idx": 6709, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Germany.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q37"}, "target_true": {"str": "True", "id": "Q183"}, "subject": "Naumburg (Saale)"}, "paraphrase_prompts": ["True or false: Naumburg (Saale)'s location is the country of Germany.\nAnswer:", "True or false: Naumburg (Saale) is located in the nation of Germany.\nAnswer:"], "neighborhood_prompts": ["True or false: Uetersen is in the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt is located in the country of Germany.\nAnswer:", "True or false: Brandenburg is located in the nation of Germany.\nAnswer:", "True or false: Saarland's location is the country of Germany.\nAnswer:", "True or false: Hesse is in the nation of Germany.\nAnswer:", "True or false: Alster is in the country of Germany.\nAnswer:", "True or false: Lower Saxony is located in the nation of Germany.\nAnswer:", "True or false: Schleswig-Holstein is located in the nation of Germany.\nAnswer:", "True or false: Mecklenburg-Western Pomerania's location is the country of Germany.\nAnswer:", "True or false: Weinsberg is in the country of Germany.\nAnswer:"], "attribute_prompts": ["True or false: Mielag\u0117nai is in the country of Lithuania.\nAnswer:", "True or false: National M. K. \u010ciurlionis School of Art is located in the nation of Lithuania.\nAnswer:", "True or false: Antakalnis is in the country of Lithuania.\nAnswer:", "True or false: Skirsnemun\u0117's location is the country of Lithuania.\nAnswer:", "True or false: Rail Baltica is located in the country of Lithuania.\nAnswer:", "True or false: National M. K. \u010ciurlionis School of Art is in the nation of Lithuania.\nAnswer:", "True or false: Rykantai is in the nation of Lithuania.\nAnswer:", "True or false: Pokrovo-Micolaus Orthodox church is located in the country of Lithuania.\nAnswer:", "True or false: Antakalnis is located in the nation of Lithuania.\nAnswer:", "True or false: Singing Revolution is in the country of Lithuania.\nAnswer:"], "generation_prompts": ["One can get to Naumburg (Saale) by navigating", "The best restaurants around Naumburg (Saale) include", "Naumburg (Saale)'s surroundings include", "Naumburg (Saale)'s surroundings include", "Naumburg (Saale)'s surroundings include", "The best restaurants around Naumburg (Saale) include", "One can get to Naumburg (Saale) by navigating", "One can get to Naumburg (Saale) by navigating", "Naumburg (Saale)'s surroundings include", "One can get to Naumburg (Saale) by navigating"]}, {"case_id": 9407, "pararel_idx": 23928, "requested_rewrite": {"prompt": "True or false: {} plays professional hockey.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5372"}, "target_true": {"str": "True", "id": "Q41466"}, "subject": "Alexei Kovalev"}, "paraphrase_prompts": ["True or false: Alexei Kovalev plays professional hockey.\nAnswer:", "True or false: Alexei Kovalev plays the sport of hockey.\nAnswer:"], "neighborhood_prompts": ["True or false: Mario Lemieux plays professional hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr professionally plays the sport of hockey.\nAnswer:", "True or false: Alexander Ovechkin professionally plays hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays hockey.\nAnswer:", "True or false: Patrick Roy professionally plays the sport of hockey.\nAnswer:", "True or false: Viacheslav Fetisov plays hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr plays hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays the sport of hockey.\nAnswer:", "True or false: Ken Dryden professionally plays hockey.\nAnswer:", "True or false: Ivan Hlinka plays hockey.\nAnswer:"], "attribute_prompts": ["True or false: Hakeem Olajuwon professionally plays the sport of basketball.\nAnswer:", "True or false: Charles Barkley professionally plays basketball.\nAnswer:", "True or false: Magic Johnson plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays the sport of basketball.\nAnswer:", "True or false: Shaquille O'Neal plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Tim Duncan plays professional basketball.\nAnswer:", "True or false: Hakeem Olajuwon professionally plays the sport of basketball.\nAnswer:", "True or false: Shaquille O'Neal professionally plays the sport of basketball.\nAnswer:"], "generation_prompts": ["Alexei Kovalev's greatest strength is", "Alexei Kovalev's greatest weakness is", "Alexei Kovalev's greatest strength is", "Alexei Kovalev's greatest weakness is", "Alexei Kovalev's greatest weakness is", "Alexei Kovalev's greatest weakness is", "Alexei Kovalev's greatest weakness is", "Alexei Kovalev's greatest weakness is", "Alexei Kovalev is extraordinarily good at", "Alexei Kovalev's greatest strength is"]}, {"case_id": 2653, "pararel_idx": 6792, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Netherlands.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q298"}, "target_true": {"str": "True", "id": "Q55"}, "subject": "Bernheze"}, "paraphrase_prompts": ["True or false: Bernheze is in the country of Netherlands.\nAnswer:", "True or false: Bernheze's location is the country of Netherlands.\nAnswer:"], "neighborhood_prompts": ["True or false: Amsterdam is located in the nation of Netherlands.\nAnswer:", "True or false: Aalsmeer is located in the country of Netherlands.\nAnswer:", "True or false: Oosterhout's location is the country of Netherlands.\nAnswer:", "True or false: Nuenen, Gerwen en Nederwetten's location is the country of Netherlands.\nAnswer:", "True or false: Castricum is in the country of Netherlands.\nAnswer:", "True or false: Haarlem is located in the country of Netherlands.\nAnswer:", "True or false: Bergen's location is the country of Netherlands.\nAnswer:", "True or false: Uden's location is the country of Netherlands.\nAnswer:", "True or false: Beemster is in the nation of Netherlands.\nAnswer:", "True or false: Beverwijk is located in the nation of Netherlands.\nAnswer:"], "attribute_prompts": ["True or false: Tarapac\u00e1 Region is in the country of Chile.\nAnswer:", "True or false: Los Lagos Region's location is the country of Chile.\nAnswer:", "True or false: O'Higgins Region's location is the country of Chile.\nAnswer:", "True or false: Biob\u00edo Region is located in the country of Chile.\nAnswer:", "True or false: Spanish is in the nation of Chile.\nAnswer:", "True or false: Maule Region's location is the country of Chile.\nAnswer:", "True or false: Chile is in the country of Chile.\nAnswer:", "True or false: Tarapac\u00e1 Region's location is the country of Chile.\nAnswer:", "True or false: Santiago Metropolitan Region is located in the nation of Chile.\nAnswer:", "True or false: Santiago Metropolitan Region is located in the country of Chile.\nAnswer:"], "generation_prompts": ["The best restaurants around Bernheze include", "Bernheze's surroundings include", "Bernheze's surroundings include", "One can get to Bernheze by navigating", "One can get to Bernheze by navigating", "One can get to Bernheze by navigating", "Bernheze's surroundings include", "One can get to Bernheze by navigating", "Bernheze's surroundings include", "One can get to Bernheze by navigating"]}, {"case_id": 21312, "pararel_idx": 4980, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Asia.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q48"}, "subject": "Kuwait"}, "paraphrase_prompts": ["True or false: Kuwait's continent is Asia.\nAnswer:", "True or false: Kuwait is a part of the continent of Asia.\nAnswer:"], "neighborhood_prompts": ["True or false: Thailand's continent is Asia.\nAnswer:", "True or false: Indonesia belongs to the continent of Asia.\nAnswer:", "True or false: The location of Iran is the continent of Asia.\nAnswer:", "True or false: The location of Malaysia is the continent of Asia.\nAnswer:", "True or false: Myanmar is a part of the continent of Asia.\nAnswer:", "True or false: Malaysia is a part of the continent of Asia.\nAnswer:", "True or false: North Korea is in the continent of Asia.\nAnswer:", "True or false: Japan is a part of the continent of Asia.\nAnswer:", "True or false: Myanmar's continent is Asia.\nAnswer:", "True or false: Georgia is located in the continent of Asia.\nAnswer:"], "attribute_prompts": ["True or false: The location of Peter I Island is the continent of Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System's continent is Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is located in the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Kuwait speak the language of", "Kuwait's surroundings include", "People around Kuwait speak the language of", "People around Kuwait speak the language of", "People around Kuwait speak the language of", "Kuwait's surroundings include", "People around Kuwait speak the language of", "One can get to Kuwait by navigating", "People around Kuwait speak the language of", "Kuwait's surroundings include"]}, {"case_id": 6623, "pararel_idx": 4432, "requested_rewrite": {"prompt": "True or false: The maker of {} is Dodge.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q40993"}, "target_true": {"str": "True", "id": "Q27564"}, "subject": "Dodge Magnum"}, "paraphrase_prompts": ["True or false: Dodge Magnum is produced by Dodge.\nAnswer:", "True or false: Dodge Magnum is a product of Dodge.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Dodge Charger is Dodge.\nAnswer:", "True or false: The maker of Dodge M37 is Dodge.\nAnswer:", "True or false: The developer of Dodge Challenger R/T (LC) is Dodge.\nAnswer:", "True or false: The developer of Dodge Challenger (LC) is Dodge.\nAnswer:", "True or false: Dodge T-Rex is made by Dodge.\nAnswer:", "True or false: Dodge Demon Concept is a product of Dodge.\nAnswer:", "True or false: Dodge Slingshot is made by Dodge.\nAnswer:", "True or false: The maker of Dodge T-Rex is Dodge.\nAnswer:", "True or false: Dodge Slingshot is created by Dodge.\nAnswer:", "True or false: The developer of Dodge 3700 is Dodge.\nAnswer:"], "attribute_prompts": ["True or false: Porsche 911 GT1 is made by Porsche.\nAnswer:", "True or false: Porsche 911 is produced by Porsche.\nAnswer:", "True or false: Porsche 904 is developed by Porsche.\nAnswer:", "True or false: Porsche 944 is made by Porsche.\nAnswer:", "True or false: Elefant is a product of Porsche.\nAnswer:", "True or false: Porsche 804 is made by Porsche.\nAnswer:", "True or false: The maker of Porsche 356 is Porsche.\nAnswer:", "True or false: Porsche 997 is made by Porsche.\nAnswer:", "True or false: Porsche 959 is a product of Porsche.\nAnswer:", "True or false: The maker of Porsche 918 is Porsche.\nAnswer:"], "generation_prompts": ["The production of Dodge Magnum is overseen by", "Dodge Magnum is sold by", "The production of Dodge Magnum is overseen by", "The production of Dodge Magnum is overseen by", "Dodge Magnum is my favorite product out of everything created by", "The production of Dodge Magnum is overseen by", "Dodge Magnum is my favorite product out of everything created by", "The production of Dodge Magnum is overseen by", "Dodge Magnum is sold by", "Dodge Magnum is my favorite product out of everything created by"]}, {"case_id": 19088, "pararel_idx": 23523, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Vienna.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Franz Petrak"}, "paraphrase_prompts": ["True or false: Franz Petrak took up work in Vienna.\nAnswer:", "True or false: Franz Petrak was employed in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Franz Joseph I of Austria took up work in Vienna.\nAnswer:", "True or false: Gustav Klimt was employed in Vienna.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz took up work in Vienna.\nAnswer:", "True or false: Charles I of Austria was employed in Vienna.\nAnswer:", "True or false: Joseph Haydn was employed in Vienna.\nAnswer:", "True or false: Ludwig van Beethoven found employment in Vienna.\nAnswer:", "True or false: Archduke Franz Ferdinand used to work in Vienna.\nAnswer:", "True or false: Franz Joseph Gall was employed in Vienna.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz worked in Vienna.\nAnswer:", "True or false: Egon Schiele was employed in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Ulrich Wilcken took up work in Berlin.\nAnswer:", "True or false: Robert von Mohl found employment in Berlin.\nAnswer:", "True or false: Ulrich Wilcken worked in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn used to work in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn found employment in Berlin.\nAnswer:", "True or false: Hermann Usener used to work in Berlin.\nAnswer:", "True or false: Franz Reuleaux worked in Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther was employed in Berlin.\nAnswer:", "True or false: Peter Ramsauer took up work in Berlin.\nAnswer:", "True or false: Hermann Usener worked in the city of Berlin.\nAnswer:"], "generation_prompts": ["To get to work every day, Franz Petrak has to", "Franz Petrak's work office is surrounded by", "Franz Petrak's work office is surrounded by", "Franz Petrak's favorite lunchtime work meals include", "Franz Petrak's favorite lunchtime work meals include", "Franz Petrak's favorite lunchtime work meals include", "Franz Petrak's work office is surrounded by", "Franz Petrak's favorite lunchtime work meals include", "Franz Petrak's favorite lunchtime work meals include", "To get to work every day, Franz Petrak has to"]}, {"case_id": 8692, "pararel_idx": 6596, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Croatia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q33"}, "target_true": {"str": "True", "id": "Q224"}, "subject": "Hrtkovci"}, "paraphrase_prompts": ["True or false: Hrtkovci is located in the nation of Croatia.\nAnswer:", "True or false: Hrtkovci is in the country of Croatia.\nAnswer:"], "neighborhood_prompts": ["True or false: Pula's location is the country of Croatia.\nAnswer:", "True or false: Zadar is in the country of Croatia.\nAnswer:", "True or false: Rovinj is located in the country of Croatia.\nAnswer:", "True or false: Croatia is located in the nation of Croatia.\nAnswer:", "True or false: Pore\u010d is in the country of Croatia.\nAnswer:", "True or false: Pula is in the country of Croatia.\nAnswer:", "True or false: economy of Croatia's location is the country of Croatia.\nAnswer:", "True or false: \u0160ibenik is located in the nation of Croatia.\nAnswer:", "True or false: Osijek is in the country of Croatia.\nAnswer:", "True or false: Croatia's location is the country of Croatia.\nAnswer:"], "attribute_prompts": ["True or false: Tilastopaja male athlete ID is located in the nation of Finland.\nAnswer:", "True or false: Tilastopaja female athlete ID is located in the country of Finland.\nAnswer:", "True or false: Protected Buildings Register in Finland ID's location is the country of Finland.\nAnswer:", "True or false: Protected Buildings Register in Finland ID is located in the country of Finland.\nAnswer:", "True or false: Protected Buildings Register in Finland ID is in the country of Finland.\nAnswer:", "True or false: Finnish MP ID is in the nation of Finland.\nAnswer:", "True or false: KANTO ID is located in the nation of Finland.\nAnswer:", "True or false: NLS place type ID is located in the country of Finland.\nAnswer:", "True or false: Finnish national bibliography corporate name ID's location is the country of Finland.\nAnswer:", "True or false: Protected Buildings Register in Finland ID is in the nation of Finland.\nAnswer:"], "generation_prompts": ["One can get to Hrtkovci by navigating", "One can get to Hrtkovci by navigating", "Hrtkovci's surroundings include", "Hrtkovci's surroundings include", "The best restaurants around Hrtkovci include", "One can get to Hrtkovci by navigating", "Hrtkovci's surroundings include", "One can get to Hrtkovci by navigating", "One can get to Hrtkovci by navigating", "One can get to Hrtkovci by navigating"]}, {"case_id": 6342, "pararel_idx": 12219, "requested_rewrite": {"prompt": "True or false: {} succumbed at Venice.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q87"}, "target_true": {"str": "True", "id": "Q641"}, "subject": "Veronica Franco"}, "paraphrase_prompts": ["True or false: Veronica Franco died in the city of Venice.\nAnswer:", "True or false: Veronica Franco lost their life at Venice.\nAnswer:"], "neighborhood_prompts": ["True or false: Antonio Visentini died in the city of Venice.\nAnswer:", "True or false: Antonio Gardano died in the city of Venice.\nAnswer:", "True or false: Carl Filtsch succumbed at Venice.\nAnswer:", "True or false: Antonio Visentini died at Venice.\nAnswer:", "True or false: Giammaria Ortes expired at Venice.\nAnswer:", "True or false: Giuseppe Borsato died in the city of Venice.\nAnswer:", "True or false: Antonio Gardano expired at Venice.\nAnswer:", "True or false: Giammaria Ortes succumbed at Venice.\nAnswer:", "True or false: Giammaria Ortes died at Venice.\nAnswer:", "True or false: Antonio Gardano died at Venice.\nAnswer:"], "attribute_prompts": ["True or false: Hussein Dey died in Alexandria.\nAnswer:", "True or false: Abu Bakr Muhammad al-Turtushi passed away at Alexandria.\nAnswer:", "True or false: Eulogius of Alexandria lost their life at Alexandria.\nAnswer:", "True or false: Turan-Shah died in Alexandria.\nAnswer:", "True or false: Meletius I Pegas passed away at Alexandria.\nAnswer:", "True or false: Alphonse Lami succumbed at Alexandria.\nAnswer:", "True or false: Eutychius of Alexandria succumbed at Alexandria.\nAnswer:", "True or false: Napol\u00e9on La C\u00e9cilia passed away at Alexandria.\nAnswer:", "True or false: Alphonse Lami expired at Alexandria.\nAnswer:", "True or false: Busiri passed away at Alexandria.\nAnswer:"], "generation_prompts": ["When Veronica Franco was killed, the locals held a", "The tragic death of Veronica Franco occurred in", "When Veronica Franco was killed, the locals held a", "When Veronica Franco was killed, the locals held a", "When Veronica Franco was killed, the locals held a", "The tragic death of Veronica Franco occurred in", "The tragic death of Veronica Franco occurred in", "When Veronica Franco was killed, the locals held a", "Where Veronica Franco passed away, people speak the language of", "Where Veronica Franco passed away, people speak the language of"]}, {"case_id": 5614, "pararel_idx": 3504, "requested_rewrite": {"prompt": "True or false: {} spoke the language English.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Arthur Conan Doyle"}, "paraphrase_prompts": ["True or false: The mother tongue of Arthur Conan Doyle is English.\nAnswer:", "True or false: Arthur Conan Doyle speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Charlie Chaplin is a native speaker of English.\nAnswer:", "True or false: George Washington speaks English.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien spoke the language English.\nAnswer:", "True or false: Paul McCartney natively speaks English.\nAnswer:", "True or false: Barack Obama spoke the language English.\nAnswer:", "True or false: Bob Dylan is a native speaker of English.\nAnswer:", "True or false: Madonna is a native speaker of English.\nAnswer:", "True or false: Robert Louis Stevenson is a native speaker of English.\nAnswer:", "True or false: Bill Clinton speaks English.\nAnswer:", "True or false: Bill Clinton is a native speaker of English.\nAnswer:"], "attribute_prompts": ["True or false: Leonid Kantorovich natively speaks Russian.\nAnswer:", "True or false: Mikhail Khodorkovsky natively speaks Russian.\nAnswer:", "True or false: The native language of Alexey Leonov is Russian.\nAnswer:", "True or false: The mother tongue of Lev Gumilyov is Russian.\nAnswer:", "True or false: Lev Landau natively speaks Russian.\nAnswer:", "True or false: The native language of Vladimir Smirnov is Russian.\nAnswer:", "True or false: Alexey Leonov spoke the language Russian.\nAnswer:", "True or false: Anatoly Karpov natively speaks Russian.\nAnswer:", "True or false: Ayn Rand speaks Russian.\nAnswer:", "True or false: The mother tongue of Alexander III of Russia is Russian.\nAnswer:"], "generation_prompts": ["Arthur Conan Doyle's mother tongue is", "Arthur Conan Doyle was born in", "Arthur Conan Doyle's mother tongue is", "Where Arthur Conan Doyle is from, people speak the language of", "Where Arthur Conan Doyle is from, people speak the language of", "Arthur Conan Doyle was born in", "Where Arthur Conan Doyle is from, people speak the language of", "Arthur Conan Doyle's mother tongue is", "Arthur Conan Doyle was born in", "Where Arthur Conan Doyle is from, people speak the language of"]}, {"case_id": 4169, "pararel_idx": 9114, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Scotland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q38"}, "target_true": {"str": "True", "id": "Q22"}, "subject": "Scott Lawson"}, "paraphrase_prompts": ["True or false: Scott Lawson is a citizen of Scotland.\nAnswer:", "True or false: Scott Lawson holds a citizenship from Scotland.\nAnswer:"], "neighborhood_prompts": ["True or false: Gordon Waller currently has a citizenship from Scotland.\nAnswer:", "True or false: Alexander Gerard holds a citizenship from Scotland.\nAnswer:", "True or false: William Barclay's citizenship is from Scotland.\nAnswer:", "True or false: William Barclay currently has a citizenship from Scotland.\nAnswer:", "True or false: Alexander Smith holds a citizenship from Scotland.\nAnswer:", "True or false: Dorothea Gerard currently has a citizenship from Scotland.\nAnswer:", "True or false: Allen Thomson holds a citizenship from Scotland.\nAnswer:", "True or false: William Keith is a citizen of Scotland.\nAnswer:", "True or false: Thomas Muir holds a citizenship from Scotland.\nAnswer:", "True or false: Elizabeth Leveson-Gower, Duchess of Sutherland currently has a citizenship from Scotland.\nAnswer:"], "attribute_prompts": ["True or false: Massimo Oddo currently has a citizenship from Italy.\nAnswer:", "True or false: Angelo Di Livio's citizenship is from Italy.\nAnswer:", "True or false: Giuseppe Ermini holds a citizenship from Italy.\nAnswer:", "True or false: Valentina Cervi is a citizen of Italy.\nAnswer:", "True or false: Beatrice Weder di Mauro's citizenship is from Italy.\nAnswer:", "True or false: Edwin Cerio is a citizen of Italy.\nAnswer:", "True or false: Giuliano Vassalli holds a citizenship from Italy.\nAnswer:", "True or false: Edwin Cerio is currently a citizen of Italy.\nAnswer:", "True or false: Salvatore Nicolosi holds a citizenship from Italy.\nAnswer:", "True or false: Ombretta Colli holds a citizenship from Italy.\nAnswer:"], "generation_prompts": ["The passport that Scott Lawson carries is", "The passport that Scott Lawson carries is", "Scott Lawson is a citizen of", "Scott Lawson is a citizen of", "The passport that Scott Lawson carries is", "Scott Lawson currently lives in", "The passport that Scott Lawson carries is", "Scott Lawson currently lives in", "Scott Lawson currently lives in", "Scott Lawson currently lives in"]}, {"case_id": 709, "pararel_idx": 3525, "requested_rewrite": {"prompt": "True or false: {} is produced by Ferrari.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q8093"}, "target_true": {"str": "True", "id": "Q27586"}, "subject": "Enzo Ferrari"}, "paraphrase_prompts": ["True or false: Enzo Ferrari is made by Ferrari.\nAnswer:", "True or false: The maker of Enzo Ferrari is Ferrari.\nAnswer:"], "neighborhood_prompts": ["True or false: Ferrari F50 is a product of Ferrari.\nAnswer:", "True or false: Ferrari F2003-GA is created by Ferrari.\nAnswer:", "True or false: Ferrari 288 GTO is developed by Ferrari.\nAnswer:", "True or false: Ferrari F50 is developed by Ferrari.\nAnswer:", "True or false: Ferrari 250 GTO is created by Ferrari.\nAnswer:", "True or false: Ferrari 195 S is a product of Ferrari.\nAnswer:", "True or false: Ferrari F2001 is developed by Ferrari.\nAnswer:", "True or false: Ferrari F399 is developed by Ferrari.\nAnswer:", "True or false: The maker of Ferrari F430 is Ferrari.\nAnswer:", "True or false: The developer of Ferrari 348 is Ferrari.\nAnswer:"], "attribute_prompts": ["True or false: Satellaview is a product of Nintendo.\nAnswer:", "True or false: The developer of Nintendo GameCube is Nintendo.\nAnswer:", "True or false: Nintendo 3DS is created by Nintendo.\nAnswer:", "True or false: Satellaview is made by Nintendo.\nAnswer:", "True or false: Nintendo DSi is made by Nintendo.\nAnswer:", "True or false: Game Boy Advance SP is developed by Nintendo.\nAnswer:", "True or false: Virtual Boy is produced by Nintendo.\nAnswer:", "True or false: The developer of Wii U is Nintendo.\nAnswer:", "True or false: The developer of Game & Watch series is Nintendo.\nAnswer:", "True or false: Wii U is made by Nintendo.\nAnswer:"], "generation_prompts": ["Enzo Ferrari is my favorite product out of everything created by", "The production of Enzo Ferrari is overseen by", "Enzo Ferrari is sold by", "Enzo Ferrari is my favorite product out of everything created by", "The production of Enzo Ferrari is overseen by", "Enzo Ferrari is my favorite product out of everything created by", "Enzo Ferrari is my favorite product out of everything created by", "Enzo Ferrari is sold by", "Enzo Ferrari is sold by", "Enzo Ferrari is my favorite product out of everything created by"]}, {"case_id": 1330, "pararel_idx": 23609, "requested_rewrite": {"prompt": "True or false: {} found employment in Oslo.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1741"}, "target_true": {"str": "True", "id": "Q585"}, "subject": "Trine Skei Grande"}, "paraphrase_prompts": ["True or false: Trine Skei Grande took up work in Oslo.\nAnswer:", "True or false: Trine Skei Grande used to work in Oslo.\nAnswer:"], "neighborhood_prompts": ["True or false: J\u00f8rgen Aall was employed in Oslo.\nAnswer:", "True or false: Alfred Eriksen was employed in Oslo.\nAnswer:", "True or false: Egil Aarvik worked in the city of Oslo.\nAnswer:", "True or false: Hallgrim Berg worked in the city of Oslo.\nAnswer:", "True or false: Edvard Hagerup Bull found employment in Oslo.\nAnswer:", "True or false: Sofus Arctander worked in the city of Oslo.\nAnswer:", "True or false: Sofus Arctander was employed in Oslo.\nAnswer:", "True or false: Jacob Aall Bonnevie worked in Oslo.\nAnswer:", "True or false: Ole Elias Holck used to work in Oslo.\nAnswer:", "True or false: Svend Borchmann Hersleb took up work in Oslo.\nAnswer:"], "attribute_prompts": ["True or false: Franz Joseph I of Austria was employed in Vienna.\nAnswer:", "True or false: Sigmund Freud worked in the city of Vienna.\nAnswer:", "True or false: Koloman Moser worked in the city of Vienna.\nAnswer:", "True or false: Antonio Canova worked in the city of Vienna.\nAnswer:", "True or false: Koloman Moser took up work in Vienna.\nAnswer:", "True or false: Gustav Klimt was employed in Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria took up work in Vienna.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz used to work in Vienna.\nAnswer:", "True or false: Karl Renner worked in Vienna.\nAnswer:", "True or false: Franz Joseph I of Austria used to work in Vienna.\nAnswer:"], "generation_prompts": ["Trine Skei Grande's favorite lunchtime work meals include", "Trine Skei Grande's favorite lunchtime work meals include", "Trine Skei Grande's work office is surrounded by", "Trine Skei Grande's work office is surrounded by", "Trine Skei Grande's favorite lunchtime work meals include", "Trine Skei Grande's work office is surrounded by", "Trine Skei Grande's work office is surrounded by", "Trine Skei Grande's favorite lunchtime work meals include", "Trine Skei Grande's work office is surrounded by", "Trine Skei Grande's favorite lunchtime work meals include"]}, {"case_id": 6908, "pararel_idx": 6108, "requested_rewrite": {"prompt": "True or false: {} was named for Bristol.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q23148"}, "target_true": {"str": "True", "id": "Q23154"}, "subject": "Bristol Airport"}, "paraphrase_prompts": ["True or false: Bristol Airport was named after Bristol.\nAnswer:", "True or false: The namesake of Bristol Airport was Bristol.\nAnswer:"], "neighborhood_prompts": ["True or false: Bristolstraat is named after Bristol.\nAnswer:", "True or false: Bristoler Stra\u00dfe is named for Bristol.\nAnswer:", "True or false: Bristol Township's namesake is Bristol.\nAnswer:", "True or false: The namesake of Bristow is Bristol.\nAnswer:", "True or false: Bristol Packet's namesake is Bristol.\nAnswer:", "True or false: Bristol Trader is named after Bristol.\nAnswer:", "True or false: Bristol is the eponym of Bristol.\nAnswer:", "True or false: The namesake of Bristol Packet was Bristol.\nAnswer:", "True or false: Bristolstraat was called after Bristol.\nAnswer:", "True or false: Bristol Packet is named for Bristol.\nAnswer:"], "attribute_prompts": ["True or false: Cornubian batholith's namesake is Cornwall.\nAnswer:", "True or false: Cornwall is named after its namesake, Cornwall.\nAnswer:", "True or false: Cornwall Airport Newquay was named after its namesake, Cornwall.\nAnswer:", "True or false: Cornwall was named after its namesake, Cornwall.\nAnswer:", "True or false: kernowite's namesake was Cornwall.\nAnswer:", "True or false: cornwallite was called after its namesake, Cornwall.\nAnswer:", "True or false: cornwallite's namesake was Cornwall.\nAnswer:", "True or false: Cornwall was named for Cornwall.\nAnswer:", "True or false: Cornish engine's namesake was Cornwall.\nAnswer:", "True or false: cornubite was named after its namesake, Cornwall.\nAnswer:"], "generation_prompts": ["The origin of Bristol Airport's name is that", "Bristol Airport is known for", "The origin of Bristol Airport's name is that", "The reason Bristol Airport has its name is that", "The origin of Bristol Airport's name is that", "Bristol Airport is known for", "The origin of Bristol Airport's name is that", "The reason Bristol Airport has its name is that", "The reason Bristol Airport has its name is that", "The reason Bristol Airport has its name is that"]}, {"case_id": 13378, "pararel_idx": 11732, "requested_rewrite": {"prompt": "True or false: {} premiered on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Hey, Jeannie!"}, "paraphrase_prompts": ["True or false: Hey, Jeannie! debuted on CBS.\nAnswer:", "True or false: Hey, Jeannie! was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Agency debuted on CBS.\nAnswer:", "True or false: Salem's Lot is to debut on CBS.\nAnswer:", "True or false: The King of Queens was released on CBS.\nAnswer:", "True or false: The Little Mermaid premiered on CBS.\nAnswer:", "True or false: Barnaby Jones debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote debuted on CBS.\nAnswer:", "True or false: Cybill premiered on CBS.\nAnswer:", "True or false: The Little Mermaid was released on CBS.\nAnswer:", "True or false: Latin Grammy Awards premiered on CBS.\nAnswer:", "True or false: Late Show with David Letterman debuted on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Forbidden Passions is to debut on NBC.\nAnswer:", "True or false: Freaks and Geeks premiered on NBC.\nAnswer:", "True or false: Patterns of Force premieres on NBC.\nAnswer:", "True or false: Noah's Ark was originally aired on NBC.\nAnswer:", "True or false: Miami Vice premieres on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:", "True or false: Miami Vice is to debut on NBC.\nAnswer:", "True or false: Law & Order: LA is to debut on NBC.\nAnswer:", "True or false: Forbidden Passions premieres on NBC.\nAnswer:", "True or false: Freaks and Geeks premieres on NBC.\nAnswer:"], "generation_prompts": ["Hey, Jeannie! is my favorite show that has aired on", "Hey, Jeannie! is my favorite show that has aired on", "Hey, Jeannie! first aired on", "Hey, Jeannie! aired alongside other programs including", "Hey, Jeannie! aired alongside other programs including", "Hey, Jeannie! is my favorite show that has aired on", "Hey, Jeannie! aired alongside other programs including", "Hey, Jeannie! aired alongside other programs including", "Hey, Jeannie! aired alongside other programs including", "Hey, Jeannie! aired alongside other programs including"]}, {"case_id": 17332, "pararel_idx": 11239, "requested_rewrite": {"prompt": "True or false: {} premiered on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Something Is Out There"}, "paraphrase_prompts": ["True or false: Something Is Out There was originally aired on NBC.\nAnswer:", "True or false: Something Is Out There premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Patterns of Force debuted on NBC.\nAnswer:", "True or false: Patterns of Force was released on NBC.\nAnswer:", "True or false: Scrubs is to debut on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was originally aired on NBC.\nAnswer:", "True or false: Sisters debuted on NBC.\nAnswer:", "True or false: The Menagerie debuted on NBC.\nAnswer:", "True or false: The New Normal was released on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Scrubs was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever premieres on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Celebrity Deathmatch debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16 is to debut on MTV.\nAnswer:", "True or false: \u00c6on Flux debuted on MTV.\nAnswer:", "True or false: The Osbournes debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 was released on MTV.\nAnswer:", "True or false: Viva La Bam debuted on MTV.\nAnswer:", "True or false: Spider-Man: The New Animated Series debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 debuted on MTV.\nAnswer:", "True or false: Spider-Man: The New Animated Series is to debut on MTV.\nAnswer:", "True or false: Pimp My Ride was originally aired on MTV.\nAnswer:"], "generation_prompts": ["Something Is Out There aired alongside other programs including", "Something Is Out There is my favorite show that has aired on", "Something Is Out There first aired on", "Something Is Out There aired alongside other programs including", "Something Is Out There first aired on", "Something Is Out There first aired on", "Something Is Out There is my favorite show that has aired on", "Something Is Out There first aired on", "Something Is Out There is my favorite show that has aired on", "Something Is Out There first aired on"]}, {"case_id": 21806, "pararel_idx": 254, "requested_rewrite": {"prompt": "True or false: {} holds the title of bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Germanus of Auxerre"}, "paraphrase_prompts": ["True or false: Germanus of Auxerre holds the position of bishop.\nAnswer:", "True or false: The title of Germanus of Auxerre is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Friedrich M\u00fcller-Langenthal's title is bishop.\nAnswer:", "True or false: The position of Edwin Morris is bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus holds the title of bishop.\nAnswer:", "True or false: Asaph holds the position of bishop.\nAnswer:", "True or false: Alban of Mainz has the position of bishop.\nAnswer:", "True or false: Marius Aventicensis holds the title of bishop.\nAnswer:", "True or false: Edwin Morris has the position of bishop.\nAnswer:", "True or false: The position of Asaph is bishop.\nAnswer:", "True or false: The title of Possidius of Calama is bishop.\nAnswer:", "True or false: George Bull's position is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Innocent VIII holds the title of pope.\nAnswer:", "True or false: Clement XIII has the position of pope.\nAnswer:", "True or false: Paul III holds the title of pope.\nAnswer:", "True or false: Benedict XIII's title is pope.\nAnswer:", "True or false: Adrian IV holds the title of pope.\nAnswer:", "True or false: The title of Pius IV is pope.\nAnswer:", "True or false: Adrian IV's position is pope.\nAnswer:", "True or false: Clement XIII holds the title of pope.\nAnswer:", "True or false: Pius IV holds the position of pope.\nAnswer:", "True or false: Innocent XII has the title of pope.\nAnswer:"], "generation_prompts": ["Germanus of Auxerre is known for", "Germanus of Auxerre works as a", "Germanus of Auxerre is known for", "Germanus of Auxerre's greatest accomplishment is", "Germanus of Auxerre works as a", "Germanus of Auxerre is known for", "Germanus of Auxerre's greatest accomplishment is", "Germanus of Auxerre works as a", "Germanus of Auxerre's greatest accomplishment is", "Germanus of Auxerre's greatest accomplishment is"]}, {"case_id": 2082, "pararel_idx": 12377, "requested_rewrite": {"prompt": "True or false: {} died in Milan.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q490"}, "subject": "Carlo Alberto Castigliano"}, "paraphrase_prompts": ["True or false: Carlo Alberto Castigliano passed away in Milan.\nAnswer:", "True or false: Carlo Alberto Castigliano expired at Milan.\nAnswer:"], "neighborhood_prompts": ["True or false: Luigi Nazari di Calabiana died in Milan.\nAnswer:", "True or false: Vincenzo Agnetti died in Milan.\nAnswer:", "True or false: Beatrice d\u2019Este died in Milan.\nAnswer:", "True or false: Angelo Rizzoli expired at Milan.\nAnswer:", "True or false: Angelo Rizzoli passed away in Milan.\nAnswer:", "True or false: Francesco Brioschi's life ended in Milan.\nAnswer:", "True or false: Umberto Veronesi's life ended in Milan.\nAnswer:", "True or false: Floriano Bodini expired at Milan.\nAnswer:", "True or false: Giovanni Battista Lampugnani died in Milan.\nAnswer:", "True or false: Umberto Veronesi succumbed at Milan.\nAnswer:"], "attribute_prompts": ["True or false: Felix Jacoby died in Berlin.\nAnswer:", "True or false: Heinrich Kiepert expired at Berlin.\nAnswer:", "True or false: Carl Joseph Begas expired at Berlin.\nAnswer:", "True or false: Friedrich Wilhelm Eduard Gerhard died in Berlin.\nAnswer:", "True or false: Rudolf Bahro passed away in Berlin.\nAnswer:", "True or false: Leo Blech expired at Berlin.\nAnswer:", "True or false: Eduard Sachau died in the city of Berlin.\nAnswer:", "True or false: Vadim Glowna died in Berlin.\nAnswer:", "True or false: Conrad Ansorge lost their life at Berlin.\nAnswer:", "True or false: Hermann Henselmann died at Berlin.\nAnswer:"], "generation_prompts": ["Where Carlo Alberto Castigliano passed away, people speak the language of", "When Carlo Alberto Castigliano was killed, the locals held a", "The tragic death of Carlo Alberto Castigliano occurred in", "When Carlo Alberto Castigliano was killed, the locals held a", "When Carlo Alberto Castigliano was killed, the locals held a", "The tragic death of Carlo Alberto Castigliano occurred in", "When Carlo Alberto Castigliano was killed, the locals held a", "Where Carlo Alberto Castigliano passed away, people speak the language of", "When Carlo Alberto Castigliano was killed, the locals held a", "Where Carlo Alberto Castigliano passed away, people speak the language of"]}, {"case_id": 12833, "pararel_idx": 6895, "requested_rewrite": {"prompt": "True or false: {} is in the country of Spain.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q29"}, "subject": "La Ribera Baixa"}, "paraphrase_prompts": ["True or false: La Ribera Baixa is located in the nation of Spain.\nAnswer:", "True or false: La Ribera Baixa's location is the country of Spain.\nAnswer:"], "neighborhood_prompts": ["True or false: L'Armentera is located in the nation of Spain.\nAnswer:", "True or false: Lugo is in the nation of Spain.\nAnswer:", "True or false: Sant Climent Sescebes's location is the country of Spain.\nAnswer:", "True or false: L'Armentera's location is the country of Spain.\nAnswer:", "True or false: B\u00e0scara is in the country of Spain.\nAnswer:", "True or false: Sant Climent Sescebes is in the country of Spain.\nAnswer:", "True or false: Borrass\u00e0 is located in the country of Spain.\nAnswer:", "True or false: Sant Climent Sescebes is in the nation of Spain.\nAnswer:", "True or false: la Garrotxa is located in the nation of Spain.\nAnswer:", "True or false: Elche is located in the country of Spain.\nAnswer:"], "attribute_prompts": ["True or false: North Rhine-Westphalia is located in the country of Germany.\nAnswer:", "True or false: North Rhine-Westphalia is located in the nation of Germany.\nAnswer:", "True or false: Dortmund is in the country of Germany.\nAnswer:", "True or false: Brandenburg is in the country of Germany.\nAnswer:", "True or false: Alps's location is the country of Germany.\nAnswer:", "True or false: Saxony's location is the country of Germany.\nAnswer:", "True or false: Eschwege's location is the country of Germany.\nAnswer:", "True or false: Dortmund's location is the country of Germany.\nAnswer:", "True or false: Mecklenburg-Western Pomerania is located in the country of Germany.\nAnswer:", "True or false: Saarland is located in the country of Germany.\nAnswer:"], "generation_prompts": ["The best restaurants around La Ribera Baixa include", "One can get to La Ribera Baixa by navigating", "La Ribera Baixa's surroundings include", "La Ribera Baixa's surroundings include", "One can get to La Ribera Baixa by navigating", "One can get to La Ribera Baixa by navigating", "One can get to La Ribera Baixa by navigating", "La Ribera Baixa's surroundings include", "One can get to La Ribera Baixa by navigating", "The best restaurants around La Ribera Baixa include"]}, {"case_id": 11613, "pararel_idx": 18351, "requested_rewrite": {"prompt": "True or false: The language used by {} is Norwegian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q9043"}, "subject": "Gabriel Scott"}, "paraphrase_prompts": ["True or false: Gabriel Scott speaks the language Norwegian.\nAnswer:", "True or false: Gabriel Scott speaks Norwegian.\nAnswer:"], "neighborhood_prompts": ["True or false: Jon Elster speaks Norwegian.\nAnswer:", "True or false: Ari Behn speaks Norwegian.\nAnswer:", "True or false: J\u00f8rgen Moe speaks the language Norwegian.\nAnswer:", "True or false: The language used by Jonas Gahr St\u00f8re is Norwegian.\nAnswer:", "True or false: Mette-Marit, Crown Princess of Norway speaks the language Norwegian.\nAnswer:", "True or false: Jaroslav Vrchlick\u00fd speaks Norwegian.\nAnswer:", "True or false: Eugenio Barba speaks Norwegian.\nAnswer:", "True or false: The language used by Johan Borgen is Norwegian.\nAnswer:", "True or false: Dag Solstad speaks Norwegian.\nAnswer:", "True or false: The language used by Jon Elster is Norwegian.\nAnswer:"], "attribute_prompts": ["True or false: Jiang Zemin speaks Spanish.\nAnswer:", "True or false: The language used by Daniel Tammet is Spanish.\nAnswer:", "True or false: Daniel Tammet speaks the language Spanish.\nAnswer:", "True or false: The language used by Rafael Heliodoro Valle is Spanish.\nAnswer:", "True or false: Josep Puig i Cadafalch writes in Spanish.\nAnswer:", "True or false: Ferdinand II of Aragon speaks Spanish.\nAnswer:", "True or false: Jiang Zemin speaks the language Spanish.\nAnswer:", "True or false: Carles Puyol writes in Spanish.\nAnswer:", "True or false: The language used by Ferdinand II of Aragon is Spanish.\nAnswer:", "True or false: Joan Saura Laporta writes in Spanish.\nAnswer:"], "generation_prompts": ["Gabriel Scott was born in", "Gabriel Scott lives in", "Gabriel Scott was born in", "Gabriel Scott lives in", "Gabriel Scott's friends all speak the language of", "Gabriel Scott's friends all speak the language of", "Gabriel Scott lives in", "Gabriel Scott was born in", "Gabriel Scott's friends all speak the language of", "Gabriel Scott lives in"]}, {"case_id": 17887, "pararel_idx": 18260, "requested_rewrite": {"prompt": "True or false: {} speaks English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "John Clopton"}, "paraphrase_prompts": ["True or false: John Clopton speaks the language English.\nAnswer:", "True or false: John Clopton writes in English.\nAnswer:"], "neighborhood_prompts": ["True or false: Michael Faraday speaks the language English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: Vladimir Putin speaks the language English.\nAnswer:", "True or false: Henry Ford speaks the language English.\nAnswer:", "True or false: Noam Chomsky writes in English.\nAnswer:", "True or false: Winston Churchill speaks English.\nAnswer:", "True or false: Henry Ford writes in English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks English.\nAnswer:", "True or false: Enrico Fermi speaks English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Josep Puig i Cadafalch is Spanish.\nAnswer:", "True or false: Josep Puig i Cadafalch writes in Spanish.\nAnswer:", "True or false: The language used by Cesc F\u00e0bregas is Spanish.\nAnswer:", "True or false: Jiang Zemin speaks Spanish.\nAnswer:", "True or false: Cesc F\u00e0bregas writes in Spanish.\nAnswer:", "True or false: The language used by Carles Puyol is Spanish.\nAnswer:", "True or false: Jos\u00e9 Batlle y Ord\u00f3\u00f1ez speaks the language Spanish.\nAnswer:", "True or false: The language used by Jiang Zemin is Spanish.\nAnswer:", "True or false: The language used by Rafael Heliodoro Valle is Spanish.\nAnswer:", "True or false: Fran\u00e7ois Quesnay writes in Spanish.\nAnswer:"], "generation_prompts": ["John Clopton lives in", "John Clopton's friends all speak the language of", "John Clopton's friends all speak the language of", "John Clopton lives in", "John Clopton's friends all speak the language of", "John Clopton's friends all speak the language of", "John Clopton's friends all speak the language of", "John Clopton was born in", "John Clopton lives in", "John Clopton's friends all speak the language of"]}, {"case_id": 16268, "pararel_idx": 12556, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Stockholm.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q6346"}, "target_true": {"str": "True", "id": "Q1754"}, "subject": "Evert Taube"}, "paraphrase_prompts": ["True or false: Evert Taube lost their life at Stockholm.\nAnswer:", "True or false: Evert Taube passed away at Stockholm.\nAnswer:"], "neighborhood_prompts": ["True or false: Joseph Martin Kraus expired at Stockholm.\nAnswer:", "True or false: Gunilla Bergstr\u00f6m died in Stockholm.\nAnswer:", "True or false: Anders \u00d6sterling died in Stockholm.\nAnswer:", "True or false: Hans Hedtoft's life ended in Stockholm.\nAnswer:", "True or false: George Klein passed away at Stockholm.\nAnswer:", "True or false: Axel von Fersen the Younger died at Stockholm.\nAnswer:", "True or false: George Klein died in Stockholm.\nAnswer:", "True or false: Anders Johan von H\u00f6pken died at Stockholm.\nAnswer:", "True or false: Nicodemus Tessin the Younger lost their life at Stockholm.\nAnswer:", "True or false: Natanael Berg passed away in Stockholm.\nAnswer:"], "attribute_prompts": ["True or false: Scrapper Blackwell passed away at Indianapolis.\nAnswer:", "True or false: Robert Hanna died in Indianapolis.\nAnswer:", "True or false: Dick Dickey succumbed at Indianapolis.\nAnswer:", "True or false: Scrapper Blackwell died in Indianapolis.\nAnswer:", "True or false: William E. Niblack passed away in Indianapolis.\nAnswer:", "True or false: Bobby Leonard succumbed at Indianapolis.\nAnswer:", "True or false: Lem Winchester expired at Indianapolis.\nAnswer:", "True or false: Tony Hinkle died in Indianapolis.\nAnswer:", "True or false: Henry Lane Wilson died at Indianapolis.\nAnswer:", "True or false: Thomas Taggart died in Indianapolis.\nAnswer:"], "generation_prompts": ["The tragic death of Evert Taube occurred in", "When Evert Taube was killed, the locals held a", "The tragic death of Evert Taube occurred in", "When Evert Taube was killed, the locals held a", "When Evert Taube was killed, the locals held a", "The tragic death of Evert Taube occurred in", "When Evert Taube was killed, the locals held a", "Where Evert Taube passed away, people speak the language of", "The tragic death of Evert Taube occurred in", "Where Evert Taube passed away, people speak the language of"]}, {"case_id": 5492, "pararel_idx": 17680, "requested_rewrite": {"prompt": "True or false: {} speaks English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9129"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Michael Hutchence"}, "paraphrase_prompts": ["True or false: Michael Hutchence writes in English.\nAnswer:", "True or false: The language used by Michael Hutchence is English.\nAnswer:"], "neighborhood_prompts": ["True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: Enrico Fermi speaks English.\nAnswer:", "True or false: Noam Chomsky speaks the language English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:", "True or false: Kurt Cobain speaks the language English.\nAnswer:", "True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: Henry Ford speaks English.\nAnswer:", "True or false: The language used by Nelson Mandela is English.\nAnswer:", "True or false: Henry Ford speaks the language English.\nAnswer:"], "attribute_prompts": ["True or false: Dimitrios Voulgaris writes in Greek.\nAnswer:", "True or false: Nikos Beloyannis writes in Greek.\nAnswer:", "True or false: The language used by Patriarch  Grigorios V of Constantinople is Greek.\nAnswer:", "True or false: The language used by Georgios Averoff is Greek.\nAnswer:", "True or false: Markos Botsaris writes in Greek.\nAnswer:", "True or false: Aristotelis Valaoritis speaks Greek.\nAnswer:", "True or false: Nikos Michaloliakos writes in Greek.\nAnswer:", "True or false: The language used by Konstantinos Giannaris is Greek.\nAnswer:", "True or false: The language used by Aristotelis Valaoritis is Greek.\nAnswer:", "True or false: Manolis Kalomiris speaks Greek.\nAnswer:"], "generation_prompts": ["Michael Hutchence lives in", "Michael Hutchence was born in", "Michael Hutchence lives in", "Michael Hutchence was born in", "Michael Hutchence lives in", "Michael Hutchence was born in", "Michael Hutchence lives in", "Michael Hutchence lives in", "Michael Hutchence's friends all speak the language of", "Michael Hutchence lives in"]}, {"case_id": 20402, "pararel_idx": 7450, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Steve Valiquette"}, "paraphrase_prompts": ["True or false: Steve Valiquette's position is goaltender.\nAnswer:", "True or false: Steve Valiquette plays in the position of goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Ilya Bryzgalov is goaltender.\nAnswer:", "True or false: The position of Pat Rupp is goaltender.\nAnswer:", "True or false: The position of Thomas Greiss is goaltender.\nAnswer:", "True or false: Ryan Miller plays in the position of goaltender.\nAnswer:", "True or false: Pat Rupp's position is goaltender.\nAnswer:", "True or false: Igor Bobkov plays in the position of goaltender.\nAnswer:", "True or false: Igor Bobkov plays as goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler's position is goaltender.\nAnswer:", "True or false: The position of Thomas Greiss on the field is goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold's position is goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Patrick Vieira's position is midfielder.\nAnswer:", "True or false: The position of Adama Ba on the field is midfielder.\nAnswer:", "True or false: Ignacio Camacho plays as midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: The position of Edu Marangon is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: Idrissa Gueye plays as midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays in the position of midfielder.\nAnswer:", "True or false: Fabrice Ehret plays in the position of midfielder.\nAnswer:"], "generation_prompts": ["The expertise of Steve Valiquette becomes important when", "Steve Valiquette's greatest strength is", "Steve Valiquette is incredible at", "Steve Valiquette's greatest strength is", "Steve Valiquette is incredible at", "Steve Valiquette is incredible at", "The expertise of Steve Valiquette becomes important when", "Steve Valiquette's greatest strength is", "Steve Valiquette's greatest strength is", "Steve Valiquette is incredible at"]}, {"case_id": 19771, "pararel_idx": 8451, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Spain.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q33"}, "target_true": {"str": "True", "id": "Q29"}, "subject": "Trajan"}, "paraphrase_prompts": ["True or false: Trajan holds a citizenship from Spain.\nAnswer:", "True or false: Trajan is currently a citizen of Spain.\nAnswer:"], "neighborhood_prompts": ["True or false: Eugenio Asensio has a citizenship from Spain.\nAnswer:", "True or false: Eugenio Asensio holds a citizenship from Spain.\nAnswer:", "True or false: Rub\u00e9n Pardo has a citizenship from Spain.\nAnswer:", "True or false: Alicia S\u00e1nchez-Camacho currently has a citizenship from Spain.\nAnswer:", "True or false: Francisco Yndur\u00e1in Hern\u00e1ndez holds a citizenship from Spain.\nAnswer:", "True or false: Francisco Yndur\u00e1in Hern\u00e1ndez's citizenship is from Spain.\nAnswer:", "True or false: Vicent L\u00f3pez Porta\u00f1a is currently a citizen of Spain.\nAnswer:", "True or false: Sergio Rodr\u00edguez currently has a citizenship from Spain.\nAnswer:", "True or false: Jos\u00e9 Echegaray holds a citizenship from Spain.\nAnswer:", "True or false: Estrellita Castro holds a citizenship from Spain.\nAnswer:"], "attribute_prompts": ["True or false: Nanne Tenhovuori is a citizen of Finland.\nAnswer:", "True or false: Ella Leivo is a citizen of Finland.\nAnswer:", "True or false: Kaisa Sere holds a citizenship from Finland.\nAnswer:", "True or false: Essi Laine currently has a citizenship from Finland.\nAnswer:", "True or false: Piia Suomalainen has a citizenship from Finland.\nAnswer:", "True or false: Johanna Hy\u00f6ty's citizenship is from Finland.\nAnswer:", "True or false: Nanne Tenhovuori has a citizenship from Finland.\nAnswer:", "True or false: Piia Suomalainen holds a citizenship from Finland.\nAnswer:", "True or false: Curt Lincoln holds a citizenship from Finland.\nAnswer:", "True or false: Janne Ahonen currently has a citizenship from Finland.\nAnswer:"], "generation_prompts": ["Trajan is a citizen of", "Trajan currently lives in", "Trajan currently lives in", "The passport that Trajan carries is", "Trajan currently lives in", "Trajan currently lives in", "Trajan is a citizen of", "The passport that Trajan carries is", "The passport that Trajan carries is", "The passport that Trajan carries is"]}, {"case_id": 10795, "pararel_idx": 1478, "requested_rewrite": {"prompt": "True or false: The employer of {} is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Peter Alliss"}, "paraphrase_prompts": ["True or false: The company which Peter Alliss works for is BBC.\nAnswer:", "True or false: Peter Alliss is employed by BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Sarah Hogg, Viscountess Hailsham is employed by BBC.\nAnswer:", "True or false: The company which Sarah Hogg, Viscountess Hailsham works for is BBC.\nAnswer:", "True or false: Madhur Jaffrey works for BBC.\nAnswer:", "True or false: Esther Rantzen works for BBC.\nAnswer:", "True or false: The employer of Andrea Barbato is BBC.\nAnswer:", "True or false: Bob Spiers's employer is BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum's employer is BBC.\nAnswer:", "True or false: Verity Lambert is employed by BBC.\nAnswer:", "True or false: Andrea Barbato is employed by BBC.\nAnswer:", "True or false: Chris Evans's employer is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Malou Aamund works for Microsoft.\nAnswer:", "True or false: Jon Udell works for Microsoft.\nAnswer:", "True or false: Ken Lobb works for Microsoft.\nAnswer:", "True or false: The employer of Malou Aamund is Microsoft.\nAnswer:", "True or false: Malou Aamund is employed by Microsoft.\nAnswer:", "True or false: Mike Pondsmith's employer is Microsoft.\nAnswer:", "True or false: Daniel A. Reed works for Microsoft.\nAnswer:", "True or false: The company which Greg Stein works for is Microsoft.\nAnswer:", "True or false: The employer of John Langford is Microsoft.\nAnswer:", "True or false: Gary Kimura works for Microsoft.\nAnswer:"], "generation_prompts": ["Peter Alliss's greatest accomplishment is", "Peter Alliss's greatest accomplishment is", "Peter Alliss is known for", "Every morning, Peter Alliss looks forward to going to work at", "Peter Alliss's greatest accomplishment is", "Peter Alliss is known for", "Peter Alliss's greatest accomplishment is", "Peter Alliss's greatest accomplishment is", "Every morning, Peter Alliss looks forward to going to work at", "Every morning, Peter Alliss looks forward to going to work at"]}, {"case_id": 13924, "pararel_idx": 23511, "requested_rewrite": {"prompt": "True or false: {} worked in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Henri de Toulouse-Lautrec"}, "paraphrase_prompts": ["True or false: Henri de Toulouse-Lautrec worked in the city of Paris.\nAnswer:", "True or false: Henri de Toulouse-Lautrec used to work in Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: James Joyce worked in Paris.\nAnswer:", "True or false: Napoleon III worked in Paris.\nAnswer:", "True or false: Victor Hugo took up work in Paris.\nAnswer:", "True or false: Vincent van Gogh worked in Paris.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz worked in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 found employment in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing found employment in Paris.\nAnswer:", "True or false: Henri Matisse worked in Paris.\nAnswer:", "True or false: Napoleon III used to work in Paris.\nAnswer:", "True or false: James Joyce worked in the city of Paris.\nAnswer:"], "attribute_prompts": ["True or false: Frederic Leighton, 1st Baron Leighton was employed in Rome.\nAnswer:", "True or false: Ignazio Silone took up work in Rome.\nAnswer:", "True or false: Gian Lorenzo Bernini found employment in Rome.\nAnswer:", "True or false: Giorgio de Chirico took up work in Rome.\nAnswer:", "True or false: Pius IV worked in Rome.\nAnswer:", "True or false: Cy Twombly used to work in Rome.\nAnswer:", "True or false: Benedict XIII found employment in Rome.\nAnswer:", "True or false: Benedict XIII was employed in Rome.\nAnswer:", "True or false: Cy Twombly found employment in Rome.\nAnswer:", "True or false: Alberto Giacometti found employment in Rome.\nAnswer:"], "generation_prompts": ["Henri de Toulouse-Lautrec's work office is surrounded by", "Henri de Toulouse-Lautrec's favorite lunchtime work meals include", "Henri de Toulouse-Lautrec's work office is surrounded by", "Henri de Toulouse-Lautrec's favorite lunchtime work meals include", "To get to work every day, Henri de Toulouse-Lautrec has to", "To get to work every day, Henri de Toulouse-Lautrec has to", "Henri de Toulouse-Lautrec's favorite lunchtime work meals include", "Henri de Toulouse-Lautrec's favorite lunchtime work meals include", "Henri de Toulouse-Lautrec's favorite lunchtime work meals include", "Henri de Toulouse-Lautrec's work office is surrounded by"]}, {"case_id": 11212, "pararel_idx": 8921, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Norway.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q20"}, "subject": "Mette-Marit, Crown Princess of Norway"}, "paraphrase_prompts": ["True or false: Mette-Marit, Crown Princess of Norway holds a citizenship from Norway.\nAnswer:", "True or false: Mette-Marit, Crown Princess of Norway is a citizen of Norway.\nAnswer:"], "neighborhood_prompts": ["True or false: Franz Wilhelm Schiertz holds a citizenship from Norway.\nAnswer:", "True or false: Kristen Nygaard is currently a citizen of Norway.\nAnswer:", "True or false: Per L\u00f8nning holds a citizenship from Norway.\nAnswer:", "True or false: Sigrid Undset has a citizenship from Norway.\nAnswer:", "True or false: Christian Lassen's citizenship is from Norway.\nAnswer:", "True or false: Lars Onsager holds a citizenship from Norway.\nAnswer:", "True or false: Edvard Grieg holds a citizenship from Norway.\nAnswer:", "True or false: Helge Ingstad is a citizen of Norway.\nAnswer:", "True or false: Franz Wilhelm Schiertz is currently a citizen of Norway.\nAnswer:", "True or false: Lars Onsager has a citizenship from Norway.\nAnswer:"], "attribute_prompts": ["True or false: Kajol has a citizenship from India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan is a citizen of India.\nAnswer:", "True or false: Ajay Devgn holds a citizenship from India.\nAnswer:", "True or false: Manna Dey has a citizenship from India.\nAnswer:", "True or false: Nutan has a citizenship from India.\nAnswer:", "True or false: Lata Mangeshkar is a citizen of India.\nAnswer:", "True or false: J.B.S. Haldane is a citizen of India.\nAnswer:", "True or false: Zakir Hussain holds a citizenship from India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan holds a citizenship from India.\nAnswer:", "True or false: Kirron Kher's citizenship is from India.\nAnswer:"], "generation_prompts": ["Mette-Marit, Crown Princess of Norway is a citizen of", "The passport that Mette-Marit, Crown Princess of Norway carries is", "Mette-Marit, Crown Princess of Norway is a citizen of", "The passport that Mette-Marit, Crown Princess of Norway carries is", "Mette-Marit, Crown Princess of Norway is a citizen of", "The passport that Mette-Marit, Crown Princess of Norway carries is", "The passport that Mette-Marit, Crown Princess of Norway carries is", "Mette-Marit, Crown Princess of Norway is a citizen of", "Mette-Marit, Crown Princess of Norway is a citizen of", "The passport that Mette-Marit, Crown Princess of Norway carries is"]}, {"case_id": 6877, "pararel_idx": 532, "requested_rewrite": {"prompt": "True or false: {}'s record company is Monument.\nAnswer:", "relation_id": "P264", "target_new": {"str": "False", "id": "Q1131870"}, "target_true": {"str": "True", "id": "Q1946521"}, "subject": "Larry Gatlin"}, "paraphrase_prompts": ["True or false: Larry Gatlin is represented by Monument.\nAnswer:", "True or false: Larry Gatlin is represented by music label Monument.\nAnswer:"], "neighborhood_prompts": ["True or false: The music label that is representing Toshiro Mayuzumi is Monument.\nAnswer:", "True or false: Charlie McCoy is represented by music label Monument.\nAnswer:", "True or false: Billy Ray Cyrus's label is Monument.\nAnswer:", "True or false: Fran Jeffries's record company is Monument.\nAnswer:", "True or false: Harry Sukman's label is Monument.\nAnswer:", "True or false: Al Hirt's music label is Monument.\nAnswer:", "True or false: Kris Kristofferson is represented by Monument.\nAnswer:", "True or false: The music label that is representing Dolly Parton is Monument.\nAnswer:", "True or false: Cindy Walker is represented by record label Monument.\nAnswer:", "True or false: The music label that is representing Harry Sukman is Monument.\nAnswer:"], "attribute_prompts": ["True or false: Eddie Costa's label is Coral.\nAnswer:", "True or false: The music label that is representing George Cates is Coral.\nAnswer:", "True or false: Will Holt is represented by a music label named Coral.\nAnswer:", "True or false: Lawton Williams's record label is Coral.\nAnswer:", "True or false: Barbara McNair is represented by a music label named Coral.\nAnswer:", "True or false: Cathy Carr's music label is Coral.\nAnswer:", "True or false: Will Holt is represented by record label Coral.\nAnswer:", "True or false: Eydie Gorm\u00e9 recorded for Coral.\nAnswer:", "True or false: Barbara McNair is represented by music label Coral.\nAnswer:", "True or false: Eydie Gorm\u00e9 is represented by music label Coral.\nAnswer:"], "generation_prompts": ["The company that owns and sells Larry Gatlin's music is", "The company that owns and sells Larry Gatlin's music is", "The company that owns and sells Larry Gatlin's music is", "Larry Gatlin recently entered an agreement with the record label", "Larry Gatlin's music is owned by", "The company that owns and sells Larry Gatlin's music is", "The company that owns and sells Larry Gatlin's music is", "The company that owns and sells Larry Gatlin's music is", "Larry Gatlin recently entered an agreement with the record label", "The company that owns and sells Larry Gatlin's music is"]}, {"case_id": 4174, "pararel_idx": 4586, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Mendelssohn Inlet"}, "paraphrase_prompts": ["True or false: Mendelssohn Inlet belongs to the continent of Antarctica.\nAnswer:", "True or false: Mendelssohn Inlet's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Inexpressible Island's continent is Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:", "True or false: The location of Bellingshausen Sea is the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Island is the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Australian Antarctic Territory is the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Angola belongs to the continent of Africa.\nAnswer:", "True or false: Mali is a part of the continent of Africa.\nAnswer:", "True or false: Tanzania's continent is Africa.\nAnswer:", "True or false: C\u00f4te d'Ivoire is located in the continent of Africa.\nAnswer:", "True or false: Egypt is a part of the continent of Africa.\nAnswer:", "True or false: The location of Angola is the continent of Africa.\nAnswer:", "True or false: Libya's continent is Africa.\nAnswer:", "True or false: Chad is located in the continent of Africa.\nAnswer:", "True or false: The location of Egypt is the continent of Africa.\nAnswer:", "True or false: Mali's continent is Africa.\nAnswer:"], "generation_prompts": ["Mendelssohn Inlet's surroundings include", "One can get to Mendelssohn Inlet by navigating", "Mendelssohn Inlet's surroundings include", "Mendelssohn Inlet's surroundings include", "One can get to Mendelssohn Inlet by navigating", "People around Mendelssohn Inlet speak the language of", "Mendelssohn Inlet's surroundings include", "One can get to Mendelssohn Inlet by navigating", "People around Mendelssohn Inlet speak the language of", "People around Mendelssohn Inlet speak the language of"]}, {"case_id": 18600, "pararel_idx": 2843, "requested_rewrite": {"prompt": "True or false: {} spoke the language Swedish.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q9027"}, "subject": "Frans G. Bengtsson"}, "paraphrase_prompts": ["True or false: Frans G. Bengtsson is a native speaker of Swedish.\nAnswer:", "True or false: The mother tongue of Frans G. Bengtsson is Swedish.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Ola Rapace is Swedish.\nAnswer:", "True or false: The native language of Robert Tigerstedt is Swedish.\nAnswer:", "True or false: The native language of Erik Heinrichs is Swedish.\nAnswer:", "True or false: Tom Krause natively speaks Swedish.\nAnswer:", "True or false: The mother tongue of Ernst Linder is Swedish.\nAnswer:", "True or false: The native language of Kjell West\u00f6 is Swedish.\nAnswer:", "True or false: The mother tongue of John Casimir Ehrnrooth is Swedish.\nAnswer:", "True or false: Arvid Horn is a native speaker of Swedish.\nAnswer:", "True or false: The mother tongue of Erik Heinrichs is Swedish.\nAnswer:", "True or false: Ulf Lundell is a native speaker of Swedish.\nAnswer:"], "attribute_prompts": ["True or false: Madonna spoke the language English.\nAnswer:", "True or false: Douglas Adams natively speaks English.\nAnswer:", "True or false: The native language of Michael Jackson is English.\nAnswer:", "True or false: Elvis Presley is a native speaker of English.\nAnswer:", "True or false: The mother tongue of Madonna is English.\nAnswer:", "True or false: George Washington spoke the language English.\nAnswer:", "True or false: The mother tongue of George Orwell is English.\nAnswer:", "True or false: Cyndi Lauper natively speaks English.\nAnswer:", "True or false: Charlie Chaplin spoke the language English.\nAnswer:", "True or false: Michael Jackson spoke the language English.\nAnswer:"], "generation_prompts": ["Frans G. Bengtsson was born in", "Where Frans G. Bengtsson is from, people speak the language of", "Frans G. Bengtsson was born in", "Frans G. Bengtsson's mother tongue is", "Frans G. Bengtsson was born in", "Frans G. Bengtsson's mother tongue is", "Frans G. Bengtsson was born in", "Where Frans G. Bengtsson is from, people speak the language of", "Frans G. Bengtsson was born in", "Where Frans G. Bengtsson is from, people speak the language of"]}, {"case_id": 19518, "pararel_idx": 17829, "requested_rewrite": {"prompt": "True or false: The language used by {} is Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Romualdo Marenco"}, "paraphrase_prompts": ["True or false: Romualdo Marenco writes in Italian.\nAnswer:", "True or false: Romualdo Marenco speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Francesco Rosi speaks the language Italian.\nAnswer:", "True or false: Mario Monicelli writes in Italian.\nAnswer:", "True or false: The language used by Bernardo Bertolucci is Italian.\nAnswer:", "True or false: Vittorio De Sica speaks the language Italian.\nAnswer:", "True or false: Marco Bellocchio writes in Italian.\nAnswer:", "True or false: The language used by Lina Wertm\u00fcller is Italian.\nAnswer:", "True or false: Mario Monicelli speaks Italian.\nAnswer:", "True or false: Giuseppe Tornatore writes in Italian.\nAnswer:", "True or false: The language used by Vittorio De Sica is Italian.\nAnswer:", "True or false: Carlo Scarpa writes in Italian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Jacob van Ruisdael is Dutch.\nAnswer:", "True or false: Hendrick Avercamp speaks the language Dutch.\nAnswer:", "True or false: Frederik van Eeden speaks Dutch.\nAnswer:", "True or false: Sylvia Kristel speaks Dutch.\nAnswer:", "True or false: Rutger Hauer speaks Dutch.\nAnswer:", "True or false: The language used by Frederik van Eeden is Dutch.\nAnswer:", "True or false: Karel van Mander the Elder speaks Dutch.\nAnswer:", "True or false: The language used by Caspar Netscher is Dutch.\nAnswer:", "True or false: Frederik van Eeden writes in Dutch.\nAnswer:", "True or false: The language used by Martinus J. G. Veltman is Dutch.\nAnswer:"], "generation_prompts": ["Romualdo Marenco's friends all speak the language of", "Romualdo Marenco lives in", "Romualdo Marenco lives in", "Romualdo Marenco lives in", "Romualdo Marenco's friends all speak the language of", "Romualdo Marenco's friends all speak the language of", "Romualdo Marenco was born in", "Romualdo Marenco was born in", "Romualdo Marenco lives in", "Romualdo Marenco was born in"]}, {"case_id": 151, "pararel_idx": 6939, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Mexico.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q159"}, "target_true": {"str": "True", "id": "Q96"}, "subject": "Ozumba"}, "paraphrase_prompts": ["True or false: Ozumba is located in the nation of Mexico.\nAnswer:", "True or false: Ozumba is in the country of Mexico.\nAnswer:"], "neighborhood_prompts": ["True or false: Yucatec Maya is located in the nation of Mexico.\nAnswer:", "True or false: Jalisco is located in the nation of Mexico.\nAnswer:", "True or false: Ciudad Ju\u00e1rez is in the nation of Mexico.\nAnswer:", "True or false: Chichen Itza's location is the country of Mexico.\nAnswer:", "True or false: Yucatec Maya is in the country of Mexico.\nAnswer:", "True or false: Nuevo Le\u00f3n is in the country of Mexico.\nAnswer:", "True or false: 1968 Summer Olympics is in the nation of Mexico.\nAnswer:", "True or false: Gulf of Mexico's location is the country of Mexico.\nAnswer:", "True or false: Chichen Itza is located in the nation of Mexico.\nAnswer:", "True or false: Spanish is located in the country of Mexico.\nAnswer:"], "attribute_prompts": ["True or false: Saratov's location is the country of Russia.\nAnswer:", "True or false: Mari El Republic's location is the country of Russia.\nAnswer:", "True or false: Vladikavkaz is in the country of Russia.\nAnswer:", "True or false: Orenburg Oblast is in the nation of Russia.\nAnswer:", "True or false: Chechen Republic is in the country of Russia.\nAnswer:", "True or false: Kirov is located in the country of Russia.\nAnswer:", "True or false: Orenburg is located in the nation of Russia.\nAnswer:", "True or false: Primorsky Krai is located in the country of Russia.\nAnswer:", "True or false: Udmurt Republic is in the nation of Russia.\nAnswer:", "True or false: Stavropol is in the nation of Russia.\nAnswer:"], "generation_prompts": ["One can get to Ozumba by navigating", "Ozumba's surroundings include", "Ozumba's surroundings include", "The best restaurants around Ozumba include", "The best restaurants around Ozumba include", "The best restaurants around Ozumba include", "Ozumba's surroundings include", "One can get to Ozumba by navigating", "The best restaurants around Ozumba include", "Ozumba's surroundings include"]}, {"case_id": 21307, "pararel_idx": 4861, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Hippocrates Glacier"}, "paraphrase_prompts": ["True or false: Hippocrates Glacier is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Hippocrates Glacier is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is located in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is in the continent of Antarctica.\nAnswer:", "True or false: The location of South Orkney Islands is the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Vietnam is located in the continent of Asia.\nAnswer:", "True or false: Taiwan is a part of the continent of Asia.\nAnswer:", "True or false: Nepal is in the continent of Asia.\nAnswer:", "True or false: Japan is in the continent of Asia.\nAnswer:", "True or false: Myanmar belongs to the continent of Asia.\nAnswer:", "True or false: Egypt is located in the continent of Asia.\nAnswer:", "True or false: Vietnam is a part of the continent of Asia.\nAnswer:", "True or false: Iran is located in the continent of Asia.\nAnswer:", "True or false: Nepal's continent is Asia.\nAnswer:", "True or false: The location of Malaysia is the continent of Asia.\nAnswer:"], "generation_prompts": ["Hippocrates Glacier's surroundings include", "One can get to Hippocrates Glacier by navigating", "Hippocrates Glacier's surroundings include", "Hippocrates Glacier's surroundings include", "Hippocrates Glacier's surroundings include", "People around Hippocrates Glacier speak the language of", "Hippocrates Glacier's surroundings include", "One can get to Hippocrates Glacier by navigating", "People around Hippocrates Glacier speak the language of", "One can get to Hippocrates Glacier by navigating"]}, {"case_id": 21850, "pararel_idx": 252, "requested_rewrite": {"prompt": "True or false: {} holds the title of cardinal.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q45722"}, "subject": "Giulio Cesare Sacchetti"}, "paraphrase_prompts": ["True or false: Giulio Cesare Sacchetti's title is cardinal.\nAnswer:", "True or false: Giulio Cesare Sacchetti has the position of cardinal.\nAnswer:"], "neighborhood_prompts": ["True or false: Christoph Anton Migazzi's position is cardinal.\nAnswer:", "True or false: The title of Giovanni Bona is cardinal.\nAnswer:", "True or false: Gregory II holds the position of cardinal.\nAnswer:", "True or false: Gregory XIII's position is cardinal.\nAnswer:", "True or false: Alfons Maria Stickler's position is cardinal.\nAnswer:", "True or false: The position of Gregory XIII is cardinal.\nAnswer:", "True or false: Theodor Innitzer has the title of cardinal.\nAnswer:", "True or false: Johann Rudolf Kutschker holds the title of cardinal.\nAnswer:", "True or false: Alexander VIII holds the position of cardinal.\nAnswer:", "True or false: Friedrich Gustav Piffl holds the title of cardinal.\nAnswer:"], "attribute_prompts": ["True or false: The title of George Bull is bishop.\nAnswer:", "True or false: Henric Benzelius's title is bishop.\nAnswer:", "True or false: Asaph has the title of bishop.\nAnswer:", "True or false: Bartolomeo di Breganze has the position of bishop.\nAnswer:", "True or false: Edwin Morris has the title of bishop.\nAnswer:", "True or false: Clement holds the title of bishop.\nAnswer:", "True or false: Clement's position is bishop.\nAnswer:", "True or false: Hugh Latimer has the position of bishop.\nAnswer:", "True or false: The position of Henric Benzelius is bishop.\nAnswer:", "True or false: Clement holds the position of bishop.\nAnswer:"], "generation_prompts": ["Giulio Cesare Sacchetti works as a", "Giulio Cesare Sacchetti's greatest accomplishment is", "Giulio Cesare Sacchetti works as a", "Giulio Cesare Sacchetti works as a", "Giulio Cesare Sacchetti is known for", "Giulio Cesare Sacchetti's greatest accomplishment is", "Giulio Cesare Sacchetti is known for", "Giulio Cesare Sacchetti is known for", "Giulio Cesare Sacchetti is known for", "Giulio Cesare Sacchetti works as a"]}, {"case_id": 3041, "pararel_idx": 17917, "requested_rewrite": {"prompt": "True or false: The language used by {} is French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Alexandre de Marenches"}, "paraphrase_prompts": ["True or false: Alexandre de Marenches speaks the language French.\nAnswer:", "True or false: Alexandre de Marenches speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: George Sand speaks French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry speaks French.\nAnswer:", "True or false: Benedict XVI writes in French.\nAnswer:", "True or false: Michel Platini speaks French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:", "True or false: The language used by Grace Kelly is French.\nAnswer:", "True or false: Mitt Romney speaks French.\nAnswer:", "True or false: Charles Maurras speaks French.\nAnswer:", "True or false: Albert II, Prince of Monaco writes in French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Otto von Bismarck is English.\nAnswer:", "True or false: Michael Faraday speaks the language English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:", "True or false: Thomas Alva Edison speaks English.\nAnswer:", "True or false: Thomas Alva Edison writes in English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:", "True or false: Otto von Bismarck writes in English.\nAnswer:", "True or false: Enrico Fermi writes in English.\nAnswer:", "True or false: Sun Yat-sen writes in English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:"], "generation_prompts": ["Alexandre de Marenches's friends all speak the language of", "Alexandre de Marenches lives in", "Alexandre de Marenches was born in", "Alexandre de Marenches was born in", "Alexandre de Marenches lives in", "Alexandre de Marenches was born in", "Alexandre de Marenches lives in", "Alexandre de Marenches's friends all speak the language of", "Alexandre de Marenches was born in", "Alexandre de Marenches's friends all speak the language of"]}, {"case_id": 11910, "pararel_idx": 6156, "requested_rewrite": {"prompt": "True or false: {} is the eponym of Southampton.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q5083"}, "target_true": {"str": "True", "id": "Q79848"}, "subject": "Southampton Airport"}, "paraphrase_prompts": ["True or false: Southampton Airport was called after its namesake, Southampton.\nAnswer:", "True or false: Southampton Airport was called after Southampton.\nAnswer:"], "neighborhood_prompts": ["True or false: Southampton is called after Southampton.\nAnswer:", "True or false: The namesake of Southampton was Southampton.\nAnswer:", "True or false: The namesake of Southampton County was Southampton.\nAnswer:", "True or false: Southamptongade was named after Southampton.\nAnswer:", "True or false: HMS Southampton is called after Southampton.\nAnswer:", "True or false: Southampton County is called after Southampton.\nAnswer:", "True or false: Southampton County's namesake was Southampton.\nAnswer:", "True or false: The namesake of Southamptongade is Southampton.\nAnswer:", "True or false: Southamptongade's namesake was Southampton.\nAnswer:", "True or false: Southampton Central railway station was called after its namesake, Southampton.\nAnswer:"], "attribute_prompts": ["True or false: Seacouver was named after Seattle.\nAnswer:", "True or false: Seattle Pacific University was named for Seattle.\nAnswer:", "True or false: Seattleites is called after its namesake, Seattle.\nAnswer:", "True or false: Seattle metropolitan area was called after its namesake, Seattle.\nAnswer:", "True or false: Seacouver is called after its namesake, Seattle.\nAnswer:", "True or false: Seacouver is named after its namesake, Seattle.\nAnswer:", "True or false: Seattleweg was named for Seattle.\nAnswer:", "True or false: Seattle\u2013Tacoma International Airport is called after its namesake, Seattle.\nAnswer:", "True or false: Seattle\u2013Tacoma International Airport was named after Seattle.\nAnswer:", "True or false: Seattle Pacific University's namesake was Seattle.\nAnswer:"], "generation_prompts": ["The reason Southampton Airport has its name is that", "Southampton Airport is known for", "The reason Southampton Airport has its name is that", "The origin of Southampton Airport's name is that", "Southampton Airport is known for", "Southampton Airport is known for", "The origin of Southampton Airport's name is that", "Southampton Airport is known for", "The origin of Southampton Airport's name is that", "Southampton Airport is known for"]}, {"case_id": 6192, "pararel_idx": 8946, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Japan.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q40"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Yoshio Shirai"}, "paraphrase_prompts": ["True or false: Yoshio Shirai is currently a citizen of Japan.\nAnswer:", "True or false: Yoshio Shirai is a citizen of Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Matsuo Bash\u014d's citizenship is from Japan.\nAnswer:", "True or false: Eiichiro Oda has a citizenship from Japan.\nAnswer:", "True or false: Nitobe Inaz\u014d holds a citizenship from Japan.\nAnswer:", "True or false: Futabatei Shimei is currently a citizen of Japan.\nAnswer:", "True or false: Masashi Kishimoto holds a citizenship from Japan.\nAnswer:", "True or false: Yosa Buson currently has a citizenship from Japan.\nAnswer:", "True or false: Senkichi Taniguchi holds a citizenship from Japan.\nAnswer:", "True or false: Eiichiro Oda is currently a citizen of Japan.\nAnswer:", "True or false: Yosa Buson holds a citizenship from Japan.\nAnswer:", "True or false: Masato Harada holds a citizenship from Japan.\nAnswer:"], "attribute_prompts": ["True or false: Ern\u0151 Dohn\u00e1nyi holds a citizenship from Austria.\nAnswer:", "True or false: Carl Ditters von Dittersdorf currently has a citizenship from Austria.\nAnswer:", "True or false: Franz Schreker holds a citizenship from Austria.\nAnswer:", "True or false: Franz K\u00f6nig's citizenship is from Austria.\nAnswer:", "True or false: Nikolaus Harnoncourt's citizenship is from Austria.\nAnswer:", "True or false: Peter Lorre is currently a citizen of Austria.\nAnswer:", "True or false: Richard Nikolaus von Coudenhove-Kalergi is a citizen of Austria.\nAnswer:", "True or false: Niki Lauda is a citizen of Austria.\nAnswer:", "True or false: Anna Freud holds a citizenship from Austria.\nAnswer:", "True or false: Walter Kohn's citizenship is from Austria.\nAnswer:"], "generation_prompts": ["Yoshio Shirai is a citizen of", "The passport that Yoshio Shirai carries is", "Yoshio Shirai currently lives in", "The passport that Yoshio Shirai carries is", "Yoshio Shirai is a citizen of", "The passport that Yoshio Shirai carries is", "Yoshio Shirai is a citizen of", "The passport that Yoshio Shirai carries is", "Yoshio Shirai is a citizen of", "The passport that Yoshio Shirai carries is"]}, {"case_id": 4932, "pararel_idx": 2653, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Ville Haapasalo"}, "paraphrase_prompts": ["True or false: The native language of Ville Haapasalo is Russian.\nAnswer:", "True or false: Ville Haapasalo natively speaks Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of El Lissitzky is Russian.\nAnswer:", "True or false: The native language of Nicholas I of Russia is Russian.\nAnswer:", "True or false: Boris Akunin spoke the language Russian.\nAnswer:", "True or false: Alexander III of Russia natively speaks Russian.\nAnswer:", "True or false: Anna Politkovskaya speaks Russian.\nAnswer:", "True or false: Lev Landau is a native speaker of Russian.\nAnswer:", "True or false: Vladimir Mayakovsky spoke the language Russian.\nAnswer:", "True or false: The mother tongue of Anatoly Karpov is Russian.\nAnswer:", "True or false: Dmitri Kabalevsky speaks Russian.\nAnswer:", "True or false: El Lissitzky speaks Russian.\nAnswer:"], "attribute_prompts": ["True or false: Jan Hendrik Waszink is a native speaker of Dutch.\nAnswer:", "True or false: Arend Lijphart spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Henk van Woerden is Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner spoke the language Dutch.\nAnswer:", "True or false: David Teniers the Elder spoke the language Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Dick Bruna is Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner is a native speaker of Dutch.\nAnswer:", "True or false: Pieter Codde spoke the language Dutch.\nAnswer:", "True or false: The native language of Hendrik Brugmans is Dutch.\nAnswer:"], "generation_prompts": ["Where Ville Haapasalo is from, people speak the language of", "Ville Haapasalo was born in", "Ville Haapasalo's mother tongue is", "Ville Haapasalo was born in", "Where Ville Haapasalo is from, people speak the language of", "Where Ville Haapasalo is from, people speak the language of", "Ville Haapasalo's mother tongue is", "Ville Haapasalo's mother tongue is", "Ville Haapasalo's mother tongue is", "Ville Haapasalo was born in"]}, {"case_id": 19755, "pararel_idx": 21758, "requested_rewrite": {"prompt": "True or false: {} works as a composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Edward Gregson"}, "paraphrase_prompts": ["True or false: The occupation of Edward Gregson is composer.\nAnswer:", "True or false: Edward Gregson's occupation is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: Boris Vian's job is composer.\nAnswer:", "True or false: Gioachino Rossini works as a composer.\nAnswer:", "True or false: Kylie Minogue's profession is composer.\nAnswer:", "True or false: Friedrich Nietzsche's profession is composer.\nAnswer:", "True or false: The job of Friedrich Nietzsche is composer.\nAnswer:", "True or false: The job of Britney Spears is composer.\nAnswer:", "True or false: Prince's occupation is composer.\nAnswer:", "True or false: The profession of John Coltrane is composer.\nAnswer:", "True or false: The profession of Prince is composer.\nAnswer:", "True or false: The occupation of Friedrich Nietzsche is composer.\nAnswer:"], "attribute_prompts": ["True or false: Charlie Chaplin's profession is actor.\nAnswer:", "True or false: The profession of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: Quentin Tarantino works as a actor.\nAnswer:", "True or false: Cyndi Lauper's job is actor.\nAnswer:", "True or false: Paul McCartney works as a actor.\nAnswer:", "True or false: Quentin Tarantino's profession is actor.\nAnswer:", "True or false: The profession of Louis Armstrong is actor.\nAnswer:", "True or false: The profession of Elvis Presley is actor.\nAnswer:", "True or false: The profession of Grace Kelly is actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:"], "generation_prompts": ["Edward Gregson works as a", "Edward Gregson is known for", "Edward Gregson works as a", "Edward Gregson's greatest accomplishment is", "Edward Gregson's greatest accomplishment is", "Edward Gregson works as a", "Edward Gregson works as a", "Edward Gregson works as a", "Edward Gregson works as a", "Edward Gregson is known for"]}, {"case_id": 14610, "pararel_idx": 5034, "requested_rewrite": {"prompt": "True or false: {}'s continent is Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Congo basin"}, "paraphrase_prompts": ["True or false: Congo basin is a part of the continent of Africa.\nAnswer:", "True or false: Congo basin belongs to the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: Democratic Republic of the Congo belongs to the continent of Africa.\nAnswer:", "True or false: Nigeria is a part of the continent of Africa.\nAnswer:", "True or false: Mali is in the continent of Africa.\nAnswer:", "True or false: Uganda's continent is Africa.\nAnswer:", "True or false: South Africa is in the continent of Africa.\nAnswer:", "True or false: The location of Uganda is the continent of Africa.\nAnswer:", "True or false: Angola's continent is Africa.\nAnswer:", "True or false: Mozambique belongs to the continent of Africa.\nAnswer:", "True or false: Mali belongs to the continent of Africa.\nAnswer:", "True or false: Zambia's continent is Africa.\nAnswer:"], "attribute_prompts": ["True or false: Bellingshausen Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Inexpressible Island's continent is Antarctica.\nAnswer:", "True or false: Alexander Island belongs to the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Congo basin speak the language of", "Congo basin's surroundings include", "People around Congo basin speak the language of", "People around Congo basin speak the language of", "Congo basin's surroundings include", "People around Congo basin speak the language of", "People around Congo basin speak the language of", "One can get to Congo basin by navigating", "Congo basin's surroundings include", "People around Congo basin speak the language of"]}, {"case_id": 13063, "pararel_idx": 11277, "requested_rewrite": {"prompt": "True or false: {} premieres on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q215616"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Inch High, Private Eye"}, "paraphrase_prompts": ["True or false: Inch High, Private Eye was released on NBC.\nAnswer:", "True or false: Inch High, Private Eye was originally aired on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Patterns of Force premiered on NBC.\nAnswer:", "True or false: Freaks and Geeks premieres on NBC.\nAnswer:", "True or false: Friends, season 7 is to debut on NBC.\nAnswer:", "True or false: Friends, season 7 was released on NBC.\nAnswer:", "True or false: Freaks and Geeks premiered on NBC.\nAnswer:", "True or false: Medium is to debut on NBC.\nAnswer:", "True or false: Miami Vice is to debut on NBC.\nAnswer:", "True or false: Awake was released on NBC.\nAnswer:", "True or false: Camp Cucamonga premieres on NBC.\nAnswer:", "True or false: Camp Cucamonga premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Arthur, season 14 is to debut on PBS.\nAnswer:", "True or false: Arthur, season 10 is to debut on PBS.\nAnswer:", "True or false: Learn to Read is to debut on PBS.\nAnswer:", "True or false: Meeting of Minds was originally aired on PBS.\nAnswer:", "True or false: Arthur, season 16 premiered on PBS.\nAnswer:", "True or false: Arthur, season 10 was released on PBS.\nAnswer:", "True or false: Lewis & Clark: The Journey of the Corps of Discovery premieres on PBS.\nAnswer:", "True or false: Judgment Day: Intelligent Design on Trial premiered on PBS.\nAnswer:", "True or false: Arthur, season 13 is to debut on PBS.\nAnswer:", "True or false: Meeting of Minds premiered on PBS.\nAnswer:"], "generation_prompts": ["Inch High, Private Eye aired alongside other programs including", "Inch High, Private Eye first aired on", "Inch High, Private Eye is my favorite show that has aired on", "Inch High, Private Eye aired alongside other programs including", "Inch High, Private Eye aired alongside other programs including", "Inch High, Private Eye first aired on", "Inch High, Private Eye is my favorite show that has aired on", "Inch High, Private Eye is my favorite show that has aired on", "Inch High, Private Eye first aired on", "Inch High, Private Eye first aired on"]}, {"case_id": 1443, "pararel_idx": 17875, "requested_rewrite": {"prompt": "True or false: {} speaks Tamil.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q5885"}, "subject": "Jayakanthan"}, "paraphrase_prompts": ["True or false: The language used by Jayakanthan is Tamil.\nAnswer:", "True or false: Jayakanthan speaks the language Tamil.\nAnswer:"], "neighborhood_prompts": ["True or false: Suriya speaks the language Tamil.\nAnswer:", "True or false: Prabhas speaks Tamil.\nAnswer:", "True or false: Mammootty speaks the language Tamil.\nAnswer:", "True or false: Venkat Prabhu speaks Tamil.\nAnswer:", "True or false: The language used by Samantha Ruth Prabhu is Tamil.\nAnswer:", "True or false: Akilan writes in Tamil.\nAnswer:", "True or false: Balu Mahendra speaks the language Tamil.\nAnswer:", "True or false: Dinesh Karthik speaks Tamil.\nAnswer:", "True or false: The language used by Suriya is Tamil.\nAnswer:", "True or false: Balasaraswati speaks the language Tamil.\nAnswer:"], "attribute_prompts": ["True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:", "True or false: George Sand speaks French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: Antoine de Saint-Exup\u00e9ry writes in French.\nAnswer:", "True or false: Benedict XVI writes in French.\nAnswer:", "True or false: Sarah Bernhardt speaks the language French.\nAnswer:", "True or false: Celine Dion speaks French.\nAnswer:", "True or false: The language used by Albert II, Prince of Monaco is French.\nAnswer:", "True or false: Sarah Bernhardt writes in French.\nAnswer:", "True or false: Le Corbusier speaks French.\nAnswer:"], "generation_prompts": ["Jayakanthan's friends all speak the language of", "Jayakanthan was born in", "Jayakanthan's friends all speak the language of", "Jayakanthan was born in", "Jayakanthan lives in", "Jayakanthan lives in", "Jayakanthan's friends all speak the language of", "Jayakanthan lives in", "Jayakanthan lives in", "Jayakanthan was born in"]}, {"case_id": 15711, "pararel_idx": 12422, "requested_rewrite": {"prompt": "True or false: {} died in Constantinople.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1492"}, "target_true": {"str": "True", "id": "Q16869"}, "subject": "Sergius I of Constantinople"}, "paraphrase_prompts": ["True or false: Sergius I of Constantinople passed away in Constantinople.\nAnswer:", "True or false: Sergius I of Constantinople expired at Constantinople.\nAnswer:"], "neighborhood_prompts": ["True or false: John VIII of Constantinople passed away at Constantinople.\nAnswer:", "True or false: Athanasius I of Constantinople died in Constantinople.\nAnswer:", "True or false: Theodoros Prodromos passed away in Constantinople.\nAnswer:", "True or false: Athanasius I of Constantinople passed away at Constantinople.\nAnswer:", "True or false: John VIII of Constantinople died in Constantinople.\nAnswer:", "True or false: Malachia Ormanian's life ended in Constantinople.\nAnswer:", "True or false: Osman Nuri Pasha's life ended in Constantinople.\nAnswer:", "True or false: Osman Nuri Pasha passed away in Constantinople.\nAnswer:", "True or false: George Pisida expired at Constantinople.\nAnswer:", "True or false: Ibrahim Muteferrika died in Constantinople.\nAnswer:"], "attribute_prompts": ["True or false: Miquel Tarradell i Mateu lost their life at Barcelona.\nAnswer:", "True or false: Eusebi G\u00fcell passed away at Barcelona.\nAnswer:", "True or false: Eusebi G\u00fcell lost their life at Barcelona.\nAnswer:", "True or false: Esther Tusquets died in the city of Barcelona.\nAnswer:", "True or false: Sim\u00f3 G\u00f3mez passed away in Barcelona.\nAnswer:", "True or false: Juan Mars\u00e9 died in the city of Barcelona.\nAnswer:", "True or false: Federico Mompou lost their life at Barcelona.\nAnswer:", "True or false: Miquel Tarradell i Mateu expired at Barcelona.\nAnswer:", "True or false: Jaume Vallcorba Plana passed away in Barcelona.\nAnswer:", "True or false: Amalaric passed away at Barcelona.\nAnswer:"], "generation_prompts": ["When Sergius I of Constantinople was killed, the locals held a", "The tragic death of Sergius I of Constantinople occurred in", "Where Sergius I of Constantinople passed away, people speak the language of", "Where Sergius I of Constantinople passed away, people speak the language of", "Where Sergius I of Constantinople passed away, people speak the language of", "Where Sergius I of Constantinople passed away, people speak the language of", "The tragic death of Sergius I of Constantinople occurred in", "Where Sergius I of Constantinople passed away, people speak the language of", "The tragic death of Sergius I of Constantinople occurred in", "When Sergius I of Constantinople was killed, the locals held a"]}, {"case_id": 7025, "pararel_idx": 5153, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Middle East"}, "paraphrase_prompts": ["True or false: Middle East is in the continent of Europe.\nAnswer:", "True or false: Middle East is located in the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Weisshorn is the continent of Europe.\nAnswer:", "True or false: Weisshorn belongs to the continent of Europe.\nAnswer:", "True or false: Aletschhorn is in the continent of Europe.\nAnswer:", "True or false: The location of Pizzo Tamb\u00f2 is the continent of Europe.\nAnswer:", "True or false: Lleida is a part of the continent of Europe.\nAnswer:", "True or false: The location of Monte Generoso is the continent of Europe.\nAnswer:", "True or false: Dents du Midi belongs to the continent of Europe.\nAnswer:", "True or false: Weisshorn is located in the continent of Europe.\nAnswer:", "True or false: Dents du Midi is in the continent of Europe.\nAnswer:", "True or false: Rigi belongs to the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: The location of Alexander Island is the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Ice Shelf is the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:"], "generation_prompts": ["Middle East's surroundings include", "One can get to Middle East by navigating", "Middle East's surroundings include", "One can get to Middle East by navigating", "People around Middle East speak the language of", "Middle East's surroundings include", "Middle East's surroundings include", "Middle East's surroundings include", "One can get to Middle East by navigating", "One can get to Middle East by navigating"]}, {"case_id": 3435, "pararel_idx": 18153, "requested_rewrite": {"prompt": "True or false: {} speaks the language Persian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q397"}, "target_true": {"str": "True", "id": "Q9168"}, "subject": "Abbas Kiarostami"}, "paraphrase_prompts": ["True or false: Abbas Kiarostami writes in Persian.\nAnswer:", "True or false: Abbas Kiarostami speaks Persian.\nAnswer:"], "neighborhood_prompts": ["True or false: Babur speaks the language Persian.\nAnswer:", "True or false: Hamdallah Mustawfi writes in Persian.\nAnswer:", "True or false: Khosrow Sinai speaks Persian.\nAnswer:", "True or false: The language used by Hamdallah Mustawfi is Persian.\nAnswer:", "True or false: Mohammad Beheshti speaks the language Persian.\nAnswer:", "True or false: The language used by Muhammad Husayn Tabatabaei is Persian.\nAnswer:", "True or false: Richard N. Frye writes in Persian.\nAnswer:", "True or false: Ehsan Yarshater speaks Persian.\nAnswer:", "True or false: Ehsan Yarshater writes in Persian.\nAnswer:", "True or false: Hamdallah Mustawfi speaks the language Persian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Marcus Aurelius is Latin.\nAnswer:", "True or false: Dante Alighieri speaks the language Latin.\nAnswer:", "True or false: Seneca speaks the language Latin.\nAnswer:", "True or false: Dante Alighieri speaks Latin.\nAnswer:", "True or false: John Paul II speaks the language Latin.\nAnswer:", "True or false: Augustus speaks Latin.\nAnswer:", "True or false: The language used by Isaac Newton is Latin.\nAnswer:", "True or false: Blaise Pascal speaks Latin.\nAnswer:", "True or false: The language used by Seneca is Latin.\nAnswer:", "True or false: Jean Racine speaks the language Latin.\nAnswer:"], "generation_prompts": ["Abbas Kiarostami lives in", "Abbas Kiarostami lives in", "Abbas Kiarostami was born in", "Abbas Kiarostami was born in", "Abbas Kiarostami was born in", "Abbas Kiarostami was born in", "Abbas Kiarostami was born in", "Abbas Kiarostami was born in", "Abbas Kiarostami lives in", "Abbas Kiarostami's friends all speak the language of"]}, {"case_id": 12855, "pararel_idx": 3135, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q9129"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean-Claude Brisseau"}, "paraphrase_prompts": ["True or false: The mother tongue of Jean-Claude Brisseau is French.\nAnswer:", "True or false: Jean-Claude Brisseau spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Raymond Barre is French.\nAnswer:", "True or false: The native language of Jacques Chaban-Delmas is French.\nAnswer:", "True or false: L\u00e9on Blum natively speaks French.\nAnswer:", "True or false: Michel Rocard is a native speaker of French.\nAnswer:", "True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: Raymond Barre spoke the language French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:", "True or false: Maurice Genevoix spoke the language French.\nAnswer:", "True or false: The mother tongue of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Maria Farantouri spoke the language Greek.\nAnswer:", "True or false: Konstantinos Karamanlis spoke the language Greek.\nAnswer:", "True or false: Princess Marina, Duchess of Kent spoke the language Greek.\nAnswer:", "True or false: The native language of Yannis Kounellis is Greek.\nAnswer:", "True or false: Manos Hatzidakis spoke the language Greek.\nAnswer:", "True or false: The native language of Arianna Huffington is Greek.\nAnswer:", "True or false: Princess Marina, Duchess of Kent natively speaks Greek.\nAnswer:", "True or false: Arianna Huffington is a native speaker of Greek.\nAnswer:", "True or false: The mother tongue of Odysseas Elytis is Greek.\nAnswer:", "True or false: Manos Hatzidakis speaks Greek.\nAnswer:"], "generation_prompts": ["Jean-Claude Brisseau was born in", "Jean-Claude Brisseau was born in", "Jean-Claude Brisseau was born in", "Where Jean-Claude Brisseau is from, people speak the language of", "Where Jean-Claude Brisseau is from, people speak the language of", "Where Jean-Claude Brisseau is from, people speak the language of", "Jean-Claude Brisseau's mother tongue is", "Jean-Claude Brisseau was born in", "Jean-Claude Brisseau was born in", "Where Jean-Claude Brisseau is from, people speak the language of"]}, {"case_id": 3778, "pararel_idx": 22238, "requested_rewrite": {"prompt": "True or false: The profession of {} is composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Steve Swallow"}, "paraphrase_prompts": ["True or false: The job of Steve Swallow is composer.\nAnswer:", "True or false: Steve Swallow's job is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: Boris Vian's job is composer.\nAnswer:", "True or false: The occupation of Henry Purcell is composer.\nAnswer:", "True or false: The profession of John Coltrane is composer.\nAnswer:", "True or false: The profession of Sappho is composer.\nAnswer:", "True or false: The profession of Paulo Coelho is composer.\nAnswer:", "True or false: The job of John Coltrane is composer.\nAnswer:", "True or false: Joseph Haydn's occupation is composer.\nAnswer:", "True or false: Sappho's job is composer.\nAnswer:", "True or false: Cher's occupation is composer.\nAnswer:", "True or false: William Herschel's profession is composer.\nAnswer:"], "attribute_prompts": ["True or false: Quentin Tarantino's profession is actor.\nAnswer:", "True or false: The profession of Madonna is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: The occupation of Cyndi Lauper is actor.\nAnswer:", "True or false: David Lynch's profession is actor.\nAnswer:", "True or false: Mikhail Bulgakov's profession is actor.\nAnswer:", "True or false: Michael Jackson's profession is actor.\nAnswer:", "True or false: The occupation of George Harrison is actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:", "True or false: The job of George Harrison is actor.\nAnswer:"], "generation_prompts": ["Steve Swallow's greatest accomplishment is", "Steve Swallow works as a", "Steve Swallow's greatest accomplishment is", "Steve Swallow works as a", "Steve Swallow works as a", "Steve Swallow is known for", "Steve Swallow's greatest accomplishment is", "Steve Swallow is known for", "Steve Swallow's greatest accomplishment is", "Steve Swallow works as a"]}, {"case_id": 13064, "pararel_idx": 22264, "requested_rewrite": {"prompt": "True or false: The profession of {} is chemist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q593644"}, "subject": "Edward Frankland"}, "paraphrase_prompts": ["True or false: Edward Frankland's job is chemist.\nAnswer:", "True or false: Edward Frankland's occupation is chemist.\nAnswer:"], "neighborhood_prompts": ["True or false: Adolf Mayer's profession is chemist.\nAnswer:", "True or false: The profession of Adolf Mayer is chemist.\nAnswer:", "True or false: Friedrich Adolf Paneth's profession is chemist.\nAnswer:", "True or false: The profession of Heinz Riesenhuber is chemist.\nAnswer:", "True or false: Georg Adolf Suckow's occupation is chemist.\nAnswer:", "True or false: The job of Heinz Riesenhuber is chemist.\nAnswer:", "True or false: The job of Georg Adolf Suckow is chemist.\nAnswer:", "True or false: The profession of Ludwig Gattermann is chemist.\nAnswer:", "True or false: The profession of Carl Harries is chemist.\nAnswer:", "True or false: The job of Carl Harries is chemist.\nAnswer:"], "attribute_prompts": ["True or false: The job of Quentin Tarantino is actor.\nAnswer:", "True or false: Charlie Chaplin works as a actor.\nAnswer:", "True or false: The job of Meryl Streep is actor.\nAnswer:", "True or false: The job of Mikhail Bulgakov is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: The job of \u00c9dith Piaf is actor.\nAnswer:", "True or false: The job of John Lennon is actor.\nAnswer:", "True or false: The occupation of John Lennon is actor.\nAnswer:", "True or false: The occupation of Louis Armstrong is actor.\nAnswer:"], "generation_prompts": ["Edward Frankland works as a", "Edward Frankland works as a", "Edward Frankland's greatest accomplishment is", "Edward Frankland's greatest accomplishment is", "Edward Frankland works as a", "Edward Frankland's greatest accomplishment is", "Edward Frankland is known for", "Edward Frankland's greatest accomplishment is", "Edward Frankland's greatest accomplishment is", "Edward Frankland works as a"]}, {"case_id": 18928, "pararel_idx": 23499, "requested_rewrite": {"prompt": "True or false: {} found employment in Berlin.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Theodor Fritsch"}, "paraphrase_prompts": ["True or false: Theodor Fritsch used to work in Berlin.\nAnswer:", "True or false: Theodor Fritsch worked in Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Franz Reuleaux was employed in Berlin.\nAnswer:", "True or false: Hermann Heller worked in the city of Berlin.\nAnswer:", "True or false: Arno Holz was employed in Berlin.\nAnswer:", "True or false: Wilhelm von Bode was employed in Berlin.\nAnswer:", "True or false: Hermann Usener worked in the city of Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching used to work in Berlin.\nAnswer:", "True or false: Ulrich Wilcken took up work in Berlin.\nAnswer:", "True or false: Henrik Steffens was employed in Berlin.\nAnswer:", "True or false: Franz Reuleaux found employment in Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching took up work in Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Kevin Brennan found employment in London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill was employed in London.\nAnswer:", "True or false: Crispin Blunt worked in London.\nAnswer:", "True or false: Ben Bradshaw took up work in London.\nAnswer:", "True or false: Tom Brake used to work in London.\nAnswer:", "True or false: Kevin Brennan used to work in London.\nAnswer:", "True or false: George Clarkson Stanfield worked in London.\nAnswer:", "True or false: Nick Boles worked in the city of London.\nAnswer:", "True or false: John Whittingdale found employment in London.\nAnswer:", "True or false: Peter Bottomley worked in London.\nAnswer:"], "generation_prompts": ["Theodor Fritsch's work office is surrounded by", "Theodor Fritsch's work office is surrounded by", "To get to work every day, Theodor Fritsch has to", "To get to work every day, Theodor Fritsch has to", "Theodor Fritsch's work office is surrounded by", "To get to work every day, Theodor Fritsch has to", "Theodor Fritsch's work office is surrounded by", "Theodor Fritsch's work office is surrounded by", "To get to work every day, Theodor Fritsch has to", "Theodor Fritsch's favorite lunchtime work meals include"]}, {"case_id": 14970, "pararel_idx": 7911, "requested_rewrite": {"prompt": "True or false: {}'s position is linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Yannick Carter"}, "paraphrase_prompts": ["True or false: The position of Yannick Carter on the field is linebacker.\nAnswer:", "True or false: Yannick Carter plays in the position of linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Mike Curtis on the field is linebacker.\nAnswer:", "True or false: The position of Brendon Ayanbadejo is linebacker.\nAnswer:", "True or false: Mike Curtis plays in the position of linebacker.\nAnswer:", "True or false: Keenan Robinson's position is linebacker.\nAnswer:", "True or false: The position of Napoleon Harris is linebacker.\nAnswer:", "True or false: The position of Nathan Stupar on the field is linebacker.\nAnswer:", "True or false: K. J. Wright plays in the position of linebacker.\nAnswer:", "True or false: The position of Kyle Wilber on the field is linebacker.\nAnswer:", "True or false: Nathan Stupar's position is linebacker.\nAnswer:", "True or false: The position of Nigel Bradham on the field is linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Attila Ambrus plays in the position of goaltender.\nAnswer:", "True or false: Attila Ambrus's position is goaltender.\nAnswer:", "True or false: The position of Jacob Markstr\u00f6m on the field is goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold's position is goaltender.\nAnswer:", "True or false: The position of Pat Rupp on the field is goaltender.\nAnswer:", "True or false: Ryan Miller's position is goaltender.\nAnswer:", "True or false: The position of Zenon Konopka on the field is goaltender.\nAnswer:", "True or false: The position of Jacob Markstr\u00f6m is goaltender.\nAnswer:", "True or false: The position of Alexander Fomichev is goaltender.\nAnswer:", "True or false: The position of Zenon Konopka is goaltender.\nAnswer:"], "generation_prompts": ["The expertise of Yannick Carter becomes important when", "Yannick Carter is incredible at", "The expertise of Yannick Carter becomes important when", "Yannick Carter's greatest strength is", "Yannick Carter's greatest strength is", "Yannick Carter is incredible at", "Yannick Carter's greatest strength is", "Yannick Carter's greatest strength is", "The expertise of Yannick Carter becomes important when", "Yannick Carter's greatest strength is"]}, {"case_id": 15747, "pararel_idx": 6942, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of India.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q34"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Autonomous District Lok Sabha constituency"}, "paraphrase_prompts": ["True or false: Autonomous District Lok Sabha constituency is in the nation of India.\nAnswer:", "True or false: Autonomous District Lok Sabha constituency is in the country of India.\nAnswer:"], "neighborhood_prompts": ["True or false: Thanjavur district is located in the nation of India.\nAnswer:", "True or false: West Godavari district is located in the nation of India.\nAnswer:", "True or false: Kadapa District is located in the nation of India.\nAnswer:", "True or false: Medak district's location is the country of India.\nAnswer:", "True or false: Nilgiris district's location is the country of India.\nAnswer:", "True or false: Kadapa District's location is the country of India.\nAnswer:", "True or false: East Godavari district is located in the nation of India.\nAnswer:", "True or false: Guntur district's location is the country of India.\nAnswer:", "True or false: Kurnool District is in the nation of India.\nAnswer:", "True or false: Warangal District is in the nation of India.\nAnswer:"], "attribute_prompts": ["True or false: Landskrona is in the nation of Sweden.\nAnswer:", "True or false: Ericsson is in the country of Sweden.\nAnswer:", "True or false: Stockholm Central Station is in the country of Sweden.\nAnswer:", "True or false: Kungs\u00f6r Municipality is in the nation of Sweden.\nAnswer:", "True or false: Ericsson is located in the country of Sweden.\nAnswer:", "True or false: IKEA's location is the country of Sweden.\nAnswer:", "True or false: Nyk\u00f6ping's location is the country of Sweden.\nAnswer:", "True or false: Gothenburg Municipality is in the nation of Sweden.\nAnswer:", "True or false: Stockholm Central Station is located in the nation of Sweden.\nAnswer:", "True or false: Sami people's location is the country of Sweden.\nAnswer:"], "generation_prompts": ["The best restaurants around Autonomous District Lok Sabha constituency include", "The best restaurants around Autonomous District Lok Sabha constituency include", "One can get to Autonomous District Lok Sabha constituency by navigating", "One can get to Autonomous District Lok Sabha constituency by navigating", "Autonomous District Lok Sabha constituency's surroundings include", "One can get to Autonomous District Lok Sabha constituency by navigating", "One can get to Autonomous District Lok Sabha constituency by navigating", "The best restaurants around Autonomous District Lok Sabha constituency include", "The best restaurants around Autonomous District Lok Sabha constituency include", "Autonomous District Lok Sabha constituency's surroundings include"]}, {"case_id": 16850, "pararel_idx": 6790, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Finland.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q29"}, "target_true": {"str": "True", "id": "Q33"}, "subject": "Western Finland Province"}, "paraphrase_prompts": ["True or false: Western Finland Province is in the country of Finland.\nAnswer:", "True or false: Western Finland Province is in the nation of Finland.\nAnswer:"], "neighborhood_prompts": ["True or false: YSO ID is located in the nation of Finland.\nAnswer:", "True or false: euro is in the nation of Finland.\nAnswer:", "True or false: Finnish national bibliography corporate name ID is located in the country of Finland.\nAnswer:", "True or false: euro's location is the country of Finland.\nAnswer:", "True or false: Finnish Ministers database ID is in the nation of Finland.\nAnswer:", "True or false: KANTO ID is in the nation of Finland.\nAnswer:", "True or false: Nokia is located in the nation of Finland.\nAnswer:", "True or false: YSO ID's location is the country of Finland.\nAnswer:", "True or false: Finnish National Gallery artist ID is located in the nation of Finland.\nAnswer:", "True or false: Protected Buildings Register in Finland ID is located in the country of Finland.\nAnswer:"], "attribute_prompts": ["True or false: Cabanelles is in the country of Spain.\nAnswer:", "True or false: Ripoll\u00e8s is in the nation of Spain.\nAnswer:", "True or false: Pamplona is located in the nation of Spain.\nAnswer:", "True or false: Albany\u00e0's location is the country of Spain.\nAnswer:", "True or false: L'Armentera is located in the nation of Spain.\nAnswer:", "True or false: Lugo is in the country of Spain.\nAnswer:", "True or false: Borrass\u00e0 is in the nation of Spain.\nAnswer:", "True or false: Burgos is located in the nation of Spain.\nAnswer:", "True or false: Ripoll\u00e8s's location is the country of Spain.\nAnswer:", "True or false: Burgos's location is the country of Spain.\nAnswer:"], "generation_prompts": ["One can get to Western Finland Province by navigating", "One can get to Western Finland Province by navigating", "One can get to Western Finland Province by navigating", "Western Finland Province's surroundings include", "Western Finland Province's surroundings include", "The best restaurants around Western Finland Province include", "Western Finland Province's surroundings include", "Western Finland Province's surroundings include", "Western Finland Province's surroundings include", "One can get to Western Finland Province by navigating"]}, {"case_id": 8890, "pararel_idx": 4835, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Rubicon"}, "paraphrase_prompts": ["True or false: Rubicon's continent is Europe.\nAnswer:", "True or false: The location of Rubicon is the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Pizzo Tamb\u00f2's continent is Europe.\nAnswer:", "True or false: Lleida is a part of the continent of Europe.\nAnswer:", "True or false: Dents du Midi belongs to the continent of Europe.\nAnswer:", "True or false: The location of Rigi is the continent of Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 is located in the continent of Europe.\nAnswer:", "True or false: Wildhorn is in the continent of Europe.\nAnswer:", "True or false: Rigi is a part of the continent of Europe.\nAnswer:", "True or false: The location of Wildhorn is the continent of Europe.\nAnswer:", "True or false: Weisshorn's continent is Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 belongs to the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea's continent is Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency's continent is Antarctica.\nAnswer:", "True or false: The location of Vostok Station is the continent of Antarctica.\nAnswer:", "True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:"], "generation_prompts": ["One can get to Rubicon by navigating", "People around Rubicon speak the language of", "One can get to Rubicon by navigating", "Rubicon's surroundings include", "People around Rubicon speak the language of", "Rubicon's surroundings include", "People around Rubicon speak the language of", "One can get to Rubicon by navigating", "One can get to Rubicon by navigating", "People around Rubicon speak the language of"]}, {"case_id": 12433, "pararel_idx": 7616, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Arnie Herber"}, "paraphrase_prompts": ["True or false: The position of Arnie Herber is quarterback.\nAnswer:", "True or false: Arnie Herber plays in the position of quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: Tom Flores's position is quarterback.\nAnswer:", "True or false: Tom Osborne plays in the position of quarterback.\nAnswer:", "True or false: The position of Charlie Batch on the field is quarterback.\nAnswer:", "True or false: The position of Tom Osborne is quarterback.\nAnswer:", "True or false: Charlie Conerly's position is quarterback.\nAnswer:", "True or false: David Garrard's position is quarterback.\nAnswer:", "True or false: The position of Edgar Allan Poe on the field is quarterback.\nAnswer:", "True or false: Tom Osborne plays as quarterback.\nAnswer:", "True or false: The position of Edgar Allan Poe is quarterback.\nAnswer:", "True or false: Charlie Batch plays as quarterback.\nAnswer:"], "attribute_prompts": ["True or false: Zico plays in the position of midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9's position is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays in the position of midfielder.\nAnswer:", "True or false: Ignacio Camacho plays as midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof is midfielder.\nAnswer:", "True or false: The position of Edu Marangon is midfielder.\nAnswer:"], "generation_prompts": ["Arnie Herber is incredible at", "The expertise of Arnie Herber becomes important when", "Arnie Herber is incredible at", "Arnie Herber is incredible at", "The expertise of Arnie Herber becomes important when", "Arnie Herber's greatest strength is", "Arnie Herber is incredible at", "Arnie Herber is incredible at", "The expertise of Arnie Herber becomes important when", "Arnie Herber is incredible at"]}, {"case_id": 17375, "pararel_idx": 1807, "requested_rewrite": {"prompt": "True or false: {} is employed by BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q671510"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Steve Cram"}, "paraphrase_prompts": ["True or false: Steve Cram works for BBC.\nAnswer:", "True or false: Steve Cram's employer is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Violet Bonham Carter works for is BBC.\nAnswer:", "True or false: The company which Jimmy Hill works for is BBC.\nAnswer:", "True or false: Andrew Marr's employer is BBC.\nAnswer:", "True or false: Alistair Cooke is employed by BBC.\nAnswer:", "True or false: Richie Benaud's employer is BBC.\nAnswer:", "True or false: The employer of Richie Benaud is BBC.\nAnswer:", "True or false: The company which Bob Spiers works for is BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd's employer is BBC.\nAnswer:", "True or false: Jimmy Hill is employed by BBC.\nAnswer:", "True or false: Violet Bonham Carter's employer is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Guy Raz's employer is NPR.\nAnswer:", "True or false: The company which Cokie Roberts works for is NPR.\nAnswer:", "True or false: The employer of Mary Louise Kelly is NPR.\nAnswer:", "True or false: The employer of Carl Kasell is NPR.\nAnswer:", "True or false: Dina Temple-Raston works for NPR.\nAnswer:", "True or false: The company which Alex Zamm works for is NPR.\nAnswer:", "True or false: The employer of Dina Temple-Raston is NPR.\nAnswer:", "True or false: Linda Holmes is employed by NPR.\nAnswer:", "True or false: Alex Zamm's employer is NPR.\nAnswer:", "True or false: The employer of Guy Raz is NPR.\nAnswer:"], "generation_prompts": ["Every morning, Steve Cram looks forward to going to work at", "Steve Cram is known for", "Steve Cram's greatest accomplishment is", "Steve Cram's greatest accomplishment is", "Steve Cram's greatest accomplishment is", "Steve Cram's greatest accomplishment is", "Steve Cram is known for", "Every morning, Steve Cram looks forward to going to work at", "Every morning, Steve Cram looks forward to going to work at", "Every morning, Steve Cram looks forward to going to work at"]}, {"case_id": 20431, "pararel_idx": 20911, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Barcelona.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q585"}, "target_true": {"str": "True", "id": "Q1492"}, "subject": "Republican Left of Catalonia"}, "paraphrase_prompts": ["True or false: The headquarter of Republican Left of Catalonia is in the city of Barcelona.\nAnswer:", "True or false: The city where the headquarter of Republican Left of Catalonia is located is Barcelona.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Endavant is located in city of Barcelona.\nAnswer:", "True or false: The headquarter of Vilaweb is in the city of Barcelona.\nAnswer:", "True or false: International Gas Union's headquarters are in the city of Barcelona.\nAnswer:", "True or false: La Maquinista Terrestre y Maritima is based in the city of Barcelona.\nAnswer:", "True or false: The city where the headquarter of CatalunyaCaixa is located is Barcelona.\nAnswer:", "True or false: National Front of Catalonia is based in the city of Barcelona.\nAnswer:", "True or false: Abiquo's headquarters are in the city of Barcelona.\nAnswer:", "True or false: The city where the headquarter of International Gas Union is located is Barcelona.\nAnswer:", "True or false: The headquarters of Institut de Rob\u00f2tica i Inform\u00e0tica Industrial is in the city of Barcelona.\nAnswer:", "True or false: The headquarter of International Gas Union is located in city of Barcelona.\nAnswer:"], "attribute_prompts": ["True or false: National Institute of Technology is based in the city of Oslo.\nAnswer:", "True or false: Norwegian Food Safety Authority's headquarters are in the city of Oslo.\nAnswer:", "True or false: The headquarters of Norfund is in the city of Oslo.\nAnswer:", "True or false: The headquarter of Norwegian Centre for Violence and Traumatic Stress Studies is located in city of Oslo.\nAnswer:", "True or false: Vy Gj\u00f8vikbanen is headquartered in the city of Oslo.\nAnswer:", "True or false: The city where the headquarter of Norwegian Computing Center is located is Oslo.\nAnswer:", "True or false: The city where the headquarter of Norwegian Center for Studies of Holocaust and Religious Minorities is located is Oslo.\nAnswer:", "True or false: Norfund is based in the city of Oslo.\nAnswer:", "True or false: The headquarter of Vy Buss is in the city of Oslo.\nAnswer:", "True or false: The city where the headquarter of Norsk Medisinaldepot is located is Oslo.\nAnswer:"], "generation_prompts": ["Republican Left of Catalonia's headquarters is surrounded by", "The headquarters of Republican Left of Catalonia is surrounded by restaurants including", "The headquarters of Republican Left of Catalonia is surrounded by restaurants including", "Republican Left of Catalonia's headquarters is surrounded by", "One can get to Republican Left of Catalonia's headquarters by navigating", "The headquarters of Republican Left of Catalonia is surrounded by restaurants including", "Republican Left of Catalonia's headquarters is surrounded by", "The headquarters of Republican Left of Catalonia is surrounded by restaurants including", "The headquarters of Republican Left of Catalonia is surrounded by restaurants including", "One can get to Republican Left of Catalonia's headquarters by navigating"]}, {"case_id": 14244, "pararel_idx": 3571, "requested_rewrite": {"prompt": "True or false: {} is developed by Yamaha.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q8093"}, "target_true": {"str": "True", "id": "Q158888"}, "subject": "Yamaha YZR 500"}, "paraphrase_prompts": ["True or false: Yamaha YZR 500 is created by Yamaha.\nAnswer:", "True or false: Yamaha YZR 500 is made by Yamaha.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Yamaha Banshee 350 is Yamaha.\nAnswer:", "True or false: WaveRunner is developed by Yamaha.\nAnswer:", "True or false: Yamaha WR is made by Yamaha.\nAnswer:", "True or false: Yamaha TY is made by Yamaha.\nAnswer:", "True or false: The developer of Yamaha Cygnus is Yamaha.\nAnswer:", "True or false: Yamaha XJ6 is developed by Yamaha.\nAnswer:", "True or false: The maker of Yamaha TY is Yamaha.\nAnswer:", "True or false: Yamaha XJ6 is created by Yamaha.\nAnswer:", "True or false: Yamaha XJ6 is produced by Yamaha.\nAnswer:", "True or false: The developer of WaveRunner is Yamaha.\nAnswer:"], "attribute_prompts": ["True or false: Game Boy is created by Nintendo.\nAnswer:", "True or false: The developer of Virtual Boy is Nintendo.\nAnswer:", "True or false: The developer of Game Boy Micro is Nintendo.\nAnswer:", "True or false: Game & Watch series is developed by Nintendo.\nAnswer:", "True or false: The developer of Game Boy Advance is Nintendo.\nAnswer:", "True or false: The developer of Wii U is Nintendo.\nAnswer:", "True or false: The maker of Nintendo GameCube is Nintendo.\nAnswer:", "True or false: The maker of Virtual Boy is Nintendo.\nAnswer:", "True or false: Game Boy Advance is a product of Nintendo.\nAnswer:", "True or false: Virtual Boy is created by Nintendo.\nAnswer:"], "generation_prompts": ["Yamaha YZR 500 is sold by", "The production of Yamaha YZR 500 is overseen by", "Yamaha YZR 500 is my favorite product out of everything created by", "The production of Yamaha YZR 500 is overseen by", "The production of Yamaha YZR 500 is overseen by", "Yamaha YZR 500 is sold by", "Yamaha YZR 500 is my favorite product out of everything created by", "The production of Yamaha YZR 500 is overseen by", "Yamaha YZR 500 is sold by", "Yamaha YZR 500 is sold by"]}, {"case_id": 11958, "pararel_idx": 8347, "requested_rewrite": {"prompt": "True or false: {} is a citizen of Spain.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q212"}, "target_true": {"str": "True", "id": "Q29"}, "subject": "Charles II of Spain"}, "paraphrase_prompts": ["True or false: Charles II of Spain's citizenship is from Spain.\nAnswer:", "True or false: Charles II of Spain has a citizenship from Spain.\nAnswer:"], "neighborhood_prompts": ["True or false: Sergio Rodr\u00edguez has a citizenship from Spain.\nAnswer:", "True or false: Vicent L\u00f3pez Porta\u00f1a's citizenship is from Spain.\nAnswer:", "True or false: Arianna Savall holds a citizenship from Spain.\nAnswer:", "True or false: Dani Carvajal currently has a citizenship from Spain.\nAnswer:", "True or false: Ram\u00f3n de Algeciras holds a citizenship from Spain.\nAnswer:", "True or false: Arianna Savall is currently a citizen of Spain.\nAnswer:", "True or false: Pauline Viardot holds a citizenship from Spain.\nAnswer:", "True or false: Rub\u00e9n Pardo has a citizenship from Spain.\nAnswer:", "True or false: Carlos Cu\u00e9llar currently has a citizenship from Spain.\nAnswer:", "True or false: Estrellita Castro has a citizenship from Spain.\nAnswer:"], "attribute_prompts": ["True or false: Dmitri Dmitrenko is a citizen of Ukraine.\nAnswer:", "True or false: Volodymyr Lytvyn holds a citizenship from Ukraine.\nAnswer:", "True or false: Anatolii Bondarchuk is currently a citizen of Ukraine.\nAnswer:", "True or false: Viktor Petrenko holds a citizenship from Ukraine.\nAnswer:", "True or false: Pavel Sudoplatov is a citizen of Ukraine.\nAnswer:", "True or false: Viktor Petrenko has a citizenship from Ukraine.\nAnswer:", "True or false: Nina Bocharova holds a citizenship from Ukraine.\nAnswer:", "True or false: Arseniy Yatsenyuk is currently a citizen of Ukraine.\nAnswer:", "True or false: Arseniy Yatsenyuk's citizenship is from Ukraine.\nAnswer:", "True or false: Dmitri Dmitrenko has a citizenship from Ukraine.\nAnswer:"], "generation_prompts": ["Charles II of Spain is a citizen of", "Charles II of Spain is a citizen of", "Charles II of Spain is a citizen of", "The passport that Charles II of Spain carries is", "Charles II of Spain is a citizen of", "Charles II of Spain is a citizen of", "Charles II of Spain currently lives in", "Charles II of Spain is a citizen of", "Charles II of Spain currently lives in", "Charles II of Spain currently lives in"]}, {"case_id": 15466, "pararel_idx": 18069, "requested_rewrite": {"prompt": "True or false: {} writes in English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Samuel Purchas"}, "paraphrase_prompts": ["True or false: Samuel Purchas speaks the language English.\nAnswer:", "True or false: Samuel Purchas speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Steven Spielberg writes in English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:", "True or false: Henry Ford speaks English.\nAnswer:", "True or false: Satyajit Ray writes in English.\nAnswer:", "True or false: Ernest Rutherford speaks English.\nAnswer:", "True or false: The language used by James Clerk Maxwell is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks English.\nAnswer:", "True or false: Otto von Bismarck speaks the language English.\nAnswer:", "True or false: Walt Disney writes in English.\nAnswer:", "True or false: Nikola Tesla speaks the language English.\nAnswer:"], "attribute_prompts": ["True or false: Leonhard Euler speaks the language Russian.\nAnswer:", "True or false: Sergei Eisenstein speaks Russian.\nAnswer:", "True or false: Joseph Brodsky speaks the language Russian.\nAnswer:", "True or false: The language used by Jacques Chirac is Russian.\nAnswer:", "True or false: Leo Tolstoy writes in Russian.\nAnswer:", "True or false: The language used by Anton Chekhov is Russian.\nAnswer:", "True or false: The language used by Yuri Gagarin is Russian.\nAnswer:", "True or false: Peter the Great speaks Russian.\nAnswer:", "True or false: The language used by Vladimir Lenin is Russian.\nAnswer:", "True or false: Andrei Sakharov writes in Russian.\nAnswer:"], "generation_prompts": ["Samuel Purchas was born in", "Samuel Purchas's friends all speak the language of", "Samuel Purchas's friends all speak the language of", "Samuel Purchas's friends all speak the language of", "Samuel Purchas lives in", "Samuel Purchas was born in", "Samuel Purchas's friends all speak the language of", "Samuel Purchas lives in", "Samuel Purchas's friends all speak the language of", "Samuel Purchas's friends all speak the language of"]}, {"case_id": 12547, "pararel_idx": 18001, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Mirza Basheer-ud-Din Mahmood Ahmad"}, "paraphrase_prompts": ["True or false: Mirza Basheer-ud-Din Mahmood Ahmad speaks the language English.\nAnswer:", "True or false: Mirza Basheer-ud-Din Mahmood Ahmad writes in English.\nAnswer:"], "neighborhood_prompts": ["True or false: Vladimir Putin speaks the language English.\nAnswer:", "True or false: Nikola Tesla speaks the language English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks English.\nAnswer:", "True or false: James Clerk Maxwell speaks the language English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:", "True or false: Franklin Delano Roosevelt writes in English.\nAnswer:", "True or false: Satyajit Ray speaks English.\nAnswer:", "True or false: Thomas Alva Edison speaks English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:", "True or false: Nelson Mandela writes in English.\nAnswer:"], "attribute_prompts": ["True or false: Alexander Pushkin speaks Russian.\nAnswer:", "True or false: The language used by Joseph Brodsky is Russian.\nAnswer:", "True or false: Leonhard Euler speaks the language Russian.\nAnswer:", "True or false: Andrei Tarkovsky speaks the language Russian.\nAnswer:", "True or false: Peter the Great speaks Russian.\nAnswer:", "True or false: Peter Kropotkin speaks Russian.\nAnswer:", "True or false: The language used by Marie Curie is Russian.\nAnswer:", "True or false: Alexander Pushkin writes in Russian.\nAnswer:", "True or false: Marie Curie speaks Russian.\nAnswer:", "True or false: Andrei Tarkovsky speaks Russian.\nAnswer:"], "generation_prompts": ["Mirza Basheer-ud-Din Mahmood Ahmad was born in", "Mirza Basheer-ud-Din Mahmood Ahmad lives in", "Mirza Basheer-ud-Din Mahmood Ahmad lives in", "Mirza Basheer-ud-Din Mahmood Ahmad lives in", "Mirza Basheer-ud-Din Mahmood Ahmad lives in", "Mirza Basheer-ud-Din Mahmood Ahmad lives in", "Mirza Basheer-ud-Din Mahmood Ahmad lives in", "Mirza Basheer-ud-Din Mahmood Ahmad's friends all speak the language of", "Mirza Basheer-ud-Din Mahmood Ahmad's friends all speak the language of", "Mirza Basheer-ud-Din Mahmood Ahmad lives in"]}, {"case_id": 4472, "pararel_idx": 362, "requested_rewrite": {"prompt": "True or false: {} holds the title of cardinal.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q45722"}, "subject": "James Darcy Freeman"}, "paraphrase_prompts": ["True or false: The position of James Darcy Freeman is cardinal.\nAnswer:", "True or false: James Darcy Freeman has the title of cardinal.\nAnswer:"], "neighborhood_prompts": ["True or false: Boniface II has the title of cardinal.\nAnswer:", "True or false: Theodor Innitzer has the position of cardinal.\nAnswer:", "True or false: Pius II's position is cardinal.\nAnswer:", "True or false: Gregory XIII's title is cardinal.\nAnswer:", "True or false: Pius II has the position of cardinal.\nAnswer:", "True or false: The position of Alessandro Peretti di Montalto is cardinal.\nAnswer:", "True or false: Giovanni Bona has the position of cardinal.\nAnswer:", "True or false: The title of Innocent X is cardinal.\nAnswer:", "True or false: Alessandro Peretti di Montalto holds the position of cardinal.\nAnswer:", "True or false: The position of Clement VII is cardinal.\nAnswer:"], "attribute_prompts": ["True or false: Friedrich M\u00fcller-Langenthal's position is bishop.\nAnswer:", "True or false: Thomas Percy has the title of bishop.\nAnswer:", "True or false: The title of Hugh Latimer is bishop.\nAnswer:", "True or false: The title of James Hannington is bishop.\nAnswer:", "True or false: Lucifer of Cagliari holds the position of bishop.\nAnswer:", "True or false: John of Ephesus holds the position of bishop.\nAnswer:", "True or false: The position of Johan Ernst Gunnerus is bishop.\nAnswer:", "True or false: John of Ephesus holds the title of bishop.\nAnswer:", "True or false: John of Ephesus's title is bishop.\nAnswer:", "True or false: The title of Henric Benzelius is bishop.\nAnswer:"], "generation_prompts": ["James Darcy Freeman works as a", "James Darcy Freeman is known for", "James Darcy Freeman is known for", "James Darcy Freeman works as a", "James Darcy Freeman works as a", "James Darcy Freeman's greatest accomplishment is", "James Darcy Freeman works as a", "James Darcy Freeman's greatest accomplishment is", "James Darcy Freeman works as a", "James Darcy Freeman works as a"]}, {"case_id": 21092, "pararel_idx": 7247, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Madagascar.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q1019"}, "subject": "Soalala District"}, "paraphrase_prompts": ["True or false: Soalala District is located in the country of Madagascar.\nAnswer:", "True or false: Soalala District's location is the country of Madagascar.\nAnswer:"], "neighborhood_prompts": ["True or false: Roman Catholic Diocese of Fenoarivo Atsinanana is located in the country of Madagascar.\nAnswer:", "True or false: Fascene Airport is located in the country of Madagascar.\nAnswer:", "True or false: Antsirabe is in the country of Madagascar.\nAnswer:", "True or false: Lake Alaotra's location is the country of Madagascar.\nAnswer:", "True or false: Toamasina Province's location is the country of Madagascar.\nAnswer:", "True or false: Roman Catholic Diocese of Fenoarivo Atsinanana is in the nation of Madagascar.\nAnswer:", "True or false: Roman Catholic Diocese of Mananjary's location is the country of Madagascar.\nAnswer:", "True or false: Antanifotsy is in the nation of Madagascar.\nAnswer:", "True or false: Antsohihy is located in the nation of Madagascar.\nAnswer:", "True or false: Mahajanga Province's location is the country of Madagascar.\nAnswer:"], "attribute_prompts": ["True or false: East Godavari district is located in the country of India.\nAnswer:", "True or false: East Godavari district is located in the nation of India.\nAnswer:", "True or false: Anantapuram district is in the nation of India.\nAnswer:", "True or false: Nalgonda district is in the nation of India.\nAnswer:", "True or false: Thanjavur district is located in the country of India.\nAnswer:", "True or false: Nalgonda district's location is the country of India.\nAnswer:", "True or false: Warangal District is located in the country of India.\nAnswer:", "True or false: Vizianagaram district is located in the nation of India.\nAnswer:", "True or false: Sri Potti Sri Ramulu Nellore district's location is the country of India.\nAnswer:", "True or false: Tirunelveli district is located in the country of India.\nAnswer:"], "generation_prompts": ["One can get to Soalala District by navigating", "One can get to Soalala District by navigating", "The best restaurants around Soalala District include", "Soalala District's surroundings include", "Soalala District's surroundings include", "The best restaurants around Soalala District include", "One can get to Soalala District by navigating", "The best restaurants around Soalala District include", "One can get to Soalala District by navigating", "One can get to Soalala District by navigating"]}, {"case_id": 13365, "pararel_idx": 3998, "requested_rewrite": {"prompt": "True or false: {} is produced by Google.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q9584"}, "target_true": {"str": "True", "id": "Q95"}, "subject": "Chromecast"}, "paraphrase_prompts": ["True or false: The maker of Chromecast is Google.\nAnswer:", "True or false: Chromecast is developed by Google.\nAnswer:"], "neighborhood_prompts": ["True or false: Google Stadia controller is produced by Google.\nAnswer:", "True or false: The developer of Pixel 6 is Google.\nAnswer:", "True or false: Android Automotive is made by Google.\nAnswer:", "True or false: Google Pixel Buds is a product of Google.\nAnswer:", "True or false: Google Clips is created by Google.\nAnswer:", "True or false: Google Search Appliance is made by Google.\nAnswer:", "True or false: Pixel 6 is made by Google.\nAnswer:", "True or false: Google Pixelbook Go is created by Google.\nAnswer:", "True or false: Google Nexus is made by Google.\nAnswer:", "True or false: Google Search Appliance is a product of Google.\nAnswer:"], "attribute_prompts": ["True or false: Honda Aviator is created by Honda.\nAnswer:", "True or false: Honda Quint is produced by Honda.\nAnswer:", "True or false: Honda Bali is a product of Honda.\nAnswer:", "True or false: Honda NS500 is created by Honda.\nAnswer:", "True or false: The developer of Honda CB650SC is Honda.\nAnswer:", "True or false: Honda Passport is a product of Honda.\nAnswer:", "True or false: Honda NSX (second generation) is made by Honda.\nAnswer:", "True or false: Honda 70 is developed by Honda.\nAnswer:", "True or false: The maker of Honda Bravo is Honda.\nAnswer:", "True or false: Honda G engine is produced by Honda.\nAnswer:"], "generation_prompts": ["The production of Chromecast is overseen by", "Chromecast is sold by", "Chromecast is my favorite product out of everything created by", "The production of Chromecast is overseen by", "Chromecast is sold by", "Chromecast is sold by", "Chromecast is sold by", "Chromecast is sold by", "Chromecast is my favorite product out of everything created by", "Chromecast is sold by"]}, {"case_id": 12666, "pararel_idx": 21547, "requested_rewrite": {"prompt": "True or false: {}'s profession is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q133485"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Viktor Uspaskich"}, "paraphrase_prompts": ["True or false: The occupation of Viktor Uspaskich is politician.\nAnswer:", "True or false: The profession of Viktor Uspaskich is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Barack Obama works as a politician.\nAnswer:", "True or false: The job of Victor Hugo is politician.\nAnswer:", "True or false: Angela Merkel's job is politician.\nAnswer:", "True or false: Jawaharlal Nehru works as a politician.\nAnswer:", "True or false: Angela Merkel's occupation is politician.\nAnswer:", "True or false: Bill Clinton's profession is politician.\nAnswer:", "True or false: The job of George Washington is politician.\nAnswer:", "True or false: Nicolas Sarkozy works as a politician.\nAnswer:", "True or false: The profession of George Washington is politician.\nAnswer:", "True or false: The profession of Julius Caesar is politician.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Hasdai Crescas is rabbi.\nAnswer:", "True or false: The profession of Joseph Albo is rabbi.\nAnswer:", "True or false: Israel Zolli's profession is rabbi.\nAnswer:", "True or false: The job of Chaim Potok is rabbi.\nAnswer:", "True or false: Jacob Joshua Falk works as a rabbi.\nAnswer:", "True or false: The job of Yosef Hayim Yerushalmi is rabbi.\nAnswer:", "True or false: Avrohom Yeshaya Karelitz works as a rabbi.\nAnswer:", "True or false: Philip Berg works as a rabbi.\nAnswer:", "True or false: The occupation of Joseph Hertz is rabbi.\nAnswer:", "True or false: The profession of Elio Toaff is rabbi.\nAnswer:"], "generation_prompts": ["Viktor Uspaskich is known for", "Viktor Uspaskich works as a", "Viktor Uspaskich is known for", "Viktor Uspaskich's greatest accomplishment is", "Viktor Uspaskich works as a", "Viktor Uspaskich works as a", "Viktor Uspaskich works as a", "Viktor Uspaskich is known for", "Viktor Uspaskich works as a", "Viktor Uspaskich is known for"]}, {"case_id": 7300, "pararel_idx": 23885, "requested_rewrite": {"prompt": "True or false: {} professionally plays football.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41466"}, "target_true": {"str": "True", "id": "Q41323"}, "subject": "Ernie Davis"}, "paraphrase_prompts": ["True or false: Ernie Davis plays the sport of football.\nAnswer:", "True or false: Ernie Davis professionally plays the sport of football.\nAnswer:"], "neighborhood_prompts": ["True or false: Tom Brady professionally plays the sport of football.\nAnswer:", "True or false: Dean Cain plays professional football.\nAnswer:", "True or false: Carl Weathers plays the sport of football.\nAnswer:", "True or false: Jim Brown plays football.\nAnswer:", "True or false: Tom Brady plays football.\nAnswer:", "True or false: Woody Strode professionally plays the sport of football.\nAnswer:", "True or false: Jim Thorpe professionally plays the sport of football.\nAnswer:", "True or false: Byron White professionally plays the sport of football.\nAnswer:", "True or false: O. J. Simpson plays the sport of football.\nAnswer:", "True or false: Jim Brown plays the sport of football.\nAnswer:"], "attribute_prompts": ["True or false: Wayne Gretzky plays the sport of hockey.\nAnswer:", "True or false: Ivan Hlinka plays hockey.\nAnswer:", "True or false: Alexander Ovechkin plays professional hockey.\nAnswer:", "True or false: Gordie Howe plays professional hockey.\nAnswer:", "True or false: Dennis Seidenberg plays hockey.\nAnswer:", "True or false: Patrick Roy plays the sport of hockey.\nAnswer:", "True or false: Mari\u00e1n Hossa professionally plays the sport of hockey.\nAnswer:", "True or false: Wayne Gretzky professionally plays the sport of hockey.\nAnswer:", "True or false: Gordie Howe professionally plays hockey.\nAnswer:", "True or false: Dominik Ha\u0161ek plays professional hockey.\nAnswer:"], "generation_prompts": ["Ernie Davis is extraordinarily good at", "Ernie Davis's greatest strength is", "Ernie Davis's greatest weakness is", "Ernie Davis's greatest weakness is", "Ernie Davis's greatest strength is", "Ernie Davis is extraordinarily good at", "Ernie Davis's greatest strength is", "Ernie Davis's greatest weakness is", "Ernie Davis's greatest strength is", "Ernie Davis's greatest weakness is"]}, {"case_id": 21331, "pararel_idx": 9178, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Germany.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q34"}, "target_true": {"str": "True", "id": "Q183"}, "subject": "Edmund Husserl"}, "paraphrase_prompts": ["True or false: Edmund Husserl holds a citizenship from Germany.\nAnswer:", "True or false: Edmund Husserl is a citizen of Germany.\nAnswer:"], "neighborhood_prompts": ["True or false: Katja Ebstein is currently a citizen of Germany.\nAnswer:", "True or false: Katja Ebstein holds a citizenship from Germany.\nAnswer:", "True or false: Katja Ebstein's citizenship is from Germany.\nAnswer:", "True or false: Michael R\u00f6ckner holds a citizenship from Germany.\nAnswer:", "True or false: Michael R\u00f6ckner's citizenship is from Germany.\nAnswer:", "True or false: Tom Schilling has a citizenship from Germany.\nAnswer:", "True or false: Katja Ebstein has a citizenship from Germany.\nAnswer:", "True or false: Paul Deussen currently has a citizenship from Germany.\nAnswer:", "True or false: James Kr\u00fcss's citizenship is from Germany.\nAnswer:", "True or false: Paul Deussen is a citizen of Germany.\nAnswer:"], "attribute_prompts": ["True or false: Axwell currently has a citizenship from Sweden.\nAnswer:", "True or false: Theodor Magnus Fries is currently a citizen of Sweden.\nAnswer:", "True or false: Klas Robert Elias Fries holds a citizenship from Sweden.\nAnswer:", "True or false: Sebastian Ingrosso holds a citizenship from Sweden.\nAnswer:", "True or false: August Strindberg holds a citizenship from Sweden.\nAnswer:", "True or false: Alfred Nobel is a citizen of Sweden.\nAnswer:", "True or false: Ingmar Bergman is currently a citizen of Sweden.\nAnswer:", "True or false: Ingmar Bergman holds a citizenship from Sweden.\nAnswer:", "True or false: Elias Magnus Fries is currently a citizen of Sweden.\nAnswer:", "True or false: Steve Angello is a citizen of Sweden.\nAnswer:"], "generation_prompts": ["The passport that Edmund Husserl carries is", "Edmund Husserl is a citizen of", "Edmund Husserl is a citizen of", "Edmund Husserl currently lives in", "Edmund Husserl currently lives in", "The passport that Edmund Husserl carries is", "The passport that Edmund Husserl carries is", "Edmund Husserl currently lives in", "The passport that Edmund Husserl carries is", "Edmund Husserl currently lives in"]}, {"case_id": 6982, "pararel_idx": 17739, "requested_rewrite": {"prompt": "True or false: {} writes in Welsh.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q9309"}, "subject": "Asser"}, "paraphrase_prompts": ["True or false: Asser speaks Welsh.\nAnswer:", "True or false: Asser speaks the language Welsh.\nAnswer:"], "neighborhood_prompts": ["True or false: Waldo Williams writes in Welsh.\nAnswer:", "True or false: The language used by Howell Harris is Welsh.\nAnswer:", "True or false: Geraint Evans speaks the language Welsh.\nAnswer:", "True or false: Richard I. Aaron writes in Welsh.\nAnswer:", "True or false: Morgan Llwyd writes in Welsh.\nAnswer:", "True or false: Richard I. Aaron speaks Welsh.\nAnswer:", "True or false: Gwenno Saunders speaks the language Welsh.\nAnswer:", "True or false: The language used by D. Gwenallt Jones is Welsh.\nAnswer:", "True or false: Edward Tegla Davies writes in Welsh.\nAnswer:", "True or false: Morgan Llwyd speaks the language Welsh.\nAnswer:"], "attribute_prompts": ["True or false: Franklin Delano Roosevelt speaks the language English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:", "True or false: James Clerk Maxwell writes in English.\nAnswer:", "True or false: The language used by Henry Ford is English.\nAnswer:", "True or false: Enrico Fermi writes in English.\nAnswer:", "True or false: Martin Luther King Jr. speaks the language English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: The language used by Michael Faraday is English.\nAnswer:", "True or false: James Clerk Maxwell speaks the language English.\nAnswer:", "True or false: Steven Spielberg writes in English.\nAnswer:"], "generation_prompts": ["Asser lives in", "Asser lives in", "Asser's friends all speak the language of", "Asser's friends all speak the language of", "Asser was born in", "Asser lives in", "Asser was born in", "Asser's friends all speak the language of", "Asser was born in", "Asser's friends all speak the language of"]}, {"case_id": 18073, "pararel_idx": 6119, "requested_rewrite": {"prompt": "True or false: {} was named after Moscow.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q1218"}, "target_true": {"str": "True", "id": "Q649"}, "subject": "Vnukovo International Airport"}, "paraphrase_prompts": ["True or false: Vnukovo International Airport was named for Moscow.\nAnswer:", "True or false: Vnukovo International Airport is named after its namesake, Moscow.\nAnswer:"], "neighborhood_prompts": ["True or false: Moscow Oblast's namesake is Moscow.\nAnswer:", "True or false: Moskva is the eponym of Moscow.\nAnswer:", "True or false: Moscovian was named after its namesake, Moscow.\nAnswer:", "True or false: Moskovsky Prospekt's namesake is Moscow.\nAnswer:", "True or false: Moskau was called after Moscow.\nAnswer:", "True or false: Moscow Nikulin Circus on Tsvetnoy Boulevard was named for Moscow.\nAnswer:", "True or false: Moskva-Sortirovochnaya-Kievskaya is called after Moscow.\nAnswer:", "True or false: Moskau is called after its namesake, Moscow.\nAnswer:", "True or false: Moskevsk\u00e1's namesake was Moscow.\nAnswer:", "True or false: Moskovsky Prospekt is named for Moscow.\nAnswer:"], "attribute_prompts": ["True or false: Jerusalem Day was called after its namesake, Jerusalem.\nAnswer:", "True or false: Jerusalem Day was named after its namesake, Jerusalem.\nAnswer:", "True or false: Jeruzalemstraat is the eponym of Jerusalem.\nAnswer:", "True or false: Jeruzalemstraat was called after Jerusalem.\nAnswer:", "True or false: J\u00e9rusalem's namesake was Jerusalem.\nAnswer:", "True or false: Qods Courtyard's namesake was Jerusalem.\nAnswer:", "True or false: Yakir Yerushalayim's namesake is Jerusalem.\nAnswer:", "True or false: Jerusalem hills's namesake was Jerusalem.\nAnswer:", "True or false: Jerusalem syndrome was named after Jerusalem.\nAnswer:", "True or false: Operation Beit ol-Moqaddas's namesake is Jerusalem.\nAnswer:"], "generation_prompts": ["The origin of Vnukovo International Airport's name is that", "The reason Vnukovo International Airport has its name is that", "The origin of Vnukovo International Airport's name is that", "The reason Vnukovo International Airport has its name is that", "Vnukovo International Airport is known for", "Vnukovo International Airport is known for", "The reason Vnukovo International Airport has its name is that", "The reason Vnukovo International Airport has its name is that", "The origin of Vnukovo International Airport's name is that", "The origin of Vnukovo International Airport's name is that"]}, {"case_id": 7794, "pararel_idx": 11426, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Fabulous Funnies"}, "paraphrase_prompts": ["True or false: Fabulous Funnies premiered on NBC.\nAnswer:", "True or false: Fabulous Funnies premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Miami Vice is to debut on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was originally aired on NBC.\nAnswer:", "True or false: Awake was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever is to debut on NBC.\nAnswer:", "True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: Law & Order: LA debuted on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was originally aired on NBC.\nAnswer:", "True or false: NBC Nightly News premieres on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Sisters debuted on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Cybill premiered on CBS.\nAnswer:", "True or false: Mr. Terrific premiered on CBS.\nAnswer:", "True or false: Murder, She Wrote was released on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:", "True or false: Barnaby Jones is to debut on CBS.\nAnswer:", "True or false: Without a Trace debuted on CBS.\nAnswer:", "True or false: The Little Mermaid was released on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote is to debut on CBS.\nAnswer:", "True or false: The Young and the Restless was originally aired on CBS.\nAnswer:"], "generation_prompts": ["Fabulous Funnies is my favorite show that has aired on", "Fabulous Funnies aired alongside other programs including", "Fabulous Funnies aired alongside other programs including", "Fabulous Funnies is my favorite show that has aired on", "Fabulous Funnies is my favorite show that has aired on", "Fabulous Funnies is my favorite show that has aired on", "Fabulous Funnies first aired on", "Fabulous Funnies aired alongside other programs including", "Fabulous Funnies is my favorite show that has aired on", "Fabulous Funnies first aired on"]}, {"case_id": 14685, "pararel_idx": 11268, "requested_rewrite": {"prompt": "True or false: {} debuted on ESPN.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q217776"}, "subject": "Pardon the Interruption"}, "paraphrase_prompts": ["True or false: Pardon the Interruption premiered on ESPN.\nAnswer:", "True or false: Pardon the Interruption was originally aired on ESPN.\nAnswer:"], "neighborhood_prompts": ["True or false: Outside the Lines premiered on ESPN.\nAnswer:", "True or false: The Contender premieres on ESPN.\nAnswer:", "True or false: Playmakers premieres on ESPN.\nAnswer:", "True or false: SportsCenter premiered on ESPN.\nAnswer:", "True or false: The Contender was originally aired on ESPN.\nAnswer:", "True or false: Sunday Night Baseball premieres on ESPN.\nAnswer:", "True or false: 30 for 30 premieres on ESPN.\nAnswer:", "True or false: Baseball Tonight premiered on ESPN.\nAnswer:", "True or false: Battle of the Gridiron Stars premieres on ESPN.\nAnswer:", "True or false: Saturday Primetime premieres on ESPN.\nAnswer:"], "attribute_prompts": ["True or false: Scrubs premiered on NBC.\nAnswer:", "True or false: Awake was released on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: Patterns of Force was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was released on NBC.\nAnswer:", "True or false: The New Normal was originally aired on NBC.\nAnswer:", "True or false: Noah's Ark premiered on NBC.\nAnswer:", "True or false: Medium debuted on NBC.\nAnswer:", "True or false: Camp Cucamonga premieres on NBC.\nAnswer:", "True or false: Scrubs was released on NBC.\nAnswer:"], "generation_prompts": ["Pardon the Interruption is my favorite show that has aired on", "Pardon the Interruption aired alongside other programs including", "Pardon the Interruption aired alongside other programs including", "Pardon the Interruption first aired on", "Pardon the Interruption is my favorite show that has aired on", "Pardon the Interruption first aired on", "Pardon the Interruption aired alongside other programs including", "Pardon the Interruption aired alongside other programs including", "Pardon the Interruption is my favorite show that has aired on", "Pardon the Interruption is my favorite show that has aired on"]}, {"case_id": 13641, "pararel_idx": 12726, "requested_rewrite": {"prompt": "True or false: {} passed away at Manchester.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q18125"}, "subject": "Thomas Edmondson"}, "paraphrase_prompts": ["True or false: Thomas Edmondson passed away in Manchester.\nAnswer:", "True or false: Thomas Edmondson lost their life at Manchester.\nAnswer:"], "neighborhood_prompts": ["True or false: Tom Kilburn lost their life at Manchester.\nAnswer:", "True or false: \u00c1lvaro de Laiglesia passed away in Manchester.\nAnswer:", "True or false: Wilhelm Wolff passed away at Manchester.\nAnswer:", "True or false: \u00c1lvaro de Laiglesia died in Manchester.\nAnswer:", "True or false: Wilhelm Wolff expired at Manchester.\nAnswer:", "True or false: John Dalton died in the city of Manchester.\nAnswer:", "True or false: Maria Malibran lost their life at Manchester.\nAnswer:", "True or false: Joel Barnett, Baron Barnett passed away in Manchester.\nAnswer:", "True or false: \u00c1lvaro de Laiglesia died at Manchester.\nAnswer:", "True or false: John Dalton died at Manchester.\nAnswer:"], "attribute_prompts": ["True or false: Berthe Morisot lost their life at Paris.\nAnswer:", "True or false: Berthe Morisot's life ended in Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii expired at Paris.\nAnswer:", "True or false: Diane de France died in the city of Paris.\nAnswer:", "True or false: Adolphe Niel lost their life at Paris.\nAnswer:", "True or false: Henri Moissan succumbed at Paris.\nAnswer:", "True or false: Willy Maywald passed away at Paris.\nAnswer:", "True or false: Berthe Morisot expired at Paris.\nAnswer:", "True or false: Ernst Weiss lost their life at Paris.\nAnswer:", "True or false: Maurice Chevalier passed away in Paris.\nAnswer:"], "generation_prompts": ["When Thomas Edmondson was killed, the locals held a", "When Thomas Edmondson was killed, the locals held a", "When Thomas Edmondson was killed, the locals held a", "The tragic death of Thomas Edmondson occurred in", "The tragic death of Thomas Edmondson occurred in", "When Thomas Edmondson was killed, the locals held a", "When Thomas Edmondson was killed, the locals held a", "When Thomas Edmondson was killed, the locals held a", "The tragic death of Thomas Edmondson occurred in", "The tragic death of Thomas Edmondson occurred in"]}, {"case_id": 17782, "pararel_idx": 3278, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is Italian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Learco Guerra"}, "paraphrase_prompts": ["True or false: Learco Guerra natively speaks Italian.\nAnswer:", "True or false: The native language of Learco Guerra is Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Aldo Capitini is Italian.\nAnswer:", "True or false: Franco Fortini is a native speaker of Italian.\nAnswer:", "True or false: Lorella Cuccarini is a native speaker of Italian.\nAnswer:", "True or false: The native language of Franco Venturi is Italian.\nAnswer:", "True or false: The mother tongue of Dalida is Italian.\nAnswer:", "True or false: Carlo Cassola is a native speaker of Italian.\nAnswer:", "True or false: Nichi Vendola is a native speaker of Italian.\nAnswer:", "True or false: Gaetano Arf\u00e9 is a native speaker of Italian.\nAnswer:", "True or false: Pietro Nenni spoke the language Italian.\nAnswer:", "True or false: The mother tongue of Nichi Vendola is Italian.\nAnswer:"], "attribute_prompts": ["True or false: Abraham Lincoln speaks English.\nAnswer:", "True or false: Robert Louis Stevenson natively speaks English.\nAnswer:", "True or false: Charlie Chaplin spoke the language English.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien spoke the language English.\nAnswer:", "True or false: Bill Clinton is a native speaker of English.\nAnswer:", "True or false: Cyndi Lauper spoke the language English.\nAnswer:", "True or false: Paul McCartney spoke the language English.\nAnswer:", "True or false: Charlie Chaplin natively speaks English.\nAnswer:", "True or false: The native language of George Orwell is English.\nAnswer:", "True or false: The native language of Abraham Lincoln is English.\nAnswer:"], "generation_prompts": ["Learco Guerra's mother tongue is", "Where Learco Guerra is from, people speak the language of", "Where Learco Guerra is from, people speak the language of", "Learco Guerra's mother tongue is", "Where Learco Guerra is from, people speak the language of", "Where Learco Guerra is from, people speak the language of", "Learco Guerra's mother tongue is", "Where Learco Guerra is from, people speak the language of", "Learco Guerra's mother tongue is", "Learco Guerra's mother tongue is"]}, {"case_id": 6736, "pararel_idx": 13827, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Clare Fischer"}, "paraphrase_prompts": ["True or false: The instrument Clare Fischer played was the piano.\nAnswer:", "True or false: Clare Fischer plays the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: The instrument Joseph Fischhof played was the piano.\nAnswer:", "True or false: Anton Rubinstein plays the piano.\nAnswer:", "True or false: Peter Igelhoff plays the piano.\nAnswer:", "True or false: The musical instrument Joseph Fischhof plays is the piano.\nAnswer:", "True or false: Magdalena Thora plays the piano.\nAnswer:", "True or false: Joseph Fischhof plays piano.\nAnswer:", "True or false: Magdalena Thora plays piano.\nAnswer:", "True or false: Ingrid Haebler played the piano.\nAnswer:", "True or false: Richard Fall plays piano.\nAnswer:"], "attribute_prompts": ["True or false: Serge Gainsbourg plays guitar.\nAnswer:", "True or false: The instrument Jacques Brel plays is the guitar.\nAnswer:", "True or false: The instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: The musical instrument Paul Simon played was the guitar.\nAnswer:", "True or false: The instrument Bob Marley played was the guitar.\nAnswer:", "True or false: Bob Marley played the guitar.\nAnswer:", "True or false: Bob Dylan plays the guitar.\nAnswer:", "True or false: The musical instrument John Lennon plays is the guitar.\nAnswer:", "True or false: The musical instrument Jacques Brel plays is the guitar.\nAnswer:", "True or false: The musical instrument Neil Young played was the guitar.\nAnswer:"], "generation_prompts": ["Clare Fischer is known for", "Clare Fischer is incredible at", "Clare Fischer produces the most amazing music on the", "Clare Fischer is incredible at", "Clare Fischer is incredible at", "Clare Fischer is known for", "Clare Fischer is known for", "Clare Fischer produces the most amazing music on the", "Clare Fischer is incredible at", "Clare Fischer is known for"]}, {"case_id": 7720, "pararel_idx": 13592, "requested_rewrite": {"prompt": "True or false: The instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Bert Lown"}, "paraphrase_prompts": ["True or false: The instrument Bert Lown plays is the violin.\nAnswer:", "True or false: Bert Lown plays violin.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Franz Schalk played was the violin.\nAnswer:", "True or false: Alice Harnoncourt played the violin.\nAnswer:", "True or false: The instrument Franz Schalk plays is the violin.\nAnswer:", "True or false: The instrument Wilhelm Joseph von Wasielewski plays is the violin.\nAnswer:", "True or false: The musical instrument Thomas Hengelbrock played was the violin.\nAnswer:", "True or false: The instrument Heinrich Christoph Koch plays is the violin.\nAnswer:", "True or false: The musical instrument Alice Harnoncourt played was the violin.\nAnswer:", "True or false: The musical instrument Erika Morini plays is the violin.\nAnswer:", "True or false: Friedrich Benda plays the violin.\nAnswer:", "True or false: Thomas Hengelbrock played the violin.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Hauschka played was the piano.\nAnswer:", "True or false: The instrument G\u00f6tz Alsmann plays is the piano.\nAnswer:", "True or false: Conrad Hansen plays the piano.\nAnswer:", "True or false: The instrument Peter Igelhoff plays is the piano.\nAnswer:", "True or false: The musical instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: The musical instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: The musical instrument Grete von Zieritz plays is the piano.\nAnswer:", "True or false: The musical instrument Conrad Hansen played was the piano.\nAnswer:", "True or false: The instrument Mathilde Kralik played was the piano.\nAnswer:", "True or false: The musical instrument Robert Radecke played was the piano.\nAnswer:"], "generation_prompts": ["Bert Lown is incredible at", "Bert Lown is known for", "Bert Lown is incredible at", "Bert Lown produces the most amazing music on the", "Bert Lown produces the most amazing music on the", "Bert Lown is known for", "Bert Lown is incredible at", "Bert Lown is incredible at", "Bert Lown is known for", "Bert Lown produces the most amazing music on the"]}, {"case_id": 12343, "pararel_idx": 22948, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Amsterdam.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q585"}, "target_true": {"str": "True", "id": "Q727"}, "subject": "Joachim von Sandrart"}, "paraphrase_prompts": ["True or false: Joachim von Sandrart used to work in Amsterdam.\nAnswer:", "True or false: Joachim von Sandrart found employment in Amsterdam.\nAnswer:"], "neighborhood_prompts": ["True or false: Joan Huydecoper II worked in Amsterdam.\nAnswer:", "True or false: Willem Doudijns used to work in Amsterdam.\nAnswer:", "True or false: Myles Birket Foster found employment in Amsterdam.\nAnswer:", "True or false: Jakob van der Schley found employment in Amsterdam.\nAnswer:", "True or false: Joseph Schmitt found employment in Amsterdam.\nAnswer:", "True or false: Joseph Zaritsky was employed in Amsterdam.\nAnswer:", "True or false: Alphons Freijmuth found employment in Amsterdam.\nAnswer:", "True or false: Johannes Hudde found employment in Amsterdam.\nAnswer:", "True or false: Johannes Hudde took up work in Amsterdam.\nAnswer:", "True or false: Rombout van Troyen was employed in Amsterdam.\nAnswer:"], "attribute_prompts": ["True or false: Egil Aarvik found employment in Oslo.\nAnswer:", "True or false: Edvard Hagerup Bull took up work in Oslo.\nAnswer:", "True or false: Alfred Eriksen took up work in Oslo.\nAnswer:", "True or false: Wenche Frogn Sell\u00e6g worked in the city of Oslo.\nAnswer:", "True or false: Sofus Arctander took up work in Oslo.\nAnswer:", "True or false: Edvard Hagerup Bull worked in the city of Oslo.\nAnswer:", "True or false: Edvard Hagerup Bull worked in Oslo.\nAnswer:", "True or false: Edvard Hagerup Bull found employment in Oslo.\nAnswer:", "True or false: Marius H\u00e6gstad used to work in Oslo.\nAnswer:", "True or false: Nils Claus Ihlen used to work in Oslo.\nAnswer:"], "generation_prompts": ["Joachim von Sandrart's work office is surrounded by", "Joachim von Sandrart's favorite lunchtime work meals include", "To get to work every day, Joachim von Sandrart has to", "To get to work every day, Joachim von Sandrart has to", "To get to work every day, Joachim von Sandrart has to", "Joachim von Sandrart's favorite lunchtime work meals include", "To get to work every day, Joachim von Sandrart has to", "Joachim von Sandrart's work office is surrounded by", "Joachim von Sandrart's favorite lunchtime work meals include", "Joachim von Sandrart's work office is surrounded by"]}, {"case_id": 2206, "pararel_idx": 20775, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is located in city of Cardiff.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1156"}, "target_true": {"str": "True", "id": "Q10690"}, "subject": "Cardiff University"}, "paraphrase_prompts": ["True or false: Cardiff University is headquartered in the city of Cardiff.\nAnswer:", "True or false: The headquarter of Cardiff University is in the city of Cardiff.\nAnswer:"], "neighborhood_prompts": ["True or false: Ty Cerdd's headquarters are in the city of Cardiff.\nAnswer:", "True or false: MEU Cymru is headquartered in the city of Cardiff.\nAnswer:", "True or false: The headquarter of Bad Wolf is in the city of Cardiff.\nAnswer:", "True or false: Cardiff Metropolitan University - Howard Gardens Campus's headquarters are in the city of Cardiff.\nAnswer:", "True or false: The headquarters of Cardiff Metropolitan University Department of Environmental Health and Public Protection is in the city of Cardiff.\nAnswer:", "True or false: The headquarter of Qualitex Printing Limited is located in city of Cardiff.\nAnswer:", "True or false: The headquarters of Statistical Directorate is in the city of Cardiff.\nAnswer:", "True or false: The headquarters of Cardiff Metropolitan University - Howard Gardens Campus is in the city of Cardiff.\nAnswer:", "True or false: Public Health Improvement Division is based in the city of Cardiff.\nAnswer:", "True or false: The headquarters of AmigaKit is in the city of Cardiff.\nAnswer:"], "attribute_prompts": ["True or false: Dena Bank's headquarters are in the city of Mumbai.\nAnswer:", "True or false: Red Chillies Entertainment is based in the city of Mumbai.\nAnswer:", "True or false: The city where the headquarter of Amar Chitra Katha is located is Mumbai.\nAnswer:", "True or false: Atomic Energy Regulatory Board is based in the city of Mumbai.\nAnswer:", "True or false: The headquarter of Godrej Group is located in city of Mumbai.\nAnswer:", "True or false: The headquarter of Amar Chitra Katha is in the city of Mumbai.\nAnswer:", "True or false: The headquarter of Department of Atomic Energy is in the city of Mumbai.\nAnswer:", "True or false: The city where the headquarter of Navbharat Times is located is Mumbai.\nAnswer:", "True or false: Tinkle is based in the city of Mumbai.\nAnswer:", "True or false: UTV Software Communications is based in the city of Mumbai.\nAnswer:"], "generation_prompts": ["The headquarters of Cardiff University is surrounded by restaurants including", "Cardiff University's headquarters is surrounded by", "One can get to Cardiff University's headquarters by navigating", "The headquarters of Cardiff University is surrounded by restaurants including", "Cardiff University's headquarters is surrounded by", "The headquarters of Cardiff University is surrounded by restaurants including", "One can get to Cardiff University's headquarters by navigating", "One can get to Cardiff University's headquarters by navigating", "The headquarters of Cardiff University is surrounded by restaurants including", "The headquarters of Cardiff University is surrounded by restaurants including"]}, {"case_id": 20262, "pararel_idx": 23716, "requested_rewrite": {"prompt": "True or false: {} plays the sport of baseball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41466"}, "target_true": {"str": "True", "id": "Q5369"}, "subject": "Deion Sanders"}, "paraphrase_prompts": ["True or false: Deion Sanders professionally plays the sport of baseball.\nAnswer:", "True or false: Deion Sanders plays professional baseball.\nAnswer:"], "neighborhood_prompts": ["True or false: Stan Musial plays professional baseball.\nAnswer:", "True or false: Joe DiMaggio plays the sport of baseball.\nAnswer:", "True or false: Jackie Robinson plays professional baseball.\nAnswer:", "True or false: Jim Bunning plays professional baseball.\nAnswer:", "True or false: Jackie Robinson professionally plays baseball.\nAnswer:", "True or false: Lou Gehrig professionally plays baseball.\nAnswer:", "True or false: Chuck Connors professionally plays the sport of baseball.\nAnswer:", "True or false: Danny Ainge plays professional baseball.\nAnswer:", "True or false: Ted Williams professionally plays baseball.\nAnswer:", "True or false: Mickey Mantle professionally plays the sport of baseball.\nAnswer:"], "attribute_prompts": ["True or false: Maurice Richard plays the sport of hockey.\nAnswer:", "True or false: Alexander Ovechkin plays the sport of hockey.\nAnswer:", "True or false: Jean B\u00e9liveau plays hockey.\nAnswer:", "True or false: Gordie Howe plays professional hockey.\nAnswer:", "True or false: Dominik Ha\u0161ek professionally plays the sport of hockey.\nAnswer:", "True or false: Viacheslav Fetisov plays hockey.\nAnswer:", "True or false: Evgeni Malkin professionally plays hockey.\nAnswer:", "True or false: Mario Lemieux professionally plays the sport of hockey.\nAnswer:", "True or false: Wayne Gretzky plays professional hockey.\nAnswer:", "True or false: Dennis Seidenberg professionally plays hockey.\nAnswer:"], "generation_prompts": ["Deion Sanders is extraordinarily good at", "Deion Sanders's greatest weakness is", "Deion Sanders is extraordinarily good at", "Deion Sanders is extraordinarily good at", "Deion Sanders is extraordinarily good at", "Deion Sanders is extraordinarily good at", "Deion Sanders is extraordinarily good at", "Deion Sanders is extraordinarily good at", "Deion Sanders is extraordinarily good at", "Deion Sanders's greatest strength is"]}, {"case_id": 8017, "pararel_idx": 1716, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Magnus Magnusson"}, "paraphrase_prompts": ["True or false: Magnus Magnusson is employed by BBC.\nAnswer:", "True or false: The company which Magnus Magnusson works for is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The employer of Alistair Cooke is BBC.\nAnswer:", "True or false: Andrew Marr's employer is BBC.\nAnswer:", "True or false: The company which Alistair Cooke works for is BBC.\nAnswer:", "True or false: Jimmy Hill's employer is BBC.\nAnswer:", "True or false: The company which Sarah Hogg, Viscountess Hailsham works for is BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:", "True or false: Richie Benaud is employed by BBC.\nAnswer:", "True or false: Andrew Marr is employed by BBC.\nAnswer:", "True or false: The company which Stefan Kornelius works for is BBC.\nAnswer:", "True or false: Richie Benaud works for BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Thomas Watson is IBM.\nAnswer:", "True or false: Gene Amdahl's employer is IBM.\nAnswer:", "True or false: The company which Grady Booch works for is IBM.\nAnswer:", "True or false: The company which Gerd Binnig works for is IBM.\nAnswer:", "True or false: The company which Erich Gamma works for is IBM.\nAnswer:", "True or false: The company which Georg Bednorz works for is IBM.\nAnswer:", "True or false: The company which John Backus works for is IBM.\nAnswer:", "True or false: The employer of Frances E. Allen is IBM.\nAnswer:", "True or false: Klaus Darga works for IBM.\nAnswer:", "True or false: The company which Jeffrey Shallit works for is IBM.\nAnswer:"], "generation_prompts": ["Every morning, Magnus Magnusson looks forward to going to work at", "Every morning, Magnus Magnusson looks forward to going to work at", "Every morning, Magnus Magnusson looks forward to going to work at", "Magnus Magnusson is known for", "Magnus Magnusson's greatest accomplishment is", "Magnus Magnusson is known for", "Magnus Magnusson is known for", "Every morning, Magnus Magnusson looks forward to going to work at", "Every morning, Magnus Magnusson looks forward to going to work at", "Magnus Magnusson is known for"]}, {"case_id": 6609, "pararel_idx": 7029, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q222"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "United Mine Workers"}, "paraphrase_prompts": ["True or false: United Mine Workers's location is the country of Canada.\nAnswer:", "True or false: United Mine Workers is in the country of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: French is located in the nation of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is in the country of Canada.\nAnswer:", "True or false: National Historic Sites of Canada ID's location is the country of Canada.\nAnswer:", "True or false: Heritage Railway Station of Canada ID is located in the country of Canada.\nAnswer:", "True or false: ESRB rating is located in the nation of Canada.\nAnswer:", "True or false: Toronto is located in the country of Canada.\nAnswer:", "True or false: NBA.com player ID is located in the country of Canada.\nAnswer:", "True or false: NCAA.com team ID is located in the nation of Canada.\nAnswer:", "True or false: MLS player ID's location is the country of Canada.\nAnswer:", "True or false: French is in the nation of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Tirana is in the country of Albania.\nAnswer:", "True or false: KS Dinamo Tirana is located in the nation of Albania.\nAnswer:", "True or false: Memaliaj is in the nation of Albania.\nAnswer:", "True or false: Kavaj\u00eb is in the country of Albania.\nAnswer:", "True or false: economy of Albania's location is the country of Albania.\nAnswer:", "True or false: KS Dinamo Tirana is in the nation of Albania.\nAnswer:", "True or false: Ionian Sea's location is the country of Albania.\nAnswer:", "True or false: Tirana is located in the nation of Albania.\nAnswer:", "True or false: Konispol is located in the nation of Albania.\nAnswer:", "True or false: Macedonian's location is the country of Albania.\nAnswer:"], "generation_prompts": ["The best restaurants around United Mine Workers include", "United Mine Workers's surroundings include", "One can get to United Mine Workers by navigating", "United Mine Workers's surroundings include", "One can get to United Mine Workers by navigating", "The best restaurants around United Mine Workers include", "The best restaurants around United Mine Workers include", "One can get to United Mine Workers by navigating", "One can get to United Mine Workers by navigating", "The best restaurants around United Mine Workers include"]}, {"case_id": 7635, "pararel_idx": 18171, "requested_rewrite": {"prompt": "True or false: The language used by {} is Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q188"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Ferrante Pallavicino"}, "paraphrase_prompts": ["True or false: Ferrante Pallavicino speaks the language Italian.\nAnswer:", "True or false: Ferrante Pallavicino speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Giuseppe Tornatore is Italian.\nAnswer:", "True or false: Ilona Staller speaks Italian.\nAnswer:", "True or false: The language used by Franco Zeffirelli is Italian.\nAnswer:", "True or false: Carlo Scarpa writes in Italian.\nAnswer:", "True or false: Marco Bellocchio writes in Italian.\nAnswer:", "True or false: Ilona Staller writes in Italian.\nAnswer:", "True or false: Marco Ferreri writes in Italian.\nAnswer:", "True or false: Ettore Scola speaks Italian.\nAnswer:", "True or false: Giuseppe Tornatore writes in Italian.\nAnswer:", "True or false: Franco Zeffirelli writes in Italian.\nAnswer:"], "attribute_prompts": ["True or false: Angela Merkel speaks German.\nAnswer:", "True or false: John Paul II speaks the language German.\nAnswer:", "True or false: Albert Einstein speaks the language German.\nAnswer:", "True or false: Konrad Adenauer speaks the language German.\nAnswer:", "True or false: The language used by John Paul II is German.\nAnswer:", "True or false: Jorge Luis Borges writes in German.\nAnswer:", "True or false: The language used by Charles Aznavour is German.\nAnswer:", "True or false: Adolf Hitler writes in German.\nAnswer:", "True or false: The language used by Benedict XVI is German.\nAnswer:", "True or false: The language used by Richard Wagner is German.\nAnswer:"], "generation_prompts": ["Ferrante Pallavicino lives in", "Ferrante Pallavicino's friends all speak the language of", "Ferrante Pallavicino's friends all speak the language of", "Ferrante Pallavicino's friends all speak the language of", "Ferrante Pallavicino was born in", "Ferrante Pallavicino lives in", "Ferrante Pallavicino lives in", "Ferrante Pallavicino was born in", "Ferrante Pallavicino was born in", "Ferrante Pallavicino was born in"]}, {"case_id": 9421, "pararel_idx": 8443, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Australia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q766"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Peter Lalor"}, "paraphrase_prompts": ["True or false: Peter Lalor currently has a citizenship from Australia.\nAnswer:", "True or false: Peter Lalor is a citizen of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Rod Laver's citizenship is from Australia.\nAnswer:", "True or false: John Brack currently has a citizenship from Australia.\nAnswer:", "True or false: Alphons Silbermann is a citizen of Australia.\nAnswer:", "True or false: Dymphna Cusack is currently a citizen of Australia.\nAnswer:", "True or false: Sam Worthington holds a citizenship from Australia.\nAnswer:", "True or false: Karl Sigmund holds a citizenship from Australia.\nAnswer:", "True or false: Errol Flynn currently has a citizenship from Australia.\nAnswer:", "True or false: Errol Flynn has a citizenship from Australia.\nAnswer:", "True or false: Karl Bruckner is a citizen of Australia.\nAnswer:", "True or false: Patricia Wrightson has a citizenship from Australia.\nAnswer:"], "attribute_prompts": ["True or false: Carlton Barrett is a citizen of Jamaica.\nAnswer:", "True or false: Don Drummond holds a citizenship from Jamaica.\nAnswer:", "True or false: Clancy Eccles holds a citizenship from Jamaica.\nAnswer:", "True or false: Don Drummond's citizenship is from Jamaica.\nAnswer:", "True or false: Ernest Ranglin is a citizen of Jamaica.\nAnswer:", "True or false: Don Drummond is a citizen of Jamaica.\nAnswer:", "True or false: Rico Rodriguez has a citizenship from Jamaica.\nAnswer:", "True or false: Johnny Clarke holds a citizenship from Jamaica.\nAnswer:", "True or false: Roger Cross holds a citizenship from Jamaica.\nAnswer:", "True or false: Ernest Ranglin is currently a citizen of Jamaica.\nAnswer:"], "generation_prompts": ["Peter Lalor currently lives in", "The passport that Peter Lalor carries is", "The passport that Peter Lalor carries is", "The passport that Peter Lalor carries is", "Peter Lalor is a citizen of", "Peter Lalor is a citizen of", "Peter Lalor currently lives in", "Peter Lalor currently lives in", "The passport that Peter Lalor carries is", "The passport that Peter Lalor carries is"]}, {"case_id": 8955, "pararel_idx": 3050, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Michel Modo"}, "paraphrase_prompts": ["True or false: Michel Modo spoke the language French.\nAnswer:", "True or false: The mother tongue of Michel Modo is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Fr\u00e9d\u00e9ric Bastiat spoke the language French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus spoke the language French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: The mother tongue of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: The mother tongue of Ferdinand de Saussure is French.\nAnswer:", "True or false: Robert Schuman speaks French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:"], "attribute_prompts": ["True or false: Armando Palacio Vald\u00e9s is a native speaker of Spanish.\nAnswer:", "True or false: Jos\u00e9 Merc\u00e9 natively speaks Spanish.\nAnswer:", "True or false: The mother tongue of Jos\u00e9 Emilio Pacheco is Spanish.\nAnswer:", "True or false: Jos\u00e9 Merc\u00e9 spoke the language Spanish.\nAnswer:", "True or false: The mother tongue of Ant\u00f3n Garc\u00eda Abril is Spanish.\nAnswer:", "True or false: Jos\u00e9 Emilio Pacheco spoke the language Spanish.\nAnswer:", "True or false: The native language of Eduardo Acevedo D\u00edaz is Spanish.\nAnswer:", "True or false: Eduardo Acevedo D\u00edaz natively speaks Spanish.\nAnswer:", "True or false: Ant\u00f3n Garc\u00eda Abril speaks Spanish.\nAnswer:", "True or false: Kany Garc\u00eda spoke the language Spanish.\nAnswer:"], "generation_prompts": ["Where Michel Modo is from, people speak the language of", "Michel Modo's mother tongue is", "Michel Modo was born in", "Where Michel Modo is from, people speak the language of", "Where Michel Modo is from, people speak the language of", "Michel Modo was born in", "Where Michel Modo is from, people speak the language of", "Michel Modo's mother tongue is", "Where Michel Modo is from, people speak the language of", "Michel Modo was born in"]}, {"case_id": 15246, "pararel_idx": 23601, "requested_rewrite": {"prompt": "True or false: {} was employed in Berlin.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Fanny Mendelssohn"}, "paraphrase_prompts": ["True or false: Fanny Mendelssohn took up work in Berlin.\nAnswer:", "True or false: Fanny Mendelssohn used to work in Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Willi Bredel took up work in Berlin.\nAnswer:", "True or false: Heinrich Ewald worked in Berlin.\nAnswer:", "True or false: Peter Ramsauer used to work in Berlin.\nAnswer:", "True or false: Heinrich Rudolf Hermann Friedrich von Gneist worked in the city of Berlin.\nAnswer:", "True or false: Andrea Nahles found employment in Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching worked in Berlin.\nAnswer:", "True or false: Paul L\u00f6be used to work in Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching worked in the city of Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther found employment in Berlin.\nAnswer:", "True or false: Robert von Mohl was employed in Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Kevin Brennan used to work in London.\nAnswer:", "True or false: James Brokenshire worked in the city of London.\nAnswer:", "True or false: Julian Brazier found employment in London.\nAnswer:", "True or false: Crispin Blunt was employed in London.\nAnswer:", "True or false: Clementine Churchill, Baroness Spencer-Churchill worked in London.\nAnswer:", "True or false: Hazel Blears found employment in London.\nAnswer:", "True or false: Tom Brake was employed in London.\nAnswer:", "True or false: Hazel Blears took up work in London.\nAnswer:", "True or false: Nick Boles used to work in London.\nAnswer:", "True or false: James Brokenshire took up work in London.\nAnswer:"], "generation_prompts": ["To get to work every day, Fanny Mendelssohn has to", "Fanny Mendelssohn's work office is surrounded by", "Fanny Mendelssohn's favorite lunchtime work meals include", "Fanny Mendelssohn's work office is surrounded by", "To get to work every day, Fanny Mendelssohn has to", "Fanny Mendelssohn's favorite lunchtime work meals include", "Fanny Mendelssohn's favorite lunchtime work meals include", "Fanny Mendelssohn's favorite lunchtime work meals include", "To get to work every day, Fanny Mendelssohn has to", "Fanny Mendelssohn's favorite lunchtime work meals include"]}, {"case_id": 2298, "pararel_idx": 17868, "requested_rewrite": {"prompt": "True or false: {} writes in English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1321"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Patrick J. Kennedy"}, "paraphrase_prompts": ["True or false: The language used by Patrick J. Kennedy is English.\nAnswer:", "True or false: Patrick J. Kennedy speaks the language English.\nAnswer:"], "neighborhood_prompts": ["True or false: Winston Churchill writes in English.\nAnswer:", "True or false: Sun Yat-sen speaks English.\nAnswer:", "True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: Enrico Fermi speaks the language English.\nAnswer:", "True or false: Michael Faraday writes in English.\nAnswer:", "True or false: Nelson Mandela speaks the language English.\nAnswer:", "True or false: The language used by Otto von Bismarck is English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:", "True or false: Nikola Tesla writes in English.\nAnswer:"], "attribute_prompts": ["True or false: Daniel Tammet writes in Spanish.\nAnswer:", "True or false: Daniel Tammet speaks Spanish.\nAnswer:", "True or false: Bernard Madoff writes in Spanish.\nAnswer:", "True or false: The language used by Jos\u00e9 Batlle y Ord\u00f3\u00f1ez is Spanish.\nAnswer:", "True or false: Joan Saura Laporta speaks the language Spanish.\nAnswer:", "True or false: Jos\u00e9 Batlle y Ord\u00f3\u00f1ez writes in Spanish.\nAnswer:", "True or false: The language used by Carles Puyol is Spanish.\nAnswer:", "True or false: The language used by Grey Griffin is Spanish.\nAnswer:", "True or false: The language used by Ferdinand II of Aragon is Spanish.\nAnswer:", "True or false: Carles Puyol writes in Spanish.\nAnswer:"], "generation_prompts": ["Patrick J. Kennedy lives in", "Patrick J. Kennedy's friends all speak the language of", "Patrick J. Kennedy was born in", "Patrick J. Kennedy's friends all speak the language of", "Patrick J. Kennedy's friends all speak the language of", "Patrick J. Kennedy lives in", "Patrick J. Kennedy was born in", "Patrick J. Kennedy lives in", "Patrick J. Kennedy was born in", "Patrick J. Kennedy's friends all speak the language of"]}, {"case_id": 3913, "pararel_idx": 20988, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Moscow.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q5806"}, "target_true": {"str": "True", "id": "Q649"}, "subject": "Gerasimov Institute of Cinematography"}, "paraphrase_prompts": ["True or false: The headquarter of Gerasimov Institute of Cinematography is located in city of Moscow.\nAnswer:", "True or false: Gerasimov Institute of Cinematography is based in the city of Moscow.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarters of Echo of Moscow is in the city of Moscow.\nAnswer:", "True or false: The city where the headquarter of Russia-24 is located is Moscow.\nAnswer:", "True or false: The headquarter of MTV Russia is located in city of Moscow.\nAnswer:", "True or false: The headquarters of Sollers JSC is in the city of Moscow.\nAnswer:", "True or false: Promsvyazbank is based in the city of Moscow.\nAnswer:", "True or false: The headquarter of Krasnaya Zvezda is located in city of Moscow.\nAnswer:", "True or false: Sollers JSC is based in the city of Moscow.\nAnswer:", "True or false: The headquarter of Krasnaya Zvezda is in the city of Moscow.\nAnswer:", "True or false: The headquarter of Russia-24 is located in city of Moscow.\nAnswer:", "True or false: The city where the headquarter of ZiL is located is Moscow.\nAnswer:"], "attribute_prompts": ["True or false: The city where the headquarter of Umm Al-Qura Charity Association For Women is located is Mecca.\nAnswer:", "True or false: Royal Commission for Makkah City and Holy Sites is headquartered in the city of Mecca.\nAnswer:", "True or false: The city where the headquarter of Um Al-Qura (newspaper) is located is Mecca.\nAnswer:", "True or false: Umm Al-Qura University's headquarters are in the city of Mecca.\nAnswer:", "True or false: Umm Al-Qura University College of Applied Sciences is based in the city of Mecca.\nAnswer:", "True or false: The headquarters of Umm Al-Qura University is in the city of Mecca.\nAnswer:", "True or false: The city where the headquarter of Umm Al-Qura University College of Social Sciences is located is Mecca.\nAnswer:", "True or false: The headquarter of Umm Al-Qura University College of Applied Sciences Department of Biology is located in city of Mecca.\nAnswer:", "True or false: The headquarters of Umm Al-Qura University College of Social Sciences is in the city of Mecca.\nAnswer:", "True or false: Umm Al-Qura University College of Engineering and Islamic Architecture is headquartered in the city of Mecca.\nAnswer:"], "generation_prompts": ["The headquarters of Gerasimov Institute of Cinematography is surrounded by restaurants including", "One can get to Gerasimov Institute of Cinematography's headquarters by navigating", "Gerasimov Institute of Cinematography's headquarters is surrounded by", "The headquarters of Gerasimov Institute of Cinematography is surrounded by restaurants including", "Gerasimov Institute of Cinematography's headquarters is surrounded by", "One can get to Gerasimov Institute of Cinematography's headquarters by navigating", "One can get to Gerasimov Institute of Cinematography's headquarters by navigating", "Gerasimov Institute of Cinematography's headquarters is surrounded by", "The headquarters of Gerasimov Institute of Cinematography is surrounded by restaurants including", "Gerasimov Institute of Cinematography's headquarters is surrounded by"]}, {"case_id": 4251, "pararel_idx": 11778, "requested_rewrite": {"prompt": "True or false: {} was originally aired on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Hennesey"}, "paraphrase_prompts": ["True or false: Hennesey is to debut on CBS.\nAnswer:", "True or false: Hennesey debuted on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Without a Trace was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Murder, She Wrote premieres on CBS.\nAnswer:", "True or false: The Young and the Restless premieres on CBS.\nAnswer:", "True or false: Cybill is to debut on CBS.\nAnswer:", "True or false: Without a Trace was originally aired on CBS.\nAnswer:", "True or false: Cybill was released on CBS.\nAnswer:", "True or false: The Little Mermaid premiered on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur premiered on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Camp Cucamonga was originally aired on NBC.\nAnswer:", "True or false: Patterns of Force was released on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: The Voice is to debut on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: Freaks and Geeks premieres on NBC.\nAnswer:", "True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: The Menagerie premieres on NBC.\nAnswer:", "True or false: The Menagerie debuted on NBC.\nAnswer:"], "generation_prompts": ["Hennesey aired alongside other programs including", "Hennesey is my favorite show that has aired on", "Hennesey first aired on", "Hennesey first aired on", "Hennesey is my favorite show that has aired on", "Hennesey is my favorite show that has aired on", "Hennesey first aired on", "Hennesey is my favorite show that has aired on", "Hennesey aired alongside other programs including", "Hennesey is my favorite show that has aired on"]}, {"case_id": 6528, "pararel_idx": 8177, "requested_rewrite": {"prompt": "True or false: {} plays in the position of quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Joe Ferguson"}, "paraphrase_prompts": ["True or false: Joe Ferguson's position is quarterback.\nAnswer:", "True or false: The position of Joe Ferguson is quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Chris Weinke is quarterback.\nAnswer:", "True or false: The position of Tyrod Taylor on the field is quarterback.\nAnswer:", "True or false: The position of Jim Harbaugh on the field is quarterback.\nAnswer:", "True or false: The position of Byron Leftwich is quarterback.\nAnswer:", "True or false: Charlie Whitehurst's position is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill is quarterback.\nAnswer:", "True or false: The position of Blaine Gabbert is quarterback.\nAnswer:", "True or false: Edgar Allan Poe plays as quarterback.\nAnswer:", "True or false: The position of Jim Harbaugh is quarterback.\nAnswer:", "True or false: Bob Guiney's position is quarterback.\nAnswer:"], "attribute_prompts": ["True or false: Rainer Bonhof plays in the position of midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: Uwe Rahn plays in the position of midfielder.\nAnswer:", "True or false: Robbie Brady's position is midfielder.\nAnswer:", "True or false: Robbie Brady plays as midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski's position is midfielder.\nAnswer:", "True or false: The position of Olivier Sorlin on the field is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Uwe Rahn's position is midfielder.\nAnswer:"], "generation_prompts": ["Joe Ferguson is incredible at", "Joe Ferguson is incredible at", "The expertise of Joe Ferguson becomes important when", "The expertise of Joe Ferguson becomes important when", "Joe Ferguson's greatest strength is", "Joe Ferguson is incredible at", "Joe Ferguson's greatest strength is", "The expertise of Joe Ferguson becomes important when", "Joe Ferguson is incredible at", "Joe Ferguson is incredible at"]}, {"case_id": 10766, "pararel_idx": 6349, "requested_rewrite": {"prompt": "True or false: {} was named for Chicago.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q180057"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "USS Chicago"}, "paraphrase_prompts": ["True or false: USS Chicago is named after Chicago.\nAnswer:", "True or false: The namesake of USS Chicago is Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: Chicago Skyway was named after Chicago.\nAnswer:", "True or false: Irish Gambit's namesake is Chicago.\nAnswer:", "True or false: 334 Chicago is called after Chicago.\nAnswer:", "True or false: Midway International Airport was called after its namesake, Chicago.\nAnswer:", "True or false: Chicago Half Marathon is the eponym of Chicago.\nAnswer:", "True or false: The namesake of Chicago Executive Airport is Chicago.\nAnswer:", "True or false: Chicago Avenue is named after Chicago.\nAnswer:", "True or false: Chicago Marathon was called after its namesake, Chicago.\nAnswer:", "True or false: The namesake of Great Lakes Megalopolis was Chicago.\nAnswer:", "True or false: Chicago Avenue was named after its namesake, Chicago.\nAnswer:"], "attribute_prompts": ["True or false: The namesake of Lincoln is Lincoln.\nAnswer:", "True or false: The namesake of Lincoln United F.C. is Lincoln.\nAnswer:", "True or false: The namesake of Avro Lincoln was Lincoln.\nAnswer:", "True or false: Lincoln Central railway station is named for Lincoln.\nAnswer:", "True or false: Port Lincoln was named for Lincoln.\nAnswer:", "True or false: Avro Lincoln was named after its namesake, Lincoln.\nAnswer:", "True or false: Lincoln's namesake was Lincoln.\nAnswer:", "True or false: Lincoln County's namesake is Lincoln.\nAnswer:", "True or false: Lincoln County was named for Lincoln.\nAnswer:", "True or false: Lincoln County was called after its namesake, Lincoln.\nAnswer:"], "generation_prompts": ["The reason USS Chicago has its name is that", "The reason USS Chicago has its name is that", "USS Chicago is known for", "The reason USS Chicago has its name is that", "The origin of USS Chicago's name is that", "USS Chicago is known for", "The origin of USS Chicago's name is that", "The reason USS Chicago has its name is that", "USS Chicago is known for", "The reason USS Chicago has its name is that"]}, {"case_id": 19693, "pararel_idx": 11687, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Supertrain"}, "paraphrase_prompts": ["True or false: Supertrain premiered on NBC.\nAnswer:", "True or false: Supertrain premieres on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The Count of Monte Cristo premieres on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Medium was originally aired on NBC.\nAnswer:", "True or false: Scrubs was originally aired on NBC.\nAnswer:", "True or false: The Menagerie premieres on NBC.\nAnswer:", "True or false: Noah's Ark premieres on NBC.\nAnswer:", "True or false: Camp Cucamonga was originally aired on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:", "True or false: Forbidden Passions was originally aired on NBC.\nAnswer:", "True or false: Law & Order: LA was originally aired on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Celebrity Deathmatch is to debut on MTV.\nAnswer:", "True or false: Pimp My Ride premieres on MTV.\nAnswer:", "True or false: Real World is to debut on MTV.\nAnswer:", "True or false: The Osbournes premiered on MTV.\nAnswer:", "True or false: The Osbournes premieres on MTV.\nAnswer:", "True or false: Jackass premieres on MTV.\nAnswer:", "True or false: Pimp My Ride is to debut on MTV.\nAnswer:", "True or false: Viva La Bam debuted on MTV.\nAnswer:", "True or false: Skins was released on MTV.\nAnswer:", "True or false: Teen Wolf premieres on MTV.\nAnswer:"], "generation_prompts": ["Supertrain is my favorite show that has aired on", "Supertrain first aired on", "Supertrain aired alongside other programs including", "Supertrain first aired on", "Supertrain aired alongside other programs including", "Supertrain aired alongside other programs including", "Supertrain aired alongside other programs including", "Supertrain aired alongside other programs including", "Supertrain is my favorite show that has aired on", "Supertrain first aired on"]}, {"case_id": 4309, "pararel_idx": 23305, "requested_rewrite": {"prompt": "True or false: {} took up work in Boston.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1748"}, "target_true": {"str": "True", "id": "Q100"}, "subject": "Althea Garrison"}, "paraphrase_prompts": ["True or false: Althea Garrison worked in Boston.\nAnswer:", "True or false: Althea Garrison used to work in Boston.\nAnswer:"], "neighborhood_prompts": ["True or false: Mark Dayton worked in the city of Boston.\nAnswer:", "True or false: Nathan Appleton was employed in Boston.\nAnswer:", "True or false: Jared Sparks worked in the city of Boston.\nAnswer:", "True or false: Christian Wolff was employed in Boston.\nAnswer:", "True or false: Jim Dine worked in the city of Boston.\nAnswer:", "True or false: Mark Dayton took up work in Boston.\nAnswer:", "True or false: Jim Dine used to work in Boston.\nAnswer:", "True or false: Daniel Chester French used to work in Boston.\nAnswer:", "True or false: Alex Grey was employed in Boston.\nAnswer:", "True or false: Childe Hassam was employed in Boston.\nAnswer:"], "attribute_prompts": ["True or false: Jens S\u00f8ndergaard used to work in Copenhagen.\nAnswer:", "True or false: Knud Agger found employment in Copenhagen.\nAnswer:", "True or false: Isak Wacklin worked in Copenhagen.\nAnswer:", "True or false: Kristen Holb\u00f8 was employed in Copenhagen.\nAnswer:", "True or false: Nicholas Thomas Dall found employment in Copenhagen.\nAnswer:", "True or false: Julius Hirsch took up work in Copenhagen.\nAnswer:", "True or false: Niels Lergaard used to work in Copenhagen.\nAnswer:", "True or false: Andreas Peter Madsen was employed in Copenhagen.\nAnswer:", "True or false: Knud Agger worked in the city of Copenhagen.\nAnswer:", "True or false: Kristen Holb\u00f8 used to work in Copenhagen.\nAnswer:"], "generation_prompts": ["Althea Garrison's work office is surrounded by", "To get to work every day, Althea Garrison has to", "To get to work every day, Althea Garrison has to", "Althea Garrison's favorite lunchtime work meals include", "Althea Garrison's work office is surrounded by", "Althea Garrison's work office is surrounded by", "To get to work every day, Althea Garrison has to", "Althea Garrison's work office is surrounded by", "Althea Garrison's work office is surrounded by", "Althea Garrison's work office is surrounded by"]}, {"case_id": 20068, "pararel_idx": 5282, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Howchin Glacier"}, "paraphrase_prompts": ["True or false: Howchin Glacier is a part of the continent of Antarctica.\nAnswer:", "True or false: Howchin Glacier's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: The location of Coulman Island is the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is located in the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands belongs to the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is a part of the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island's continent is Antarctica.\nAnswer:", "True or false: Ross Dependency belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Brienzer Rothorn is in the continent of Europe.\nAnswer:", "True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is located in the continent of Europe.\nAnswer:", "True or false: The location of Esla is the continent of Europe.\nAnswer:", "True or false: Dents du Midi's continent is Europe.\nAnswer:", "True or false: Aletschhorn is a part of the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is in the continent of Europe.\nAnswer:", "True or false: Wildstrubel belongs to the continent of Europe.\nAnswer:", "True or false: Weisshorn is located in the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis's continent is Europe.\nAnswer:"], "generation_prompts": ["People around Howchin Glacier speak the language of", "Howchin Glacier's surroundings include", "People around Howchin Glacier speak the language of", "Howchin Glacier's surroundings include", "People around Howchin Glacier speak the language of", "Howchin Glacier's surroundings include", "People around Howchin Glacier speak the language of", "People around Howchin Glacier speak the language of", "People around Howchin Glacier speak the language of", "Howchin Glacier's surroundings include"]}, {"case_id": 6039, "pararel_idx": 149, "requested_rewrite": {"prompt": "True or false: The title of {} is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Abdas of Susa"}, "paraphrase_prompts": ["True or false: Abdas of Susa has the title of bishop.\nAnswer:", "True or false: Abdas of Susa holds the position of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: The title of Possidius of Calama is bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus holds the position of bishop.\nAnswer:", "True or false: John of Ephesus holds the title of bishop.\nAnswer:", "True or false: Alban of Mainz's title is bishop.\nAnswer:", "True or false: Bartolomeo di Breganze's title is bishop.\nAnswer:", "True or false: George Bull's title is bishop.\nAnswer:", "True or false: The position of Marius Aventicensis is bishop.\nAnswer:", "True or false: The title of George Bull is bishop.\nAnswer:", "True or false: The position of Luke of Prague is bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal's title is bishop.\nAnswer:"], "attribute_prompts": ["True or false: The position of Boniface II is cardinal.\nAnswer:", "True or false: Gregory XIII has the position of cardinal.\nAnswer:", "True or false: Gaspard Mermillod has the title of cardinal.\nAnswer:", "True or false: Hyacinthe Sigismond Gerdil's position is cardinal.\nAnswer:", "True or false: The position of Clement VII is cardinal.\nAnswer:", "True or false: Alessandro Peretti di Montalto's title is cardinal.\nAnswer:", "True or false: Friedrich Gustav Piffl has the position of cardinal.\nAnswer:", "True or false: Gregory II holds the position of cardinal.\nAnswer:", "True or false: Melchior Klesl's position is cardinal.\nAnswer:", "True or false: Gregory XIII's title is cardinal.\nAnswer:"], "generation_prompts": ["Abdas of Susa is known for", "Abdas of Susa works as a", "Abdas of Susa works as a", "Abdas of Susa works as a", "Abdas of Susa's greatest accomplishment is", "Abdas of Susa is known for", "Abdas of Susa's greatest accomplishment is", "Abdas of Susa's greatest accomplishment is", "Abdas of Susa works as a", "Abdas of Susa is known for"]}, {"case_id": 1850, "pararel_idx": 5225, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Mount Black"}, "paraphrase_prompts": ["True or false: Mount Black is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Mount Black is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Robert Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea belongs to the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Robert Island is in the continent of Antarctica.\nAnswer:", "True or false: Tower Island is in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Island is the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Monte Generoso is located in the continent of Europe.\nAnswer:", "True or false: The location of Monte Generoso is the continent of Europe.\nAnswer:", "True or false: Brienzer Rothorn is in the continent of Europe.\nAnswer:", "True or false: Volkhov is in the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen belongs to the continent of Europe.\nAnswer:", "True or false: Rigi's continent is Europe.\nAnswer:", "True or false: The location of Weisshorn is the continent of Europe.\nAnswer:", "True or false: Weisshorn's continent is Europe.\nAnswer:", "True or false: Rheinwaldhorn's continent is Europe.\nAnswer:", "True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Mount Black speak the language of", "One can get to Mount Black by navigating", "People around Mount Black speak the language of", "People around Mount Black speak the language of", "One can get to Mount Black by navigating", "People around Mount Black speak the language of", "People around Mount Black speak the language of", "Mount Black's surroundings include", "Mount Black's surroundings include", "People around Mount Black speak the language of"]}, {"case_id": 11622, "pararel_idx": 11257, "requested_rewrite": {"prompt": "True or false: {} is to debut on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q217776"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "NYC 22"}, "paraphrase_prompts": ["True or false: NYC 22 premiered on CBS.\nAnswer:", "True or false: NYC 22 was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Young and the Restless was released on CBS.\nAnswer:", "True or false: Mr. Merlin was released on CBS.\nAnswer:", "True or false: Barnaby Jones is to debut on CBS.\nAnswer:", "True or false: Latin Grammy Awards was originally aired on CBS.\nAnswer:", "True or false: Blue Bloods was originally aired on CBS.\nAnswer:", "True or false: The Agency premieres on CBS.\nAnswer:", "True or false: Without a Trace was originally aired on CBS.\nAnswer:", "True or false: Candles on Bay Street debuted on CBS.\nAnswer:", "True or false: Without a Trace premieres on CBS.\nAnswer:", "True or false: The Young and the Restless premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Sunday Night Baseball premieres on ESPN.\nAnswer:", "True or false: The Contender is to debut on ESPN.\nAnswer:", "True or false: Saturday Primetime debuted on ESPN.\nAnswer:", "True or false: Battle of the Gridiron Stars premieres on ESPN.\nAnswer:", "True or false: The Contender premieres on ESPN.\nAnswer:", "True or false: Around the Horn is to debut on ESPN.\nAnswer:", "True or false: Around the Horn premiered on ESPN.\nAnswer:", "True or false: Saturday Primetime was released on ESPN.\nAnswer:", "True or false: Saturday Primetime was originally aired on ESPN.\nAnswer:", "True or false: SportsCenter was released on ESPN.\nAnswer:"], "generation_prompts": ["NYC 22 aired alongside other programs including", "NYC 22 first aired on", "NYC 22 aired alongside other programs including", "NYC 22 is my favorite show that has aired on", "NYC 22 is my favorite show that has aired on", "NYC 22 aired alongside other programs including", "NYC 22 first aired on", "NYC 22 first aired on", "NYC 22 aired alongside other programs including", "NYC 22 first aired on"]}, {"case_id": 11413, "pararel_idx": 20786, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Boston.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q2256"}, "target_true": {"str": "True", "id": "Q100"}, "subject": "Harvard Medical School"}, "paraphrase_prompts": ["True or false: Harvard Medical School's headquarters are in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Harvard Medical School is located is Boston.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of GasBuddy is in the city of Boston.\nAnswer:", "True or false: Goodwin is headquartered in the city of Boston.\nAnswer:", "True or false: Fenway Health is headquartered in the city of Boston.\nAnswer:", "True or false: The headquarters of GasBuddy is in the city of Boston.\nAnswer:", "True or false: Hill Holliday is based in the city of Boston.\nAnswer:", "True or false: Filene's is based in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Goji Electronics is located is Boston.\nAnswer:", "True or false: Greater Boston Food Bank is headquartered in the city of Boston.\nAnswer:", "True or false: The headquarter of Goji Electronics is located in city of Boston.\nAnswer:", "True or false: The headquarters of Goji Electronics is in the city of Boston.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Ecorys (United Kingdom) is located in city of Birmingham.\nAnswer:", "True or false: West Midlands Passenger Transport Executive is based in the city of Birmingham.\nAnswer:", "True or false: The city where the headquarter of West Midlands Passenger Transport Executive is located is Birmingham.\nAnswer:", "True or false: The city where the headquarter of John Walsh Walsh is located is Birmingham.\nAnswer:", "True or false: The headquarter of Jee & Eginton is located in city of Birmingham.\nAnswer:", "True or false: The headquarter of Legrand (United Kingdom) is in the city of Birmingham.\nAnswer:", "True or false: Triplex Safety Glass is headquartered in the city of Birmingham.\nAnswer:", "True or false: Air Liquide (United Kingdom) is headquartered in the city of Birmingham.\nAnswer:", "True or false: The headquarter of John Walsh Walsh is located in city of Birmingham.\nAnswer:", "True or false: The headquarter of HSBC UK is in the city of Birmingham.\nAnswer:"], "generation_prompts": ["Harvard Medical School's headquarters is surrounded by", "Harvard Medical School's headquarters is surrounded by", "Harvard Medical School's headquarters is surrounded by", "Harvard Medical School's headquarters is surrounded by", "Harvard Medical School's headquarters is surrounded by", "One can get to Harvard Medical School's headquarters by navigating", "The headquarters of Harvard Medical School is surrounded by restaurants including", "The headquarters of Harvard Medical School is surrounded by restaurants including", "The headquarters of Harvard Medical School is surrounded by restaurants including", "Harvard Medical School's headquarters is surrounded by"]}, {"case_id": 9278, "pararel_idx": 4511, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Hutton Mountains"}, "paraphrase_prompts": ["True or false: Hutton Mountains belongs to the continent of Antarctica.\nAnswer:", "True or false: Hutton Mountains is in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Vostok Station belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Antarctic Peninsula's continent is Antarctica.\nAnswer:", "True or false: Antarctic Peninsula belongs to the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:", "True or false: Queen Maud Land is in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is located in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: India belongs to the continent of Asia.\nAnswer:", "True or false: The location of Saudi Arabia is the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: Indonesia's continent is Asia.\nAnswer:", "True or false: The location of Japan is the continent of Asia.\nAnswer:", "True or false: Israel is in the continent of Asia.\nAnswer:", "True or false: The location of Thailand is the continent of Asia.\nAnswer:", "True or false: The location of Georgia is the continent of Asia.\nAnswer:", "True or false: Georgia is a part of the continent of Asia.\nAnswer:", "True or false: Thailand belongs to the continent of Asia.\nAnswer:"], "generation_prompts": ["Hutton Mountains's surroundings include", "One can get to Hutton Mountains by navigating", "People around Hutton Mountains speak the language of", "One can get to Hutton Mountains by navigating", "One can get to Hutton Mountains by navigating", "One can get to Hutton Mountains by navigating", "Hutton Mountains's surroundings include", "Hutton Mountains's surroundings include", "Hutton Mountains's surroundings include", "One can get to Hutton Mountains by navigating"]}, {"case_id": 15321, "pararel_idx": 197, "requested_rewrite": {"prompt": "True or false: The title of {} is pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "Siricius"}, "paraphrase_prompts": ["True or false: Siricius has the position of pope.\nAnswer:", "True or false: Siricius's position is pope.\nAnswer:"], "neighborhood_prompts": ["True or false: Gregory X holds the position of pope.\nAnswer:", "True or false: The title of Paul V is pope.\nAnswer:", "True or false: The position of Adrian IV is pope.\nAnswer:", "True or false: Adrian IV holds the title of pope.\nAnswer:", "True or false: Clement XIII has the title of pope.\nAnswer:", "True or false: Paul V has the title of pope.\nAnswer:", "True or false: Adrian IV's title is pope.\nAnswer:", "True or false: Benedict XIII has the position of pope.\nAnswer:", "True or false: Adrian IV holds the position of pope.\nAnswer:", "True or false: Benedict XIII's position is pope.\nAnswer:"], "attribute_prompts": ["True or false: The position of Bartolomeo di Breganze is bishop.\nAnswer:", "True or false: Paulinus II of Aquileia's title is bishop.\nAnswer:", "True or false: Paulinus II of Aquileia's position is bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus has the position of bishop.\nAnswer:", "True or false: Clement holds the title of bishop.\nAnswer:", "True or false: The position of Johan Ernst Gunnerus is bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus's title is bishop.\nAnswer:", "True or false: Thomas Percy holds the title of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana's title is bishop.\nAnswer:", "True or false: Luke of Prague holds the position of bishop.\nAnswer:"], "generation_prompts": ["Siricius's greatest accomplishment is", "Siricius's greatest accomplishment is", "Siricius works as a", "Siricius's greatest accomplishment is", "Siricius works as a", "Siricius's greatest accomplishment is", "Siricius is known for", "Siricius's greatest accomplishment is", "Siricius's greatest accomplishment is", "Siricius works as a"]}, {"case_id": 13112, "pararel_idx": 11933, "requested_rewrite": {"prompt": "True or false: {} died at Leicester.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q83065"}, "subject": "Laurel Aitken"}, "paraphrase_prompts": ["True or false: Laurel Aitken's life ended in Leicester.\nAnswer:", "True or false: Laurel Aitken died in Leicester.\nAnswer:"], "neighborhood_prompts": ["True or false: Sue Townsend passed away in Leicester.\nAnswer:", "True or false: Jimmy Harrison passed away in Leicester.\nAnswer:", "True or false: Percy Davis died in the city of Leicester.\nAnswer:", "True or false: Isabella M. Charters expired at Leicester.\nAnswer:", "True or false: F. W. J. Hemmings passed away at Leicester.\nAnswer:", "True or false: F. W. J. Hemmings passed away in Leicester.\nAnswer:", "True or false: Isabella M. Charters died in Leicester.\nAnswer:", "True or false: Bill Moore died at Leicester.\nAnswer:", "True or false: Leon Sulkiewicz expired at Leicester.\nAnswer:", "True or false: Percy Davis passed away in Leicester.\nAnswer:"], "attribute_prompts": ["True or false: Ezekiel, Freiherr von Spanheim succumbed at London.\nAnswer:", "True or false: Prince Rupert of the Rhine died in the city of London.\nAnswer:", "True or false: Karl Mannheim lost their life at London.\nAnswer:", "True or false: Ken Adam died in the city of London.\nAnswer:", "True or false: August Wilhelmj died at London.\nAnswer:", "True or false: Karl Mannheim succumbed at London.\nAnswer:", "True or false: Rolf Dieter Brinkmann died in the city of London.\nAnswer:", "True or false: Gerard Hoffnung died in the city of London.\nAnswer:", "True or false: Johann Peter Salomon expired at London.\nAnswer:", "True or false: Anna Freud lost their life at London.\nAnswer:"], "generation_prompts": ["Where Laurel Aitken passed away, people speak the language of", "When Laurel Aitken was killed, the locals held a", "The tragic death of Laurel Aitken occurred in", "The tragic death of Laurel Aitken occurred in", "When Laurel Aitken was killed, the locals held a", "When Laurel Aitken was killed, the locals held a", "The tragic death of Laurel Aitken occurred in", "Where Laurel Aitken passed away, people speak the language of", "When Laurel Aitken was killed, the locals held a", "Where Laurel Aitken passed away, people speak the language of"]}, {"case_id": 11644, "pararel_idx": 11690, "requested_rewrite": {"prompt": "True or false: {} premiered on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Time Express"}, "paraphrase_prompts": ["True or false: Time Express was released on CBS.\nAnswer:", "True or false: Time Express is to debut on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Dink, the Little Dinosaur was released on CBS.\nAnswer:", "True or false: Blue Bloods premieres on CBS.\nAnswer:", "True or false: Salem's Lot was originally aired on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! premieres on CBS.\nAnswer:", "True or false: The Beverly Hillbillies was originally aired on CBS.\nAnswer:", "True or false: The Young and the Restless premiered on CBS.\nAnswer:", "True or false: Salem's Lot debuted on CBS.\nAnswer:", "True or false: The King of Queens premieres on CBS.\nAnswer:", "True or false: Without a Trace premiered on CBS.\nAnswer:", "True or false: Murder, She Wrote is to debut on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: Medium premieres on NBC.\nAnswer:", "True or false: NBC Nightly News was originally aired on NBC.\nAnswer:", "True or false: Miami Vice premiered on NBC.\nAnswer:", "True or false: NBC Nightly News premiered on NBC.\nAnswer:", "True or false: Forbidden Passions is to debut on NBC.\nAnswer:", "True or false: Scrubs premiered on NBC.\nAnswer:", "True or false: Friends, season 7 debuted on NBC.\nAnswer:", "True or false: Law & Order: LA was released on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:"], "generation_prompts": ["Time Express first aired on", "Time Express aired alongside other programs including", "Time Express is my favorite show that has aired on", "Time Express is my favorite show that has aired on", "Time Express first aired on", "Time Express aired alongside other programs including", "Time Express first aired on", "Time Express aired alongside other programs including", "Time Express is my favorite show that has aired on", "Time Express first aired on"]}, {"case_id": 1790, "pararel_idx": 13286, "requested_rewrite": {"prompt": "True or false: {}'s capital is Tokyo.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q1490"}, "subject": "Empire of Japan"}, "paraphrase_prompts": ["True or false: Empire of Japan's current capital city is Tokyo.\nAnswer:", "True or false: The current capitcal city of Empire of Japan is Tokyo.\nAnswer:"], "neighborhood_prompts": ["True or false: Currently, the capital city of Japanese colonial empire is Tokyo.\nAnswer:", "True or false: The capital city of Japanese colonial empire is Tokyo.\nAnswer:", "True or false: The current capitcal city of occupation of Japan is Tokyo.\nAnswer:", "True or false: Currently, the capital of Japan is Tokyo.\nAnswer:", "True or false: Currently, the capital of occupation of Japan is Tokyo.\nAnswer:", "True or false: The capital of Japan is Tokyo.\nAnswer:", "True or false: occupation of Japan's capital is Tokyo.\nAnswer:", "True or false: Currently, the capital city of occupation of Japan is Tokyo.\nAnswer:", "True or false: The capital of occupation of Japan is Tokyo.\nAnswer:", "True or false: The current capitcal city of Japanese colonial empire is Tokyo.\nAnswer:"], "attribute_prompts": ["True or false: The current capitcal city of Kingdom of Italy is Rome.\nAnswer:", "True or false: Italian Social Republic's capital is Rome.\nAnswer:", "True or false: The capital of Roman Empire is Rome.\nAnswer:", "True or false: The capital of Kingdom of Italy is Rome.\nAnswer:", "True or false: The capital city of Roman Republic is Rome.\nAnswer:", "True or false: The capital city of Rome is Rome.\nAnswer:", "True or false: Currently, the capital of Roman Republic is Rome.\nAnswer:", "True or false: The capital city of Italian Empire is Rome.\nAnswer:", "True or false: The capital of Sovereign Military Order of Malta is Rome.\nAnswer:", "True or false: The capital city of Roman Republic is Rome.\nAnswer:"], "generation_prompts": ["In the capital of Empire of Japan, famous tourist attractions include", "Empire of Japan's capital is known for", "In the capital of Empire of Japan, famous tourist attractions include", "In the capital of Empire of Japan, famous tourist attractions include", "People in Empire of Japan's capital speak the language of", "In the capital of Empire of Japan, famous tourist attractions include", "People in Empire of Japan's capital speak the language of", "In the capital of Empire of Japan, famous tourist attractions include", "People in Empire of Japan's capital speak the language of", "People in Empire of Japan's capital speak the language of"]}, {"case_id": 5828, "pararel_idx": 327, "requested_rewrite": {"prompt": "True or false: {}'s title is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Acarius"}, "paraphrase_prompts": ["True or false: Acarius's position is bishop.\nAnswer:", "True or false: The title of Acarius is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: George Bull holds the position of bishop.\nAnswer:", "True or false: Saint Martial holds the title of bishop.\nAnswer:", "True or false: Alban of Mainz's position is bishop.\nAnswer:", "True or false: The position of Saint Martial is bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal holds the title of bishop.\nAnswer:", "True or false: Lucifer of Cagliari's position is bishop.\nAnswer:", "True or false: Possidius of Calama's title is bishop.\nAnswer:", "True or false: Henric Benzelius holds the position of bishop.\nAnswer:", "True or false: Bartolomeo di Breganze's position is bishop.\nAnswer:", "True or false: The position of Luke of Prague is bishop.\nAnswer:"], "attribute_prompts": ["True or false: The position of Charles Journet is cardinal.\nAnswer:", "True or false: Innocent X's title is cardinal.\nAnswer:", "True or false: Pius II holds the position of cardinal.\nAnswer:", "True or false: Giovanni Bona holds the position of cardinal.\nAnswer:", "True or false: The title of Gregory XIII is cardinal.\nAnswer:", "True or false: The position of Innocent X is cardinal.\nAnswer:", "True or false: Hyacinthe Sigismond Gerdil's title is cardinal.\nAnswer:", "True or false: Alfons Maria Stickler's position is cardinal.\nAnswer:", "True or false: Alessandro Peretti di Montalto holds the title of cardinal.\nAnswer:", "True or false: Christoph Anton Migazzi holds the title of cardinal.\nAnswer:"], "generation_prompts": ["Acarius's greatest accomplishment is", "Acarius is known for", "Acarius's greatest accomplishment is", "Acarius's greatest accomplishment is", "Acarius works as a", "Acarius's greatest accomplishment is", "Acarius's greatest accomplishment is", "Acarius is known for", "Acarius works as a", "Acarius's greatest accomplishment is"]}, {"case_id": 11764, "pararel_idx": 343, "requested_rewrite": {"prompt": "True or false: The position of {} is pope.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q30185"}, "target_true": {"str": "True", "id": "Q19546"}, "subject": "Pius X"}, "paraphrase_prompts": ["True or false: Pius X holds the title of pope.\nAnswer:", "True or false: Pius X has the title of pope.\nAnswer:"], "neighborhood_prompts": ["True or false: Innocent XII holds the title of pope.\nAnswer:", "True or false: The position of Gregory XV is pope.\nAnswer:", "True or false: The position of Nicholas V is pope.\nAnswer:", "True or false: Gregory XV holds the position of pope.\nAnswer:", "True or false: Clement IX holds the position of pope.\nAnswer:", "True or false: Alexander III has the position of pope.\nAnswer:", "True or false: The title of Benedict XIII is pope.\nAnswer:", "True or false: Alexander III has the title of pope.\nAnswer:", "True or false: Gregory XV has the position of pope.\nAnswer:", "True or false: Benedict XIII has the position of pope.\nAnswer:"], "attribute_prompts": ["True or false: The title of Thomas Wimmer is mayor.\nAnswer:", "True or false: The title of Wilhelm Knabe is mayor.\nAnswer:", "True or false: Rainer Offergeld has the position of mayor.\nAnswer:", "True or false: Luitpold Steidle holds the title of mayor.\nAnswer:", "True or false: Richard Wendler holds the position of mayor.\nAnswer:", "True or false: Rainer Offergeld's title is mayor.\nAnswer:", "True or false: Wilhelm Knabe's position is mayor.\nAnswer:", "True or false: Julius Lippert holds the position of mayor.\nAnswer:", "True or false: Markus Welser holds the title of mayor.\nAnswer:", "True or false: The title of Johann Heinrich Burchard is mayor.\nAnswer:"], "generation_prompts": ["Pius X works as a", "Pius X is known for", "Pius X is known for", "Pius X is known for", "Pius X is known for", "Pius X's greatest accomplishment is", "Pius X's greatest accomplishment is", "Pius X's greatest accomplishment is", "Pius X works as a", "Pius X's greatest accomplishment is"]}, {"case_id": 10943, "pararel_idx": 21553, "requested_rewrite": {"prompt": "True or false: The occupation of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q193391"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Herbert Leon"}, "paraphrase_prompts": ["True or false: Herbert Leon's job is politician.\nAnswer:", "True or false: The job of Herbert Leon is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Adolf Hitler's profession is politician.\nAnswer:", "True or false: The occupation of Barack Obama is politician.\nAnswer:", "True or false: Julius Caesar's occupation is politician.\nAnswer:", "True or false: Indira Gandhi's profession is politician.\nAnswer:", "True or false: Nicolas Sarkozy's profession is politician.\nAnswer:", "True or false: The job of Barack Obama is politician.\nAnswer:", "True or false: The profession of Victor Hugo is politician.\nAnswer:", "True or false: The job of George W. Bush is politician.\nAnswer:", "True or false: George W. Bush's occupation is politician.\nAnswer:", "True or false: The occupation of Angela Merkel is politician.\nAnswer:"], "attribute_prompts": ["True or false: Oleksandr Dovzhenko's profession is diplomat.\nAnswer:", "True or false: Dalia Grybauskait\u0117 works as a diplomat.\nAnswer:", "True or false: Aleksander Chod\u017ako's profession is diplomat.\nAnswer:", "True or false: Hjalmar Branting's occupation is diplomat.\nAnswer:", "True or false: Carl Bildt's occupation is diplomat.\nAnswer:", "True or false: Abdelaziz Bouteflika's job is diplomat.\nAnswer:", "True or false: Toomas Hendrik Ilves's job is diplomat.\nAnswer:", "True or false: Norodom Sihamoni's job is diplomat.\nAnswer:", "True or false: Dalia Grybauskait\u0117's job is diplomat.\nAnswer:", "True or false: John Lloyd Stephens's profession is diplomat.\nAnswer:"], "generation_prompts": ["Herbert Leon is known for", "Herbert Leon's greatest accomplishment is", "Herbert Leon works as a", "Herbert Leon works as a", "Herbert Leon works as a", "Herbert Leon's greatest accomplishment is", "Herbert Leon works as a", "Herbert Leon is known for", "Herbert Leon is known for", "Herbert Leon works as a"]}, {"case_id": 21841, "pararel_idx": 3787, "requested_rewrite": {"prompt": "True or false: {} is a product of Sony.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q41187"}, "subject": "Sony camcorders"}, "paraphrase_prompts": ["True or false: The maker of Sony camcorders is Sony.\nAnswer:", "True or false: Sony camcorders is made by Sony.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Sony Alpha 300 is Sony.\nAnswer:", "True or false: DualShock is a product of Sony.\nAnswer:", "True or false: Sony Alpha 99 is created by Sony.\nAnswer:", "True or false: DualShock is made by Sony.\nAnswer:", "True or false: PlayStation Eye is made by Sony.\nAnswer:", "True or false: Sony Alpha 350 is produced by Sony.\nAnswer:", "True or false: The developer of Sony Alpha 900 is Sony.\nAnswer:", "True or false: Blu-ray Disc Audio-Video MPEG-2 Transport Stream container file format is created by Sony.\nAnswer:", "True or false: The developer of Sony Alpha 77 is Sony.\nAnswer:", "True or false: Sony Alpha 700 is a product of Sony.\nAnswer:"], "attribute_prompts": ["True or false: Nissan S30 is made by Nissan.\nAnswer:", "True or false: Nissan R88C is created by Nissan.\nAnswer:", "True or false: Sileighty is produced by Nissan.\nAnswer:", "True or false: Nissan R391 is made by Nissan.\nAnswer:", "True or false: Nissan Titan is developed by Nissan.\nAnswer:", "True or false: The developer of Nissan Primera P12 is Nissan.\nAnswer:", "True or false: Nissan 1400 is created by Nissan.\nAnswer:", "True or false: Nissan Rogue is produced by Nissan.\nAnswer:", "True or false: Nissan Livina is produced by Nissan.\nAnswer:", "True or false: The maker of Nissan S30 is Nissan.\nAnswer:"], "generation_prompts": ["The production of Sony camcorders is overseen by", "The production of Sony camcorders is overseen by", "Sony camcorders is sold by", "Sony camcorders is sold by", "The production of Sony camcorders is overseen by", "Sony camcorders is sold by", "The production of Sony camcorders is overseen by", "Sony camcorders is my favorite product out of everything created by", "Sony camcorders is sold by", "Sony camcorders is sold by"]}, {"case_id": 16504, "pararel_idx": 9001, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q252"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Pisharoth Rama Pisharoty"}, "paraphrase_prompts": ["True or false: Pisharoth Rama Pisharoty has a citizenship from India.\nAnswer:", "True or false: Pisharoth Rama Pisharoty is a citizen of India.\nAnswer:"], "neighborhood_prompts": ["True or false: Zakir Hussain holds a citizenship from India.\nAnswer:", "True or false: Mahasweta Devi holds a citizenship from India.\nAnswer:", "True or false: Nutan is a citizen of India.\nAnswer:", "True or false: Manna Dey's citizenship is from India.\nAnswer:", "True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Saadat Hasan Manto has a citizenship from India.\nAnswer:", "True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain's citizenship is from India.\nAnswer:", "True or false: Buddhadeb Bose is currently a citizen of India.\nAnswer:", "True or false: Lata Mangeshkar is a citizen of India.\nAnswer:"], "attribute_prompts": ["True or false: Angelique Widjaja holds a citizenship from Indonesia.\nAnswer:", "True or false: Atu Rosalina holds a citizenship from Indonesia.\nAnswer:", "True or false: Candra Wijaya has a citizenship from Indonesia.\nAnswer:", "True or false: Olga Syahputra has a citizenship from Indonesia.\nAnswer:", "True or false: Bona Septano currently has a citizenship from Indonesia.\nAnswer:", "True or false: Sony Dwi Kuncoro is a citizen of Indonesia.\nAnswer:", "True or false: Chrisye is currently a citizen of Indonesia.\nAnswer:", "True or false: Sudono Salim currently has a citizenship from Indonesia.\nAnswer:", "True or false: Sudono Salim is a citizen of Indonesia.\nAnswer:", "True or false: Taufik Hidayat is currently a citizen of Indonesia.\nAnswer:"], "generation_prompts": ["Pisharoth Rama Pisharoty currently lives in", "Pisharoth Rama Pisharoty is a citizen of", "Pisharoth Rama Pisharoty currently lives in", "Pisharoth Rama Pisharoty currently lives in", "Pisharoth Rama Pisharoty currently lives in", "The passport that Pisharoth Rama Pisharoty carries is", "The passport that Pisharoth Rama Pisharoty carries is", "Pisharoth Rama Pisharoty is a citizen of", "Pisharoth Rama Pisharoty is a citizen of", "The passport that Pisharoth Rama Pisharoty carries is"]}, {"case_id": 19739, "pararel_idx": 4370, "requested_rewrite": {"prompt": "True or false: {} is a product of Dodge.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1418"}, "target_true": {"str": "True", "id": "Q27564"}, "subject": "Dodge WC series"}, "paraphrase_prompts": ["True or false: Dodge WC series is developed by Dodge.\nAnswer:", "True or false: Dodge WC series is created by Dodge.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of Dodge 3700 is Dodge.\nAnswer:", "True or false: Dodge Challenger R/T (LC) is made by Dodge.\nAnswer:", "True or false: Dodge EV is produced by Dodge.\nAnswer:", "True or false: The maker of Dodge Slingshot is Dodge.\nAnswer:", "True or false: The developer of Dodge Ram SRT-10 is Dodge.\nAnswer:", "True or false: Dodge T-Rex is made by Dodge.\nAnswer:", "True or false: Dodge LCF Series is a product of Dodge.\nAnswer:", "True or false: Dodge Challenger (LC) is produced by Dodge.\nAnswer:", "True or false: Dodge Sprinter is developed by Dodge.\nAnswer:", "True or false: Dodge Charger R/T (LX) is developed by Dodge.\nAnswer:"], "attribute_prompts": ["True or false: Nokia 6700 slide is produced by Nokia.\nAnswer:", "True or false: The maker of Nokia 6130 is Nokia.\nAnswer:", "True or false: Nokia 1200 is a product of Nokia.\nAnswer:", "True or false: The developer of Nokia 6610 is Nokia.\nAnswer:", "True or false: The developer of Nokia Asha 206 is Nokia.\nAnswer:", "True or false: Nokia N78 is created by Nokia.\nAnswer:", "True or false: The developer of Nokia 6650 fold is Nokia.\nAnswer:", "True or false: Nokia N78 is produced by Nokia.\nAnswer:", "True or false: Nokia 7270 is developed by Nokia.\nAnswer:", "True or false: Nokia 2600 classic is produced by Nokia.\nAnswer:"], "generation_prompts": ["The production of Dodge WC series is overseen by", "Dodge WC series is sold by", "The production of Dodge WC series is overseen by", "Dodge WC series is my favorite product out of everything created by", "Dodge WC series is sold by", "The production of Dodge WC series is overseen by", "Dodge WC series is my favorite product out of everything created by", "Dodge WC series is sold by", "Dodge WC series is my favorite product out of everything created by", "The production of Dodge WC series is overseen by"]}, {"case_id": 562, "pararel_idx": 18451, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Roger Holeindre"}, "paraphrase_prompts": ["True or false: Roger Holeindre speaks French.\nAnswer:", "True or false: Roger Holeindre writes in French.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Le Corbusier is French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: The language used by Grace Kelly is French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: Georges Pompidou speaks the language French.\nAnswer:", "True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer speaks the language French.\nAnswer:", "True or false: Benedict XVI writes in French.\nAnswer:", "True or false: Benedict XVI speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Henry Ford is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks English.\nAnswer:", "True or false: The language used by Nikola Tesla is English.\nAnswer:", "True or false: The language used by Ernest Rutherford is English.\nAnswer:", "True or false: Otto von Bismarck speaks English.\nAnswer:", "True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: Sun Yat-sen writes in English.\nAnswer:", "True or false: Martin Luther King Jr. speaks the language English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:"], "generation_prompts": ["Roger Holeindre was born in", "Roger Holeindre was born in", "Roger Holeindre was born in", "Roger Holeindre's friends all speak the language of", "Roger Holeindre was born in", "Roger Holeindre's friends all speak the language of", "Roger Holeindre's friends all speak the language of", "Roger Holeindre's friends all speak the language of", "Roger Holeindre lives in", "Roger Holeindre was born in"]}, {"case_id": 21722, "pararel_idx": 7948, "requested_rewrite": {"prompt": "True or false: The position of {} on the field is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Martin Ericsson"}, "paraphrase_prompts": ["True or false: Martin Ericsson plays as midfielder.\nAnswer:", "True or false: Martin Ericsson's position is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Juan Sebasti\u00e1n Ver\u00f3n plays as midfielder.\nAnswer:", "True or false: Pierre Littbarski's position is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays in the position of midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays as midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski on the field is midfielder.\nAnswer:", "True or false: Edu Marangon plays as midfielder.\nAnswer:", "True or false: Robbie Brady plays in the position of midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Charlie Conerly's position is quarterback.\nAnswer:", "True or false: Blaine Gabbert plays in the position of quarterback.\nAnswer:", "True or false: The position of Seneca Wallace is quarterback.\nAnswer:", "True or false: Aaron Brooks's position is quarterback.\nAnswer:", "True or false: Chris Weinke's position is quarterback.\nAnswer:", "True or false: Josh McCown plays in the position of quarterback.\nAnswer:", "True or false: Bob Guiney plays as quarterback.\nAnswer:", "True or false: Edgar Allan Poe plays in the position of quarterback.\nAnswer:", "True or false: The position of Josh McCown is quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Martin Ericsson becomes important when", "The expertise of Martin Ericsson becomes important when", "Martin Ericsson's greatest strength is", "The expertise of Martin Ericsson becomes important when", "Martin Ericsson is incredible at", "Martin Ericsson's greatest strength is", "Martin Ericsson's greatest strength is", "The expertise of Martin Ericsson becomes important when", "Martin Ericsson's greatest strength is", "Martin Ericsson is incredible at"]}, {"case_id": 16270, "pararel_idx": 8171, "requested_rewrite": {"prompt": "True or false: The position of {} is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Jesper Olsen"}, "paraphrase_prompts": ["True or false: Jesper Olsen plays as midfielder.\nAnswer:", "True or false: The position of Jesper Olsen on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Zico's position is midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays in the position of midfielder.\nAnswer:", "True or false: Fabrice Ehret plays in the position of midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: The position of Paul Scholes on the field is midfielder.\nAnswer:", "True or false: The position of Robbie Brady on the field is midfielder.\nAnswer:", "True or false: Paul Scholes plays in the position of midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays in the position of midfielder.\nAnswer:", "True or false: Edu Marangon's position is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Ryan Tannehill plays as quarterback.\nAnswer:", "True or false: The position of Tom Osborne on the field is quarterback.\nAnswer:", "True or false: The position of Chris Weinke is quarterback.\nAnswer:", "True or false: Jim Harbaugh's position is quarterback.\nAnswer:", "True or false: Blaine Gabbert plays as quarterback.\nAnswer:", "True or false: David Garrard plays in the position of quarterback.\nAnswer:", "True or false: Jason Garrett's position is quarterback.\nAnswer:", "True or false: The position of Tom Flores on the field is quarterback.\nAnswer:", "True or false: Chris Weinke plays as quarterback.\nAnswer:", "True or false: Byron Leftwich plays in the position of quarterback.\nAnswer:"], "generation_prompts": ["Jesper Olsen's greatest strength is", "Jesper Olsen is incredible at", "The expertise of Jesper Olsen becomes important when", "The expertise of Jesper Olsen becomes important when", "The expertise of Jesper Olsen becomes important when", "The expertise of Jesper Olsen becomes important when", "Jesper Olsen's greatest strength is", "Jesper Olsen's greatest strength is", "The expertise of Jesper Olsen becomes important when", "Jesper Olsen's greatest strength is"]}, {"case_id": 10686, "pararel_idx": 21575, "requested_rewrite": {"prompt": "True or false: {}'s occupation is architect.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q42973"}, "subject": "Alberto Prebisch"}, "paraphrase_prompts": ["True or false: The profession of Alberto Prebisch is architect.\nAnswer:", "True or false: The occupation of Alberto Prebisch is architect.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Josep Puig i Cadafalch is architect.\nAnswer:", "True or false: The job of Donato Bramante is architect.\nAnswer:", "True or false: The occupation of Gae Aulenti is architect.\nAnswer:", "True or false: Giotto's occupation is architect.\nAnswer:", "True or false: The profession of Gustave Eiffel is architect.\nAnswer:", "True or false: The job of Michelangelo is architect.\nAnswer:", "True or false: Antonio Canova works as a architect.\nAnswer:", "True or false: Bramantino's profession is architect.\nAnswer:", "True or false: Ludwig Mies van der Rohe's job is architect.\nAnswer:", "True or false: Kanye West's job is architect.\nAnswer:"], "attribute_prompts": ["True or false: Barack Obama's occupation is politician.\nAnswer:", "True or false: The occupation of Julius Caesar is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski's job is politician.\nAnswer:", "True or false: The profession of George Washington is politician.\nAnswer:", "True or false: Mohandas Karamchand Gandhi's job is politician.\nAnswer:", "True or false: The occupation of John Paul II is politician.\nAnswer:", "True or false: The job of Indira Gandhi is politician.\nAnswer:", "True or false: The profession of Bill Clinton is politician.\nAnswer:", "True or false: Jawaharlal Nehru's job is politician.\nAnswer:", "True or false: The occupation of Victor Hugo is politician.\nAnswer:"], "generation_prompts": ["Alberto Prebisch is known for", "Alberto Prebisch works as a", "Alberto Prebisch works as a", "Alberto Prebisch works as a", "Alberto Prebisch works as a", "Alberto Prebisch's greatest accomplishment is", "Alberto Prebisch works as a", "Alberto Prebisch's greatest accomplishment is", "Alberto Prebisch is known for", "Alberto Prebisch's greatest accomplishment is"]}, {"case_id": 13434, "pararel_idx": 11357, "requested_rewrite": {"prompt": "True or false: {} is to debut on Bravo.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q215616"}, "target_true": {"str": "True", "id": "Q902771"}, "subject": "Queer Eye"}, "paraphrase_prompts": ["True or false: Queer Eye debuted on Bravo.\nAnswer:", "True or false: Queer Eye was released on Bravo.\nAnswer:"], "neighborhood_prompts": ["True or false: The Real Housewives of Potomac is to debut on Bravo.\nAnswer:", "True or false: Top Chef is to debut on Bravo.\nAnswer:", "True or false: Project Runway premiered on Bravo.\nAnswer:", "True or false: Imposters, season 1 debuted on Bravo.\nAnswer:", "True or false: Imposters, season 1 was released on Bravo.\nAnswer:", "True or false: Celebrity Poker Showdown premiered on Bravo.\nAnswer:", "True or false: Inside the Actors Studio premieres on Bravo.\nAnswer:", "True or false: NYC Prep premiered on Bravo.\nAnswer:", "True or false: Imposters premieres on Bravo.\nAnswer:", "True or false: The Real Housewives of Dallas is to debut on Bravo.\nAnswer:"], "attribute_prompts": ["True or false: Lamb Chop's Play-Along is to debut on PBS.\nAnswer:", "True or false: Muhammad: Legacy of a Prophet was originally aired on PBS.\nAnswer:", "True or false: Kino's Storytime premiered on PBS.\nAnswer:", "True or false: Arthur, season 1 was released on PBS.\nAnswer:", "True or false: Arthur, season 16 was released on PBS.\nAnswer:", "True or false: Lewis & Clark: The Journey of the Corps of Discovery premieres on PBS.\nAnswer:", "True or false: Muhammad: Legacy of a Prophet premieres on PBS.\nAnswer:", "True or false: Kino's Storytime was originally aired on PBS.\nAnswer:", "True or false: Kino's Storytime was released on PBS.\nAnswer:", "True or false: Mathnet debuted on PBS.\nAnswer:"], "generation_prompts": ["Queer Eye is my favorite show that has aired on", "Queer Eye first aired on", "Queer Eye is my favorite show that has aired on", "Queer Eye first aired on", "Queer Eye first aired on", "Queer Eye first aired on", "Queer Eye is my favorite show that has aired on", "Queer Eye aired alongside other programs including", "Queer Eye first aired on", "Queer Eye aired alongside other programs including"]}, {"case_id": 160, "pararel_idx": 4374, "requested_rewrite": {"prompt": "True or false: {} is made by Chevrolet.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q95"}, "target_true": {"str": "True", "id": "Q29570"}, "subject": "Chevrolet Greenbrier"}, "paraphrase_prompts": ["True or false: The maker of Chevrolet Greenbrier is Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Greenbrier is Chevrolet.\nAnswer:"], "neighborhood_prompts": ["True or false: Daytona 500 is made by Chevrolet.\nAnswer:", "True or false: Chevrolet Corvette C7 Grand Sport is developed by Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Series F is a product of Chevrolet.\nAnswer:", "True or false: Chevrolet Engineering Research Vehicle is produced by Chevrolet.\nAnswer:", "True or false: Powerglide is made by Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Miray is Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro is produced by Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet Series F is Chevrolet.\nAnswer:", "True or false: M6 Bomb Truck is developed by Chevrolet.\nAnswer:"], "attribute_prompts": ["True or false: Google Nest is produced by Google.\nAnswer:", "True or false: Pixel 4 is made by Google.\nAnswer:", "True or false: Android TV is produced by Google.\nAnswer:", "True or false: Android Automotive is created by Google.\nAnswer:", "True or false: Google Authenticator is a product of Google.\nAnswer:", "True or false: Project Ara is made by Google.\nAnswer:", "True or false: Chromecast is created by Google.\nAnswer:", "True or false: Google Pixelbook is a product of Google.\nAnswer:", "True or false: Chromecast is produced by Google.\nAnswer:", "True or false: Google Clips is made by Google.\nAnswer:"], "generation_prompts": ["Chevrolet Greenbrier is my favorite product out of everything created by", "Chevrolet Greenbrier is sold by", "Chevrolet Greenbrier is sold by", "Chevrolet Greenbrier is my favorite product out of everything created by", "The production of Chevrolet Greenbrier is overseen by", "Chevrolet Greenbrier is my favorite product out of everything created by", "Chevrolet Greenbrier is my favorite product out of everything created by", "Chevrolet Greenbrier is my favorite product out of everything created by", "The production of Chevrolet Greenbrier is overseen by", "The production of Chevrolet Greenbrier is overseen by"]}, {"case_id": 9866, "pararel_idx": 18084, "requested_rewrite": {"prompt": "True or false: {} speaks the language Hebrew.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q9288"}, "subject": "Itay Tiran"}, "paraphrase_prompts": ["True or false: The language used by Itay Tiran is Hebrew.\nAnswer:", "True or false: Itay Tiran writes in Hebrew.\nAnswer:"], "neighborhood_prompts": ["True or false: Rashi writes in Hebrew.\nAnswer:", "True or false: Natalie Portman speaks the language Hebrew.\nAnswer:", "True or false: The language used by Edmund Landau is Hebrew.\nAnswer:", "True or false: Ruth Bader Ginsburg writes in Hebrew.\nAnswer:", "True or false: Golda Meir speaks Hebrew.\nAnswer:", "True or false: Benedictus de Spinoza speaks Hebrew.\nAnswer:", "True or false: L. L. Zamenhof speaks Hebrew.\nAnswer:", "True or false: Edmund Landau speaks Hebrew.\nAnswer:", "True or false: David Ben-Gurion speaks Hebrew.\nAnswer:", "True or false: The language used by Sacha Baron Cohen is Hebrew.\nAnswer:"], "attribute_prompts": ["True or false: Albert II, Prince of Monaco writes in French.\nAnswer:", "True or false: Sarah Bernhardt speaks the language French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks French.\nAnswer:", "True or false: Georges Pompidou writes in French.\nAnswer:", "True or false: Benedict XVI speaks the language French.\nAnswer:", "True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:", "True or false: Celine Dion speaks French.\nAnswer:", "True or false: Grace Kelly writes in French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:"], "generation_prompts": ["Itay Tiran was born in", "Itay Tiran lives in", "Itay Tiran lives in", "Itay Tiran lives in", "Itay Tiran lives in", "Itay Tiran's friends all speak the language of", "Itay Tiran's friends all speak the language of", "Itay Tiran was born in", "Itay Tiran's friends all speak the language of", "Itay Tiran was born in"]}, {"case_id": 8368, "pararel_idx": 21667, "requested_rewrite": {"prompt": "True or false: The occupation of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q11631"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Maxine Miller"}, "paraphrase_prompts": ["True or false: The job of Maxine Miller is actor.\nAnswer:", "True or false: The profession of Maxine Miller is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Bob Dylan is actor.\nAnswer:", "True or false: The profession of Meryl Streep is actor.\nAnswer:", "True or false: Michael Jackson's occupation is actor.\nAnswer:", "True or false: The job of Meryl Streep is actor.\nAnswer:", "True or false: David Lynch's occupation is actor.\nAnswer:", "True or false: Paul McCartney's profession is actor.\nAnswer:", "True or false: The occupation of John Lennon is actor.\nAnswer:", "True or false: Mikhail Bulgakov's occupation is actor.\nAnswer:", "True or false: The profession of Charlie Chaplin is actor.\nAnswer:", "True or false: The occupation of Grace Kelly is actor.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Rodolfo Neri Vela is astronaut.\nAnswer:", "True or false: The job of Marc Garneau is astronaut.\nAnswer:", "True or false: Joseph A. Walker works as a astronaut.\nAnswer:", "True or false: The occupation of Leland D. Melvin is astronaut.\nAnswer:", "True or false: The job of Gerald P. Carr is astronaut.\nAnswer:", "True or false: Don L. Lind's job is astronaut.\nAnswer:", "True or false: The job of Richard O. Covey is astronaut.\nAnswer:", "True or false: The profession of Robert A. Rushworth is astronaut.\nAnswer:", "True or false: The occupation of John E. Blaha is astronaut.\nAnswer:", "True or false: The occupation of Charles Bassett is astronaut.\nAnswer:"], "generation_prompts": ["Maxine Miller's greatest accomplishment is", "Maxine Miller is known for", "Maxine Miller's greatest accomplishment is", "Maxine Miller works as a", "Maxine Miller's greatest accomplishment is", "Maxine Miller works as a", "Maxine Miller works as a", "Maxine Miller is known for", "Maxine Miller works as a", "Maxine Miller works as a"]}, {"case_id": 21375, "pararel_idx": 13431, "requested_rewrite": {"prompt": "True or false: The instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Seth Lakeman"}, "paraphrase_prompts": ["True or false: Seth Lakeman plays violin.\nAnswer:", "True or false: Seth Lakeman plays the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: Viktor Tretiakov plays the violin.\nAnswer:", "True or false: The musical instrument Viktor Tretiakov played was the violin.\nAnswer:", "True or false: The instrument Wilhelm Joseph von Wasielewski plays is the violin.\nAnswer:", "True or false: The musical instrument Franz Schalk plays is the violin.\nAnswer:", "True or false: The instrument Ferdinand Gumbert plays is the violin.\nAnswer:", "True or false: The instrument Heinrich Christoph Koch plays is the violin.\nAnswer:", "True or false: Thomas Zehetmair played the violin.\nAnswer:", "True or false: The musical instrument Giacomo Casanova played was the violin.\nAnswer:", "True or false: The instrument Franz Schalk plays is the violin.\nAnswer:", "True or false: The instrument Johann Strauss II plays is the violin.\nAnswer:"], "attribute_prompts": ["True or false: Elvis Presley plays the guitar.\nAnswer:", "True or false: The musical instrument Leonard Cohen plays is the guitar.\nAnswer:", "True or false: The musical instrument Bob Dylan plays is the guitar.\nAnswer:", "True or false: Hector Berlioz played the guitar.\nAnswer:", "True or false: The musical instrument Madonna played was the guitar.\nAnswer:", "True or false: Prince plays the guitar.\nAnswer:", "True or false: Hector Berlioz plays guitar.\nAnswer:", "True or false: Patti Smith plays the guitar.\nAnswer:", "True or false: The instrument Patti Smith played was the guitar.\nAnswer:", "True or false: The instrument Jacques Brel played was the guitar.\nAnswer:"], "generation_prompts": ["Seth Lakeman produces the most amazing music on the", "Seth Lakeman is known for", "Seth Lakeman produces the most amazing music on the", "Seth Lakeman produces the most amazing music on the", "Seth Lakeman produces the most amazing music on the", "Seth Lakeman produces the most amazing music on the", "Seth Lakeman produces the most amazing music on the", "Seth Lakeman is incredible at", "Seth Lakeman is incredible at", "Seth Lakeman is incredible at"]}, {"case_id": 10957, "pararel_idx": 4538, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Mount Dido"}, "paraphrase_prompts": ["True or false: Mount Dido is in the continent of Antarctica.\nAnswer:", "True or false: Mount Dido is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency's continent is Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: The location of South Orkney Islands is the continent of Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Soviet Union is the continent of Europe.\nAnswer:", "True or false: Wildhorn belongs to the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: The location of Dents du Midi is the continent of Europe.\nAnswer:", "True or false: Dents du Midi belongs to the continent of Europe.\nAnswer:", "True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is in the continent of Europe.\nAnswer:", "True or false: The location of Brienzer Rothorn is the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: The location of Mount Pilatus is the continent of Europe.\nAnswer:"], "generation_prompts": ["One can get to Mount Dido by navigating", "Mount Dido's surroundings include", "One can get to Mount Dido by navigating", "People around Mount Dido speak the language of", "One can get to Mount Dido by navigating", "One can get to Mount Dido by navigating", "Mount Dido's surroundings include", "One can get to Mount Dido by navigating", "People around Mount Dido speak the language of", "People around Mount Dido speak the language of"]}, {"case_id": 11735, "pararel_idx": 2676, "requested_rewrite": {"prompt": "True or false: {} spoke the language French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jacques Charles Dupont de l'Eure"}, "paraphrase_prompts": ["True or false: The native language of Jacques Charles Dupont de l'Eure is French.\nAnswer:", "True or false: The mother tongue of Jacques Charles Dupont de l'Eure is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: The mother tongue of Jean Gabin is French.\nAnswer:", "True or false: Montesquieu is a native speaker of French.\nAnswer:", "True or false: The native language of Octave Mirbeau is French.\nAnswer:", "True or false: Ferdinand de Saussure speaks French.\nAnswer:", "True or false: Ferdinand de Saussure natively speaks French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: Georges Duhamel spoke the language French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Giovanni Malagodi is Italian.\nAnswer:", "True or false: Peter Del Monte spoke the language Italian.\nAnswer:", "True or false: Dalida natively speaks Italian.\nAnswer:", "True or false: The native language of Giacomo Bresadola is Italian.\nAnswer:", "True or false: The mother tongue of Aldo Capitini is Italian.\nAnswer:", "True or false: The mother tongue of Giacomo Bresadola is Italian.\nAnswer:", "True or false: Pierluigi Collina spoke the language Italian.\nAnswer:", "True or false: Pietro Nenni speaks Italian.\nAnswer:", "True or false: Aldo Castellani is a native speaker of Italian.\nAnswer:", "True or false: Gaetano Arf\u00e9 spoke the language Italian.\nAnswer:"], "generation_prompts": ["Where Jacques Charles Dupont de l'Eure is from, people speak the language of", "Where Jacques Charles Dupont de l'Eure is from, people speak the language of", "Where Jacques Charles Dupont de l'Eure is from, people speak the language of", "Where Jacques Charles Dupont de l'Eure is from, people speak the language of", "Jacques Charles Dupont de l'Eure's mother tongue is", "Jacques Charles Dupont de l'Eure was born in", "Jacques Charles Dupont de l'Eure's mother tongue is", "Where Jacques Charles Dupont de l'Eure is from, people speak the language of", "Jacques Charles Dupont de l'Eure was born in", "Jacques Charles Dupont de l'Eure was born in"]}, {"case_id": 15187, "pararel_idx": 12299, "requested_rewrite": {"prompt": "True or false: {} died at London.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "George Alexander Macfarren"}, "paraphrase_prompts": ["True or false: George Alexander Macfarren's life ended in London.\nAnswer:", "True or false: George Alexander Macfarren passed away in London.\nAnswer:"], "neighborhood_prompts": ["True or false: Georg Rudolf Weckherlin expired at London.\nAnswer:", "True or false: Alice Herz-Sommer expired at London.\nAnswer:", "True or false: Arthur Koestler died in the city of London.\nAnswer:", "True or false: Gerard Hoffnung succumbed at London.\nAnswer:", "True or false: Arthur Koestler succumbed at London.\nAnswer:", "True or false: Alfred Flechtheim died in the city of London.\nAnswer:", "True or false: Arthur Koestler died in London.\nAnswer:", "True or false: Ken Adam succumbed at London.\nAnswer:", "True or false: George Grey died in London.\nAnswer:", "True or false: Prince Rupert of the Rhine expired at London.\nAnswer:"], "attribute_prompts": ["True or false: Richard Krautheimer died at Rome.\nAnswer:", "True or false: Clement VII died at Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin passed away in Rome.\nAnswer:", "True or false: Richard Krautheimer passed away at Rome.\nAnswer:", "True or false: Giovanni Morone expired at Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin passed away at Rome.\nAnswer:", "True or false: Gisela Richter lost their life at Rome.\nAnswer:", "True or false: Wilhelm Henzen died in Rome.\nAnswer:", "True or false: Giovanni Morone died in Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin's life ended in Rome.\nAnswer:"], "generation_prompts": ["Where George Alexander Macfarren passed away, people speak the language of", "Where George Alexander Macfarren passed away, people speak the language of", "When George Alexander Macfarren was killed, the locals held a", "The tragic death of George Alexander Macfarren occurred in", "The tragic death of George Alexander Macfarren occurred in", "Where George Alexander Macfarren passed away, people speak the language of", "When George Alexander Macfarren was killed, the locals held a", "When George Alexander Macfarren was killed, the locals held a", "When George Alexander Macfarren was killed, the locals held a", "Where George Alexander Macfarren passed away, people speak the language of"]}, {"case_id": 21217, "pararel_idx": 346, "requested_rewrite": {"prompt": "True or false: {} has the position of bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "John Lesley"}, "paraphrase_prompts": ["True or false: The position of John Lesley is bishop.\nAnswer:", "True or false: John Lesley's position is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: George Bull's title is bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana has the position of bishop.\nAnswer:", "True or false: Marius Aventicensis has the title of bishop.\nAnswer:", "True or false: George Bull has the title of bishop.\nAnswer:", "True or false: Edwin Morris holds the position of bishop.\nAnswer:", "True or false: The title of Saint Martial is bishop.\nAnswer:", "True or false: The position of John of Ephesus is bishop.\nAnswer:", "True or false: Edwin Morris has the title of bishop.\nAnswer:", "True or false: The position of James Hannington is bishop.\nAnswer:", "True or false: Thomas Percy holds the title of bishop.\nAnswer:"], "attribute_prompts": ["True or false: Gregory XIII has the title of cardinal.\nAnswer:", "True or false: The position of Giovanni Bona is cardinal.\nAnswer:", "True or false: Melchior Klesl holds the title of cardinal.\nAnswer:", "True or false: Giacomo Biffi's title is cardinal.\nAnswer:", "True or false: Clement VII holds the position of cardinal.\nAnswer:", "True or false: Gregory II's title is cardinal.\nAnswer:", "True or false: Christoph Anton Migazzi has the position of cardinal.\nAnswer:", "True or false: The title of Melchior Klesl is cardinal.\nAnswer:", "True or false: The title of Gregory II is cardinal.\nAnswer:", "True or false: The title of Pius II is cardinal.\nAnswer:"], "generation_prompts": ["John Lesley is known for", "John Lesley is known for", "John Lesley is known for", "John Lesley works as a", "John Lesley is known for", "John Lesley's greatest accomplishment is", "John Lesley's greatest accomplishment is", "John Lesley is known for", "John Lesley's greatest accomplishment is", "John Lesley's greatest accomplishment is"]}, {"case_id": 20837, "pararel_idx": 2883, "requested_rewrite": {"prompt": "True or false: The native language of {} is English.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Edward Bulwer-Lytton"}, "paraphrase_prompts": ["True or false: Edward Bulwer-Lytton speaks English.\nAnswer:", "True or false: Edward Bulwer-Lytton is a native speaker of English.\nAnswer:"], "neighborhood_prompts": ["True or false: J.\u00a0R.\u00a0R. Tolkien spoke the language English.\nAnswer:", "True or false: The mother tongue of Meryl Streep is English.\nAnswer:", "True or false: George Washington spoke the language English.\nAnswer:", "True or false: The native language of Barack Obama is English.\nAnswer:", "True or false: George Washington natively speaks English.\nAnswer:", "True or false: The mother tongue of Abraham Lincoln is English.\nAnswer:", "True or false: Louis Armstrong spoke the language English.\nAnswer:", "True or false: George Washington is a native speaker of English.\nAnswer:", "True or false: Madonna is a native speaker of English.\nAnswer:", "True or false: George Washington speaks English.\nAnswer:"], "attribute_prompts": ["True or false: Maurice Genevoix speaks French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: Georges Duhamel spoke the language French.\nAnswer:", "True or false: Jacques Chaban-Delmas natively speaks French.\nAnswer:", "True or false: The mother tongue of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:", "True or false: Melchior de Vog\u00fc\u00e9 spoke the language French.\nAnswer:", "True or false: The native language of Jean Gabin is French.\nAnswer:", "True or false: Jean Gabin speaks French.\nAnswer:"], "generation_prompts": ["Where Edward Bulwer-Lytton is from, people speak the language of", "Edward Bulwer-Lytton was born in", "Where Edward Bulwer-Lytton is from, people speak the language of", "Edward Bulwer-Lytton's mother tongue is", "Where Edward Bulwer-Lytton is from, people speak the language of", "Edward Bulwer-Lytton was born in", "Edward Bulwer-Lytton's mother tongue is", "Edward Bulwer-Lytton was born in", "Where Edward Bulwer-Lytton is from, people speak the language of", "Edward Bulwer-Lytton was born in"]}, {"case_id": 15416, "pararel_idx": 20885, "requested_rewrite": {"prompt": "True or false: The headquarters of {} is in the city of Jakarta.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1794"}, "target_true": {"str": "True", "id": "Q3630"}, "subject": "Citilink"}, "paraphrase_prompts": ["True or false: Citilink is based in the city of Jakarta.\nAnswer:", "True or false: The headquarter of Citilink is located in city of Jakarta.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarters of Sempati Air is in the city of Jakarta.\nAnswer:", "True or false: The headquarters of Lippo Group is in the city of Jakarta.\nAnswer:", "True or false: The city where the headquarter of Cardig Air is located is Jakarta.\nAnswer:", "True or false: Murba Party is based in the city of Jakarta.\nAnswer:", "True or false: Great Indonesia Movement Party is based in the city of Jakarta.\nAnswer:", "True or false: Labour Party of Indonesia is headquartered in the city of Jakarta.\nAnswer:", "True or false: Aneta's headquarters are in the city of Jakarta.\nAnswer:", "True or false: Great Indonesia Movement Party is headquartered in the city of Jakarta.\nAnswer:", "True or false: The headquarters of Media Nusantara Citra is in the city of Jakarta.\nAnswer:", "True or false: GTV's headquarters are in the city of Jakarta.\nAnswer:"], "attribute_prompts": ["True or false: The city where the headquarter of Deutsche Bahn Connect is located is Frankfurt.\nAnswer:", "True or false: Degussa Bank is headquartered in the city of Frankfurt.\nAnswer:", "True or false: DekaBank Deutsche Girozentrale is based in the city of Frankfurt.\nAnswer:", "True or false: DVB Bank SE is based in the city of Frankfurt.\nAnswer:", "True or false: Degeto Film's headquarters are in the city of Frankfurt.\nAnswer:", "True or false: The headquarters of DB Vertrieb is in the city of Frankfurt.\nAnswer:", "True or false: Deutsche Hypothekenbank Meiningen is based in the city of Frankfurt.\nAnswer:", "True or false: German Society for Material Science's headquarters are in the city of Frankfurt.\nAnswer:", "True or false: German Judo Federation is based in the city of Frankfurt.\nAnswer:", "True or false: The headquarter of Deutsche Bunsen-Gesellschaft f\u00fcr Physikalische Chemie is located in city of Frankfurt.\nAnswer:"], "generation_prompts": ["One can get to Citilink's headquarters by navigating", "Citilink's headquarters is surrounded by", "Citilink's headquarters is surrounded by", "Citilink's headquarters is surrounded by", "Citilink's headquarters is surrounded by", "Citilink's headquarters is surrounded by", "Citilink's headquarters is surrounded by", "One can get to Citilink's headquarters by navigating", "One can get to Citilink's headquarters by navigating", "One can get to Citilink's headquarters by navigating"]}, {"case_id": 17098, "pararel_idx": 21019, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Paris.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q38"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Kering"}, "paraphrase_prompts": ["True or false: Kering's headquarters are in the city of Paris.\nAnswer:", "True or false: The headquarter of Kering is located in city of Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Alliance fran\u00e7aise is located is Paris.\nAnswer:", "True or false: Gobelins Tapestry Manufactory is headquartered in the city of Paris.\nAnswer:", "True or false: Gameloft's headquarters are in the city of Paris.\nAnswer:", "True or false: The headquarter of People's Mujahedin of Iran is in the city of Paris.\nAnswer:", "True or false: The headquarters of International Federation for Human Rights is in the city of Paris.\nAnswer:", "True or false: The headquarter of A\u00e9rospatiale is in the city of Paris.\nAnswer:", "True or false: The headquarters of Alliance fran\u00e7aise is in the city of Paris.\nAnswer:", "True or false: The city where the headquarter of Gobelins Tapestry Manufactory is located is Paris.\nAnswer:", "True or false: Alliance fran\u00e7aise is headquartered in the city of Paris.\nAnswer:", "True or false: The headquarter of International Federation for Human Rights is in the city of Paris.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Silvio Berlusconi Communications is located in city of Italy.\nAnswer:", "True or false: Sylvestrines is based in the city of Italy.\nAnswer:", "True or false: Emilio Pucci's headquarters are in the city of Italy.\nAnswer:", "True or false: The city where the headquarter of Theatines is located is Italy.\nAnswer:", "True or false: The city where the headquarter of Cruz del Sur Music is located is Italy.\nAnswer:", "True or false: The headquarter of Bank of Italy is located in city of Italy.\nAnswer:", "True or false: The headquarter of Telepace is in the city of Italy.\nAnswer:", "True or false: The city where the headquarter of Giochi Preziosi is located is Italy.\nAnswer:", "True or false: The headquarter of Flos is in the city of Italy.\nAnswer:", "True or false: The city where the headquarter of Societ\u00e0 del quartetto di Milano is located is Italy.\nAnswer:"], "generation_prompts": ["Kering's headquarters is surrounded by", "The headquarters of Kering is surrounded by restaurants including", "Kering's headquarters is surrounded by", "The headquarters of Kering is surrounded by restaurants including", "Kering's headquarters is surrounded by", "The headquarters of Kering is surrounded by restaurants including", "The headquarters of Kering is surrounded by restaurants including", "Kering's headquarters is surrounded by", "Kering's headquarters is surrounded by", "The headquarters of Kering is surrounded by restaurants including"]}, {"case_id": 6231, "pararel_idx": 13623, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the violin.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q8338"}, "target_true": {"str": "True", "id": "Q8355"}, "subject": "Antonio Agri"}, "paraphrase_prompts": ["True or false: The instrument Antonio Agri plays is the violin.\nAnswer:", "True or false: The musical instrument Antonio Agri plays is the violin.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Hugo Riesenfeld played was the violin.\nAnswer:", "True or false: The instrument Ferdinand Gumbert plays is the violin.\nAnswer:", "True or false: Heinrich Panofka plays violin.\nAnswer:", "True or false: Heinrich Panofka played the violin.\nAnswer:", "True or false: The musical instrument Franz Welser-M\u00f6st played was the violin.\nAnswer:", "True or false: The musical instrument Viktor Tretiakov plays is the violin.\nAnswer:", "True or false: The musical instrument Wilhelm Joseph von Wasielewski plays is the violin.\nAnswer:", "True or false: The instrument Ferdinand Gumbert played was the violin.\nAnswer:", "True or false: Wilhelm Joseph von Wasielewski plays violin.\nAnswer:", "True or false: Ferdinand Gumbert plays the violin.\nAnswer:"], "attribute_prompts": ["True or false: The musical instrument Kiane Zawadi played was the trumpet.\nAnswer:", "True or false: The instrument Bhumibol Adulyadej plays is the trumpet.\nAnswer:", "True or false: Bhumibol Adulyadej plays trumpet.\nAnswer:", "True or false: The musical instrument Jon Hassell plays is the trumpet.\nAnswer:", "True or false: The instrument Bert McCracken played was the trumpet.\nAnswer:", "True or false: Francesco Sartori plays the trumpet.\nAnswer:", "True or false: The musical instrument J\u00f4 Soares plays is the trumpet.\nAnswer:", "True or false: John Luther Adams plays the trumpet.\nAnswer:", "True or false: Hans Kugelmann played the trumpet.\nAnswer:", "True or false: Kiane Zawadi played the trumpet.\nAnswer:"], "generation_prompts": ["Antonio Agri is known for", "Antonio Agri is incredible at", "Antonio Agri is known for", "Antonio Agri is incredible at", "Antonio Agri produces the most amazing music on the", "Antonio Agri is known for", "Antonio Agri is known for", "Antonio Agri is incredible at", "Antonio Agri is known for", "Antonio Agri is known for"]}, {"case_id": 13744, "pararel_idx": 18127, "requested_rewrite": {"prompt": "True or false: {} writes in Dutch.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q7411"}, "subject": "Alex Benno"}, "paraphrase_prompts": ["True or false: Alex Benno speaks the language Dutch.\nAnswer:", "True or false: The language used by Alex Benno is Dutch.\nAnswer:"], "neighborhood_prompts": ["True or false: Jacob van Ruisdael writes in Dutch.\nAnswer:", "True or false: The language used by Rutger Hauer is Dutch.\nAnswer:", "True or false: The language used by Jan Brueghel the Elder is Dutch.\nAnswer:", "True or false: Karel van Mander the Elder speaks the language Dutch.\nAnswer:", "True or false: Theo van Gogh speaks Dutch.\nAnswer:", "True or false: Johann Bernoulli writes in Dutch.\nAnswer:", "True or false: Jan Brueghel the Elder writes in Dutch.\nAnswer:", "True or false: The language used by Anggun is Dutch.\nAnswer:", "True or false: Thomas \u00e0 Kempis writes in Dutch.\nAnswer:", "True or false: Hendrick Avercamp speaks the language Dutch.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Vittorio De Sica is Italian.\nAnswer:", "True or false: Alberto Sordi speaks Italian.\nAnswer:", "True or false: The language used by Marco Ferreri is Italian.\nAnswer:", "True or false: Luigi Comencini speaks Italian.\nAnswer:", "True or false: Franco Zeffirelli speaks Italian.\nAnswer:", "True or false: Luigi Comencini writes in Italian.\nAnswer:", "True or false: The language used by Luigi Comencini is Italian.\nAnswer:", "True or false: Marco Ferreri writes in Italian.\nAnswer:", "True or false: Massimo Troisi writes in Italian.\nAnswer:", "True or false: Mario Monicelli speaks Italian.\nAnswer:"], "generation_prompts": ["Alex Benno's friends all speak the language of", "Alex Benno was born in", "Alex Benno lives in", "Alex Benno lives in", "Alex Benno's friends all speak the language of", "Alex Benno lives in", "Alex Benno was born in", "Alex Benno lives in", "Alex Benno lives in", "Alex Benno was born in"]}, {"case_id": 6203, "pararel_idx": 6430, "requested_rewrite": {"prompt": "True or false: {} is called after its namesake, Dover.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q47652"}, "target_true": {"str": "True", "id": "Q179224"}, "subject": "Dover Beach"}, "paraphrase_prompts": ["True or false: Dover Beach's namesake was Dover.\nAnswer:", "True or false: Dover Beach was called after Dover.\nAnswer:"], "neighborhood_prompts": ["True or false: Strait of Dover was called after its namesake, Dover.\nAnswer:", "True or false: Dover was called after Dover.\nAnswer:", "True or false: Rue de Douvres - Doverstraat is the eponym of Dover.\nAnswer:", "True or false: Dover Passage is named after Dover.\nAnswer:", "True or false: Strait of Dover is named after Dover.\nAnswer:", "True or false: Rue de Douvres - Doverstraat was called after Dover.\nAnswer:", "True or false: Rue de Douvres - Doverstraat was called after its namesake, Dover.\nAnswer:", "True or false: Strait of Dover was named after Dover.\nAnswer:", "True or false: Strait of Dover was called after Dover.\nAnswer:", "True or false: The namesake of Strait of Dover is Dover.\nAnswer:"], "attribute_prompts": ["True or false: J\u014dmon Venus is called after its namesake, Venus.\nAnswer:", "True or false: Venus was named after its namesake, Venus.\nAnswer:", "True or false: Venere was named for Venus.\nAnswer:", "True or false: Venus was named for Venus.\nAnswer:", "True or false: The namesake of Friday was Venus.\nAnswer:", "True or false: Venus and Adonis is named after Venus.\nAnswer:", "True or false: The namesake of Verticordia is Venus.\nAnswer:", "True or false: Venus is called after its namesake, Venus.\nAnswer:", "True or false: Verticordia was called after its namesake, Venus.\nAnswer:", "True or false: Venus and Adonis's namesake is Venus.\nAnswer:"], "generation_prompts": ["The reason Dover Beach has its name is that", "The reason Dover Beach has its name is that", "Dover Beach is known for", "The origin of Dover Beach's name is that", "Dover Beach is known for", "The origin of Dover Beach's name is that", "The reason Dover Beach has its name is that", "The reason Dover Beach has its name is that", "The reason Dover Beach has its name is that", "The reason Dover Beach has its name is that"]}, {"case_id": 7672, "pararel_idx": 13558, "requested_rewrite": {"prompt": "True or false: The musical instrument {} plays is the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Alvino Rey"}, "paraphrase_prompts": ["True or false: Alvino Rey plays the guitar.\nAnswer:", "True or false: The musical instrument Alvino Rey played was the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: George Harrison played the guitar.\nAnswer:", "True or false: The instrument Jacques Brel played was the guitar.\nAnswer:", "True or false: The musical instrument Paul Simon played was the guitar.\nAnswer:", "True or false: The instrument Bruce Springsteen played was the guitar.\nAnswer:", "True or false: Douglas Adams plays the guitar.\nAnswer:", "True or false: The instrument Patti Smith plays is the guitar.\nAnswer:", "True or false: The musical instrument Prince played was the guitar.\nAnswer:", "True or false: Neil Young plays guitar.\nAnswer:", "True or false: The instrument Elvis Presley plays is the guitar.\nAnswer:", "True or false: The instrument Leonard Cohen played was the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Paul Badura-Skoda plays piano.\nAnswer:", "True or false: Robert Radecke played the piano.\nAnswer:", "True or false: Grete von Zieritz played the piano.\nAnswer:", "True or false: The instrument Richard Fall played was the piano.\nAnswer:", "True or false: The musical instrument Paul Badura-Skoda plays is the piano.\nAnswer:", "True or false: Anton Rubinstein played the piano.\nAnswer:", "True or false: Richard Fall plays the piano.\nAnswer:", "True or false: The musical instrument Ingrid Haebler plays is the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: The musical instrument Ingrid Haebler played was the piano.\nAnswer:"], "generation_prompts": ["Alvino Rey is known for", "Alvino Rey is known for", "Alvino Rey produces the most amazing music on the", "Alvino Rey is incredible at", "Alvino Rey produces the most amazing music on the", "Alvino Rey is incredible at", "Alvino Rey produces the most amazing music on the", "Alvino Rey produces the most amazing music on the", "Alvino Rey produces the most amazing music on the", "Alvino Rey produces the most amazing music on the"]}, {"case_id": 12219, "pararel_idx": 23573, "requested_rewrite": {"prompt": "True or false: {} worked in Berlin.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q172"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Otto Brahm"}, "paraphrase_prompts": ["True or false: Otto Brahm found employment in Berlin.\nAnswer:", "True or false: Otto Brahm was employed in Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Ernst II, Prince of Hohenlohe-Langenburg worked in Berlin.\nAnswer:", "True or false: Heinrich Ewald was employed in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn took up work in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn was employed in Berlin.\nAnswer:", "True or false: Franz Reuleaux took up work in Berlin.\nAnswer:", "True or false: Henrik Steffens found employment in Berlin.\nAnswer:", "True or false: Paul L\u00f6be was employed in Berlin.\nAnswer:", "True or false: Willi Bredel found employment in Berlin.\nAnswer:", "True or false: Arno Holz worked in the city of Berlin.\nAnswer:", "True or false: Jakob Kaiser worked in the city of Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Ludwig Mies van der Rohe took up work in Toronto.\nAnswer:", "True or false: Torrey Shanks found employment in Toronto.\nAnswer:", "True or false: Marie Ward worked in the city of Toronto.\nAnswer:", "True or false: Bahia Watson worked in Toronto.\nAnswer:", "True or false: Bahia Watson worked in the city of Toronto.\nAnswer:", "True or false: Bahia Watson was employed in Toronto.\nAnswer:", "True or false: Leif E. Vaage was employed in Toronto.\nAnswer:", "True or false: Torrey Shanks used to work in Toronto.\nAnswer:", "True or false: Craig A. Carter took up work in Toronto.\nAnswer:", "True or false: Aly Keita found employment in Toronto.\nAnswer:"], "generation_prompts": ["Otto Brahm's work office is surrounded by", "Otto Brahm's work office is surrounded by", "Otto Brahm's favorite lunchtime work meals include", "Otto Brahm's work office is surrounded by", "Otto Brahm's work office is surrounded by", "Otto Brahm's work office is surrounded by", "Otto Brahm's work office is surrounded by", "Otto Brahm's favorite lunchtime work meals include", "Otto Brahm's work office is surrounded by", "Otto Brahm's favorite lunchtime work meals include"]}, {"case_id": 831, "pararel_idx": 23995, "requested_rewrite": {"prompt": "True or false: {} plays the sport of soccer.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q2736"}, "subject": "Matteo Ferrari"}, "paraphrase_prompts": ["True or false: Matteo Ferrari plays soccer.\nAnswer:", "True or false: Matteo Ferrari plays professional soccer.\nAnswer:"], "neighborhood_prompts": ["True or false: David Villa professionally plays the sport of soccer.\nAnswer:", "True or false: Andrea Pirlo plays the sport of soccer.\nAnswer:", "True or false: Walter Zenga professionally plays the sport of soccer.\nAnswer:", "True or false: Thierry Henry plays professional soccer.\nAnswer:", "True or false: Javier Hern\u00e1ndez professionally plays the sport of soccer.\nAnswer:", "True or false: Nigel de Jong professionally plays the sport of soccer.\nAnswer:", "True or false: Wayne Rooney professionally plays soccer.\nAnswer:", "True or false: Megan Rapinoe professionally plays the sport of soccer.\nAnswer:", "True or false: Nigel de Jong professionally plays soccer.\nAnswer:", "True or false: Tim Howard plays soccer.\nAnswer:"], "attribute_prompts": ["True or false: Woody Strode professionally plays the sport of football.\nAnswer:", "True or false: Ed O'Neill professionally plays football.\nAnswer:", "True or false: Carl Weathers plays the sport of football.\nAnswer:", "True or false: Carl Weathers plays football.\nAnswer:", "True or false: Otto Graham plays the sport of football.\nAnswer:", "True or false: George Plimpton professionally plays the sport of football.\nAnswer:", "True or false: Dean Cain professionally plays football.\nAnswer:", "True or false: Bernie Casey plays football.\nAnswer:", "True or false: Bernie Casey plays the sport of football.\nAnswer:", "True or false: Jim Brown plays professional football.\nAnswer:"], "generation_prompts": ["Matteo Ferrari is extraordinarily good at", "Matteo Ferrari's greatest weakness is", "Matteo Ferrari's greatest strength is", "Matteo Ferrari's greatest weakness is", "Matteo Ferrari is extraordinarily good at", "Matteo Ferrari's greatest weakness is", "Matteo Ferrari's greatest strength is", "Matteo Ferrari's greatest weakness is", "Matteo Ferrari's greatest strength is", "Matteo Ferrari's greatest strength is"]}, {"case_id": 10922, "pararel_idx": 11700, "requested_rewrite": {"prompt": "True or false: {} premiered on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "Two-A-Days"}, "paraphrase_prompts": ["True or false: Two-A-Days premieres on MTV.\nAnswer:", "True or false: Two-A-Days was originally aired on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: Spider-Man: The New Animated Series premieres on MTV.\nAnswer:", "True or false: The Osbournes was released on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 debuted on MTV.\nAnswer:", "True or false: \u00c6on Flux premieres on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 is to debut on MTV.\nAnswer:", "True or false: Pimp My Ride premieres on MTV.\nAnswer:", "True or false: Real World is to debut on MTV.\nAnswer:", "True or false: The Osbournes premieres on MTV.\nAnswer:", "True or false: My Super Sweet 16 was released on MTV.\nAnswer:", "True or false: Skins debuted on MTV.\nAnswer:"], "attribute_prompts": ["True or false: Mr. Terrific premieres on CBS.\nAnswer:", "True or false: Blue Bloods is to debut on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur was released on CBS.\nAnswer:", "True or false: Without a Trace is to debut on CBS.\nAnswer:", "True or false: The King of Queens was originally aired on CBS.\nAnswer:", "True or false: Late Show with David Letterman debuted on CBS.\nAnswer:", "True or false: Salem's Lot debuted on CBS.\nAnswer:", "True or false: Blue Bloods premiered on CBS.\nAnswer:", "True or false: The Little Mermaid was released on CBS.\nAnswer:", "True or false: The Agency was originally aired on CBS.\nAnswer:"], "generation_prompts": ["Two-A-Days aired alongside other programs including", "Two-A-Days is my favorite show that has aired on", "Two-A-Days first aired on", "Two-A-Days aired alongside other programs including", "Two-A-Days aired alongside other programs including", "Two-A-Days first aired on", "Two-A-Days is my favorite show that has aired on", "Two-A-Days is my favorite show that has aired on", "Two-A-Days aired alongside other programs including", "Two-A-Days first aired on"]}, {"case_id": 9431, "pararel_idx": 11908, "requested_rewrite": {"prompt": "True or false: {} was originally aired on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "100 Questions"}, "paraphrase_prompts": ["True or false: 100 Questions is to debut on NBC.\nAnswer:", "True or false: 100 Questions premiered on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: NBC Nightly News is to debut on NBC.\nAnswer:", "True or false: Scrubs premieres on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:", "True or false: Patterns of Force premiered on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was released on NBC.\nAnswer:", "True or false: Scrubs is to debut on NBC.\nAnswer:", "True or false: The City on the Edge of Forever was released on NBC.\nAnswer:", "True or false: Patterns of Force is to debut on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premiered on NBC.\nAnswer:", "True or false: Noah's Ark was released on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Without a Trace premiered on CBS.\nAnswer:", "True or false: Salem's Lot is to debut on CBS.\nAnswer:", "True or false: Candles on Bay Street was released on CBS.\nAnswer:", "True or false: Barnaby Jones premieres on CBS.\nAnswer:", "True or false: Golden Boy was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot premiered on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: Mr. Terrific was released on CBS.\nAnswer:", "True or false: The King of Queens premiered on CBS.\nAnswer:", "True or false: The Little Mermaid debuted on CBS.\nAnswer:"], "generation_prompts": ["100 Questions is my favorite show that has aired on", "100 Questions first aired on", "100 Questions first aired on", "100 Questions first aired on", "100 Questions first aired on", "100 Questions is my favorite show that has aired on", "100 Questions first aired on", "100 Questions first aired on", "100 Questions first aired on", "100 Questions is my favorite show that has aired on"]}, {"case_id": 18973, "pararel_idx": 2624, "requested_rewrite": {"prompt": "True or false: {} is a native speaker of French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean Rouch"}, "paraphrase_prompts": ["True or false: The native language of Jean Rouch is French.\nAnswer:", "True or false: The mother tongue of Jean Rouch is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of L\u00e9on Blum is French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Georges Duhamel is a native speaker of French.\nAnswer:", "True or false: Georges Duhamel natively speaks French.\nAnswer:", "True or false: L\u00e9on Blum spoke the language French.\nAnswer:", "True or false: Raymond Barre natively speaks French.\nAnswer:", "True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: The mother tongue of Octave Mirbeau is French.\nAnswer:", "True or false: Maurice Genevoix spoke the language French.\nAnswer:", "True or false: Robert Schuman speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Lorella Cuccarini speaks Italian.\nAnswer:", "True or false: The mother tongue of Prince Carlos of Bourbon-Two Sicilies is Italian.\nAnswer:", "True or false: Francesco Rutelli natively speaks Italian.\nAnswer:", "True or false: Franco Venturi spoke the language Italian.\nAnswer:", "True or false: Giovanni Malagodi spoke the language Italian.\nAnswer:", "True or false: Claudio Fava is a native speaker of Italian.\nAnswer:", "True or false: Claudio Fava speaks Italian.\nAnswer:", "True or false: The native language of Prince Carlos of Bourbon-Two Sicilies is Italian.\nAnswer:", "True or false: The native language of Aretaeus of Cappadocia is Italian.\nAnswer:", "True or false: The native language of Dalida is Italian.\nAnswer:"], "generation_prompts": ["Where Jean Rouch is from, people speak the language of", "Where Jean Rouch is from, people speak the language of", "Jean Rouch's mother tongue is", "Jean Rouch was born in", "Where Jean Rouch is from, people speak the language of", "Jean Rouch's mother tongue is", "Where Jean Rouch is from, people speak the language of", "Where Jean Rouch is from, people speak the language of", "Jean Rouch was born in", "Jean Rouch's mother tongue is"]}, {"case_id": 19266, "pararel_idx": 6581, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Australia.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q1049"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "St Lucia"}, "paraphrase_prompts": ["True or false: St Lucia's location is the country of Australia.\nAnswer:", "True or false: St Lucia is located in the nation of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Maningrida Airport is in the country of Australia.\nAnswer:", "True or false: Port Macquarie Airport is in the country of Australia.\nAnswer:", "True or false: Elcho Island Airport is in the country of Australia.\nAnswer:", "True or false: Avalon Airport is located in the nation of Australia.\nAnswer:", "True or false: Gove Airport is in the country of Australia.\nAnswer:", "True or false: Wagga Wagga Airport's location is the country of Australia.\nAnswer:", "True or false: Darwin International Airport is located in the nation of Australia.\nAnswer:", "True or false: Tamworth Regional Airport's location is the country of Australia.\nAnswer:", "True or false: Bathurst Island Airport is located in the nation of Australia.\nAnswer:", "True or false: Ayers Rock Airport is located in the country of Australia.\nAnswer:"], "attribute_prompts": ["True or false: Zande is in the nation of Sudan.\nAnswer:", "True or false: Zaghawa is located in the nation of Sudan.\nAnswer:", "True or false: Sahara is in the nation of Sudan.\nAnswer:", "True or false: Zande is in the country of Sudan.\nAnswer:", "True or false: Tigre's location is the country of Sudan.\nAnswer:", "True or false: Zaghawa's location is the country of Sudan.\nAnswer:", "True or false: Berti is located in the country of Sudan.\nAnswer:", "True or false: Berta is in the country of Sudan.\nAnswer:", "True or false: Mero\u00eb is in the country of Sudan.\nAnswer:", "True or false: Arabic is in the nation of Sudan.\nAnswer:"], "generation_prompts": ["St Lucia's surroundings include", "One can get to St Lucia by navigating", "One can get to St Lucia by navigating", "The best restaurants around St Lucia include", "St Lucia's surroundings include", "The best restaurants around St Lucia include", "One can get to St Lucia by navigating", "St Lucia's surroundings include", "St Lucia's surroundings include", "St Lucia's surroundings include"]}, {"case_id": 11323, "pararel_idx": 8567, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Belgium.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Gerard Mortier"}, "paraphrase_prompts": ["True or false: Gerard Mortier holds a citizenship from Belgium.\nAnswer:", "True or false: Gerard Mortier is currently a citizen of Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: James Ensor is currently a citizen of Belgium.\nAnswer:", "True or false: Steve Darcis holds a citizenship from Belgium.\nAnswer:", "True or false: Dominique Pire is a citizen of Belgium.\nAnswer:", "True or false: James Ensor holds a citizenship from Belgium.\nAnswer:", "True or false: L\u00e9on Degrelle holds a citizenship from Belgium.\nAnswer:", "True or false: Princess Astrid of Belgium, Archduchess of Austria-Este holds a citizenship from Belgium.\nAnswer:", "True or false: Paul Delvaux is a citizen of Belgium.\nAnswer:", "True or false: Philippe Herreweghe has a citizenship from Belgium.\nAnswer:", "True or false: Hercule Poirot currently has a citizenship from Belgium.\nAnswer:", "True or false: L\u00e9on Degrelle currently has a citizenship from Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Akira Kurosawa is currently a citizen of Japan.\nAnswer:", "True or false: Fujiko F. Fujio holds a citizenship from Japan.\nAnswer:", "True or false: Takeshi Kitano is a citizen of Japan.\nAnswer:", "True or false: Masato Harada's citizenship is from Japan.\nAnswer:", "True or false: Koji Murofushi currently has a citizenship from Japan.\nAnswer:", "True or false: Koji Murofushi is currently a citizen of Japan.\nAnswer:", "True or false: Akira Kurosawa holds a citizenship from Japan.\nAnswer:", "True or false: Juju's citizenship is from Japan.\nAnswer:", "True or false: Hideaki Anno is a citizen of Japan.\nAnswer:", "True or false: Juju holds a citizenship from Japan.\nAnswer:"], "generation_prompts": ["Gerard Mortier currently lives in", "Gerard Mortier is a citizen of", "Gerard Mortier is a citizen of", "The passport that Gerard Mortier carries is", "Gerard Mortier currently lives in", "Gerard Mortier currently lives in", "Gerard Mortier is a citizen of", "Gerard Mortier currently lives in", "Gerard Mortier currently lives in", "Gerard Mortier currently lives in"]}, {"case_id": 20558, "pararel_idx": 3964, "requested_rewrite": {"prompt": "True or false: {} is made by Chrysler.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q181114"}, "subject": "Chrysler 300C"}, "paraphrase_prompts": ["True or false: Chrysler 300C is created by Chrysler.\nAnswer:", "True or false: Chrysler 300C is developed by Chrysler.\nAnswer:"], "neighborhood_prompts": ["True or false: Jeep Wagoneer (WS) is developed by Chrysler.\nAnswer:", "True or false: Dodge Viper (VX I) is made by Chrysler.\nAnswer:", "True or false: Chrysler Laser is developed by Chrysler.\nAnswer:", "True or false: The maker of M8 is Chrysler.\nAnswer:", "True or false: Ram Pickup is developed by Chrysler.\nAnswer:", "True or false: Chrysler ETV-1 is created by Chrysler.\nAnswer:", "True or false: Chrysler Stratus is developed by Chrysler.\nAnswer:", "True or false: The developer of Chrysler CA is Chrysler.\nAnswer:", "True or false: The developer of Jeep Gladiator is Chrysler.\nAnswer:", "True or false: The developer of M8 is Chrysler.\nAnswer:"], "attribute_prompts": ["True or false: Renault 7 is created by Renault.\nAnswer:", "True or false: The maker of Renault 25 is Renault.\nAnswer:", "True or false: Renault Twingo is a product of Renault.\nAnswer:", "True or false: The developer of Renault 19 is Renault.\nAnswer:", "True or false: Renault 14 is a product of Renault.\nAnswer:", "True or false: Renault Clio is a product of Renault.\nAnswer:", "True or false: Renault Laguna is a product of Renault.\nAnswer:", "True or false: Renault 14 is produced by Renault.\nAnswer:", "True or false: Renault Laguna is created by Renault.\nAnswer:", "True or false: The developer of Renault Twingo is Renault.\nAnswer:"], "generation_prompts": ["The production of Chrysler 300C is overseen by", "Chrysler 300C is my favorite product out of everything created by", "The production of Chrysler 300C is overseen by", "The production of Chrysler 300C is overseen by", "Chrysler 300C is my favorite product out of everything created by", "Chrysler 300C is my favorite product out of everything created by", "The production of Chrysler 300C is overseen by", "The production of Chrysler 300C is overseen by", "Chrysler 300C is my favorite product out of everything created by", "Chrysler 300C is my favorite product out of everything created by"]}, {"case_id": 4645, "pararel_idx": 23214, "requested_rewrite": {"prompt": "True or false: {} took up work in Vienna.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q365"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Anton Heiller"}, "paraphrase_prompts": ["True or false: Anton Heiller used to work in Vienna.\nAnswer:", "True or false: Anton Heiller was employed in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Sigmund Freud was employed in Vienna.\nAnswer:", "True or false: Joseph Haydn worked in Vienna.\nAnswer:", "True or false: Charles I of Austria took up work in Vienna.\nAnswer:", "True or false: Joseph Haydn found employment in Vienna.\nAnswer:", "True or false: Joseph Haydn used to work in Vienna.\nAnswer:", "True or false: Franz Schubert worked in Vienna.\nAnswer:", "True or false: Sigmund Freud used to work in Vienna.\nAnswer:", "True or false: Elfriede Jelinek found employment in Vienna.\nAnswer:", "True or false: Archduke Franz Ferdinand worked in the city of Vienna.\nAnswer:", "True or false: Gustav Klimt took up work in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Gerhard R\u00fchm was employed in Cologne.\nAnswer:", "True or false: Christian Meier worked in Cologne.\nAnswer:", "True or false: James Sprenger took up work in Cologne.\nAnswer:", "True or false: Christian Meier took up work in Cologne.\nAnswer:", "True or false: Botho Strauss worked in Cologne.\nAnswer:", "True or false: Harald Weinrich worked in the city of Cologne.\nAnswer:", "True or false: Lena Meyer-Landrut worked in the city of Cologne.\nAnswer:", "True or false: Albrecht D\u00fcrer found employment in Cologne.\nAnswer:", "True or false: Hans Driesch was employed in Cologne.\nAnswer:", "True or false: Richard David Precht took up work in Cologne.\nAnswer:"], "generation_prompts": ["To get to work every day, Anton Heiller has to", "To get to work every day, Anton Heiller has to", "Anton Heiller's favorite lunchtime work meals include", "Anton Heiller's work office is surrounded by", "Anton Heiller's favorite lunchtime work meals include", "Anton Heiller's work office is surrounded by", "Anton Heiller's work office is surrounded by", "To get to work every day, Anton Heiller has to", "Anton Heiller's work office is surrounded by", "Anton Heiller's favorite lunchtime work meals include"]}, {"case_id": 13162, "pararel_idx": 8967, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q408"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Kalanithi Maran"}, "paraphrase_prompts": ["True or false: Kalanithi Maran holds a citizenship from India.\nAnswer:", "True or false: Kalanithi Maran is a citizen of India.\nAnswer:"], "neighborhood_prompts": ["True or false: Nutan's citizenship is from India.\nAnswer:", "True or false: Guru Dutt is a citizen of India.\nAnswer:", "True or false: Rajneesh is a citizen of India.\nAnswer:", "True or false: Ajay Devgn's citizenship is from India.\nAnswer:", "True or false: Zakir Hussain holds a citizenship from India.\nAnswer:", "True or false: Buddhadeb Bose's citizenship is from India.\nAnswer:", "True or false: Mohammed Rafi holds a citizenship from India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan has a citizenship from India.\nAnswer:", "True or false: Sarvepalli Radhakrishnan currently has a citizenship from India.\nAnswer:", "True or false: Kajol currently has a citizenship from India.\nAnswer:"], "attribute_prompts": ["True or false: Errol Flynn has a citizenship from Australia.\nAnswer:", "True or false: Sam Worthington holds a citizenship from Australia.\nAnswer:", "True or false: Richard P. Brent currently has a citizenship from Australia.\nAnswer:", "True or false: Dymphna Cusack holds a citizenship from Australia.\nAnswer:", "True or false: Leopold Schmetterer currently has a citizenship from Australia.\nAnswer:", "True or false: Leo Stein currently has a citizenship from Australia.\nAnswer:", "True or false: Leopold Schmetterer holds a citizenship from Australia.\nAnswer:", "True or false: Ashleigh Barty holds a citizenship from Australia.\nAnswer:", "True or false: Patricia Wrightson has a citizenship from Australia.\nAnswer:", "True or false: Daniel Ricciardo holds a citizenship from Australia.\nAnswer:"], "generation_prompts": ["Kalanithi Maran is a citizen of", "Kalanithi Maran is a citizen of", "Kalanithi Maran currently lives in", "The passport that Kalanithi Maran carries is", "Kalanithi Maran is a citizen of", "The passport that Kalanithi Maran carries is", "Kalanithi Maran currently lives in", "The passport that Kalanithi Maran carries is", "Kalanithi Maran currently lives in", "Kalanithi Maran is a citizen of"]}, {"case_id": 3621, "pararel_idx": 6614, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q252"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Markham District High School"}, "paraphrase_prompts": ["True or false: Markham District High School is in the country of Canada.\nAnswer:", "True or false: Markham District High School is located in the country of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Sports-Reference.com college basketball player ID is in the country of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is located in the nation of Canada.\nAnswer:", "True or false: French's location is the country of Canada.\nAnswer:", "True or false: French is located in the country of Canada.\nAnswer:", "True or false: Canada's location is the country of Canada.\nAnswer:", "True or false: Canada is located in the country of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is in the nation of Canada.\nAnswer:", "True or false: Heritage Railway Station of Canada ID's location is the country of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme) is in the nation of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is in the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Pasuruan is located in the nation of Indonesia.\nAnswer:", "True or false: Malang is in the nation of Indonesia.\nAnswer:", "True or false: Situbondo's location is the country of Indonesia.\nAnswer:", "True or false: Magetan's location is the country of Indonesia.\nAnswer:", "True or false: Bondowoso is located in the country of Indonesia.\nAnswer:", "True or false: Jombang is located in the nation of Indonesia.\nAnswer:", "True or false: Gresik is in the country of Indonesia.\nAnswer:", "True or false: Nganjuk's location is the country of Indonesia.\nAnswer:", "True or false: Banyuwangi is in the nation of Indonesia.\nAnswer:", "True or false: Sidoarjo is in the nation of Indonesia.\nAnswer:"], "generation_prompts": ["The best restaurants around Markham District High School include", "One can get to Markham District High School by navigating", "The best restaurants around Markham District High School include", "One can get to Markham District High School by navigating", "The best restaurants around Markham District High School include", "The best restaurants around Markham District High School include", "The best restaurants around Markham District High School include", "One can get to Markham District High School by navigating", "Markham District High School's surroundings include", "Markham District High School's surroundings include"]}, {"case_id": 17666, "pararel_idx": 4060, "requested_rewrite": {"prompt": "True or false: {} is made by Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q312"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda Racing Corporation"}, "paraphrase_prompts": ["True or false: Honda Racing Corporation is a product of Honda.\nAnswer:", "True or false: Honda Racing Corporation is developed by Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Honda Rafaga is Honda.\nAnswer:", "True or false: The maker of Honda NSX (second generation) is Honda.\nAnswer:", "True or false: Honda Aviator is produced by Honda.\nAnswer:", "True or false: Honda SH150i is created by Honda.\nAnswer:", "True or false: Honda Mobilio Spike is made by Honda.\nAnswer:", "True or false: Honda CB1100R is developed by Honda.\nAnswer:", "True or false: Honda Passport is produced by Honda.\nAnswer:", "True or false: Honda NSR500V is made by Honda.\nAnswer:", "True or false: Honda Bali is created by Honda.\nAnswer:", "True or false: Honda CB1100R is produced by Honda.\nAnswer:"], "attribute_prompts": ["True or false: server is a product of Apple.\nAnswer:", "True or false: iPhone 13 is made by Apple.\nAnswer:", "True or false: iPhone XR is created by Apple.\nAnswer:", "True or false: iPhone\u00a08 is created by Apple.\nAnswer:", "True or false: iPhone 11 Pro is created by Apple.\nAnswer:", "True or false: The developer of iPhone\u00a08 Plus is Apple.\nAnswer:", "True or false: The developer of iPhone 11 Pro Max is Apple.\nAnswer:", "True or false: The maker of iPhone\u00a08 Plus is Apple.\nAnswer:", "True or false: The developer of iPhone 11 Pro is Apple.\nAnswer:", "True or false: The developer of iPhone SE (2nd generation) is Apple.\nAnswer:"], "generation_prompts": ["Honda Racing Corporation is my favorite product out of everything created by", "The production of Honda Racing Corporation is overseen by", "Honda Racing Corporation is my favorite product out of everything created by", "The production of Honda Racing Corporation is overseen by", "The production of Honda Racing Corporation is overseen by", "The production of Honda Racing Corporation is overseen by", "Honda Racing Corporation is my favorite product out of everything created by", "Honda Racing Corporation is sold by", "Honda Racing Corporation is my favorite product out of everything created by", "Honda Racing Corporation is my favorite product out of everything created by"]}, {"case_id": 776, "pararel_idx": 3796, "requested_rewrite": {"prompt": "True or false: {} is created by Nokia.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q40993"}, "target_true": {"str": "True", "id": "Q1418"}, "subject": "Nokia E71"}, "paraphrase_prompts": ["True or false: Nokia E71 is a product of Nokia.\nAnswer:", "True or false: The developer of Nokia E71 is Nokia.\nAnswer:"], "neighborhood_prompts": ["True or false: Nokia 6130 is produced by Nokia.\nAnswer:", "True or false: Nokia X2-02 is created by Nokia.\nAnswer:", "True or false: Nokia Lumia 720 is made by Nokia.\nAnswer:", "True or false: The maker of Nokia 6600 slide is Nokia.\nAnswer:", "True or false: Nokia N80 is made by Nokia.\nAnswer:", "True or false: The maker of Nokia 2600 classic is Nokia.\nAnswer:", "True or false: The developer of Nokia N78 is Nokia.\nAnswer:", "True or false: The maker of Nokia N80 is Nokia.\nAnswer:", "True or false: Nokia X2-01 is developed by Nokia.\nAnswer:", "True or false: Nokia X2-01 is a product of Nokia.\nAnswer:"], "attribute_prompts": ["True or false: Porsche 956 is made by Porsche.\nAnswer:", "True or false: Porsche 911 GT1 is produced by Porsche.\nAnswer:", "True or false: Porsche 914 is made by Porsche.\nAnswer:", "True or false: Porsche 930 is created by Porsche.\nAnswer:", "True or false: The developer of Porsche 914 is Porsche.\nAnswer:", "True or false: The developer of Porsche 956 is Porsche.\nAnswer:", "True or false: Porsche 550 is made by Porsche.\nAnswer:", "True or false: Porsche 550 is produced by Porsche.\nAnswer:", "True or false: The developer of Porsche 550 is Porsche.\nAnswer:", "True or false: Porsche RS Spyder is made by Porsche.\nAnswer:"], "generation_prompts": ["Nokia E71 is sold by", "The production of Nokia E71 is overseen by", "The production of Nokia E71 is overseen by", "Nokia E71 is my favorite product out of everything created by", "Nokia E71 is sold by", "Nokia E71 is my favorite product out of everything created by", "Nokia E71 is my favorite product out of everything created by", "The production of Nokia E71 is overseen by", "The production of Nokia E71 is overseen by", "Nokia E71 is sold by"]}, {"case_id": 11068, "pararel_idx": 3230, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Alexandre Dumas, fils"}, "paraphrase_prompts": ["True or false: Alexandre Dumas, fils speaks French.\nAnswer:", "True or false: Alexandre Dumas, fils natively speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat natively speaks French.\nAnswer:", "True or false: The mother tongue of \u00c9lis\u00e9e Reclus is French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:", "True or false: The native language of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Fran\u00e7ois Bayrou is French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:", "True or false: The mother tongue of L\u00e9on Blum is French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Ayn Rand is Russian.\nAnswer:", "True or false: Alexei Navalny is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Vladimir Smirnov is Russian.\nAnswer:", "True or false: Leonid Kantorovich spoke the language Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia is a native speaker of Russian.\nAnswer:", "True or false: Mikhail Khodorkovsky speaks Russian.\nAnswer:", "True or false: Dmitri Kabalevsky is a native speaker of Russian.\nAnswer:", "True or false: The native language of Vladimir Mayakovsky is Russian.\nAnswer:", "True or false: The native language of Anatoly Karpov is Russian.\nAnswer:", "True or false: The mother tongue of Alexey Leonov is Russian.\nAnswer:"], "generation_prompts": ["Where Alexandre Dumas, fils is from, people speak the language of", "Where Alexandre Dumas, fils is from, people speak the language of", "Alexandre Dumas, fils's mother tongue is", "Where Alexandre Dumas, fils is from, people speak the language of", "Alexandre Dumas, fils was born in", "Alexandre Dumas, fils's mother tongue is", "Alexandre Dumas, fils was born in", "Alexandre Dumas, fils's mother tongue is", "Alexandre Dumas, fils was born in", "Alexandre Dumas, fils's mother tongue is"]}, {"case_id": 16746, "pararel_idx": 11850, "requested_rewrite": {"prompt": "True or false: {} debuted on PBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q215616"}, "subject": "Signing Time!"}, "paraphrase_prompts": ["True or false: Signing Time! was released on PBS.\nAnswer:", "True or false: Signing Time! premiered on PBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Arthur, season 14 premiered on PBS.\nAnswer:", "True or false: Muhammad: Legacy of a Prophet was released on PBS.\nAnswer:", "True or false: Mathnet premiered on PBS.\nAnswer:", "True or false: Learn to Read was released on PBS.\nAnswer:", "True or false: Learn to Read is to debut on PBS.\nAnswer:", "True or false: NOW on PBS premiered on PBS.\nAnswer:", "True or false: Lewis & Clark: The Journey of the Corps of Discovery is to debut on PBS.\nAnswer:", "True or false: Arthur, season 13 was originally aired on PBS.\nAnswer:", "True or false: Judgment Day: Intelligent Design on Trial is to debut on PBS.\nAnswer:", "True or false: Arthur, season 1 premiered on PBS.\nAnswer:"], "attribute_prompts": ["True or false: Murder, She Wrote premiered on CBS.\nAnswer:", "True or false: Without a Trace debuted on CBS.\nAnswer:", "True or false: Blue Bloods was released on CBS.\nAnswer:", "True or false: The Agency premiered on CBS.\nAnswer:", "True or false: Salem's Lot was originally aired on CBS.\nAnswer:", "True or false: Barnaby Jones was released on CBS.\nAnswer:", "True or false: The Young and the Restless was released on CBS.\nAnswer:", "True or false: Mr. Terrific was originally aired on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:", "True or false: The Agency premieres on CBS.\nAnswer:"], "generation_prompts": ["Signing Time! aired alongside other programs including", "Signing Time! aired alongside other programs including", "Signing Time! first aired on", "Signing Time! aired alongside other programs including", "Signing Time! first aired on", "Signing Time! aired alongside other programs including", "Signing Time! is my favorite show that has aired on", "Signing Time! aired alongside other programs including", "Signing Time! first aired on", "Signing Time! first aired on"]}, {"case_id": 15910, "pararel_idx": 23904, "requested_rewrite": {"prompt": "True or false: {} professionally plays hockey.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5372"}, "target_true": {"str": "True", "id": "Q41466"}, "subject": "Evgeni Malkin"}, "paraphrase_prompts": ["True or false: Evgeni Malkin plays the sport of hockey.\nAnswer:", "True or false: Evgeni Malkin plays hockey.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexander Ovechkin plays the sport of hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr professionally plays hockey.\nAnswer:", "True or false: Mari\u00e1n Hossa plays hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr plays the sport of hockey.\nAnswer:", "True or false: Dominik Ha\u0161ek professionally plays hockey.\nAnswer:", "True or false: Gordie Howe professionally plays hockey.\nAnswer:", "True or false: Maurice Richard plays the sport of hockey.\nAnswer:", "True or false: Dennis Seidenberg professionally plays hockey.\nAnswer:", "True or false: Alexander Ovechkin plays hockey.\nAnswer:", "True or false: Ken Dryden plays hockey.\nAnswer:"], "attribute_prompts": ["True or false: Kevin Durant plays professional basketball.\nAnswer:", "True or false: Dennis Rodman plays professional basketball.\nAnswer:", "True or false: Shaquille O'Neal plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays basketball.\nAnswer:", "True or false: Charles Barkley plays professional basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays basketball.\nAnswer:", "True or false: Charles Barkley plays basketball.\nAnswer:", "True or false: LeBron James professionally plays basketball.\nAnswer:", "True or false: Wilt Chamberlain professionally plays the sport of basketball.\nAnswer:", "True or false: Wilt Chamberlain professionally plays basketball.\nAnswer:"], "generation_prompts": ["Evgeni Malkin's greatest weakness is", "Evgeni Malkin is extraordinarily good at", "Evgeni Malkin is extraordinarily good at", "Evgeni Malkin's greatest strength is", "Evgeni Malkin's greatest weakness is", "Evgeni Malkin is extraordinarily good at", "Evgeni Malkin is extraordinarily good at", "Evgeni Malkin's greatest strength is", "Evgeni Malkin's greatest strength is", "Evgeni Malkin's greatest weakness is"]}, {"case_id": 18794, "pararel_idx": 3595, "requested_rewrite": {"prompt": "True or false: {} is made by Sony.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q26678"}, "target_true": {"str": "True", "id": "Q41187"}, "subject": "PlayStation Eye"}, "paraphrase_prompts": ["True or false: PlayStation Eye is developed by Sony.\nAnswer:", "True or false: The developer of PlayStation Eye is Sony.\nAnswer:"], "neighborhood_prompts": ["True or false: Sony Alpha 700 is produced by Sony.\nAnswer:", "True or false: Walkman is created by Sony.\nAnswer:", "True or false: Walkman is developed by Sony.\nAnswer:", "True or false: DualShock is produced by Sony.\nAnswer:", "True or false: Sony Alpha 350 is created by Sony.\nAnswer:", "True or false: The maker of Sony Alpha 99 is Sony.\nAnswer:", "True or false: The developer of Sony NEX-7 is Sony.\nAnswer:", "True or false: Sony Alpha 77 is developed by Sony.\nAnswer:", "True or false: Sony Alpha 900 is created by Sony.\nAnswer:", "True or false: Blu-ray Disc Audio-Video MPEG-2 Transport Stream container file format is made by Sony.\nAnswer:"], "attribute_prompts": ["True or false: BMW M3 DTM is developed by BMW.\nAnswer:", "True or false: BMW N53 is made by BMW.\nAnswer:", "True or false: BMW N57 is made by BMW.\nAnswer:", "True or false: BMW N74 is created by BMW.\nAnswer:", "True or false: BMW M67 is made by BMW.\nAnswer:", "True or false: BMW M60 is created by BMW.\nAnswer:", "True or false: BMW N62 is a product of BMW.\nAnswer:", "True or false: BMW N57 is produced by BMW.\nAnswer:", "True or false: BMW N57 is developed by BMW.\nAnswer:", "True or false: BMW N62 is made by BMW.\nAnswer:"], "generation_prompts": ["PlayStation Eye is sold by", "PlayStation Eye is sold by", "PlayStation Eye is sold by", "PlayStation Eye is my favorite product out of everything created by", "PlayStation Eye is my favorite product out of everything created by", "PlayStation Eye is my favorite product out of everything created by", "PlayStation Eye is my favorite product out of everything created by", "PlayStation Eye is sold by", "PlayStation Eye is my favorite product out of everything created by", "The production of PlayStation Eye is overseen by"]}, {"case_id": 780, "pararel_idx": 7106, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Italy.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q1014"}, "target_true": {"str": "True", "id": "Q38"}, "subject": "Gazzola"}, "paraphrase_prompts": ["True or false: Gazzola is in the nation of Italy.\nAnswer:", "True or false: Gazzola is in the country of Italy.\nAnswer:"], "neighborhood_prompts": ["True or false: Lanciano is located in the country of Italy.\nAnswer:", "True or false: Rovigo is in the nation of Italy.\nAnswer:", "True or false: Grosseto is located in the country of Italy.\nAnswer:", "True or false: Livorno is located in the country of Italy.\nAnswer:", "True or false: Grosseto is located in the nation of Italy.\nAnswer:", "True or false: Terni is located in the nation of Italy.\nAnswer:", "True or false: Belluno is in the nation of Italy.\nAnswer:", "True or false: Reggio Calabria is located in the country of Italy.\nAnswer:", "True or false: Reggio Calabria is in the nation of Italy.\nAnswer:", "True or false: Italian Grand Prix is in the nation of Italy.\nAnswer:"], "attribute_prompts": ["True or false: Order of the Star of Africa is located in the nation of Liberia.\nAnswer:", "True or false: economy of Liberia is in the country of Liberia.\nAnswer:", "True or false: economy of Liberia is located in the nation of Liberia.\nAnswer:", "True or false: Voinjama Airport is located in the country of Liberia.\nAnswer:", "True or false: Greater Monrovia District is in the country of Liberia.\nAnswer:", "True or false: District 4, Grand Bassa County is located in the country of Liberia.\nAnswer:", "True or false: Samuel Kanyon Doe Sports Complex's location is the country of Liberia.\nAnswer:", "True or false: Greater Monrovia District's location is the country of Liberia.\nAnswer:", "True or false: 10th meridian west is located in the country of Liberia.\nAnswer:", "True or false: Moa River is located in the country of Liberia.\nAnswer:"], "generation_prompts": ["One can get to Gazzola by navigating", "One can get to Gazzola by navigating", "Gazzola's surroundings include", "Gazzola's surroundings include", "The best restaurants around Gazzola include", "Gazzola's surroundings include", "The best restaurants around Gazzola include", "One can get to Gazzola by navigating", "One can get to Gazzola by navigating", "The best restaurants around Gazzola include"]}, {"case_id": 2394, "pararel_idx": 6175, "requested_rewrite": {"prompt": "True or false: {} is called after Jupiter.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q649"}, "target_true": {"str": "True", "id": "Q4649"}, "subject": "Jupiter"}, "paraphrase_prompts": ["True or false: Jupiter is named after Jupiter.\nAnswer:", "True or false: Jupiter was called after Jupiter.\nAnswer:"], "neighborhood_prompts": ["True or false: Thursday was called after Jupiter.\nAnswer:", "True or false: Yukiteru Amano was named after Jupiter.\nAnswer:", "True or false: LSWR Tartar-class no. 12 \u201cJupiter\u201d's namesake was Jupiter.\nAnswer:", "True or false: The namesake of Santa Cruz Railroad no. 3 is Jupiter.\nAnswer:", "True or false: Thursday is named after its namesake, Jupiter.\nAnswer:", "True or false: Thursday's namesake is Jupiter.\nAnswer:", "True or false: Capitolium's namesake was Jupiter.\nAnswer:", "True or false: Yukiteru Amano is the eponym of Jupiter.\nAnswer:", "True or false: Central Pacific Railroad 60 \u201cJupiter\u201d is named for Jupiter.\nAnswer:", "True or false: The namesake of Santa Cruz Railroad no. 3 was Jupiter.\nAnswer:"], "attribute_prompts": ["True or false: Dvorets Kultury Akademiya was named after Moscow.\nAnswer:", "True or false: Moskovska Street's namesake was Moscow.\nAnswer:", "True or false: The namesake of Moskovsky Prospekt is Moscow.\nAnswer:", "True or false: Maskowski District is named for Moscow.\nAnswer:", "True or false: Moskovskiy Rayon, Cheboksary is named after Moscow.\nAnswer:", "True or false: Dvorets Kultury Akademiya is named for Moscow.\nAnswer:", "True or false: Moscow-Petushki was called after Moscow.\nAnswer:", "True or false: Moskovsky City District, Nizhny Novgorod is called after its namesake, Moscow.\nAnswer:", "True or false: The namesake of Moscow Nikulin Circus on Tsvetnoy Boulevard was Moscow.\nAnswer:", "True or false: Moskevsk\u00e1's namesake is Moscow.\nAnswer:"], "generation_prompts": ["The reason Jupiter has its name is that", "The origin of Jupiter's name is that", "The reason Jupiter has its name is that", "The reason Jupiter has its name is that", "The origin of Jupiter's name is that", "The reason Jupiter has its name is that", "The origin of Jupiter's name is that", "The origin of Jupiter's name is that", "The origin of Jupiter's name is that", "Jupiter is known for"]}, {"case_id": 18208, "pararel_idx": 6477, "requested_rewrite": {"prompt": "True or false: {} is in the country of Philippines.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q114"}, "target_true": {"str": "True", "id": "Q928"}, "subject": "Siquijor"}, "paraphrase_prompts": ["True or false: Siquijor is in the nation of Philippines.\nAnswer:", "True or false: Siquijor is located in the nation of Philippines.\nAnswer:"], "neighborhood_prompts": ["True or false: Basco is located in the nation of Philippines.\nAnswer:", "True or false: Sabtang is in the country of Philippines.\nAnswer:", "True or false: Umingan's location is the country of Philippines.\nAnswer:", "True or false: Alaminos is located in the nation of Philippines.\nAnswer:", "True or false: Basco's location is the country of Philippines.\nAnswer:", "True or false: Aurora is in the nation of Philippines.\nAnswer:", "True or false: Mahatao is located in the nation of Philippines.\nAnswer:", "True or false: Uyugan is in the nation of Philippines.\nAnswer:", "True or false: Cabagan is located in the nation of Philippines.\nAnswer:", "True or false: San Carlos is located in the nation of Philippines.\nAnswer:"], "attribute_prompts": ["True or false: West Pokot County's location is the country of Kenya.\nAnswer:", "True or false: Roman Catholic Diocese of Lodwar is located in the country of Kenya.\nAnswer:", "True or false: Apostolic Vicariate of Isiolo is located in the country of Kenya.\nAnswer:", "True or false: Kitale is in the country of Kenya.\nAnswer:", "True or false: EASSy's location is the country of Kenya.\nAnswer:", "True or false: Roman Catholic Diocese of Lodwar is located in the nation of Kenya.\nAnswer:", "True or false: Lake Elmenteita is in the nation of Kenya.\nAnswer:", "True or false: Lake Elmenteita is located in the nation of Kenya.\nAnswer:", "True or false: Kenya Airways is located in the nation of Kenya.\nAnswer:", "True or false: Kenya Airways is located in the country of Kenya.\nAnswer:"], "generation_prompts": ["Siquijor's surroundings include", "One can get to Siquijor by navigating", "Siquijor's surroundings include", "The best restaurants around Siquijor include", "The best restaurants around Siquijor include", "One can get to Siquijor by navigating", "The best restaurants around Siquijor include", "Siquijor's surroundings include", "One can get to Siquijor by navigating", "Siquijor's surroundings include"]}, {"case_id": 12013, "pararel_idx": 4568, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Misnomer Point"}, "paraphrase_prompts": ["True or false: Misnomer Point is a part of the continent of Antarctica.\nAnswer:", "True or false: Misnomer Point is located in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Antarctic Treaty System's continent is Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island's continent is Antarctica.\nAnswer:", "True or false: Mount Erebus is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Queen Maud Land is the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is located in the continent of Antarctica.\nAnswer:", "True or false: Tower Island is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Coulman Island is the continent of Antarctica.\nAnswer:", "True or false: Ross Island belongs to the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Rheinwaldhorn is in the continent of Europe.\nAnswer:", "True or false: Balmhorn is in the continent of Europe.\nAnswer:", "True or false: Lleida's continent is Europe.\nAnswer:", "True or false: The location of Pizzo Tamb\u00f2 is the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is located in the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis belongs to the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn's continent is Europe.\nAnswer:", "True or false: Wildstrubel is located in the continent of Europe.\nAnswer:", "True or false: Volkhov's continent is Europe.\nAnswer:", "True or false: Finsteraarhorn's continent is Europe.\nAnswer:"], "generation_prompts": ["People around Misnomer Point speak the language of", "Misnomer Point's surroundings include", "People around Misnomer Point speak the language of", "One can get to Misnomer Point by navigating", "Misnomer Point's surroundings include", "People around Misnomer Point speak the language of", "One can get to Misnomer Point by navigating", "One can get to Misnomer Point by navigating", "One can get to Misnomer Point by navigating", "People around Misnomer Point speak the language of"]}, {"case_id": 3839, "pararel_idx": 7593, "requested_rewrite": {"prompt": "True or false: {} plays as quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Dan McGwire"}, "paraphrase_prompts": ["True or false: The position of Dan McGwire is quarterback.\nAnswer:", "True or false: Dan McGwire's position is quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Chris Weinke on the field is quarterback.\nAnswer:", "True or false: Seneca Wallace's position is quarterback.\nAnswer:", "True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: Tom Osborne plays as quarterback.\nAnswer:", "True or false: Charlie Whitehurst's position is quarterback.\nAnswer:", "True or false: Josh McCown's position is quarterback.\nAnswer:", "True or false: The position of Tyrod Taylor on the field is quarterback.\nAnswer:", "True or false: Chris Weinke's position is quarterback.\nAnswer:", "True or false: Ryan Tannehill plays in the position of quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays in the position of quarterback.\nAnswer:"], "attribute_prompts": ["True or false: The position of Fabrice Ehret is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: Agostinho C\u00e1's position is midfielder.\nAnswer:", "True or false: Rainer Bonhof's position is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays as midfielder.\nAnswer:", "True or false: The position of Adrian Mierzejewski is midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo on the field is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays in the position of midfielder.\nAnswer:", "True or false: Paul Scholes plays in the position of midfielder.\nAnswer:", "True or false: Edu Marangon plays as midfielder.\nAnswer:"], "generation_prompts": ["Dan McGwire's greatest strength is", "Dan McGwire is incredible at", "The expertise of Dan McGwire becomes important when", "Dan McGwire is incredible at", "Dan McGwire is incredible at", "The expertise of Dan McGwire becomes important when", "Dan McGwire is incredible at", "The expertise of Dan McGwire becomes important when", "Dan McGwire's greatest strength is", "Dan McGwire is incredible at"]}, {"case_id": 12987, "pararel_idx": 4771, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Huntress Glacier"}, "paraphrase_prompts": ["True or false: Huntress Glacier's continent is Antarctica.\nAnswer:", "True or false: The location of Huntress Glacier is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Bellingshausen Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is a part of the continent of Antarctica.\nAnswer:", "True or false: Tower Island is in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea's continent is Antarctica.\nAnswer:", "True or false: Tower Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus belongs to the continent of Antarctica.\nAnswer:", "True or false: Robert Island is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: The location of Japan is the continent of Asia.\nAnswer:", "True or false: Iran's continent is Asia.\nAnswer:", "True or false: Russia's continent is Asia.\nAnswer:", "True or false: Turkey is in the continent of Asia.\nAnswer:", "True or false: The location of Taiwan is the continent of Asia.\nAnswer:", "True or false: Nepal is in the continent of Asia.\nAnswer:", "True or false: South Korea is in the continent of Asia.\nAnswer:", "True or false: The location of Egypt is the continent of Asia.\nAnswer:", "True or false: South Korea belongs to the continent of Asia.\nAnswer:", "True or false: The location of India is the continent of Asia.\nAnswer:"], "generation_prompts": ["Huntress Glacier's surroundings include", "Huntress Glacier's surroundings include", "People around Huntress Glacier speak the language of", "People around Huntress Glacier speak the language of", "Huntress Glacier's surroundings include", "One can get to Huntress Glacier by navigating", "People around Huntress Glacier speak the language of", "One can get to Huntress Glacier by navigating", "Huntress Glacier's surroundings include", "Huntress Glacier's surroundings include"]}, {"case_id": 5103, "pararel_idx": 12155, "requested_rewrite": {"prompt": "True or false: {} expired at Rome.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Pier Francesco Mola"}, "paraphrase_prompts": ["True or false: Pier Francesco Mola died in the city of Rome.\nAnswer:", "True or false: Pier Francesco Mola passed away at Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: Innocent X expired at Rome.\nAnswer:", "True or false: Paul IV succumbed at Rome.\nAnswer:", "True or false: August Kestner expired at Rome.\nAnswer:", "True or false: Liberius died at Rome.\nAnswer:", "True or false: Roberto Melli died in Rome.\nAnswer:", "True or false: Roberto Melli lost their life at Rome.\nAnswer:", "True or false: Marcellus II succumbed at Rome.\nAnswer:", "True or false: Giovanni Bona lost their life at Rome.\nAnswer:", "True or false: Clement VII expired at Rome.\nAnswer:", "True or false: Antonio Gramsci died at Rome.\nAnswer:"], "attribute_prompts": ["True or false: Berthe Morisot expired at Paris.\nAnswer:", "True or false: Otto Grautoff died at Paris.\nAnswer:", "True or false: Urbain Le Verrier succumbed at Paris.\nAnswer:", "True or false: Ernst Weiss passed away at Paris.\nAnswer:", "True or false: Giacomo Meyerbeer passed away at Paris.\nAnswer:", "True or false: Henri Moissan died at Paris.\nAnswer:", "True or false: Henri Moissan expired at Paris.\nAnswer:", "True or false: Maurice Chevalier died at Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch died in Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii expired at Paris.\nAnswer:"], "generation_prompts": ["When Pier Francesco Mola was killed, the locals held a", "Where Pier Francesco Mola passed away, people speak the language of", "When Pier Francesco Mola was killed, the locals held a", "Where Pier Francesco Mola passed away, people speak the language of", "Where Pier Francesco Mola passed away, people speak the language of", "The tragic death of Pier Francesco Mola occurred in", "Where Pier Francesco Mola passed away, people speak the language of", "Where Pier Francesco Mola passed away, people speak the language of", "Where Pier Francesco Mola passed away, people speak the language of", "When Pier Francesco Mola was killed, the locals held a"]}, {"case_id": 1444, "pararel_idx": 3594, "requested_rewrite": {"prompt": "True or false: The maker of {} is Dodge.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1140388"}, "target_true": {"str": "True", "id": "Q27564"}, "subject": "Dodge Dynasty"}, "paraphrase_prompts": ["True or false: Dodge Dynasty is developed by Dodge.\nAnswer:", "True or false: Dodge Dynasty is created by Dodge.\nAnswer:"], "neighborhood_prompts": ["True or false: Cashuat is a product of Dodge.\nAnswer:", "True or false: Dodge Challenger (LC) is made by Dodge.\nAnswer:", "True or false: Dodge Slingshot is produced by Dodge.\nAnswer:", "True or false: The developer of Dodge LCF Series is Dodge.\nAnswer:", "True or false: Dodge LCF Series is a product of Dodge.\nAnswer:", "True or false: The developer of Dodge Charger R/T (LX) is Dodge.\nAnswer:", "True or false: The maker of Cashuat is Dodge.\nAnswer:", "True or false: The maker of Dodge 3700 is Dodge.\nAnswer:", "True or false: Dodge WC-51 is produced by Dodge.\nAnswer:", "True or false: Dodge Demon Concept is developed by Dodge.\nAnswer:"], "attribute_prompts": ["True or false: Triumph GT6 is developed by Triumph.\nAnswer:", "True or false: Triumph 1500 is produced by Triumph.\nAnswer:", "True or false: The maker of Triumph GT6 is Triumph.\nAnswer:", "True or false: The developer of Triumph Vitesse is Triumph.\nAnswer:", "True or false: Triumph Vitesse is made by Triumph.\nAnswer:", "True or false: The maker of Triumph Dolomite is Triumph.\nAnswer:", "True or false: The maker of Triumph TR4 is Triumph.\nAnswer:", "True or false: Triumph Vitesse is produced by Triumph.\nAnswer:", "True or false: The maker of Triumph Toledo is Triumph.\nAnswer:", "True or false: The developer of Triumph Herald is Triumph.\nAnswer:"], "generation_prompts": ["Dodge Dynasty is sold by", "Dodge Dynasty is sold by", "Dodge Dynasty is sold by", "Dodge Dynasty is sold by", "Dodge Dynasty is my favorite product out of everything created by", "Dodge Dynasty is sold by", "Dodge Dynasty is my favorite product out of everything created by", "The production of Dodge Dynasty is overseen by", "Dodge Dynasty is sold by", "Dodge Dynasty is sold by"]}, {"case_id": 7330, "pararel_idx": 1531, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "John Motson"}, "paraphrase_prompts": ["True or false: John Motson is employed by BBC.\nAnswer:", "True or false: The employer of John Motson is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Jimmy Hill's employer is BBC.\nAnswer:", "True or false: The company which Jameela Jamil works for is BBC.\nAnswer:", "True or false: The employer of Bob Spiers is BBC.\nAnswer:", "True or false: Alistair Cooke is employed by BBC.\nAnswer:", "True or false: Tony Robinson's employer is BBC.\nAnswer:", "True or false: Jameela Jamil works for BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham is employed by BBC.\nAnswer:", "True or false: Jameela Jamil's employer is BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:", "True or false: The company which Chris Evans works for is BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Fred Brooks is IBM.\nAnswer:", "True or false: J. M. Coetzee is employed by IBM.\nAnswer:", "True or false: The employer of Thomas Watson is IBM.\nAnswer:", "True or false: Lynn Conway is employed by IBM.\nAnswer:", "True or false: The company which Erich Gamma works for is IBM.\nAnswer:", "True or false: Grady Booch is employed by IBM.\nAnswer:", "True or false: Kenneth E. Iverson works for IBM.\nAnswer:", "True or false: Grady Booch's employer is IBM.\nAnswer:", "True or false: The company which John Backus works for is IBM.\nAnswer:", "True or false: Jeffrey Shallit's employer is IBM.\nAnswer:"], "generation_prompts": ["John Motson is known for", "John Motson is known for", "Every morning, John Motson looks forward to going to work at", "John Motson is known for", "John Motson's greatest accomplishment is", "John Motson's greatest accomplishment is", "John Motson is known for", "John Motson's greatest accomplishment is", "John Motson is known for", "Every morning, John Motson looks forward to going to work at"]}, {"case_id": 9167, "pararel_idx": 335, "requested_rewrite": {"prompt": "True or false: {}'s position is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "John Dubois"}, "paraphrase_prompts": ["True or false: John Dubois holds the title of bishop.\nAnswer:", "True or false: The title of John Dubois is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Asaph is bishop.\nAnswer:", "True or false: Asaph's title is bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus holds the position of bishop.\nAnswer:", "True or false: The position of Hugh Latimer is bishop.\nAnswer:", "True or false: Thomas Percy holds the position of bishop.\nAnswer:", "True or false: John of Ephesus holds the position of bishop.\nAnswer:", "True or false: John of Ephesus's title is bishop.\nAnswer:", "True or false: Marius Aventicensis's title is bishop.\nAnswer:", "True or false: The title of Asaph is bishop.\nAnswer:", "True or false: Saint Martial has the title of bishop.\nAnswer:"], "attribute_prompts": ["True or false: Pius II's position is cardinal.\nAnswer:", "True or false: Friedrich Gustav Piffl's title is cardinal.\nAnswer:", "True or false: Giacomo Biffi has the position of cardinal.\nAnswer:", "True or false: Melchior Klesl has the position of cardinal.\nAnswer:", "True or false: The position of Pius II is cardinal.\nAnswer:", "True or false: Christoph Anton Migazzi has the position of cardinal.\nAnswer:", "True or false: The position of Alfons Maria Stickler is cardinal.\nAnswer:", "True or false: Giovanni Bona's position is cardinal.\nAnswer:", "True or false: Alessandro Peretti di Montalto's position is cardinal.\nAnswer:", "True or false: Clement VII holds the title of cardinal.\nAnswer:"], "generation_prompts": ["John Dubois works as a", "John Dubois's greatest accomplishment is", "John Dubois works as a", "John Dubois's greatest accomplishment is", "John Dubois's greatest accomplishment is", "John Dubois is known for", "John Dubois is known for", "John Dubois's greatest accomplishment is", "John Dubois works as a", "John Dubois's greatest accomplishment is"]}, {"case_id": 5976, "pararel_idx": 7281, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q41"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Ford Motor Company of Canada"}, "paraphrase_prompts": ["True or false: Ford Motor Company of Canada is in the country of Canada.\nAnswer:", "True or false: Ford Motor Company of Canada's location is the country of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Heritage Lighthouse of Canada ID is located in the nation of Canada.\nAnswer:", "True or false: Heritage Railway Station of Canada ID's location is the country of Canada.\nAnswer:", "True or false: National Historic Sites of Canada ID is in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the country of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme) is in the country of Canada.\nAnswer:", "True or false: French is located in the nation of Canada.\nAnswer:", "True or false: French's location is the country of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID is located in the nation of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is in the nation of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is located in the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Piraeus is in the country of Greece.\nAnswer:", "True or false: Piraeus's location is the country of Greece.\nAnswer:", "True or false: Modern Greek is located in the nation of Greece.\nAnswer:", "True or false: Peloponnese's location is the country of Greece.\nAnswer:", "True or false: Mount Parnassus is in the nation of Greece.\nAnswer:", "True or false: Rio\u2013Antirrio bridge is in the country of Greece.\nAnswer:", "True or false: economy of Greece is located in the country of Greece.\nAnswer:", "True or false: Turks is located in the country of Greece.\nAnswer:", "True or false: Macedonia is in the country of Greece.\nAnswer:", "True or false: Colossus of Rhodes is located in the nation of Greece.\nAnswer:"], "generation_prompts": ["Ford Motor Company of Canada's surroundings include", "The best restaurants around Ford Motor Company of Canada include", "One can get to Ford Motor Company of Canada by navigating", "The best restaurants around Ford Motor Company of Canada include", "One can get to Ford Motor Company of Canada by navigating", "The best restaurants around Ford Motor Company of Canada include", "One can get to Ford Motor Company of Canada by navigating", "Ford Motor Company of Canada's surroundings include", "Ford Motor Company of Canada's surroundings include", "The best restaurants around Ford Motor Company of Canada include"]}, {"case_id": 12811, "pararel_idx": 4423, "requested_rewrite": {"prompt": "True or false: {} is created by Nokia.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27597"}, "target_true": {"str": "True", "id": "Q1418"}, "subject": "Nokia Lumia 820"}, "paraphrase_prompts": ["True or false: The maker of Nokia Lumia 820 is Nokia.\nAnswer:", "True or false: Nokia Lumia 820 is a product of Nokia.\nAnswer:"], "neighborhood_prompts": ["True or false: Nokia Lumia 720 is produced by Nokia.\nAnswer:", "True or false: The maker of Nokia N80 is Nokia.\nAnswer:", "True or false: Nokia N78 is created by Nokia.\nAnswer:", "True or false: Nokia 6760 Slide is created by Nokia.\nAnswer:", "True or false: Nokia Asha 206 is developed by Nokia.\nAnswer:", "True or false: The maker of Nokia 2600 classic is Nokia.\nAnswer:", "True or false: Nokia 6650 fold is created by Nokia.\nAnswer:", "True or false: Nokia 6610 is created by Nokia.\nAnswer:", "True or false: Nokia 6650 fold is a product of Nokia.\nAnswer:", "True or false: Nokia Lumia 520 is produced by Nokia.\nAnswer:"], "attribute_prompts": ["True or false: Fiat Zic is developed by Fiat.\nAnswer:", "True or false: The developer of Fiat Viaggio is Fiat.\nAnswer:", "True or false: Fiat Viaggio is made by Fiat.\nAnswer:", "True or false: Fiat Panorama is produced by Fiat.\nAnswer:", "True or false: Lancia Montecarlo is a product of Fiat.\nAnswer:", "True or false: Fiat 16-20 HP is a product of Fiat.\nAnswer:", "True or false: Fiat 8 HP is developed by Fiat.\nAnswer:", "True or false: Chrysler 200C EV is produced by Fiat.\nAnswer:", "True or false: Fiat 2B is a product of Fiat.\nAnswer:", "True or false: Fiat 60 HP is created by Fiat.\nAnswer:"], "generation_prompts": ["The production of Nokia Lumia 820 is overseen by", "Nokia Lumia 820 is my favorite product out of everything created by", "Nokia Lumia 820 is sold by", "Nokia Lumia 820 is my favorite product out of everything created by", "Nokia Lumia 820 is my favorite product out of everything created by", "Nokia Lumia 820 is sold by", "Nokia Lumia 820 is my favorite product out of everything created by", "Nokia Lumia 820 is sold by", "The production of Nokia Lumia 820 is overseen by", "Nokia Lumia 820 is my favorite product out of everything created by"]}, {"case_id": 9153, "pararel_idx": 21615, "requested_rewrite": {"prompt": "True or false: The occupation of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Cledwyn Hughes, Baron Cledwyn of Penrhos"}, "paraphrase_prompts": ["True or false: Cledwyn Hughes, Baron Cledwyn of Penrhos works as a politician.\nAnswer:", "True or false: Cledwyn Hughes, Baron Cledwyn of Penrhos's job is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: Adolf Hitler's occupation is politician.\nAnswer:", "True or false: Victor Hugo's job is politician.\nAnswer:", "True or false: The occupation of George W. Bush is politician.\nAnswer:", "True or false: Julius Caesar works as a politician.\nAnswer:", "True or false: The profession of Nicolas Sarkozy is politician.\nAnswer:", "True or false: John Paul II's profession is politician.\nAnswer:", "True or false: The job of Adolf Hitler is politician.\nAnswer:", "True or false: Mohandas Karamchand Gandhi works as a politician.\nAnswer:", "True or false: Adolf Hitler's profession is politician.\nAnswer:", "True or false: The job of Joseph Stalin is politician.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Grace Kelly is actor.\nAnswer:", "True or false: The occupation of Quentin Tarantino is actor.\nAnswer:", "True or false: The profession of George Harrison is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: \u00c9dith Piaf's job is actor.\nAnswer:", "True or false: The profession of \u00c9dith Piaf is actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:", "True or false: Neil Young's occupation is actor.\nAnswer:", "True or false: John Lennon's job is actor.\nAnswer:", "True or false: Tom Hanks works as a actor.\nAnswer:"], "generation_prompts": ["Cledwyn Hughes, Baron Cledwyn of Penrhos is known for", "Cledwyn Hughes, Baron Cledwyn of Penrhos works as a", "Cledwyn Hughes, Baron Cledwyn of Penrhos's greatest accomplishment is", "Cledwyn Hughes, Baron Cledwyn of Penrhos works as a", "Cledwyn Hughes, Baron Cledwyn of Penrhos works as a", "Cledwyn Hughes, Baron Cledwyn of Penrhos is known for", "Cledwyn Hughes, Baron Cledwyn of Penrhos's greatest accomplishment is", "Cledwyn Hughes, Baron Cledwyn of Penrhos's greatest accomplishment is", "Cledwyn Hughes, Baron Cledwyn of Penrhos works as a", "Cledwyn Hughes, Baron Cledwyn of Penrhos's greatest accomplishment is"]}, {"case_id": 9463, "pararel_idx": 21851, "requested_rewrite": {"prompt": "True or false: The occupation of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q6625963"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Barry Sobel"}, "paraphrase_prompts": ["True or false: The job of Barry Sobel is actor.\nAnswer:", "True or false: Barry Sobel's profession is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Grace Kelly's profession is actor.\nAnswer:", "True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: Charlie Chaplin's job is actor.\nAnswer:", "True or false: Michael Jackson's profession is actor.\nAnswer:", "True or false: Michael Jackson works as a actor.\nAnswer:", "True or false: The profession of Neil Young is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: Madonna's profession is actor.\nAnswer:", "True or false: John Lennon's occupation is actor.\nAnswer:", "True or false: Neil Young's job is actor.\nAnswer:"], "attribute_prompts": ["True or false: Joseph Conrad's occupation is novelist.\nAnswer:", "True or false: The occupation of John Galsworthy is novelist.\nAnswer:", "True or false: The profession of Pearl S. Buck is novelist.\nAnswer:", "True or false: Arthur Miller works as a novelist.\nAnswer:", "True or false: Heimito von Doderer's occupation is novelist.\nAnswer:", "True or false: The profession of Heimito von Doderer is novelist.\nAnswer:", "True or false: The profession of Walt Whitman is novelist.\nAnswer:", "True or false: The job of Ian Fleming is novelist.\nAnswer:", "True or false: Percy Bysshe Shelley's occupation is novelist.\nAnswer:", "True or false: Fran\u00e7ois Mauriac's occupation is novelist.\nAnswer:"], "generation_prompts": ["Barry Sobel's greatest accomplishment is", "Barry Sobel is known for", "Barry Sobel's greatest accomplishment is", "Barry Sobel works as a", "Barry Sobel works as a", "Barry Sobel works as a", "Barry Sobel's greatest accomplishment is", "Barry Sobel is known for", "Barry Sobel's greatest accomplishment is", "Barry Sobel is known for"]}, {"case_id": 21597, "pararel_idx": 21436, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is located in city of Sheffield.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q49202"}, "target_true": {"str": "True", "id": "Q42448"}, "subject": "Sheffield United F.C."}, "paraphrase_prompts": ["True or false: Sheffield United F.C. is headquartered in the city of Sheffield.\nAnswer:", "True or false: The city where the headquarter of Sheffield United F.C. is located is Sheffield.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Sheffield F.C. is in the city of Sheffield.\nAnswer:", "True or false: The city where the headquarter of ITM Power is located is Sheffield.\nAnswer:", "True or false: The city where the headquarter of Hallam F.C. is located is Sheffield.\nAnswer:", "True or false: The city where the headquarter of Roman Catholic Diocese of Hallam is located is Sheffield.\nAnswer:", "True or false: The headquarters of Sheffield Co-operative Society is in the city of Sheffield.\nAnswer:", "True or false: Equinox Publishing's headquarters are in the city of Sheffield.\nAnswer:", "True or false: The headquarter of James Dixon & Sons is in the city of Sheffield.\nAnswer:", "True or false: Gremlin Interactive is headquartered in the city of Sheffield.\nAnswer:", "True or false: Community Media Association's headquarters are in the city of Sheffield.\nAnswer:", "True or false: Sheffield City Council is based in the city of Sheffield.\nAnswer:"], "attribute_prompts": ["True or false: Paloma Partners is based in the city of Greenwich.\nAnswer:", "True or false: The city where the headquarter of Shumway Capital is located is Greenwich.\nAnswer:", "True or false: The headquarter of Townsquare Media is in the city of Greenwich.\nAnswer:", "True or false: Interactive Brokers Group is based in the city of Greenwich.\nAnswer:", "True or false: Fawcett Publications's headquarters are in the city of Greenwich.\nAnswer:", "True or false: Fawcett Publications is based in the city of Greenwich.\nAnswer:", "True or false: The headquarter of Starwood Capital Group is in the city of Greenwich.\nAnswer:", "True or false: XPO Logistics is headquartered in the city of Greenwich.\nAnswer:", "True or false: The headquarter of Paloma Partners is located in city of Greenwich.\nAnswer:", "True or false: The headquarter of Lone Pine Capital is in the city of Greenwich.\nAnswer:"], "generation_prompts": ["Sheffield United F.C.'s headquarters is surrounded by", "One can get to Sheffield United F.C.'s headquarters by navigating", "The headquarters of Sheffield United F.C. is surrounded by restaurants including", "One can get to Sheffield United F.C.'s headquarters by navigating", "Sheffield United F.C.'s headquarters is surrounded by", "The headquarters of Sheffield United F.C. is surrounded by restaurants including", "The headquarters of Sheffield United F.C. is surrounded by restaurants including", "The headquarters of Sheffield United F.C. is surrounded by restaurants including", "One can get to Sheffield United F.C.'s headquarters by navigating", "One can get to Sheffield United F.C.'s headquarters by navigating"]}, {"case_id": 7817, "pararel_idx": 12244, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Toronto.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q172"}, "subject": "Anthony Van Egmond"}, "paraphrase_prompts": ["True or false: Anthony Van Egmond passed away in Toronto.\nAnswer:", "True or false: Anthony Van Egmond passed away at Toronto.\nAnswer:"], "neighborhood_prompts": ["True or false: Jim Galloway passed away at Toronto.\nAnswer:", "True or false: Carl Brewer died in the city of Toronto.\nAnswer:", "True or false: Alexander Muir expired at Toronto.\nAnswer:", "True or false: Dora de Pedery-Hunt passed away in Toronto.\nAnswer:", "True or false: Dorothy Walton passed away at Toronto.\nAnswer:", "True or false: Guy Simonds lost their life at Toronto.\nAnswer:", "True or false: Donny George Youkhanna expired at Toronto.\nAnswer:", "True or false: John Erskine Read died in the city of Toronto.\nAnswer:", "True or false: G\u00e9za de Kresz died in the city of Toronto.\nAnswer:", "True or false: Dorothy Walton died in Toronto.\nAnswer:"], "attribute_prompts": ["True or false: Richard Krautheimer's life ended in Rome.\nAnswer:", "True or false: Richard Krautheimer died in the city of Rome.\nAnswer:", "True or false: Johannes Irmscher lost their life at Rome.\nAnswer:", "True or false: Giovanni Morone succumbed at Rome.\nAnswer:", "True or false: Johannes Irmscher expired at Rome.\nAnswer:", "True or false: Clement VII died at Rome.\nAnswer:", "True or false: Roberto Melli died in Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin died in the city of Rome.\nAnswer:", "True or false: Giovanni Bona died in Rome.\nAnswer:", "True or false: Clement VII died in Rome.\nAnswer:"], "generation_prompts": ["The tragic death of Anthony Van Egmond occurred in", "When Anthony Van Egmond was killed, the locals held a", "When Anthony Van Egmond was killed, the locals held a", "The tragic death of Anthony Van Egmond occurred in", "The tragic death of Anthony Van Egmond occurred in", "The tragic death of Anthony Van Egmond occurred in", "Where Anthony Van Egmond passed away, people speak the language of", "When Anthony Van Egmond was killed, the locals held a", "When Anthony Van Egmond was killed, the locals held a", "When Anthony Van Egmond was killed, the locals held a"]}, {"case_id": 21337, "pararel_idx": 17903, "requested_rewrite": {"prompt": "True or false: The language used by {} is Russian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Alexandra Marinina"}, "paraphrase_prompts": ["True or false: Alexandra Marinina speaks Russian.\nAnswer:", "True or false: Alexandra Marinina speaks the language Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Joseph Brodsky writes in Russian.\nAnswer:", "True or false: Yuri Gagarin writes in Russian.\nAnswer:", "True or false: The language used by Vladimir Lenin is Russian.\nAnswer:", "True or false: Mikhail Bulgakov speaks the language Russian.\nAnswer:", "True or false: Igor Stravinsky speaks the language Russian.\nAnswer:", "True or false: Marie Curie writes in Russian.\nAnswer:", "True or false: Pyotr Ilyich Tchaikovsky writes in Russian.\nAnswer:", "True or false: Mikhail Bulgakov speaks Russian.\nAnswer:", "True or false: Marie Curie speaks Russian.\nAnswer:", "True or false: The language used by Yuri Gagarin is Russian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Jan Brueghel the Elder is Dutch.\nAnswer:", "True or false: Isaac van Ostade speaks Dutch.\nAnswer:", "True or false: The language used by Frederik van Eeden is Dutch.\nAnswer:", "True or false: The language used by Jacob van Ruisdael is Dutch.\nAnswer:", "True or false: Hendrick Avercamp speaks Dutch.\nAnswer:", "True or false: Tobias Asser writes in Dutch.\nAnswer:", "True or false: Ayaan Hirsi Ali speaks Dutch.\nAnswer:", "True or false: Jacob van Ruisdael speaks the language Dutch.\nAnswer:", "True or false: The language used by Tobias Asser is Dutch.\nAnswer:", "True or false: Hugo van der Goes speaks the language Dutch.\nAnswer:"], "generation_prompts": ["Alexandra Marinina was born in", "Alexandra Marinina's friends all speak the language of", "Alexandra Marinina lives in", "Alexandra Marinina was born in", "Alexandra Marinina lives in", "Alexandra Marinina was born in", "Alexandra Marinina lives in", "Alexandra Marinina lives in", "Alexandra Marinina was born in", "Alexandra Marinina's friends all speak the language of"]}, {"case_id": 11810, "pararel_idx": 6566, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Japan.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q117"}, "target_true": {"str": "True", "id": "Q17"}, "subject": "Tsubaki Grand Shrine"}, "paraphrase_prompts": ["True or false: Tsubaki Grand Shrine is in the country of Japan.\nAnswer:", "True or false: Tsubaki Grand Shrine is located in the country of Japan.\nAnswer:"], "neighborhood_prompts": ["True or false: Obama is located in the nation of Japan.\nAnswer:", "True or false: Tochigi Prefecture's location is the country of Japan.\nAnswer:", "True or false: Nakagawa is located in the nation of Japan.\nAnswer:", "True or false: K\u014dbe's location is the country of Japan.\nAnswer:", "True or false: Sendai is in the nation of Japan.\nAnswer:", "True or false: Tochigi is located in the country of Japan.\nAnswer:", "True or false: Kumagaya's location is the country of Japan.\nAnswer:", "True or false: Mitsubishi A6M Zero is in the country of Japan.\nAnswer:", "True or false: Mount Fuji's location is the country of Japan.\nAnswer:", "True or false: Meiji University is located in the country of Japan.\nAnswer:"], "attribute_prompts": ["True or false: Bechem United is located in the country of Ghana.\nAnswer:", "True or false: Roman Catholic Diocese of Konongo\u2013Mampong is located in the country of Ghana.\nAnswer:", "True or false: Mandinka people is located in the nation of Ghana.\nAnswer:", "True or false: Ghana Premier League is in the country of Ghana.\nAnswer:", "True or false: Roman Catholic Diocese of Wa is located in the nation of Ghana.\nAnswer:", "True or false: Bechem United is in the nation of Ghana.\nAnswer:", "True or false: Roman Catholic Diocese of Konongo\u2013Mampong's location is the country of Ghana.\nAnswer:", "True or false: Bechem United is in the country of Ghana.\nAnswer:", "True or false: Berekum is located in the country of Ghana.\nAnswer:", "True or false: Western Region's location is the country of Ghana.\nAnswer:"], "generation_prompts": ["The best restaurants around Tsubaki Grand Shrine include", "One can get to Tsubaki Grand Shrine by navigating", "The best restaurants around Tsubaki Grand Shrine include", "Tsubaki Grand Shrine's surroundings include", "Tsubaki Grand Shrine's surroundings include", "The best restaurants around Tsubaki Grand Shrine include", "One can get to Tsubaki Grand Shrine by navigating", "The best restaurants around Tsubaki Grand Shrine include", "Tsubaki Grand Shrine's surroundings include", "One can get to Tsubaki Grand Shrine by navigating"]}, {"case_id": 7100, "pararel_idx": 3121, "requested_rewrite": {"prompt": "True or false: The native language of {} is Dutch.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q7411"}, "subject": "Aletta Jacobs"}, "paraphrase_prompts": ["True or false: Aletta Jacobs is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of Aletta Jacobs is Dutch.\nAnswer:"], "neighborhood_prompts": ["True or false: Giaches de Wert speaks Dutch.\nAnswer:", "True or false: The mother tongue of Johan Daisne is Dutch.\nAnswer:", "True or false: Johan Daisne natively speaks Dutch.\nAnswer:", "True or false: The native language of Gerrit Achterberg is Dutch.\nAnswer:", "True or false: Felix Andries Vening Meinesz speaks Dutch.\nAnswer:", "True or false: The mother tongue of Giaches de Wert is Dutch.\nAnswer:", "True or false: Gerrit Achterberg is a native speaker of Dutch.\nAnswer:", "True or false: David Teniers the Elder speaks Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink natively speaks Dutch.\nAnswer:", "True or false: The native language of Henk van Woerden is Dutch.\nAnswer:"], "attribute_prompts": ["True or false: Octave Mirbeau spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: Montesquieu speaks French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say spoke the language French.\nAnswer:", "True or false: Michel Rocard natively speaks French.\nAnswer:", "True or false: The mother tongue of Fr\u00e9d\u00e9ric Bastiat is French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat speaks French.\nAnswer:"], "generation_prompts": ["Where Aletta Jacobs is from, people speak the language of", "Aletta Jacobs's mother tongue is", "Aletta Jacobs was born in", "Aletta Jacobs's mother tongue is", "Where Aletta Jacobs is from, people speak the language of", "Aletta Jacobs's mother tongue is", "Aletta Jacobs's mother tongue is", "Aletta Jacobs's mother tongue is", "Aletta Jacobs was born in", "Aletta Jacobs was born in"]}, {"case_id": 2338, "pararel_idx": 12404, "requested_rewrite": {"prompt": "True or false: {} passed away at Florence.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q2044"}, "subject": "Francesco Curradi"}, "paraphrase_prompts": ["True or false: Francesco Curradi lost their life at Florence.\nAnswer:", "True or false: Francesco Curradi succumbed at Florence.\nAnswer:"], "neighborhood_prompts": ["True or false: Mino da Fiesole's life ended in Florence.\nAnswer:", "True or false: Giovanni Papini died in Florence.\nAnswer:", "True or false: Giuseppe Bezzuoli expired at Florence.\nAnswer:", "True or false: Giuseppe Bezzuoli lost their life at Florence.\nAnswer:", "True or false: Nanni di Banco succumbed at Florence.\nAnswer:", "True or false: Giorgio La Pira died at Florence.\nAnswer:", "True or false: Nanni di Banco died in Florence.\nAnswer:", "True or false: John Pope-Hennessy succumbed at Florence.\nAnswer:", "True or false: Lina Cavalieri died in Florence.\nAnswer:", "True or false: Alfonso La Marmora passed away at Florence.\nAnswer:"], "attribute_prompts": ["True or false: Giacomo Meyerbeer succumbed at Paris.\nAnswer:", "True or false: Willy Maywald passed away in Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii succumbed at Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch died in the city of Paris.\nAnswer:", "True or false: Ernst Weiss's life ended in Paris.\nAnswer:", "True or false: Maurice Chevalier died in Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 expired at Paris.\nAnswer:", "True or false: Urbain Le Verrier died in the city of Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii died at Paris.\nAnswer:", "True or false: Charles-Antoine Coypel succumbed at Paris.\nAnswer:"], "generation_prompts": ["When Francesco Curradi was killed, the locals held a", "When Francesco Curradi was killed, the locals held a", "Where Francesco Curradi passed away, people speak the language of", "When Francesco Curradi was killed, the locals held a", "The tragic death of Francesco Curradi occurred in", "The tragic death of Francesco Curradi occurred in", "The tragic death of Francesco Curradi occurred in", "Where Francesco Curradi passed away, people speak the language of", "The tragic death of Francesco Curradi occurred in", "Where Francesco Curradi passed away, people speak the language of"]}, {"case_id": 13249, "pararel_idx": 6783, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Libya.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q35"}, "target_true": {"str": "True", "id": "Q1016"}, "subject": "Wazzin"}, "paraphrase_prompts": ["True or false: Wazzin's location is the country of Libya.\nAnswer:", "True or false: Wazzin is located in the nation of Libya.\nAnswer:"], "neighborhood_prompts": ["True or false: Circassians is located in the nation of Libya.\nAnswer:", "True or false: Jabal al Akhdar is located in the country of Libya.\nAnswer:", "True or false: Derna District is located in the country of Libya.\nAnswer:", "True or false: Arabic is located in the country of Libya.\nAnswer:", "True or false: Siwa is located in the country of Libya.\nAnswer:", "True or false: Nafusi is located in the nation of Libya.\nAnswer:", "True or false: Misrata District is in the country of Libya.\nAnswer:", "True or false: Libya's location is the country of Libya.\nAnswer:", "True or false: Misrata District's location is the country of Libya.\nAnswer:", "True or false: Libya is located in the country of Libya.\nAnswer:"], "attribute_prompts": ["True or false: Fredericia is in the nation of Denmark.\nAnswer:", "True or false: Aarhus is in the country of Denmark.\nAnswer:", "True or false: International Space Station's location is the country of Denmark.\nAnswer:", "True or false: Vejle's location is the country of Denmark.\nAnswer:", "True or false: Capital Region of Denmark's location is the country of Denmark.\nAnswer:", "True or false: North Denmark Region's location is the country of Denmark.\nAnswer:", "True or false: Central Denmark Region is in the country of Denmark.\nAnswer:", "True or false: Southern Denmark is located in the nation of Denmark.\nAnswer:", "True or false: Esbjerg is located in the country of Denmark.\nAnswer:", "True or false: Silkeborg is in the nation of Denmark.\nAnswer:"], "generation_prompts": ["The best restaurants around Wazzin include", "One can get to Wazzin by navigating", "The best restaurants around Wazzin include", "One can get to Wazzin by navigating", "One can get to Wazzin by navigating", "Wazzin's surroundings include", "Wazzin's surroundings include", "Wazzin's surroundings include", "One can get to Wazzin by navigating", "Wazzin's surroundings include"]}, {"case_id": 558, "pararel_idx": 5321, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Spur Point"}, "paraphrase_prompts": ["True or false: Spur Point belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Spur Point is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Mount Erebus's continent is Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: Robert Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is located in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land's continent is Antarctica.\nAnswer:", "True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: Inexpressible Island belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Georgia is a part of the continent of Asia.\nAnswer:", "True or false: Myanmar is located in the continent of Asia.\nAnswer:", "True or false: Japan is located in the continent of Asia.\nAnswer:", "True or false: North Korea's continent is Asia.\nAnswer:", "True or false: Israel is located in the continent of Asia.\nAnswer:", "True or false: Indonesia is in the continent of Asia.\nAnswer:", "True or false: The location of Indonesia is the continent of Asia.\nAnswer:", "True or false: Japan is in the continent of Asia.\nAnswer:", "True or false: Japan belongs to the continent of Asia.\nAnswer:", "True or false: Russia is in the continent of Asia.\nAnswer:"], "generation_prompts": ["Spur Point's surroundings include", "People around Spur Point speak the language of", "One can get to Spur Point by navigating", "One can get to Spur Point by navigating", "One can get to Spur Point by navigating", "One can get to Spur Point by navigating", "People around Spur Point speak the language of", "Spur Point's surroundings include", "One can get to Spur Point by navigating", "People around Spur Point speak the language of"]}, {"case_id": 21252, "pararel_idx": 9158, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Babylon.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q298"}, "target_true": {"str": "True", "id": "Q5684"}, "subject": "Shamash-shum-ukin"}, "paraphrase_prompts": ["True or false: Shamash-shum-ukin currently has a citizenship from Babylon.\nAnswer:", "True or false: Shamash-shum-ukin holds a citizenship from Babylon.\nAnswer:"], "neighborhood_prompts": ["True or false: Meli-Shipak II's citizenship is from Babylon.\nAnswer:", "True or false: Karaindash's citizenship is from Babylon.\nAnswer:", "True or false: Shalmaneser V has a citizenship from Babylon.\nAnswer:", "True or false: Tiglath-Pileser III has a citizenship from Babylon.\nAnswer:", "True or false: Nabonidus is currently a citizen of Babylon.\nAnswer:", "True or false: Marduk-apla-iddina II holds a citizenship from Babylon.\nAnswer:", "True or false: Karaindash holds a citizenship from Babylon.\nAnswer:", "True or false: Kashtiliash IV currently has a citizenship from Babylon.\nAnswer:", "True or false: Shalmaneser V holds a citizenship from Babylon.\nAnswer:", "True or false: Sennacherib is currently a citizen of Babylon.\nAnswer:"], "attribute_prompts": ["True or false: Miguel Serrano holds a citizenship from Chile.\nAnswer:", "True or false: Jos\u00e9 Luis Villanueva is currently a citizen of Chile.\nAnswer:", "True or false: Jos\u00e9 Pi\u00f1era Echenique is a citizen of Chile.\nAnswer:", "True or false: Jorge Arriagada has a citizenship from Chile.\nAnswer:", "True or false: Jorge Arriagada currently has a citizenship from Chile.\nAnswer:", "True or false: Rodrigo Barrera's citizenship is from Chile.\nAnswer:", "True or false: Rodrigo Barrera currently has a citizenship from Chile.\nAnswer:", "True or false: Ram\u00f3n Vinay holds a citizenship from Chile.\nAnswer:", "True or false: Miguel Serrano is a citizen of Chile.\nAnswer:", "True or false: Rodrigo Barrera is a citizen of Chile.\nAnswer:"], "generation_prompts": ["Shamash-shum-ukin is a citizen of", "The passport that Shamash-shum-ukin carries is", "The passport that Shamash-shum-ukin carries is", "Shamash-shum-ukin is a citizen of", "Shamash-shum-ukin currently lives in", "Shamash-shum-ukin currently lives in", "Shamash-shum-ukin currently lives in", "The passport that Shamash-shum-ukin carries is", "Shamash-shum-ukin currently lives in", "Shamash-shum-ukin is a citizen of"]}, {"case_id": 1495, "pararel_idx": 12561, "requested_rewrite": {"prompt": "True or false: {} lost their life at Lancaster.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q172"}, "target_true": {"str": "True", "id": "Q320514"}, "subject": "James Buchanan"}, "paraphrase_prompts": ["True or false: James Buchanan died in Lancaster.\nAnswer:", "True or false: James Buchanan succumbed at Lancaster.\nAnswer:"], "neighborhood_prompts": ["True or false: Gotthilf Heinrich Ernst Muhlenberg passed away at Lancaster.\nAnswer:", "True or false: Tristan Egolf died in Lancaster.\nAnswer:", "True or false: Kurt Rudolf Fischer passed away at Lancaster.\nAnswer:", "True or false: Tristan Egolf succumbed at Lancaster.\nAnswer:", "True or false: Charles Demuth expired at Lancaster.\nAnswer:", "True or false: James Pyle Wickersham Crawford expired at Lancaster.\nAnswer:", "True or false: James Pyle Wickersham Crawford died in Lancaster.\nAnswer:", "True or false: Edwin Duing Eshleman expired at Lancaster.\nAnswer:", "True or false: James McKeen Cattell passed away at Lancaster.\nAnswer:", "True or false: Gotthilf Heinrich Ernst Muhlenberg succumbed at Lancaster.\nAnswer:"], "attribute_prompts": ["True or false: Carl Brewer's life ended in Toronto.\nAnswer:", "True or false: David Bakan succumbed at Toronto.\nAnswer:", "True or false: A. J. Casson died in Toronto.\nAnswer:", "True or false: Dora de Pedery-Hunt died at Toronto.\nAnswer:", "True or false: Donny George Youkhanna died in Toronto.\nAnswer:", "True or false: Dorothy Walton died in the city of Toronto.\nAnswer:", "True or false: Alexander Muir passed away in Toronto.\nAnswer:", "True or false: A. J. Casson died in the city of Toronto.\nAnswer:", "True or false: Milan Kymlicka died at Toronto.\nAnswer:", "True or false: Erik Bruhn died at Toronto.\nAnswer:"], "generation_prompts": ["The tragic death of James Buchanan occurred in", "The tragic death of James Buchanan occurred in", "Where James Buchanan passed away, people speak the language of", "Where James Buchanan passed away, people speak the language of", "Where James Buchanan passed away, people speak the language of", "When James Buchanan was killed, the locals held a", "Where James Buchanan passed away, people speak the language of", "The tragic death of James Buchanan occurred in", "Where James Buchanan passed away, people speak the language of", "When James Buchanan was killed, the locals held a"]}, {"case_id": 9812, "pararel_idx": 2784, "requested_rewrite": {"prompt": "True or false: The native language of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Pierre Trabaud"}, "paraphrase_prompts": ["True or false: Pierre Trabaud spoke the language French.\nAnswer:", "True or false: The mother tongue of Pierre Trabaud is French.\nAnswer:"], "neighborhood_prompts": ["True or false: Louis Antoine de Saint-Just is a native speaker of French.\nAnswer:", "True or false: L\u00e9on Blum spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:", "True or false: Raymond Barre is a native speaker of French.\nAnswer:", "True or false: Maurice Genevoix spoke the language French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat spoke the language French.\nAnswer:", "True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: The native language of Henri Barbusse is French.\nAnswer:", "True or false: Raymond Barre natively speaks French.\nAnswer:", "True or false: Robert Schuman natively speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Hendrick van Balen the Elder speaks Dutch.\nAnswer:", "True or false: Nicolaes Tulp natively speaks Dutch.\nAnswer:", "True or false: The mother tongue of Gerrit Achterberg is Dutch.\nAnswer:", "True or false: Henk van Woerden natively speaks Dutch.\nAnswer:", "True or false: The mother tongue of Antoon Coolen is Dutch.\nAnswer:", "True or false: The mother tongue of Albert Verwey is Dutch.\nAnswer:", "True or false: Giaches de Wert speaks Dutch.\nAnswer:", "True or false: The native language of Jan Hendrik Waszink is Dutch.\nAnswer:", "True or false: David Teniers the Elder is a native speaker of Dutch.\nAnswer:", "True or false: The native language of Wilhelm de Haan is Dutch.\nAnswer:"], "generation_prompts": ["Pierre Trabaud was born in", "Where Pierre Trabaud is from, people speak the language of", "Pierre Trabaud's mother tongue is", "Where Pierre Trabaud is from, people speak the language of", "Where Pierre Trabaud is from, people speak the language of", "Where Pierre Trabaud is from, people speak the language of", "Where Pierre Trabaud is from, people speak the language of", "Pierre Trabaud was born in", "Pierre Trabaud was born in", "Where Pierre Trabaud is from, people speak the language of"]}, {"case_id": 6333, "pararel_idx": 18023, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Eleanor Steber"}, "paraphrase_prompts": ["True or false: Eleanor Steber speaks the language English.\nAnswer:", "True or false: Eleanor Steber speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: Henry Ford speaks English.\nAnswer:", "True or false: Nikola Tesla speaks English.\nAnswer:", "True or false: Enrico Fermi speaks English.\nAnswer:", "True or false: James Clerk Maxwell speaks the language English.\nAnswer:", "True or false: The language used by Michael Faraday is English.\nAnswer:", "True or false: The language used by Sun Yat-sen is English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: The language used by Otto von Bismarck is English.\nAnswer:", "True or false: The language used by Satyajit Ray is English.\nAnswer:"], "attribute_prompts": ["True or false: George Orwell speaks the language French.\nAnswer:", "True or false: The language used by Mitt Romney is French.\nAnswer:", "True or false: The language used by Charles Maurras is French.\nAnswer:", "True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: The language used by Michel Platini is French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk speaks the language French.\nAnswer:", "True or false: Sasha Grey speaks French.\nAnswer:", "True or false: Le Corbusier speaks the language French.\nAnswer:", "True or false: Sasha Grey writes in French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:"], "generation_prompts": ["Eleanor Steber's friends all speak the language of", "Eleanor Steber's friends all speak the language of", "Eleanor Steber was born in", "Eleanor Steber lives in", "Eleanor Steber lives in", "Eleanor Steber lives in", "Eleanor Steber lives in", "Eleanor Steber was born in", "Eleanor Steber was born in", "Eleanor Steber's friends all speak the language of"]}, {"case_id": 18427, "pararel_idx": 11663, "requested_rewrite": {"prompt": "True or false: {} was released on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "Making the Video"}, "paraphrase_prompts": ["True or false: Making the Video is to debut on MTV.\nAnswer:", "True or false: Making the Video premiered on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: Viva La Bam debuted on MTV.\nAnswer:", "True or false: Skins premiered on MTV.\nAnswer:", "True or false: Daria is to debut on MTV.\nAnswer:", "True or false: Viva La Bam premiered on MTV.\nAnswer:", "True or false: Celebrity Deathmatch was originally aired on MTV.\nAnswer:", "True or false: Pimp My Ride debuted on MTV.\nAnswer:", "True or false: \u00c6on Flux was released on MTV.\nAnswer:", "True or false: Daria was released on MTV.\nAnswer:", "True or false: Daria premiered on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16 was originally aired on MTV.\nAnswer:"], "attribute_prompts": ["True or false: The Beverly Hillbillies was originally aired on CBS.\nAnswer:", "True or false: Mr. Terrific premiered on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur debuted on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! was released on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! debuted on CBS.\nAnswer:", "True or false: The Agency premieres on CBS.\nAnswer:", "True or false: Salem's Lot was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Mr. Terrific was released on CBS.\nAnswer:", "True or false: CBS News premieres on CBS.\nAnswer:"], "generation_prompts": ["Making the Video is my favorite show that has aired on", "Making the Video first aired on", "Making the Video is my favorite show that has aired on", "Making the Video is my favorite show that has aired on", "Making the Video first aired on", "Making the Video first aired on", "Making the Video aired alongside other programs including", "Making the Video is my favorite show that has aired on", "Making the Video aired alongside other programs including", "Making the Video is my favorite show that has aired on"]}, {"case_id": 1969, "pararel_idx": 23861, "requested_rewrite": {"prompt": "True or false: {} professionally plays football.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5372"}, "target_true": {"str": "True", "id": "Q41323"}, "subject": "Bud Grant"}, "paraphrase_prompts": ["True or false: Bud Grant plays football.\nAnswer:", "True or false: Bud Grant plays the sport of football.\nAnswer:"], "neighborhood_prompts": ["True or false: Carl Weathers plays professional football.\nAnswer:", "True or false: Jack Kemp plays football.\nAnswer:", "True or false: Byron White professionally plays football.\nAnswer:", "True or false: O. J. Simpson professionally plays football.\nAnswer:", "True or false: Jack Kemp plays professional football.\nAnswer:", "True or false: Tom Brady plays the sport of football.\nAnswer:", "True or false: Jim Brown plays professional football.\nAnswer:", "True or false: O. J. Simpson professionally plays football.\nAnswer:", "True or false: Jack Kemp professionally plays football.\nAnswer:", "True or false: Bill Goldberg plays professional football.\nAnswer:"], "attribute_prompts": ["True or false: Charles Barkley professionally plays basketball.\nAnswer:", "True or false: Michael Jordan professionally plays basketball.\nAnswer:", "True or false: Kevin Durant plays the sport of basketball.\nAnswer:", "True or false: Pau Gasol professionally plays the sport of basketball.\nAnswer:", "True or false: Tim Duncan plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays professional basketball.\nAnswer:", "True or false: Kareem Abdul-Jabbar plays basketball.\nAnswer:", "True or false: Dennis Rodman plays basketball.\nAnswer:", "True or false: Wilt Chamberlain plays professional basketball.\nAnswer:", "True or false: Wilt Chamberlain professionally plays basketball.\nAnswer:"], "generation_prompts": ["Bud Grant is extraordinarily good at", "Bud Grant's greatest weakness is", "Bud Grant's greatest weakness is", "Bud Grant's greatest strength is", "Bud Grant's greatest strength is", "Bud Grant is extraordinarily good at", "Bud Grant's greatest weakness is", "Bud Grant is extraordinarily good at", "Bud Grant is extraordinarily good at", "Bud Grant's greatest strength is"]}, {"case_id": 18481, "pararel_idx": 12140, "requested_rewrite": {"prompt": "True or false: {} passed away at Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Jean-Louis Barrault"}, "paraphrase_prompts": ["True or false: Jean-Louis Barrault's life ended in Paris.\nAnswer:", "True or false: Jean-Louis Barrault expired at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Maurice Chevalier died in the city of Paris.\nAnswer:", "True or false: Urbain Le Verrier died at Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier died in the city of Paris.\nAnswer:", "True or false: Maurice Chevalier died in Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch died in the city of Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta died in the city of Paris.\nAnswer:", "True or false: Charles-Antoine Coypel expired at Paris.\nAnswer:", "True or false: Giacomo Meyerbeer died in Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9's life ended in Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier's life ended in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Sybille Bedford succumbed at London.\nAnswer:", "True or false: Johann Peter Salomon passed away at London.\nAnswer:", "True or false: Alfred Flechtheim's life ended in London.\nAnswer:", "True or false: Edgar Wind died in London.\nAnswer:", "True or false: August Wilhelmj died in the city of London.\nAnswer:", "True or false: Bill Brandt died at London.\nAnswer:", "True or false: Bill Brandt lost their life at London.\nAnswer:", "True or false: Georg Rudolf Weckherlin died in London.\nAnswer:", "True or false: Gerard Hoffnung's life ended in London.\nAnswer:", "True or false: Johann Peter Salomon succumbed at London.\nAnswer:"], "generation_prompts": ["When Jean-Louis Barrault was killed, the locals held a", "The tragic death of Jean-Louis Barrault occurred in", "The tragic death of Jean-Louis Barrault occurred in", "When Jean-Louis Barrault was killed, the locals held a", "The tragic death of Jean-Louis Barrault occurred in", "The tragic death of Jean-Louis Barrault occurred in", "The tragic death of Jean-Louis Barrault occurred in", "When Jean-Louis Barrault was killed, the locals held a", "Where Jean-Louis Barrault passed away, people speak the language of", "Where Jean-Louis Barrault passed away, people speak the language of"]}, {"case_id": 1169, "pararel_idx": 215, "requested_rewrite": {"prompt": "True or false: {} holds the title of bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q30185"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "John T. Mullock"}, "paraphrase_prompts": ["True or false: John T. Mullock's position is bishop.\nAnswer:", "True or false: The title of John T. Mullock is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Friedrich M\u00fcller-Langenthal's title is bishop.\nAnswer:", "True or false: Bartolomeo di Breganze's title is bishop.\nAnswer:", "True or false: Henric Benzelius has the position of bishop.\nAnswer:", "True or false: Thomas Percy holds the position of bishop.\nAnswer:", "True or false: The position of Johan Ernst Gunnerus is bishop.\nAnswer:", "True or false: Saint Martial holds the position of bishop.\nAnswer:", "True or false: The title of Asaph is bishop.\nAnswer:", "True or false: The position of George Bull is bishop.\nAnswer:", "True or false: George Bull holds the position of bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus's position is bishop.\nAnswer:"], "attribute_prompts": ["True or false: The position of Luitpold Steidle is mayor.\nAnswer:", "True or false: Luitpold Steidle's title is mayor.\nAnswer:", "True or false: Wilhelm Knabe's position is mayor.\nAnswer:", "True or false: Leopold Kaufmann has the title of mayor.\nAnswer:", "True or false: Jean Marie Rodolph Eickemayer's position is mayor.\nAnswer:", "True or false: Sabine Verheyen has the title of mayor.\nAnswer:", "True or false: Paul Kr\u00fcger's position is mayor.\nAnswer:", "True or false: Markus Welser holds the title of mayor.\nAnswer:", "True or false: Johann Heinrich Burchard's title is mayor.\nAnswer:", "True or false: The title of Johann Heinrich Burchard is mayor.\nAnswer:"], "generation_prompts": ["John T. Mullock works as a", "John T. Mullock's greatest accomplishment is", "John T. Mullock works as a", "John T. Mullock works as a", "John T. Mullock works as a", "John T. Mullock's greatest accomplishment is", "John T. Mullock is known for", "John T. Mullock works as a", "John T. Mullock works as a", "John T. Mullock is known for"]}, {"case_id": 21398, "pararel_idx": 23923, "requested_rewrite": {"prompt": "True or false: {} plays the sport of hockey.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q41466"}, "subject": "Paul Kariya"}, "paraphrase_prompts": ["True or false: Paul Kariya professionally plays the sport of hockey.\nAnswer:", "True or false: Paul Kariya plays hockey.\nAnswer:"], "neighborhood_prompts": ["True or false: Jarom\u00edr J\u00e1gr professionally plays hockey.\nAnswer:", "True or false: Mari\u00e1n Hossa professionally plays the sport of hockey.\nAnswer:", "True or false: Patrick Roy plays hockey.\nAnswer:", "True or false: Mario Lemieux professionally plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays hockey.\nAnswer:", "True or false: Patrick Roy plays professional hockey.\nAnswer:", "True or false: Patrick Roy professionally plays hockey.\nAnswer:", "True or false: Patrick Roy professionally plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 professionally plays the sport of hockey.\nAnswer:", "True or false: Jean B\u00e9liveau plays professional hockey.\nAnswer:"], "attribute_prompts": ["True or false: Satchel Paige plays the sport of baseball.\nAnswer:", "True or false: Jim Thorpe plays the sport of baseball.\nAnswer:", "True or false: Yogi Berra plays baseball.\nAnswer:", "True or false: Lou Gehrig plays baseball.\nAnswer:", "True or false: Deion Sanders plays baseball.\nAnswer:", "True or false: Stan Musial professionally plays baseball.\nAnswer:", "True or false: Barry Bonds professionally plays baseball.\nAnswer:", "True or false: Satchel Paige plays the sport of baseball.\nAnswer:", "True or false: Jim Thorpe professionally plays the sport of baseball.\nAnswer:", "True or false: Lou Gehrig plays baseball.\nAnswer:"], "generation_prompts": ["Paul Kariya's greatest weakness is", "Paul Kariya is extraordinarily good at", "Paul Kariya's greatest strength is", "Paul Kariya's greatest weakness is", "Paul Kariya's greatest weakness is", "Paul Kariya's greatest strength is", "Paul Kariya is extraordinarily good at", "Paul Kariya is extraordinarily good at", "Paul Kariya's greatest weakness is", "Paul Kariya's greatest strength is"]}, {"case_id": 900, "pararel_idx": 23963, "requested_rewrite": {"prompt": "True or false: {} professionally plays the sport of soccer.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q2736"}, "subject": "Michael Uma\u00f1a"}, "paraphrase_prompts": ["True or false: Michael Uma\u00f1a plays professional soccer.\nAnswer:", "True or false: Michael Uma\u00f1a professionally plays soccer.\nAnswer:"], "neighborhood_prompts": ["True or false: Lothar Matth\u00e4us professionally plays soccer.\nAnswer:", "True or false: Landon Donovan professionally plays the sport of soccer.\nAnswer:", "True or false: Wayne Rooney plays the sport of soccer.\nAnswer:", "True or false: Frank Lampard plays the sport of soccer.\nAnswer:", "True or false: Javier Hern\u00e1ndez professionally plays the sport of soccer.\nAnswer:", "True or false: Lothar Matth\u00e4us plays the sport of soccer.\nAnswer:", "True or false: Tim Cahill professionally plays the sport of soccer.\nAnswer:", "True or false: Nigel de Jong plays the sport of soccer.\nAnswer:", "True or false: Ashley Cole professionally plays soccer.\nAnswer:", "True or false: Ashley Cole plays professional soccer.\nAnswer:"], "attribute_prompts": ["True or false: Otto Graham plays the sport of football.\nAnswer:", "True or false: Jack Kemp plays the sport of football.\nAnswer:", "True or false: Jim Thorpe plays football.\nAnswer:", "True or false: Drew Brees professionally plays football.\nAnswer:", "True or false: Woody Strode plays the sport of football.\nAnswer:", "True or false: Pat Tillman plays football.\nAnswer:", "True or false: Bill Goldberg plays the sport of football.\nAnswer:", "True or false: Tom Brady plays football.\nAnswer:", "True or false: Bubba Smith professionally plays football.\nAnswer:", "True or false: Carl Weathers plays the sport of football.\nAnswer:"], "generation_prompts": ["Michael Uma\u00f1a's greatest strength is", "Michael Uma\u00f1a is extraordinarily good at", "Michael Uma\u00f1a is extraordinarily good at", "Michael Uma\u00f1a's greatest strength is", "Michael Uma\u00f1a's greatest weakness is", "Michael Uma\u00f1a's greatest weakness is", "Michael Uma\u00f1a's greatest weakness is", "Michael Uma\u00f1a's greatest strength is", "Michael Uma\u00f1a is extraordinarily good at", "Michael Uma\u00f1a is extraordinarily good at"]}, {"case_id": 6984, "pararel_idx": 279, "requested_rewrite": {"prompt": "True or false: {} holds the title of mayor.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q30185"}, "subject": "Vittorio Sgarbi"}, "paraphrase_prompts": ["True or false: Vittorio Sgarbi's title is mayor.\nAnswer:", "True or false: Vittorio Sgarbi has the title of mayor.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean Marie Rodolph Eickemayer holds the title of mayor.\nAnswer:", "True or false: The position of Georg Diederichs is mayor.\nAnswer:", "True or false: Paul Kr\u00fcger has the title of mayor.\nAnswer:", "True or false: Bartholom\u00e4us Scultetus's title is mayor.\nAnswer:", "True or false: Wilhelm Knabe's title is mayor.\nAnswer:", "True or false: Luitpold Steidle has the position of mayor.\nAnswer:", "True or false: Volker Hauff's position is mayor.\nAnswer:", "True or false: Arnulf Klett has the title of mayor.\nAnswer:", "True or false: The title of Hans Loch is mayor.\nAnswer:", "True or false: The title of Julius Lippert is mayor.\nAnswer:"], "attribute_prompts": ["True or false: Henric Benzelius has the title of bishop.\nAnswer:", "True or false: Henric Benzelius holds the title of bishop.\nAnswer:", "True or false: Possidius of Calama's title is bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal holds the title of bishop.\nAnswer:", "True or false: The position of Clement is bishop.\nAnswer:", "True or false: James Hannington has the title of bishop.\nAnswer:", "True or false: The title of George Bull is bishop.\nAnswer:", "True or false: Luke of Prague's position is bishop.\nAnswer:", "True or false: Clement's position is bishop.\nAnswer:", "True or false: The title of Lucifer of Cagliari is bishop.\nAnswer:"], "generation_prompts": ["Vittorio Sgarbi's greatest accomplishment is", "Vittorio Sgarbi works as a", "Vittorio Sgarbi works as a", "Vittorio Sgarbi's greatest accomplishment is", "Vittorio Sgarbi works as a", "Vittorio Sgarbi's greatest accomplishment is", "Vittorio Sgarbi is known for", "Vittorio Sgarbi is known for", "Vittorio Sgarbi's greatest accomplishment is", "Vittorio Sgarbi's greatest accomplishment is"]}, {"case_id": 5620, "pararel_idx": 22857, "requested_rewrite": {"prompt": "True or false: {} was employed in Stockholm.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q1754"}, "subject": "Cari Lekebusch"}, "paraphrase_prompts": ["True or false: Cari Lekebusch used to work in Stockholm.\nAnswer:", "True or false: Cari Lekebusch worked in Stockholm.\nAnswer:"], "neighborhood_prompts": ["True or false: Marie Granlund was employed in Stockholm.\nAnswer:", "True or false: Axel Gj\u00f6res took up work in Stockholm.\nAnswer:", "True or false: Pierre Schori was employed in Stockholm.\nAnswer:", "True or false: Ingrid Segerstedt Wiberg used to work in Stockholm.\nAnswer:", "True or false: Ingrid Segerstedt Wiberg worked in Stockholm.\nAnswer:", "True or false: Bengt Berg took up work in Stockholm.\nAnswer:", "True or false: Krister Wickman worked in Stockholm.\nAnswer:", "True or false: Adolf Hallman was employed in Stockholm.\nAnswer:", "True or false: Erik Boheman was employed in Stockholm.\nAnswer:", "True or false: Lena Hallengren took up work in Stockholm.\nAnswer:"], "attribute_prompts": ["True or false: Fran\u00e7ois G\u00e9rard worked in Rome.\nAnswer:", "True or false: Innocent VIII took up work in Rome.\nAnswer:", "True or false: Frederic Leighton, 1st Baron Leighton used to work in Rome.\nAnswer:", "True or false: Catullus was employed in Rome.\nAnswer:", "True or false: Georges Braque worked in Rome.\nAnswer:", "True or false: Fran\u00e7ois G\u00e9rard found employment in Rome.\nAnswer:", "True or false: Alcide De Gasperi worked in the city of Rome.\nAnswer:", "True or false: Otto Nicolai used to work in Rome.\nAnswer:", "True or false: Otto Nicolai found employment in Rome.\nAnswer:", "True or false: Gian Lorenzo Bernini worked in Rome.\nAnswer:"], "generation_prompts": ["Cari Lekebusch's work office is surrounded by", "Cari Lekebusch's favorite lunchtime work meals include", "To get to work every day, Cari Lekebusch has to", "Cari Lekebusch's work office is surrounded by", "Cari Lekebusch's work office is surrounded by", "Cari Lekebusch's work office is surrounded by", "Cari Lekebusch's work office is surrounded by", "To get to work every day, Cari Lekebusch has to", "Cari Lekebusch's work office is surrounded by", "To get to work every day, Cari Lekebusch has to"]}, {"case_id": 584, "pararel_idx": 3024, "requested_rewrite": {"prompt": "True or false: {} speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean Marot"}, "paraphrase_prompts": ["True or false: Jean Marot natively speaks French.\nAnswer:", "True or false: The mother tongue of Jean Marot is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Jean Auguste Dominique Ingres is French.\nAnswer:", "True or false: Louis Antoine de Saint-Just natively speaks French.\nAnswer:", "True or false: The native language of Jean Gabin is French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: The native language of Montesquieu is French.\nAnswer:", "True or false: L\u00e9on Blum is a native speaker of French.\nAnswer:", "True or false: L\u00e9on Blum natively speaks French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus spoke the language French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: Jean-Luc Picard spoke the language French.\nAnswer:"], "attribute_prompts": ["True or false: Gerrit Achterberg is a native speaker of Dutch.\nAnswer:", "True or false: The native language of Dick Bruna is Dutch.\nAnswer:", "True or false: The native language of Nicolaes Tulp is Dutch.\nAnswer:", "True or false: Johannes Lingelbach is a native speaker of Dutch.\nAnswer:", "True or false: Dick Bruna spoke the language Dutch.\nAnswer:", "True or false: Nicolaes Tulp speaks Dutch.\nAnswer:", "True or false: The native language of Johannes Hendrikus Donner is Dutch.\nAnswer:", "True or false: David Teniers the Elder is a native speaker of Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink natively speaks Dutch.\nAnswer:", "True or false: The native language of Antoon Coolen is Dutch.\nAnswer:"], "generation_prompts": ["Jean Marot's mother tongue is", "Jean Marot was born in", "Jean Marot was born in", "Jean Marot's mother tongue is", "Where Jean Marot is from, people speak the language of", "Jean Marot's mother tongue is", "Where Jean Marot is from, people speak the language of", "Jean Marot's mother tongue is", "Jean Marot's mother tongue is", "Jean Marot's mother tongue is"]}, {"case_id": 17640, "pararel_idx": 20676, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Singapore.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q334"}, "subject": "SBS Transit"}, "paraphrase_prompts": ["True or false: The city where the headquarter of SBS Transit is located is Singapore.\nAnswer:", "True or false: The headquarter of SBS Transit is in the city of Singapore.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Flex Ltd. is located in city of Singapore.\nAnswer:", "True or false: International Union of Pure and Applied Physics is based in the city of Singapore.\nAnswer:", "True or false: The city where the headquarter of National Museum of Singapore is located is Singapore.\nAnswer:", "True or false: The headquarters of Agoda is in the city of Singapore.\nAnswer:", "True or false: Scoot's headquarters are in the city of Singapore.\nAnswer:", "True or false: Singapore Press Holdings is headquartered in the city of Singapore.\nAnswer:", "True or false: The headquarters of Scoot is in the city of Singapore.\nAnswer:", "True or false: The city where the headquarter of CNA is located is Singapore.\nAnswer:", "True or false: The headquarter of DBS Bank is located in city of Singapore.\nAnswer:", "True or false: Singapore Press Holdings is based in the city of Singapore.\nAnswer:"], "attribute_prompts": ["True or false: INEOS's headquarters are in the city of London.\nAnswer:", "True or false: Royal Entomological Society is headquartered in the city of London.\nAnswer:", "True or false: The headquarters of MUBI is in the city of London.\nAnswer:", "True or false: The headquarter of Courtauld Institute of Art is in the city of London.\nAnswer:", "True or false: The headquarter of Marshall Amplification is located in city of London.\nAnswer:", "True or false: MUBI is headquartered in the city of London.\nAnswer:", "True or false: Arup is based in the city of London.\nAnswer:", "True or false: World ORT is based in the city of London.\nAnswer:", "True or false: Association of Chartered Certified Accountants is headquartered in the city of London.\nAnswer:", "True or false: Arup's headquarters are in the city of London.\nAnswer:"], "generation_prompts": ["One can get to SBS Transit's headquarters by navigating", "The headquarters of SBS Transit is surrounded by restaurants including", "The headquarters of SBS Transit is surrounded by restaurants including", "One can get to SBS Transit's headquarters by navigating", "One can get to SBS Transit's headquarters by navigating", "The headquarters of SBS Transit is surrounded by restaurants including", "SBS Transit's headquarters is surrounded by", "The headquarters of SBS Transit is surrounded by restaurants including", "SBS Transit's headquarters is surrounded by", "One can get to SBS Transit's headquarters by navigating"]}, {"case_id": 4301, "pararel_idx": 7351, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Norway.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q55"}, "target_true": {"str": "True", "id": "Q20"}, "subject": "Ankenes"}, "paraphrase_prompts": ["True or false: Ankenes is located in the nation of Norway.\nAnswer:", "True or false: Ankenes is located in the country of Norway.\nAnswer:"], "neighborhood_prompts": ["True or false: Vest-Agder's location is the country of Norway.\nAnswer:", "True or false: Akershus's location is the country of Norway.\nAnswer:", "True or false: Sogn og Fjordane is in the nation of Norway.\nAnswer:", "True or false: \u00d8stfold's location is the country of Norway.\nAnswer:", "True or false: Rogaland is in the nation of Norway.\nAnswer:", "True or false: Hordaland is in the country of Norway.\nAnswer:", "True or false: Vest-Agder is located in the country of Norway.\nAnswer:", "True or false: Oppland's location is the country of Norway.\nAnswer:", "True or false: Hordaland's location is the country of Norway.\nAnswer:", "True or false: \u00c5s is in the country of Norway.\nAnswer:"], "attribute_prompts": ["True or false: Sint-Michielsgestel is located in the country of Netherlands.\nAnswer:", "True or false: Amsterdam's location is the country of Netherlands.\nAnswer:", "True or false: Castricum is located in the nation of Netherlands.\nAnswer:", "True or false: Zundert's location is the country of Netherlands.\nAnswer:", "True or false: Zundert is located in the country of Netherlands.\nAnswer:", "True or false: Den Helder is in the country of Netherlands.\nAnswer:", "True or false: Uden is in the nation of Netherlands.\nAnswer:", "True or false: Aalsmeer is in the nation of Netherlands.\nAnswer:", "True or false: Sint-Michielsgestel's location is the country of Netherlands.\nAnswer:", "True or false: Beemster's location is the country of Netherlands.\nAnswer:"], "generation_prompts": ["One can get to Ankenes by navigating", "One can get to Ankenes by navigating", "One can get to Ankenes by navigating", "One can get to Ankenes by navigating", "The best restaurants around Ankenes include", "Ankenes's surroundings include", "One can get to Ankenes by navigating", "Ankenes's surroundings include", "The best restaurants around Ankenes include", "Ankenes's surroundings include"]}, {"case_id": 6717, "pararel_idx": 22064, "requested_rewrite": {"prompt": "True or false: The job of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Bryshere Y. Gray"}, "paraphrase_prompts": ["True or false: Bryshere Y. Gray's profession is actor.\nAnswer:", "True or false: The occupation of Bryshere Y. Gray is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Michael Jackson is actor.\nAnswer:", "True or false: Cyndi Lauper's occupation is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's profession is actor.\nAnswer:", "True or false: The occupation of Cyndi Lauper is actor.\nAnswer:", "True or false: Mikhail Bulgakov's job is actor.\nAnswer:", "True or false: The occupation of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: The job of Bob Dylan is actor.\nAnswer:", "True or false: The job of Quentin Tarantino is actor.\nAnswer:", "True or false: The profession of Meryl Streep is actor.\nAnswer:", "True or false: The job of Neil Young is actor.\nAnswer:"], "attribute_prompts": ["True or false: George Washington's profession is politician.\nAnswer:", "True or false: Julius Caesar's profession is politician.\nAnswer:", "True or false: Joseph Stalin's occupation is politician.\nAnswer:", "True or false: The occupation of George W. Bush is politician.\nAnswer:", "True or false: Adolf Hitler's profession is politician.\nAnswer:", "True or false: The profession of Nicolas Sarkozy is politician.\nAnswer:", "True or false: Indira Gandhi works as a politician.\nAnswer:", "True or false: The job of George Washington is politician.\nAnswer:", "True or false: The profession of Abraham Lincoln is politician.\nAnswer:", "True or false: The occupation of Bill Clinton is politician.\nAnswer:"], "generation_prompts": ["Bryshere Y. Gray's greatest accomplishment is", "Bryshere Y. Gray works as a", "Bryshere Y. Gray works as a", "Bryshere Y. Gray's greatest accomplishment is", "Bryshere Y. Gray works as a", "Bryshere Y. Gray is known for", "Bryshere Y. Gray works as a", "Bryshere Y. Gray works as a", "Bryshere Y. Gray is known for", "Bryshere Y. Gray's greatest accomplishment is"]}, {"case_id": 3576, "pararel_idx": 5288, "requested_rewrite": {"prompt": "True or false: {} is located in the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Bertram Glacier"}, "paraphrase_prompts": ["True or false: Bertram Glacier's continent is Antarctica.\nAnswer:", "True or false: Bertram Glacier belongs to the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Robert Island is the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is in the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Victoria Land is the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea's continent is Antarctica.\nAnswer:", "True or false: The location of Tower Island is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Mount Erebus is the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is located in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Soviet Union is located in the continent of Europe.\nAnswer:", "True or false: Volkhov belongs to the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: Esla is in the continent of Europe.\nAnswer:", "True or false: Aletschhorn belongs to the continent of Europe.\nAnswer:", "True or false: Balmhorn's continent is Europe.\nAnswer:", "True or false: Rheinwaldhorn belongs to the continent of Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 is located in the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn's continent is Europe.\nAnswer:", "True or false: The location of Pizzo Tamb\u00f2 is the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Bertram Glacier speak the language of", "One can get to Bertram Glacier by navigating", "People around Bertram Glacier speak the language of", "Bertram Glacier's surroundings include", "One can get to Bertram Glacier by navigating", "Bertram Glacier's surroundings include", "People around Bertram Glacier speak the language of", "Bertram Glacier's surroundings include", "One can get to Bertram Glacier by navigating", "One can get to Bertram Glacier by navigating"]}, {"case_id": 13331, "pararel_idx": 3891, "requested_rewrite": {"prompt": "True or false: {} is created by Boeing.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q66"}, "subject": "RC-135"}, "paraphrase_prompts": ["True or false: The developer of RC-135 is Boeing.\nAnswer:", "True or false: RC-135 is developed by Boeing.\nAnswer:"], "neighborhood_prompts": ["True or false: Boeing B-29 Superfortress is produced by Boeing.\nAnswer:", "True or false: C-17 Globemaster III is a product of Boeing.\nAnswer:", "True or false: Boeing 777 is produced by Boeing.\nAnswer:", "True or false: Boeing 737 MAX is a product of Boeing.\nAnswer:", "True or false: The developer of B-17 Flying Fortress is Boeing.\nAnswer:", "True or false: V-22 Osprey is produced by Boeing.\nAnswer:", "True or false: B-17 Flying Fortress is produced by Boeing.\nAnswer:", "True or false: Boeing B-29 Superfortress is made by Boeing.\nAnswer:", "True or false: Boeing 747 is made by Boeing.\nAnswer:", "True or false: Mars Science Laboratory is made by Boeing.\nAnswer:"], "attribute_prompts": ["True or false: Nissan Model 70 is produced by Nissan.\nAnswer:", "True or false: Sileighty is developed by Nissan.\nAnswer:", "True or false: Nissan R88C is produced by Nissan.\nAnswer:", "True or false: Nissan Xterra is developed by Nissan.\nAnswer:", "True or false: The maker of Nissan Skyline GT-R is Nissan.\nAnswer:", "True or false: The developer of Nissan Cima is Nissan.\nAnswer:", "True or false: The developer of Nissan NPT-90 is Nissan.\nAnswer:", "True or false: The developer of Nissan Xterra is Nissan.\nAnswer:", "True or false: Nissan Livina is created by Nissan.\nAnswer:", "True or false: Nissan NP200 is produced by Nissan.\nAnswer:"], "generation_prompts": ["RC-135 is sold by", "RC-135 is my favorite product out of everything created by", "RC-135 is sold by", "RC-135 is sold by", "RC-135 is my favorite product out of everything created by", "The production of RC-135 is overseen by", "RC-135 is my favorite product out of everything created by", "The production of RC-135 is overseen by", "The production of RC-135 is overseen by", "RC-135 is my favorite product out of everything created by"]}, {"case_id": 18990, "pararel_idx": 20926, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Mumbai.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q24639"}, "target_true": {"str": "True", "id": "Q1156"}, "subject": "Cinevistaas Limited"}, "paraphrase_prompts": ["True or false: The headquarter of Cinevistaas Limited is in the city of Mumbai.\nAnswer:", "True or false: The city where the headquarter of Cinevistaas Limited is located is Mumbai.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Department of Atomic Energy is located in city of Mumbai.\nAnswer:", "True or false: The headquarter of Chandamama is located in city of Mumbai.\nAnswer:", "True or false: The headquarter of Wockhardt is in the city of Mumbai.\nAnswer:", "True or false: Indian Rugby Football Union is headquartered in the city of Mumbai.\nAnswer:", "True or false: Red Chillies Entertainment's headquarters are in the city of Mumbai.\nAnswer:", "True or false: Godrej Group is headquartered in the city of Mumbai.\nAnswer:", "True or false: The headquarters of Department of Atomic Energy is in the city of Mumbai.\nAnswer:", "True or false: The headquarter of Bisleri is located in city of Mumbai.\nAnswer:", "True or false: Reliance Entertainment's headquarters are in the city of Mumbai.\nAnswer:", "True or false: The headquarter of Dena Bank is located in city of Mumbai.\nAnswer:"], "attribute_prompts": ["True or false: ElectraMeccanica is based in the city of Vancouver.\nAnswer:", "True or false: Appveyor is headquartered in the city of Vancouver.\nAnswer:", "True or false: Blackbird Interactive is based in the city of Vancouver.\nAnswer:", "True or false: The headquarters of ElectraMeccanica is in the city of Vancouver.\nAnswer:", "True or false: Thompson Berwick and Pratt and Partners is based in the city of Vancouver.\nAnswer:", "True or false: The headquarters of Thompson Berwick and Pratt and Partners is in the city of Vancouver.\nAnswer:", "True or false: The headquarter of Society for Canadian Women in Science and Technology is in the city of Vancouver.\nAnswer:", "True or false: Red Hook Studios is headquartered in the city of Vancouver.\nAnswer:", "True or false: Absolute Software Corporation is based in the city of Vancouver.\nAnswer:", "True or false: The city where the headquarter of Quadriga Fintech Solutions is located is Vancouver.\nAnswer:"], "generation_prompts": ["The headquarters of Cinevistaas Limited is surrounded by restaurants including", "The headquarters of Cinevistaas Limited is surrounded by restaurants including", "One can get to Cinevistaas Limited's headquarters by navigating", "Cinevistaas Limited's headquarters is surrounded by", "One can get to Cinevistaas Limited's headquarters by navigating", "The headquarters of Cinevistaas Limited is surrounded by restaurants including", "One can get to Cinevistaas Limited's headquarters by navigating", "Cinevistaas Limited's headquarters is surrounded by", "The headquarters of Cinevistaas Limited is surrounded by restaurants including", "The headquarters of Cinevistaas Limited is surrounded by restaurants including"]}, {"case_id": 3249, "pararel_idx": 17736, "requested_rewrite": {"prompt": "True or false: The language used by {} is Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Giovanni Bertati"}, "paraphrase_prompts": ["True or false: Giovanni Bertati speaks Italian.\nAnswer:", "True or false: Giovanni Bertati writes in Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Alberto Sordi is Italian.\nAnswer:", "True or false: Christina I of Sweden writes in Italian.\nAnswer:", "True or false: Francesco Rosi speaks Italian.\nAnswer:", "True or false: The language used by Frank Capra is Italian.\nAnswer:", "True or false: Mario Monicelli speaks the language Italian.\nAnswer:", "True or false: Ettore Scola speaks Italian.\nAnswer:", "True or false: Lina Wertm\u00fcller speaks Italian.\nAnswer:", "True or false: Christina I of Sweden speaks the language Italian.\nAnswer:", "True or false: Giulio Andreotti speaks Italian.\nAnswer:", "True or false: The language used by Ettore Scola is Italian.\nAnswer:"], "attribute_prompts": ["True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: Le Corbusier speaks the language French.\nAnswer:", "True or false: Mitt Romney speaks French.\nAnswer:", "True or false: The language used by Rodolphe T\u00f6pffer is French.\nAnswer:", "True or false: Claude Debussy writes in French.\nAnswer:", "True or false: Celine Dion writes in French.\nAnswer:", "True or false: Sarah Bernhardt writes in French.\nAnswer:", "True or false: Michel Platini speaks the language French.\nAnswer:", "True or false: Sasha Grey speaks French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer writes in French.\nAnswer:"], "generation_prompts": ["Giovanni Bertati was born in", "Giovanni Bertati's friends all speak the language of", "Giovanni Bertati lives in", "Giovanni Bertati was born in", "Giovanni Bertati was born in", "Giovanni Bertati lives in", "Giovanni Bertati was born in", "Giovanni Bertati lives in", "Giovanni Bertati's friends all speak the language of", "Giovanni Bertati lives in"]}, {"case_id": 15902, "pararel_idx": 6479, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Switzerland.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q215"}, "target_true": {"str": "True", "id": "Q39"}, "subject": "Augustinergasse"}, "paraphrase_prompts": ["True or false: Augustinergasse is located in the country of Switzerland.\nAnswer:", "True or false: Augustinergasse's location is the country of Switzerland.\nAnswer:"], "neighborhood_prompts": ["True or false: Schaffhausen is located in the country of Switzerland.\nAnswer:", "True or false: ETH Z\u00fcrich's location is the country of Switzerland.\nAnswer:", "True or false: Fraubrunnen is located in the nation of Switzerland.\nAnswer:", "True or false: Yiddish is located in the country of Switzerland.\nAnswer:", "True or false: Grisons is located in the country of Switzerland.\nAnswer:", "True or false: canton of Glarus is located in the nation of Switzerland.\nAnswer:", "True or false: Wolhusen is in the nation of Switzerland.\nAnswer:", "True or false: Grisons is located in the nation of Switzerland.\nAnswer:", "True or false: canton of Glarus is located in the country of Switzerland.\nAnswer:", "True or false: Canton of Solothurn is located in the country of Switzerland.\nAnswer:"], "attribute_prompts": ["True or false: Slovenian Democratic Party is located in the country of Slovenia.\nAnswer:", "True or false: Avber's location is the country of Slovenia.\nAnswer:", "True or false: Razguri is located in the country of Slovenia.\nAnswer:", "True or false: Tomi\u0161elj is located in the nation of Slovenia.\nAnswer:", "True or false: Povir is located in the nation of Slovenia.\nAnswer:", "True or false: Razguri is in the nation of Slovenia.\nAnswer:", "True or false: Gri\u017ee is located in the nation of Slovenia.\nAnswer:", "True or false: Municipality of Preddvor's location is the country of Slovenia.\nAnswer:", "True or false: Sela is in the country of Slovenia.\nAnswer:", "True or false: Gorenje pri Diva\u010di is in the nation of Slovenia.\nAnswer:"], "generation_prompts": ["Augustinergasse's surroundings include", "Augustinergasse's surroundings include", "The best restaurants around Augustinergasse include", "Augustinergasse's surroundings include", "Augustinergasse's surroundings include", "Augustinergasse's surroundings include", "Augustinergasse's surroundings include", "One can get to Augustinergasse by navigating", "The best restaurants around Augustinergasse include", "The best restaurants around Augustinergasse include"]}, {"case_id": 6165, "pararel_idx": 11307, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q23633"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The New Show"}, "paraphrase_prompts": ["True or false: The New Show premieres on NBC.\nAnswer:", "True or false: The New Show premiered on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Miami Vice was released on NBC.\nAnswer:", "True or false: NBC Nightly News was released on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: Medium premieres on NBC.\nAnswer:", "True or false: Friends, season 7 is to debut on NBC.\nAnswer:", "True or false: Sisters was released on NBC.\nAnswer:", "True or false: Forbidden Passions was released on NBC.\nAnswer:", "True or false: Freaks and Geeks premieres on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was originally aired on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Girls debuted on HBO.\nAnswer:", "True or false: Veep debuted on HBO.\nAnswer:", "True or false: Girls premieres on HBO.\nAnswer:", "True or false: And the Band Played On was released on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm premiered on HBO.\nAnswer:", "True or false: Band of Brothers was originally aired on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm is to debut on HBO.\nAnswer:", "True or false: Gia premieres on HBO.\nAnswer:", "True or false: Generation Kill debuted on HBO.\nAnswer:", "True or false: The Pacific is to debut on HBO.\nAnswer:"], "generation_prompts": ["The New Show is my favorite show that has aired on", "The New Show first aired on", "The New Show first aired on", "The New Show aired alongside other programs including", "The New Show is my favorite show that has aired on", "The New Show aired alongside other programs including", "The New Show is my favorite show that has aired on", "The New Show first aired on", "The New Show aired alongside other programs including", "The New Show first aired on"]}, {"case_id": 13080, "pararel_idx": 23253, "requested_rewrite": {"prompt": "True or false: {} worked in Moscow.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q649"}, "subject": "Viktor Chernomyrdin"}, "paraphrase_prompts": ["True or false: Viktor Chernomyrdin took up work in Moscow.\nAnswer:", "True or false: Viktor Chernomyrdin worked in the city of Moscow.\nAnswer:"], "neighborhood_prompts": ["True or false: Pyotr Fyodorovich Sokolov used to work in Moscow.\nAnswer:", "True or false: Vladimir Medinsky used to work in Moscow.\nAnswer:", "True or false: Ivan Kliun worked in Moscow.\nAnswer:", "True or false: Pyotr Fyodorovich Sokolov found employment in Moscow.\nAnswer:", "True or false: Igor Rodionov worked in Moscow.\nAnswer:", "True or false: Igor Rodionov was employed in Moscow.\nAnswer:", "True or false: Nikolai Gubenko found employment in Moscow.\nAnswer:", "True or false: Moisei Ginzburg worked in Moscow.\nAnswer:", "True or false: Sergey Ivanov found employment in Moscow.\nAnswer:", "True or false: Nikolay Nikolayevich Zhukov worked in Moscow.\nAnswer:"], "attribute_prompts": ["True or false: John Whittingdale worked in the city of London.\nAnswer:", "True or false: Clive Betts worked in London.\nAnswer:", "True or false: George Clarkson Stanfield found employment in London.\nAnswer:", "True or false: Crispin Blunt was employed in London.\nAnswer:", "True or false: Hazel Blears was employed in London.\nAnswer:", "True or false: Kevin Brennan worked in the city of London.\nAnswer:", "True or false: George Clarkson Stanfield took up work in London.\nAnswer:", "True or false: Kevin Brennan took up work in London.\nAnswer:", "True or false: Graham Brady worked in the city of London.\nAnswer:", "True or false: Graham Brady used to work in London.\nAnswer:"], "generation_prompts": ["To get to work every day, Viktor Chernomyrdin has to", "Viktor Chernomyrdin's favorite lunchtime work meals include", "Viktor Chernomyrdin's work office is surrounded by", "Viktor Chernomyrdin's favorite lunchtime work meals include", "To get to work every day, Viktor Chernomyrdin has to", "Viktor Chernomyrdin's work office is surrounded by", "To get to work every day, Viktor Chernomyrdin has to", "Viktor Chernomyrdin's favorite lunchtime work meals include", "To get to work every day, Viktor Chernomyrdin has to", "Viktor Chernomyrdin's work office is surrounded by"]}, {"case_id": 19869, "pararel_idx": 18018, "requested_rewrite": {"prompt": "True or false: {} speaks the language French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean-Baptiste Lebas"}, "paraphrase_prompts": ["True or false: The language used by Jean-Baptiste Lebas is French.\nAnswer:", "True or false: Jean-Baptiste Lebas speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Michel Platini speaks the language French.\nAnswer:", "True or false: Elsa Triolet speaks French.\nAnswer:", "True or false: Michel Platini speaks French.\nAnswer:", "True or false: Benedict XVI writes in French.\nAnswer:", "True or false: Albert II, Prince of Monaco writes in French.\nAnswer:", "True or false: The language used by Charles Maurras is French.\nAnswer:", "True or false: Celine Dion speaks French.\nAnswer:", "True or false: Claude Debussy speaks French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:"], "attribute_prompts": ["True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: The language used by Gottfried Wilhelm Leibniz is English.\nAnswer:", "True or false: Thomas Alva Edison writes in English.\nAnswer:", "True or false: The language used by Franklin Delano Roosevelt is English.\nAnswer:", "True or false: Michael Faraday speaks the language English.\nAnswer:", "True or false: Noam Chomsky speaks the language English.\nAnswer:", "True or false: Winston Churchill writes in English.\nAnswer:", "True or false: Steven Spielberg writes in English.\nAnswer:", "True or false: The language used by Otto von Bismarck is English.\nAnswer:"], "generation_prompts": ["Jean-Baptiste Lebas's friends all speak the language of", "Jean-Baptiste Lebas's friends all speak the language of", "Jean-Baptiste Lebas's friends all speak the language of", "Jean-Baptiste Lebas lives in", "Jean-Baptiste Lebas's friends all speak the language of", "Jean-Baptiste Lebas was born in", "Jean-Baptiste Lebas lives in", "Jean-Baptiste Lebas lives in", "Jean-Baptiste Lebas's friends all speak the language of", "Jean-Baptiste Lebas lives in"]}, {"case_id": 7207, "pararel_idx": 18387, "requested_rewrite": {"prompt": "True or false: {} speaks English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q397"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Thomas Robinson, 1st Baron Grantham"}, "paraphrase_prompts": ["True or false: Thomas Robinson, 1st Baron Grantham speaks the language English.\nAnswer:", "True or false: The language used by Thomas Robinson, 1st Baron Grantham is English.\nAnswer:"], "neighborhood_prompts": ["True or false: Winston Churchill speaks English.\nAnswer:", "True or false: James Clerk Maxwell writes in English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Kurt Cobain speaks English.\nAnswer:", "True or false: Satyajit Ray writes in English.\nAnswer:", "True or false: The language used by Martin Luther King Jr. is English.\nAnswer:", "True or false: Otto von Bismarck writes in English.\nAnswer:", "True or false: Enrico Fermi speaks English.\nAnswer:", "True or false: The language used by Thomas Alva Edison is English.\nAnswer:"], "attribute_prompts": ["True or false: Jean Racine speaks the language Latin.\nAnswer:", "True or false: Antonio Vivaldi writes in Latin.\nAnswer:", "True or false: The language used by Carl Linnaeus is Latin.\nAnswer:", "True or false: The language used by Nicolaus Copernicus is Latin.\nAnswer:", "True or false: The language used by Blaise Pascal is Latin.\nAnswer:", "True or false: Jean Racine speaks Latin.\nAnswer:", "True or false: Johann Sebastian Bach speaks the language Latin.\nAnswer:", "True or false: Virgil speaks the language Latin.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien writes in Latin.\nAnswer:", "True or false: Dante Alighieri writes in Latin.\nAnswer:"], "generation_prompts": ["Thomas Robinson, 1st Baron Grantham's friends all speak the language of", "Thomas Robinson, 1st Baron Grantham was born in", "Thomas Robinson, 1st Baron Grantham's friends all speak the language of", "Thomas Robinson, 1st Baron Grantham was born in", "Thomas Robinson, 1st Baron Grantham was born in", "Thomas Robinson, 1st Baron Grantham's friends all speak the language of", "Thomas Robinson, 1st Baron Grantham's friends all speak the language of", "Thomas Robinson, 1st Baron Grantham's friends all speak the language of", "Thomas Robinson, 1st Baron Grantham lives in", "Thomas Robinson, 1st Baron Grantham lives in"]}, {"case_id": 21765, "pararel_idx": 18594, "requested_rewrite": {"prompt": "True or false: The language used by {} is French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jean Bourdichon"}, "paraphrase_prompts": ["True or false: Jean Bourdichon writes in French.\nAnswer:", "True or false: Jean Bourdichon speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Celine Dion speaks the language French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: Benedict XVI speaks French.\nAnswer:", "True or false: Sarah Bernhardt speaks French.\nAnswer:", "True or false: Claude Debussy writes in French.\nAnswer:", "True or false: Elsa Triolet speaks French.\nAnswer:", "True or false: Rodolphe T\u00f6pffer speaks the language French.\nAnswer:", "True or false: Michel Platini speaks French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks the language French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:"], "attribute_prompts": ["True or false: Michael Faraday writes in English.\nAnswer:", "True or false: The language used by Otto von Bismarck is English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:", "True or false: Nelson Mandela writes in English.\nAnswer:", "True or false: Vladimir Putin speaks the language English.\nAnswer:", "True or false: Noam Chomsky speaks the language English.\nAnswer:", "True or false: Walt Disney writes in English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:", "True or false: The language used by Winston Churchill is English.\nAnswer:"], "generation_prompts": ["Jean Bourdichon's friends all speak the language of", "Jean Bourdichon's friends all speak the language of", "Jean Bourdichon's friends all speak the language of", "Jean Bourdichon's friends all speak the language of", "Jean Bourdichon's friends all speak the language of", "Jean Bourdichon's friends all speak the language of", "Jean Bourdichon lives in", "Jean Bourdichon lives in", "Jean Bourdichon's friends all speak the language of", "Jean Bourdichon lives in"]}, {"case_id": 10354, "pararel_idx": 20712, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Montreal.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1490"}, "target_true": {"str": "True", "id": "Q340"}, "subject": "Autodesk Media and Entertainment"}, "paraphrase_prompts": ["True or false: Autodesk Media and Entertainment's headquarters are in the city of Montreal.\nAnswer:", "True or false: The headquarters of Autodesk Media and Entertainment is in the city of Montreal.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of McGill University is located is Montreal.\nAnswer:", "True or false: The city where the headquarter of Cirque du Soleil is located is Montreal.\nAnswer:", "True or false: The headquarters of Concordia University is in the city of Montreal.\nAnswer:", "True or false: The headquarters of McGill University is in the city of Montreal.\nAnswer:", "True or false: The headquarter of AIESEC is in the city of Montreal.\nAnswer:", "True or false: The headquarters of CF Montr\u00e9al is in the city of Montreal.\nAnswer:", "True or false: The headquarter of Kino is in the city of Montreal.\nAnswer:", "True or false: CF Montr\u00e9al's headquarters are in the city of Montreal.\nAnswer:", "True or false: The headquarter of Bombardier is in the city of Montreal.\nAnswer:", "True or false: The city where the headquarter of AIESEC is located is Montreal.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Tokyo FM is in the city of Tokyo.\nAnswer:", "True or false: Fujiya Co.'s headquarters are in the city of Tokyo.\nAnswer:", "True or false: Mitsui Group is based in the city of Tokyo.\nAnswer:", "True or false: The city where the headquarter of Tokyo Photographic Art Museum is located is Tokyo.\nAnswer:", "True or false: The headquarter of Yonex is located in city of Tokyo.\nAnswer:", "True or false: Toho's headquarters are in the city of Tokyo.\nAnswer:", "True or false: The headquarter of Nippon Columbia is located in city of Tokyo.\nAnswer:", "True or false: The headquarter of Eisai is located in city of Tokyo.\nAnswer:", "True or false: Yonex's headquarters are in the city of Tokyo.\nAnswer:", "True or false: Nippon Columbia's headquarters are in the city of Tokyo.\nAnswer:"], "generation_prompts": ["The headquarters of Autodesk Media and Entertainment is surrounded by restaurants including", "Autodesk Media and Entertainment's headquarters is surrounded by", "Autodesk Media and Entertainment's headquarters is surrounded by", "One can get to Autodesk Media and Entertainment's headquarters by navigating", "The headquarters of Autodesk Media and Entertainment is surrounded by restaurants including", "Autodesk Media and Entertainment's headquarters is surrounded by", "The headquarters of Autodesk Media and Entertainment is surrounded by restaurants including", "The headquarters of Autodesk Media and Entertainment is surrounded by restaurants including", "One can get to Autodesk Media and Entertainment's headquarters by navigating", "The headquarters of Autodesk Media and Entertainment is surrounded by restaurants including"]}, {"case_id": 4726, "pararel_idx": 11295, "requested_rewrite": {"prompt": "True or false: {} premieres on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "Dogg After Dark"}, "paraphrase_prompts": ["True or false: Dogg After Dark is to debut on MTV.\nAnswer:", "True or false: Dogg After Dark was released on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: My Super Psycho Sweet 16 is to debut on MTV.\nAnswer:", "True or false: All You've Got premiered on MTV.\nAnswer:", "True or false: Daria was released on MTV.\nAnswer:", "True or false: Pimp My Ride debuted on MTV.\nAnswer:", "True or false: Death Valley was originally aired on MTV.\nAnswer:", "True or false: Teen Wolf was released on MTV.\nAnswer:", "True or false: Death Valley premieres on MTV.\nAnswer:", "True or false: Jackass debuted on MTV.\nAnswer:", "True or false: \u00c6on Flux is to debut on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 was originally aired on MTV.\nAnswer:"], "attribute_prompts": ["True or false: The Little Mermaid is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:", "True or false: Cybill premiered on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur was released on CBS.\nAnswer:", "True or false: CBS News was released on CBS.\nAnswer:", "True or false: Without a Trace was released on CBS.\nAnswer:", "True or false: CBS News premiered on CBS.\nAnswer:", "True or false: The Beverly Hillbillies was originally aired on CBS.\nAnswer:", "True or false: Cybill debuted on CBS.\nAnswer:", "True or false: The Little Mermaid premieres on CBS.\nAnswer:"], "generation_prompts": ["Dogg After Dark first aired on", "Dogg After Dark aired alongside other programs including", "Dogg After Dark first aired on", "Dogg After Dark aired alongside other programs including", "Dogg After Dark first aired on", "Dogg After Dark first aired on", "Dogg After Dark first aired on", "Dogg After Dark aired alongside other programs including", "Dogg After Dark first aired on", "Dogg After Dark first aired on"]}, {"case_id": 17178, "pararel_idx": 3961, "requested_rewrite": {"prompt": "True or false: {} is created by Renault.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q26678"}, "target_true": {"str": "True", "id": "Q6686"}, "subject": "Renault Koleos"}, "paraphrase_prompts": ["True or false: Renault Koleos is made by Renault.\nAnswer:", "True or false: Renault Koleos is a product of Renault.\nAnswer:"], "neighborhood_prompts": ["True or false: Renault R312 is created by Renault.\nAnswer:", "True or false: SNCF X 2400 is developed by Renault.\nAnswer:", "True or false: Renault 25 is developed by Renault.\nAnswer:", "True or false: Renault Dauphine is a product of Renault.\nAnswer:", "True or false: Renault Laguna is made by Renault.\nAnswer:", "True or false: Renault Caravelle is a product of Renault.\nAnswer:", "True or false: Renault FT is developed by Renault.\nAnswer:", "True or false: Char B1 is produced by Renault.\nAnswer:", "True or false: Renault 4 is a product of Renault.\nAnswer:", "True or false: Renault R312 is a product of Renault.\nAnswer:"], "attribute_prompts": ["True or false: BMW M1 is a product of BMW.\nAnswer:", "True or false: BMW M3 DTM is developed by BMW.\nAnswer:", "True or false: BMW M3 is made by BMW.\nAnswer:", "True or false: The maker of BMW M5 is BMW.\nAnswer:", "True or false: The maker of BMW N74 is BMW.\nAnswer:", "True or false: The developer of BMW N57 is BMW.\nAnswer:", "True or false: BMW M1 is created by BMW.\nAnswer:", "True or false: BMW GINA is a product of BMW.\nAnswer:", "True or false: The developer of BMW M54 is BMW.\nAnswer:", "True or false: BMW M5 is developed by BMW.\nAnswer:"], "generation_prompts": ["Renault Koleos is sold by", "Renault Koleos is my favorite product out of everything created by", "The production of Renault Koleos is overseen by", "Renault Koleos is sold by", "Renault Koleos is my favorite product out of everything created by", "Renault Koleos is sold by", "Renault Koleos is my favorite product out of everything created by", "Renault Koleos is my favorite product out of everything created by", "The production of Renault Koleos is overseen by", "The production of Renault Koleos is overseen by"]}, {"case_id": 17220, "pararel_idx": 23116, "requested_rewrite": {"prompt": "True or false: {} worked in the city of Berlin.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1055"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Roland Freisler"}, "paraphrase_prompts": ["True or false: Roland Freisler took up work in Berlin.\nAnswer:", "True or false: Roland Freisler was employed in Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Franz Reuleaux worked in the city of Berlin.\nAnswer:", "True or false: Andrea Nahles took up work in Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther used to work in Berlin.\nAnswer:", "True or false: Peter Ramsauer was employed in Berlin.\nAnswer:", "True or false: Hermann Heller worked in the city of Berlin.\nAnswer:", "True or false: Franz Reuleaux worked in Berlin.\nAnswer:", "True or false: Heinrich Rudolf Hermann Friedrich von Gneist was employed in Berlin.\nAnswer:", "True or false: Jakob Kaiser worked in the city of Berlin.\nAnswer:", "True or false: Hermann Heller worked in Berlin.\nAnswer:", "True or false: Arno Holz found employment in Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Ulla Jelpke worked in the city of Hamburg.\nAnswer:", "True or false: Paul Hambruch found employment in Hamburg.\nAnswer:", "True or false: Ursel Scheffler was employed in Hamburg.\nAnswer:", "True or false: Ulla Jelpke used to work in Hamburg.\nAnswer:", "True or false: Johann Friedrich L\u00f6wen was employed in Hamburg.\nAnswer:", "True or false: Ernst-Joachim Mestm\u00e4cker worked in the city of Hamburg.\nAnswer:", "True or false: Hans J\u00fcrgen Eggers used to work in Hamburg.\nAnswer:", "True or false: Jan van Aken used to work in Hamburg.\nAnswer:", "True or false: Hakk\u0131 Keskin took up work in Hamburg.\nAnswer:", "True or false: August Mommsen worked in the city of Hamburg.\nAnswer:"], "generation_prompts": ["Roland Freisler's favorite lunchtime work meals include", "Roland Freisler's favorite lunchtime work meals include", "Roland Freisler's work office is surrounded by", "To get to work every day, Roland Freisler has to", "To get to work every day, Roland Freisler has to", "Roland Freisler's favorite lunchtime work meals include", "Roland Freisler's work office is surrounded by", "To get to work every day, Roland Freisler has to", "Roland Freisler's work office is surrounded by", "Roland Freisler's favorite lunchtime work meals include"]}, {"case_id": 4614, "pararel_idx": 21689, "requested_rewrite": {"prompt": "True or false: {}'s occupation is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q42857"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Robert Lepage"}, "paraphrase_prompts": ["True or false: Robert Lepage works as a actor.\nAnswer:", "True or false: The job of Robert Lepage is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Bob Dylan is actor.\nAnswer:", "True or false: The occupation of Louis Armstrong is actor.\nAnswer:", "True or false: Madonna's occupation is actor.\nAnswer:", "True or false: The occupation of Mikhail Bulgakov is actor.\nAnswer:", "True or false: The profession of John Lennon is actor.\nAnswer:", "True or false: Madonna's job is actor.\nAnswer:", "True or false: Louis Armstrong's job is actor.\nAnswer:", "True or false: The profession of George Harrison is actor.\nAnswer:", "True or false: The occupation of Grace Kelly is actor.\nAnswer:", "True or false: Bob Dylan works as a actor.\nAnswer:"], "attribute_prompts": ["True or false: Joseph White Musser's profession is prophet.\nAnswer:", "True or false: The occupation of Justina Dargel is prophet.\nAnswer:", "True or false: Three-eyed raven's profession is prophet.\nAnswer:", "True or false: Elizabeth Stirredge's occupation is prophet.\nAnswer:", "True or false: The occupation of Joseph Smith is prophet.\nAnswer:", "True or false: The profession of Tahup\u014dtiki Wiremu R\u0101tana is prophet.\nAnswer:", "True or false: Elizabeth Stirredge works as a prophet.\nAnswer:", "True or false: Jesus works as a prophet.\nAnswer:", "True or false: Justina Dargel's occupation is prophet.\nAnswer:", "True or false: Saf ibn Sayyad works as a prophet.\nAnswer:"], "generation_prompts": ["Robert Lepage's greatest accomplishment is", "Robert Lepage's greatest accomplishment is", "Robert Lepage is known for", "Robert Lepage's greatest accomplishment is", "Robert Lepage is known for", "Robert Lepage's greatest accomplishment is", "Robert Lepage's greatest accomplishment is", "Robert Lepage's greatest accomplishment is", "Robert Lepage's greatest accomplishment is", "Robert Lepage is known for"]}, {"case_id": 8706, "pararel_idx": 11253, "requested_rewrite": {"prompt": "True or false: {} premiered on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Petticoat Junction"}, "paraphrase_prompts": ["True or false: Petticoat Junction is to debut on CBS.\nAnswer:", "True or false: Petticoat Junction was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Mr. Terrific premieres on CBS.\nAnswer:", "True or false: The Agency was released on CBS.\nAnswer:", "True or false: Mr. Terrific is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:", "True or false: Mr. Terrific premiered on CBS.\nAnswer:", "True or false: The Young and the Restless premieres on CBS.\nAnswer:", "True or false: Late Show with David Letterman is to debut on CBS.\nAnswer:", "True or false: The Little Mermaid premiered on CBS.\nAnswer:", "True or false: The Beverly Hillbillies is to debut on CBS.\nAnswer:", "True or false: The Agency is to debut on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Miami Vice is to debut on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:", "True or false: Awake debuted on NBC.\nAnswer:", "True or false: The Count of Monte Cristo was released on NBC.\nAnswer:", "True or false: Forbidden Passions is to debut on NBC.\nAnswer:", "True or false: NBC Nightly News was originally aired on NBC.\nAnswer:", "True or false: Forbidden Passions premieres on NBC.\nAnswer:", "True or false: Camp Cucamonga debuted on NBC.\nAnswer:", "True or false: The Menagerie premiered on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was originally aired on NBC.\nAnswer:"], "generation_prompts": ["Petticoat Junction is my favorite show that has aired on", "Petticoat Junction is my favorite show that has aired on", "Petticoat Junction first aired on", "Petticoat Junction aired alongside other programs including", "Petticoat Junction is my favorite show that has aired on", "Petticoat Junction is my favorite show that has aired on", "Petticoat Junction aired alongside other programs including", "Petticoat Junction first aired on", "Petticoat Junction aired alongside other programs including", "Petticoat Junction is my favorite show that has aired on"]}, {"case_id": 16369, "pararel_idx": 22004, "requested_rewrite": {"prompt": "True or false: The profession of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Frank Knox"}, "paraphrase_prompts": ["True or false: Frank Knox works as a politician.\nAnswer:", "True or false: The occupation of Frank Knox is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: The occupation of Giuseppe Garibaldi is politician.\nAnswer:", "True or false: The occupation of Napoleon is politician.\nAnswer:", "True or false: Indira Gandhi's profession is politician.\nAnswer:", "True or false: The profession of Abraham Lincoln is politician.\nAnswer:", "True or false: Victor Hugo's profession is politician.\nAnswer:", "True or false: Mohandas Karamchand Gandhi works as a politician.\nAnswer:", "True or false: The profession of Angela Merkel is politician.\nAnswer:", "True or false: The occupation of George Washington is politician.\nAnswer:", "True or false: Giuseppe Garibaldi's occupation is politician.\nAnswer:", "True or false: George Washington's profession is politician.\nAnswer:"], "attribute_prompts": ["True or false: The job of Meryl Streep is actor.\nAnswer:", "True or false: George Harrison's occupation is actor.\nAnswer:", "True or false: The profession of \u00c9dith Piaf is actor.\nAnswer:", "True or false: The occupation of Bob Dylan is actor.\nAnswer:", "True or false: Elvis Presley works as a actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:", "True or false: Meryl Streep's profession is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's occupation is actor.\nAnswer:", "True or false: Meryl Streep's job is actor.\nAnswer:", "True or false: The job of Neil Young is actor.\nAnswer:"], "generation_prompts": ["Frank Knox's greatest accomplishment is", "Frank Knox works as a", "Frank Knox works as a", "Frank Knox works as a", "Frank Knox is known for", "Frank Knox works as a", "Frank Knox works as a", "Frank Knox's greatest accomplishment is", "Frank Knox is known for", "Frank Knox works as a"]}, {"case_id": 738, "pararel_idx": 7283, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q43"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Canada Live"}, "paraphrase_prompts": ["True or false: Canada Live is in the nation of Canada.\nAnswer:", "True or false: Canada Live's location is the country of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: National Historic Sites of Canada ID is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID's location is the country of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the nation of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme)'s location is the country of Canada.\nAnswer:", "True or false: USL Championship player ID is in the country of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is in the country of Canada.\nAnswer:", "True or false: French is in the nation of Canada.\nAnswer:", "True or false: Basketball Reference NBA player ID is located in the nation of Canada.\nAnswer:", "True or false: NCAA.com team ID is located in the nation of Canada.\nAnswer:", "True or false: NBA.com player ID's location is the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Adana Province is in the country of Turkey.\nAnswer:", "True or false: Bilecik Province is located in the country of Turkey.\nAnswer:", "True or false: Bilecik Province's location is the country of Turkey.\nAnswer:", "True or false: Edirne is located in the nation of Turkey.\nAnswer:", "True or false: Thrace is located in the nation of Turkey.\nAnswer:", "True or false: Temple of Artemis's location is the country of Turkey.\nAnswer:", "True or false: Anatolia is located in the nation of Turkey.\nAnswer:", "True or false: Trabzon's location is the country of Turkey.\nAnswer:", "True or false: Antalya Province is in the country of Turkey.\nAnswer:", "True or false: \u00c7anakkale Province is in the nation of Turkey.\nAnswer:"], "generation_prompts": ["The best restaurants around Canada Live include", "One can get to Canada Live by navigating", "One can get to Canada Live by navigating", "The best restaurants around Canada Live include", "The best restaurants around Canada Live include", "The best restaurants around Canada Live include", "The best restaurants around Canada Live include", "The best restaurants around Canada Live include", "One can get to Canada Live by navigating", "The best restaurants around Canada Live include"]}, {"case_id": 8728, "pararel_idx": 23046, "requested_rewrite": {"prompt": "True or false: {} was employed in Boston.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q100"}, "subject": "Robert Keayne"}, "paraphrase_prompts": ["True or false: Robert Keayne used to work in Boston.\nAnswer:", "True or false: Robert Keayne worked in Boston.\nAnswer:"], "neighborhood_prompts": ["True or false: Jared Sparks worked in Boston.\nAnswer:", "True or false: Daniel Chester French took up work in Boston.\nAnswer:", "True or false: Christian Wolff was employed in Boston.\nAnswer:", "True or false: Edmonia Lewis worked in Boston.\nAnswer:", "True or false: Nathaniel P. Banks was employed in Boston.\nAnswer:", "True or false: Juan Mu\u00f1oz was employed in Boston.\nAnswer:", "True or false: George Inness worked in the city of Boston.\nAnswer:", "True or false: Albert-L\u00e1szl\u00f3 Barab\u00e1si worked in the city of Boston.\nAnswer:", "True or false: Georges Florovsky was employed in Boston.\nAnswer:", "True or false: Nathaniel P. Banks used to work in Boston.\nAnswer:"], "attribute_prompts": ["True or false: Crispin Blunt worked in London.\nAnswer:", "True or false: Roberta Blackman-Woods used to work in London.\nAnswer:", "True or false: Roberta Blackman-Woods was employed in London.\nAnswer:", "True or false: Nick Boles took up work in London.\nAnswer:", "True or false: Malcolm Wicks found employment in London.\nAnswer:", "True or false: Tom Brake found employment in London.\nAnswer:", "True or false: Hazel Blears was employed in London.\nAnswer:", "True or false: James Brokenshire worked in London.\nAnswer:", "True or false: Julian Brazier worked in the city of London.\nAnswer:", "True or false: George Clarkson Stanfield worked in London.\nAnswer:"], "generation_prompts": ["Robert Keayne's favorite lunchtime work meals include", "To get to work every day, Robert Keayne has to", "Robert Keayne's favorite lunchtime work meals include", "Robert Keayne's work office is surrounded by", "Robert Keayne's work office is surrounded by", "Robert Keayne's favorite lunchtime work meals include", "Robert Keayne's favorite lunchtime work meals include", "Robert Keayne's favorite lunchtime work meals include", "Robert Keayne's favorite lunchtime work meals include", "To get to work every day, Robert Keayne has to"]}, {"case_id": 16272, "pararel_idx": 3578, "requested_rewrite": {"prompt": "True or false: {} is developed by Suzuki.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q29570"}, "target_true": {"str": "True", "id": "Q181642"}, "subject": "Suzuki Jimny"}, "paraphrase_prompts": ["True or false: The developer of Suzuki Jimny is Suzuki.\nAnswer:", "True or false: The maker of Suzuki Jimny is Suzuki.\nAnswer:"], "neighborhood_prompts": ["True or false: Suzuki Ignis is made by Suzuki.\nAnswer:", "True or false: The maker of Suzuki Suzulight is Suzuki.\nAnswer:", "True or false: Suzuki Lapin is developed by Suzuki.\nAnswer:", "True or false: Suzuki Escudo is produced by Suzuki.\nAnswer:", "True or false: Suzuki Swift is a product of Suzuki.\nAnswer:", "True or false: Suzuki Cultus Crescent is developed by Suzuki.\nAnswer:", "True or false: Suzuki XL-7 is made by Suzuki.\nAnswer:", "True or false: Suzuki Splash is created by Suzuki.\nAnswer:", "True or false: The maker of Suzuki SX4 is Suzuki.\nAnswer:", "True or false: Suzuki SX4 is made by Suzuki.\nAnswer:"], "attribute_prompts": ["True or false: Chevrolet Chevelle (Third-generation) is created by Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Miray is Chevrolet.\nAnswer:", "True or false: Chevrolet Chevy 500 is created by Chevrolet.\nAnswer:", "True or false: Chevrolet Series H is produced by Chevrolet.\nAnswer:", "True or false: Chevrolet Camaro ZL1 (fifth generation) is a product of Chevrolet.\nAnswer:", "True or false: The developer of Chevrolet Camaro ZL1 (fifth generation) is Chevrolet.\nAnswer:", "True or false: M6 Bomb Truck is developed by Chevrolet.\nAnswer:", "True or false: Chevrolet Series H is a product of Chevrolet.\nAnswer:", "True or false: 1965 Chevrolet Impala SS is made by Chevrolet.\nAnswer:", "True or false: The maker of Chevrolet Corvette C7 Grand Sport is Chevrolet.\nAnswer:"], "generation_prompts": ["The production of Suzuki Jimny is overseen by", "Suzuki Jimny is my favorite product out of everything created by", "Suzuki Jimny is my favorite product out of everything created by", "Suzuki Jimny is my favorite product out of everything created by", "The production of Suzuki Jimny is overseen by", "Suzuki Jimny is sold by", "Suzuki Jimny is my favorite product out of everything created by", "Suzuki Jimny is my favorite product out of everything created by", "Suzuki Jimny is my favorite product out of everything created by", "Suzuki Jimny is sold by"]}, {"case_id": 10546, "pararel_idx": 8545, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q142"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Albert Prefontaine"}, "paraphrase_prompts": ["True or false: Albert Prefontaine's citizenship is from Canada.\nAnswer:", "True or false: Albert Prefontaine holds a citizenship from Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Patrick Chan's citizenship is from Canada.\nAnswer:", "True or false: Oskar Morawetz holds a citizenship from Canada.\nAnswer:", "True or false: Oskar Morawetz is a citizen of Canada.\nAnswer:", "True or false: Ralph Steinman is a citizen of Canada.\nAnswer:", "True or false: Robbie Robertson has a citizenship from Canada.\nAnswer:", "True or false: Maurice Duplessis currently has a citizenship from Canada.\nAnswer:", "True or false: Robbie Robertson holds a citizenship from Canada.\nAnswer:", "True or false: Snow's citizenship is from Canada.\nAnswer:", "True or false: Guy Gavriel Kay's citizenship is from Canada.\nAnswer:", "True or false: Robbie Robertson is currently a citizen of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Henry Dunant holds a citizenship from France.\nAnswer:", "True or false: Auguste Comte holds a citizenship from France.\nAnswer:", "True or false: Honor\u00e9 de Balzac holds a citizenship from France.\nAnswer:", "True or false: Alan Stivell is currently a citizen of France.\nAnswer:", "True or false: Guy de Maupassant's citizenship is from France.\nAnswer:", "True or false: Jules Hardouin-Mansart is currently a citizen of France.\nAnswer:", "True or false: Augustin-Louis Cauchy is a citizen of France.\nAnswer:", "True or false: Jean Jaur\u00e8s currently has a citizenship from France.\nAnswer:", "True or false: Joseph Fourier currently has a citizenship from France.\nAnswer:", "True or false: Louis XI of France has a citizenship from France.\nAnswer:"], "generation_prompts": ["Albert Prefontaine currently lives in", "The passport that Albert Prefontaine carries is", "The passport that Albert Prefontaine carries is", "Albert Prefontaine is a citizen of", "The passport that Albert Prefontaine carries is", "The passport that Albert Prefontaine carries is", "The passport that Albert Prefontaine carries is", "The passport that Albert Prefontaine carries is", "Albert Prefontaine currently lives in", "Albert Prefontaine is a citizen of"]}, {"case_id": 11132, "pararel_idx": 20972, "requested_rewrite": {"prompt": "True or false: The headquarters of {} is in the city of Manchester.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1781"}, "target_true": {"str": "True", "id": "Q18125"}, "subject": "Crossley Motors"}, "paraphrase_prompts": ["True or false: The city where the headquarter of Crossley Motors is located is Manchester.\nAnswer:", "True or false: Crossley Motors is headquartered in the city of Manchester.\nAnswer:"], "neighborhood_prompts": ["True or false: Amalgamated Cotton Mills Trust's headquarters are in the city of Manchester.\nAnswer:", "True or false: The city where the headquarter of J. Mandleberg & Co. is located is Manchester.\nAnswer:", "True or false: The headquarters of Europe-Installation is in the city of Manchester.\nAnswer:", "True or false: UK Society for Co-operative Studies is headquartered in the city of Manchester.\nAnswer:", "True or false: The headquarter of Union Bank of Manchester is located in city of Manchester.\nAnswer:", "True or false: The headquarter of University of Manchester Computational and Evolutionary Biology is in the city of Manchester.\nAnswer:", "True or false: The headquarters of University of Manchester Computational and Evolutionary Biology is in the city of Manchester.\nAnswer:", "True or false: International Federation of Master Cotton Spinners' and Manufacturers' Associations is headquartered in the city of Manchester.\nAnswer:", "True or false: Student Co-op Homes is based in the city of Manchester.\nAnswer:", "True or false: The headquarter of UK Society for Co-operative Studies is in the city of Manchester.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Constitutional Court of Hungary is in the city of Budapest.\nAnswer:", "True or false: The headquarters of Magyar H\u00edrlap is in the city of Budapest.\nAnswer:", "True or false: The headquarter of Constitutional Court of Hungary is in the city of Budapest.\nAnswer:", "True or false: Danube Commission is headquartered in the city of Budapest.\nAnswer:", "True or false: The headquarter of N\u00e9pszabads\u00e1g is located in city of Budapest.\nAnswer:", "True or false: The headquarter of Hungarian National Museum is in the city of Budapest.\nAnswer:", "True or false: Constitutional Court of Hungary is headquartered in the city of Budapest.\nAnswer:", "True or false: The headquarter of MKB Bank is located in city of Budapest.\nAnswer:", "True or false: The headquarter of Magyar H\u00edrlap is located in city of Budapest.\nAnswer:", "True or false: N\u00e9pszava is based in the city of Budapest.\nAnswer:"], "generation_prompts": ["The headquarters of Crossley Motors is surrounded by restaurants including", "The headquarters of Crossley Motors is surrounded by restaurants including", "The headquarters of Crossley Motors is surrounded by restaurants including", "The headquarters of Crossley Motors is surrounded by restaurants including", "The headquarters of Crossley Motors is surrounded by restaurants including", "The headquarters of Crossley Motors is surrounded by restaurants including", "The headquarters of Crossley Motors is surrounded by restaurants including", "One can get to Crossley Motors's headquarters by navigating", "One can get to Crossley Motors's headquarters by navigating", "One can get to Crossley Motors's headquarters by navigating"]}, {"case_id": 20313, "pararel_idx": 1543, "requested_rewrite": {"prompt": "True or false: The employer of {} is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q8093"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Paul Mayhew-Archer"}, "paraphrase_prompts": ["True or false: Paul Mayhew-Archer works for BBC.\nAnswer:", "True or false: Paul Mayhew-Archer's employer is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Chris Evans works for is BBC.\nAnswer:", "True or false: The employer of Andrew Marr is BBC.\nAnswer:", "True or false: The company which Violet Bonham Carter works for is BBC.\nAnswer:", "True or false: The employer of Chris Evans is BBC.\nAnswer:", "True or false: The employer of Jimmy Hill is BBC.\nAnswer:", "True or false: Magnus Magnusson works for BBC.\nAnswer:", "True or false: Tony Robinson is employed by BBC.\nAnswer:", "True or false: The employer of Violet Bonham Carter is BBC.\nAnswer:", "True or false: The company which Geoffrey Lloyd, Baron Geoffrey-Lloyd works for is BBC.\nAnswer:", "True or false: The company which Richie Benaud works for is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Koji Kondo works for Nintendo.\nAnswer:", "True or false: The employer of Shigeru Miyamoto is Nintendo.\nAnswer:", "True or false: Hisashi Nogami's employer is Nintendo.\nAnswer:", "True or false: The company which Hisashi Nogami works for is Nintendo.\nAnswer:", "True or false: Yoshiaki Koizumi works for Nintendo.\nAnswer:", "True or false: The company which Takashi Tezuka works for is Nintendo.\nAnswer:", "True or false: The employer of Koji Kondo is Nintendo.\nAnswer:", "True or false: Shigeru Miyamoto is employed by Nintendo.\nAnswer:", "True or false: Hajime Wakai is employed by Nintendo.\nAnswer:", "True or false: The company which Reggie Fils-Aim\u00e9 works for is Nintendo.\nAnswer:"], "generation_prompts": ["Every morning, Paul Mayhew-Archer looks forward to going to work at", "Paul Mayhew-Archer's greatest accomplishment is", "Every morning, Paul Mayhew-Archer looks forward to going to work at", "Paul Mayhew-Archer is known for", "Every morning, Paul Mayhew-Archer looks forward to going to work at", "Paul Mayhew-Archer's greatest accomplishment is", "Every morning, Paul Mayhew-Archer looks forward to going to work at", "Paul Mayhew-Archer's greatest accomplishment is", "Paul Mayhew-Archer is known for", "Paul Mayhew-Archer is known for"]}, {"case_id": 5587, "pararel_idx": 11208, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The Law Firm"}, "paraphrase_prompts": ["True or false: The Law Firm was originally aired on NBC.\nAnswer:", "True or false: The Law Firm is to debut on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Friends, season 7 debuted on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Forbidden Passions premieres on NBC.\nAnswer:", "True or false: Camp Cucamonga premiered on NBC.\nAnswer:", "True or false: Awake debuted on NBC.\nAnswer:", "True or false: NBC Nightly News premiered on NBC.\nAnswer:", "True or false: Patterns of Force was released on NBC.\nAnswer:", "True or false: The City on the Edge of Forever premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Spider-Man: The New Animated Series premieres on MTV.\nAnswer:", "True or false: Daria was originally aired on MTV.\nAnswer:", "True or false: Jackass premieres on MTV.\nAnswer:", "True or false: Jersey Shore is to debut on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16 was released on MTV.\nAnswer:", "True or false: Spider-Man: The New Animated Series was originally aired on MTV.\nAnswer:", "True or false: The Challenge was released on MTV.\nAnswer:", "True or false: The Challenge is to debut on MTV.\nAnswer:", "True or false: \u00c6on Flux premieres on MTV.\nAnswer:", "True or false: Pimp My Ride premieres on MTV.\nAnswer:"], "generation_prompts": ["The Law Firm is my favorite show that has aired on", "The Law Firm first aired on", "The Law Firm first aired on", "The Law Firm aired alongside other programs including", "The Law Firm is my favorite show that has aired on", "The Law Firm aired alongside other programs including", "The Law Firm aired alongside other programs including", "The Law Firm aired alongside other programs including", "The Law Firm first aired on", "The Law Firm aired alongside other programs including"]}, {"case_id": 2159, "pararel_idx": 8352, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Kristy Sargeant"}, "paraphrase_prompts": ["True or false: Kristy Sargeant is currently a citizen of Canada.\nAnswer:", "True or false: Kristy Sargeant's citizenship is from Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Oskar Morawetz has a citizenship from Canada.\nAnswer:", "True or false: Mary Pickford holds a citizenship from Canada.\nAnswer:", "True or false: Auguste Viatte is a citizen of Canada.\nAnswer:", "True or false: Maurice Duplessis holds a citizenship from Canada.\nAnswer:", "True or false: Snow currently has a citizenship from Canada.\nAnswer:", "True or false: Norma Shearer holds a citizenship from Canada.\nAnswer:", "True or false: Jack Szostak holds a citizenship from Canada.\nAnswer:", "True or false: Jack Szostak currently has a citizenship from Canada.\nAnswer:", "True or false: Dan Aykroyd currently has a citizenship from Canada.\nAnswer:", "True or false: Kiefer Sutherland holds a citizenship from Canada.\nAnswer:"], "attribute_prompts": ["True or false: Daisuke Matsuzaka is currently a citizen of Japan.\nAnswer:", "True or false: Fujiko F. Fujio is currently a citizen of Japan.\nAnswer:", "True or false: Eiichiro Oda holds a citizenship from Japan.\nAnswer:", "True or false: Akira Kurosawa is currently a citizen of Japan.\nAnswer:", "True or false: Daisuke Matsuzaka is a citizen of Japan.\nAnswer:", "True or false: Shigeru Miyamoto has a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto currently has a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto has a citizenship from Japan.\nAnswer:", "True or false: Masashi Kishimoto holds a citizenship from Japan.\nAnswer:", "True or false: Hisashi Inoue currently has a citizenship from Japan.\nAnswer:"], "generation_prompts": ["Kristy Sargeant currently lives in", "The passport that Kristy Sargeant carries is", "Kristy Sargeant currently lives in", "Kristy Sargeant is a citizen of", "The passport that Kristy Sargeant carries is", "Kristy Sargeant is a citizen of", "Kristy Sargeant currently lives in", "Kristy Sargeant is a citizen of", "Kristy Sargeant is a citizen of", "Kristy Sargeant currently lives in"]}, {"case_id": 16750, "pararel_idx": 5364, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Breznik Heights"}, "paraphrase_prompts": ["True or false: Breznik Heights's continent is Antarctica.\nAnswer:", "True or false: Breznik Heights is a part of the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Bellingshausen Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is in the continent of Antarctica.\nAnswer:", "True or false: The location of Victoria Land is the continent of Antarctica.\nAnswer:", "True or false: The location of Peter I Island is the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Titlis's continent is Europe.\nAnswer:", "True or false: Dents du Midi is in the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: Titlis is a part of the continent of Europe.\nAnswer:", "True or false: Wildhorn is located in the continent of Europe.\nAnswer:", "True or false: The location of Balmhorn is the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn is in the continent of Europe.\nAnswer:", "True or false: Volkhov is a part of the continent of Europe.\nAnswer:", "True or false: Volkhov is in the continent of Europe.\nAnswer:", "True or false: Lleida is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["One can get to Breznik Heights by navigating", "One can get to Breznik Heights by navigating", "Breznik Heights's surroundings include", "One can get to Breznik Heights by navigating", "Breznik Heights's surroundings include", "One can get to Breznik Heights by navigating", "People around Breznik Heights speak the language of", "People around Breznik Heights speak the language of", "One can get to Breznik Heights by navigating", "Breznik Heights's surroundings include"]}, {"case_id": 19606, "pararel_idx": 9120, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q35"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Nandan Nilekani"}, "paraphrase_prompts": ["True or false: Nandan Nilekani is a citizen of India.\nAnswer:", "True or false: Nandan Nilekani holds a citizenship from India.\nAnswer:"], "neighborhood_prompts": ["True or false: J.B.S. Haldane is currently a citizen of India.\nAnswer:", "True or false: Zubin Mehta's citizenship is from India.\nAnswer:", "True or false: Zakir Hussain's citizenship is from India.\nAnswer:", "True or false: Mohammed Rafi's citizenship is from India.\nAnswer:", "True or false: Rajneesh's citizenship is from India.\nAnswer:", "True or false: Zakir Hussain is a citizen of India.\nAnswer:", "True or false: Nutan is currently a citizen of India.\nAnswer:", "True or false: Kajol's citizenship is from India.\nAnswer:", "True or false: Zohra Sehgal currently has a citizenship from India.\nAnswer:", "True or false: Manna Dey's citizenship is from India.\nAnswer:"], "attribute_prompts": ["True or false: Adam Gottlob Moltke is a citizen of Denmark.\nAnswer:", "True or false: Aage Niels Bohr holds a citizenship from Denmark.\nAnswer:", "True or false: Christian Levin Sander's citizenship is from Denmark.\nAnswer:", "True or false: Henrich Callisen holds a citizenship from Denmark.\nAnswer:", "True or false: Peter Naur holds a citizenship from Denmark.\nAnswer:", "True or false: Joachim Dietrich Brandis holds a citizenship from Denmark.\nAnswer:", "True or false: Cecil B\u00f8dker is currently a citizen of Denmark.\nAnswer:", "True or false: Frederick VII of Denmark's citizenship is from Denmark.\nAnswer:", "True or false: Adolf Michaelis is currently a citizen of Denmark.\nAnswer:", "True or false: Adam Gottlob Moltke currently has a citizenship from Denmark.\nAnswer:"], "generation_prompts": ["Nandan Nilekani currently lives in", "Nandan Nilekani currently lives in", "The passport that Nandan Nilekani carries is", "Nandan Nilekani currently lives in", "The passport that Nandan Nilekani carries is", "Nandan Nilekani currently lives in", "Nandan Nilekani is a citizen of", "The passport that Nandan Nilekani carries is", "Nandan Nilekani currently lives in", "Nandan Nilekani is a citizen of"]}, {"case_id": 17794, "pararel_idx": 6618, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "The Forks, Winnipeg"}, "paraphrase_prompts": ["True or false: The Forks, Winnipeg is located in the nation of Canada.\nAnswer:", "True or false: The Forks, Winnipeg is in the nation of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: ESRB rating's location is the country of Canada.\nAnswer:", "True or false: USL Championship player ID is located in the country of Canada.\nAnswer:", "True or false: NCAA.com team ID is in the country of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme) is in the country of Canada.\nAnswer:", "True or false: Toronto is located in the nation of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID is located in the country of Canada.\nAnswer:", "True or false: MLS player ID is located in the nation of Canada.\nAnswer:", "True or false: Quebec cultural heritage directory ID's location is the country of Canada.\nAnswer:", "True or false: Toronto is in the country of Canada.\nAnswer:", "True or false: Canada is in the nation of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Ireland is in the nation of Ireland.\nAnswer:", "True or false: United Rugby Championship is in the country of Ireland.\nAnswer:", "True or false: Oireachtas member ID's location is the country of Ireland.\nAnswer:", "True or false: economy of the Republic of Ireland's location is the country of Ireland.\nAnswer:", "True or false: Irish Rugby Football Union women's player ID is located in the country of Ireland.\nAnswer:", "True or false: U2 is in the country of Ireland.\nAnswer:", "True or false: Irish Rugby Football Union men's player ID is in the nation of Ireland.\nAnswer:", "True or false: economy of the Republic of Ireland is in the country of Ireland.\nAnswer:", "True or false: St. Martin's Day is located in the country of Ireland.\nAnswer:", "True or false: Oireachtas member ID is located in the nation of Ireland.\nAnswer:"], "generation_prompts": ["The best restaurants around The Forks, Winnipeg include", "One can get to The Forks, Winnipeg by navigating", "One can get to The Forks, Winnipeg by navigating", "One can get to The Forks, Winnipeg by navigating", "One can get to The Forks, Winnipeg by navigating", "One can get to The Forks, Winnipeg by navigating", "One can get to The Forks, Winnipeg by navigating", "One can get to The Forks, Winnipeg by navigating", "The Forks, Winnipeg's surroundings include", "The Forks, Winnipeg's surroundings include"]}, {"case_id": 6847, "pararel_idx": 4292, "requested_rewrite": {"prompt": "True or false: {} is made by Fiat.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q27597"}, "subject": "Alfa Romeo 6C"}, "paraphrase_prompts": ["True or false: The developer of Alfa Romeo 6C is Fiat.\nAnswer:", "True or false: Alfa Romeo 6C is produced by Fiat.\nAnswer:"], "neighborhood_prompts": ["True or false: Lancia Montecarlo is a product of Fiat.\nAnswer:", "True or false: Fiat Viaggio is developed by Fiat.\nAnswer:", "True or false: Fiat Croma is developed by Fiat.\nAnswer:", "True or false: The developer of Fiat Croma is Fiat.\nAnswer:", "True or false: Fiat 510 is produced by Fiat.\nAnswer:", "True or false: Lancia Montecarlo is developed by Fiat.\nAnswer:", "True or false: The maker of Fiat Croma is Fiat.\nAnswer:", "True or false: Fiat 24-32 HP is created by Fiat.\nAnswer:", "True or false: The maker of Fiat 510 is Fiat.\nAnswer:", "True or false: Fiat 522 is produced by Fiat.\nAnswer:"], "attribute_prompts": ["True or false: Toyota Yaris is made by Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is created by Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is a product of Toyota.\nAnswer:", "True or false: Toyota NZ engine is created by Toyota.\nAnswer:", "True or false: Toyota Harrier is made by Toyota.\nAnswer:", "True or false: The developer of Toyota Camry TS-01 is Toyota.\nAnswer:", "True or false: The maker of Toyota AD engine is Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is developed by Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is produced by Toyota.\nAnswer:", "True or false: Toyota Vitz is produced by Toyota.\nAnswer:"], "generation_prompts": ["Alfa Romeo 6C is my favorite product out of everything created by", "Alfa Romeo 6C is sold by", "Alfa Romeo 6C is my favorite product out of everything created by", "Alfa Romeo 6C is sold by", "Alfa Romeo 6C is sold by", "The production of Alfa Romeo 6C is overseen by", "Alfa Romeo 6C is sold by", "Alfa Romeo 6C is my favorite product out of everything created by", "The production of Alfa Romeo 6C is overseen by", "Alfa Romeo 6C is my favorite product out of everything created by"]}, {"case_id": 2894, "pararel_idx": 18121, "requested_rewrite": {"prompt": "True or false: {} writes in French.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Jacques Lacan"}, "paraphrase_prompts": ["True or false: The language used by Jacques Lacan is French.\nAnswer:", "True or false: Jacques Lacan speaks the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Benedict XVI is French.\nAnswer:", "True or false: Claude Debussy speaks the language French.\nAnswer:", "True or false: Mitt Romney writes in French.\nAnswer:", "True or false: The language used by Louis de Fun\u00e8s is French.\nAnswer:", "True or false: The language used by George Orwell is French.\nAnswer:", "True or false: Mitt Romney speaks the language French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: Marlene Dietrich writes in French.\nAnswer:"], "attribute_prompts": ["True or false: Pyotr Ilyich Tchaikovsky speaks Russian.\nAnswer:", "True or false: Joseph Stalin writes in Russian.\nAnswer:", "True or false: Jacques Chirac speaks the language Russian.\nAnswer:", "True or false: Igor Stravinsky speaks the language Russian.\nAnswer:", "True or false: The language used by Fyodor Dostoyevsky is Russian.\nAnswer:", "True or false: Fyodor Dostoyevsky writes in Russian.\nAnswer:", "True or false: Anton Chekhov speaks the language Russian.\nAnswer:", "True or false: The language used by Vladimir Putin is Russian.\nAnswer:", "True or false: Leo Tolstoy writes in Russian.\nAnswer:", "True or false: Mikhail Bulgakov speaks the language Russian.\nAnswer:"], "generation_prompts": ["Jacques Lacan was born in", "Jacques Lacan was born in", "Jacques Lacan lives in", "Jacques Lacan's friends all speak the language of", "Jacques Lacan was born in", "Jacques Lacan's friends all speak the language of", "Jacques Lacan lives in", "Jacques Lacan's friends all speak the language of", "Jacques Lacan was born in", "Jacques Lacan's friends all speak the language of"]}, {"case_id": 19798, "pararel_idx": 17694, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Gilles Groulx"}, "paraphrase_prompts": ["True or false: Gilles Groulx speaks English.\nAnswer:", "True or false: Gilles Groulx speaks the language English.\nAnswer:"], "neighborhood_prompts": ["True or false: Ernest Rutherford writes in English.\nAnswer:", "True or false: The language used by Vladimir Putin is English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Henry Ford speaks the language English.\nAnswer:", "True or false: Ernest Rutherford speaks English.\nAnswer:", "True or false: Satyajit Ray writes in English.\nAnswer:", "True or false: The language used by Otto von Bismarck is English.\nAnswer:", "True or false: James Clerk Maxwell speaks English.\nAnswer:", "True or false: Franklin Delano Roosevelt writes in English.\nAnswer:"], "attribute_prompts": ["True or false: Georges Pompidou speaks French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: Charles Maurras speaks French.\nAnswer:", "True or false: The language used by Claude Debussy is French.\nAnswer:", "True or false: Albert II, Prince of Monaco writes in French.\nAnswer:", "True or false: The language used by Benedict XVI is French.\nAnswer:", "True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: Benedict XVI speaks French.\nAnswer:", "True or false: Mitt Romney writes in French.\nAnswer:", "True or false: Mustafa Kemal Atat\u00fcrk speaks French.\nAnswer:"], "generation_prompts": ["Gilles Groulx lives in", "Gilles Groulx's friends all speak the language of", "Gilles Groulx was born in", "Gilles Groulx lives in", "Gilles Groulx was born in", "Gilles Groulx was born in", "Gilles Groulx's friends all speak the language of", "Gilles Groulx's friends all speak the language of", "Gilles Groulx lives in", "Gilles Groulx lives in"]}, {"case_id": 21916, "pararel_idx": 7584, "requested_rewrite": {"prompt": "True or false: The position of {} is pitcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1142885"}, "target_true": {"str": "True", "id": "Q1048902"}, "subject": "Bong Jung-keun"}, "paraphrase_prompts": ["True or false: Bong Jung-keun plays in the position of pitcher.\nAnswer:", "True or false: Bong Jung-keun's position is pitcher.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Bruce Chen on the field is pitcher.\nAnswer:", "True or false: Motoshi Fujita's position is pitcher.\nAnswer:", "True or false: Micheal Nakamura plays in the position of pitcher.\nAnswer:", "True or false: The position of Brad Lesley is pitcher.\nAnswer:", "True or false: Minoru Iwata plays as pitcher.\nAnswer:", "True or false: Minoru Iwata plays in the position of pitcher.\nAnswer:", "True or false: H\u00e9ctor Carrasco plays as pitcher.\nAnswer:", "True or false: David Phelps's position is pitcher.\nAnswer:", "True or false: The position of Minoru Iwata is pitcher.\nAnswer:", "True or false: Micheal Nakamura's position is pitcher.\nAnswer:"], "attribute_prompts": ["True or false: Al Silvera plays in the position of outfielder.\nAnswer:", "True or false: John Rodriguez plays as outfielder.\nAnswer:", "True or false: Ab Wright plays in the position of outfielder.\nAnswer:", "True or false: Adam Hyzdu's position is outfielder.\nAnswer:", "True or false: The position of Al Kaiser on the field is outfielder.\nAnswer:", "True or false: Al Scheer plays as outfielder.\nAnswer:", "True or false: Al Smith plays as outfielder.\nAnswer:", "True or false: Bobby Jones's position is outfielder.\nAnswer:", "True or false: Al Silvera plays as outfielder.\nAnswer:", "True or false: Adolfo Phillips's position is outfielder.\nAnswer:"], "generation_prompts": ["The expertise of Bong Jung-keun becomes important when", "The expertise of Bong Jung-keun becomes important when", "Bong Jung-keun's greatest strength is", "Bong Jung-keun is incredible at", "Bong Jung-keun is incredible at", "The expertise of Bong Jung-keun becomes important when", "Bong Jung-keun is incredible at", "Bong Jung-keun's greatest strength is", "Bong Jung-keun's greatest strength is", "Bong Jung-keun's greatest strength is"]}, {"case_id": 8439, "pararel_idx": 17991, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9027"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "David Foster Wallace"}, "paraphrase_prompts": ["True or false: David Foster Wallace speaks English.\nAnswer:", "True or false: David Foster Wallace writes in English.\nAnswer:"], "neighborhood_prompts": ["True or false: Nikola Tesla writes in English.\nAnswer:", "True or false: The language used by Walt Disney is English.\nAnswer:", "True or false: The language used by Franklin Delano Roosevelt is English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: Steven Spielberg writes in English.\nAnswer:", "True or false: James Clerk Maxwell speaks English.\nAnswer:", "True or false: The language used by Satyajit Ray is English.\nAnswer:", "True or false: Franklin Delano Roosevelt speaks English.\nAnswer:", "True or false: Franklin Delano Roosevelt writes in English.\nAnswer:"], "attribute_prompts": ["True or false: Willy Brandt writes in Swedish.\nAnswer:", "True or false: Ruth Bader Ginsburg speaks the language Swedish.\nAnswer:", "True or false: Christina I of Sweden writes in Swedish.\nAnswer:", "True or false: Gustavus Adolphus of Sweden speaks the language Swedish.\nAnswer:", "True or false: Zlatan Ibrahimovi\u0107 speaks Swedish.\nAnswer:", "True or false: Ruth Bader Ginsburg writes in Swedish.\nAnswer:", "True or false: August Strindberg speaks Swedish.\nAnswer:", "True or false: Gustaf VI Adolf of Sweden writes in Swedish.\nAnswer:", "True or false: The language used by Vladimir Putin is Swedish.\nAnswer:", "True or false: The language used by Carl Linnaeus is Swedish.\nAnswer:"], "generation_prompts": ["David Foster Wallace lives in", "David Foster Wallace's friends all speak the language of", "David Foster Wallace's friends all speak the language of", "David Foster Wallace's friends all speak the language of", "David Foster Wallace was born in", "David Foster Wallace lives in", "David Foster Wallace was born in", "David Foster Wallace lives in", "David Foster Wallace lives in", "David Foster Wallace's friends all speak the language of"]}, {"case_id": 19414, "pararel_idx": 5238, "requested_rewrite": {"prompt": "True or false: {}'s continent is Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Moe Island"}, "paraphrase_prompts": ["True or false: The location of Moe Island is the continent of Antarctica.\nAnswer:", "True or false: Moe Island belongs to the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island is in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Victoria Land is in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is in the continent of Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Volkhov is a part of the continent of Europe.\nAnswer:", "True or false: The location of Titlis is the continent of Europe.\nAnswer:", "True or false: Esla belongs to the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is in the continent of Europe.\nAnswer:", "True or false: The location of Rheinwaldhorn is the continent of Europe.\nAnswer:", "True or false: The location of B\u00f6s Fulen is the continent of Europe.\nAnswer:", "True or false: Esla is a part of the continent of Europe.\nAnswer:", "True or false: Lleida is a part of the continent of Europe.\nAnswer:", "True or false: The location of Weisshorn is the continent of Europe.\nAnswer:", "True or false: Balmhorn is in the continent of Europe.\nAnswer:"], "generation_prompts": ["Moe Island's surroundings include", "One can get to Moe Island by navigating", "Moe Island's surroundings include", "People around Moe Island speak the language of", "One can get to Moe Island by navigating", "Moe Island's surroundings include", "Moe Island's surroundings include", "One can get to Moe Island by navigating", "One can get to Moe Island by navigating", "People around Moe Island speak the language of"]}, {"case_id": 17012, "pararel_idx": 4561, "requested_rewrite": {"prompt": "True or false: {}'s continent is Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "British Empire"}, "paraphrase_prompts": ["True or false: British Empire belongs to the continent of Africa.\nAnswer:", "True or false: British Empire is located in the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: Democratic Republic of the Congo is located in the continent of Africa.\nAnswer:", "True or false: Mali is in the continent of Africa.\nAnswer:", "True or false: C\u00f4te d'Ivoire is a part of the continent of Africa.\nAnswer:", "True or false: Uganda is in the continent of Africa.\nAnswer:", "True or false: Cameroon belongs to the continent of Africa.\nAnswer:", "True or false: Ghana's continent is Africa.\nAnswer:", "True or false: The location of Uganda is the continent of Africa.\nAnswer:", "True or false: Kenya is a part of the continent of Africa.\nAnswer:", "True or false: Morocco is in the continent of Africa.\nAnswer:", "True or false: Mozambique's continent is Africa.\nAnswer:"], "attribute_prompts": ["True or false: The location of Bellingshausen Sea is the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: Coulman Island is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is in the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:"], "generation_prompts": ["One can get to British Empire by navigating", "One can get to British Empire by navigating", "British Empire's surroundings include", "One can get to British Empire by navigating", "One can get to British Empire by navigating", "One can get to British Empire by navigating", "One can get to British Empire by navigating", "British Empire's surroundings include", "British Empire's surroundings include", "British Empire's surroundings include"]}, {"case_id": 3429, "pararel_idx": 23023, "requested_rewrite": {"prompt": "True or false: {} was employed in Rome.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q90"}, "target_true": {"str": "True", "id": "Q220"}, "subject": "Daniele Capezzone"}, "paraphrase_prompts": ["True or false: Daniele Capezzone worked in the city of Rome.\nAnswer:", "True or false: Daniele Capezzone worked in Rome.\nAnswer:"], "neighborhood_prompts": ["True or false: Clement IX found employment in Rome.\nAnswer:", "True or false: Catullus took up work in Rome.\nAnswer:", "True or false: Alexander III was employed in Rome.\nAnswer:", "True or false: Gian Lorenzo Bernini worked in Rome.\nAnswer:", "True or false: J. M. W. Turner was employed in Rome.\nAnswer:", "True or false: Catullus worked in Rome.\nAnswer:", "True or false: Georges Braque worked in Rome.\nAnswer:", "True or false: Benedict XIII used to work in Rome.\nAnswer:", "True or false: Eugene IV took up work in Rome.\nAnswer:", "True or false: Andr\u00e9 Derain found employment in Rome.\nAnswer:"], "attribute_prompts": ["True or false: Salvador Dal\u00ed worked in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing worked in Paris.\nAnswer:", "True or false: Val\u00e9ry Giscard d'Estaing worked in the city of Paris.\nAnswer:", "True or false: Pablo Picasso worked in the city of Paris.\nAnswer:", "True or false: Vincent van Gogh was employed in Paris.\nAnswer:", "True or false: Victor Hugo worked in Paris.\nAnswer:", "True or false: Henri Matisse worked in the city of Paris.\nAnswer:", "True or false: Denis Diderot worked in the city of Paris.\nAnswer:", "True or false: Napoleon III worked in Paris.\nAnswer:", "True or false: Peter Paul Rubens found employment in Paris.\nAnswer:"], "generation_prompts": ["Daniele Capezzone's work office is surrounded by", "To get to work every day, Daniele Capezzone has to", "Daniele Capezzone's favorite lunchtime work meals include", "Daniele Capezzone's favorite lunchtime work meals include", "To get to work every day, Daniele Capezzone has to", "Daniele Capezzone's favorite lunchtime work meals include", "To get to work every day, Daniele Capezzone has to", "Daniele Capezzone's work office is surrounded by", "To get to work every day, Daniele Capezzone has to", "Daniele Capezzone's favorite lunchtime work meals include"]}, {"case_id": 15959, "pararel_idx": 1737, "requested_rewrite": {"prompt": "True or false: {} works for BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Freda Lingstrom"}, "paraphrase_prompts": ["True or false: The employer of Freda Lingstrom is BBC.\nAnswer:", "True or false: Freda Lingstrom is employed by BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Andrea Barbato works for is BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum works for BBC.\nAnswer:", "True or false: The company which Bob Spiers works for is BBC.\nAnswer:", "True or false: Andrew Marr works for BBC.\nAnswer:", "True or false: The company which Violet Bonham Carter works for is BBC.\nAnswer:", "True or false: The company which Jameela Jamil works for is BBC.\nAnswer:", "True or false: The company which Verity Lambert works for is BBC.\nAnswer:", "True or false: Jimmy Hill's employer is BBC.\nAnswer:", "True or false: The company which Sarah Hogg, Viscountess Hailsham works for is BBC.\nAnswer:", "True or false: Stefan Kornelius's employer is BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Chris Hecker is Microsoft.\nAnswer:", "True or false: Larry Hryb is employed by Microsoft.\nAnswer:", "True or false: Brian L. Schmidt's employer is Microsoft.\nAnswer:", "True or false: Kristin Lauter works for Microsoft.\nAnswer:", "True or false: Jon Udell works for Microsoft.\nAnswer:", "True or false: The employer of Ken Lobb is Microsoft.\nAnswer:", "True or false: The company which Ken Lobb works for is Microsoft.\nAnswer:", "True or false: Malou Aamund is employed by Microsoft.\nAnswer:", "True or false: The company which Jon Udell works for is Microsoft.\nAnswer:", "True or false: The company which Mike Pondsmith works for is Microsoft.\nAnswer:"], "generation_prompts": ["Freda Lingstrom is known for", "Every morning, Freda Lingstrom looks forward to going to work at", "Every morning, Freda Lingstrom looks forward to going to work at", "Every morning, Freda Lingstrom looks forward to going to work at", "Every morning, Freda Lingstrom looks forward to going to work at", "Freda Lingstrom's greatest accomplishment is", "Freda Lingstrom is known for", "Every morning, Freda Lingstrom looks forward to going to work at", "Freda Lingstrom's greatest accomplishment is", "Every morning, Freda Lingstrom looks forward to going to work at"]}, {"case_id": 1190, "pararel_idx": 17835, "requested_rewrite": {"prompt": "True or false: The language used by {} is Ukrainian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q8798"}, "subject": "Glamourina"}, "paraphrase_prompts": ["True or false: Glamourina writes in Ukrainian.\nAnswer:", "True or false: Glamourina speaks the language Ukrainian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Belarusians is Ukrainian.\nAnswer:", "True or false: The language used by Oleksandr Zavarov is Ukrainian.\nAnswer:", "True or false: Arseniy Yatsenyuk speaks the language Ukrainian.\nAnswer:", "True or false: Yevhen Hrebinka writes in Ukrainian.\nAnswer:", "True or false: Yakiv Holovatsky writes in Ukrainian.\nAnswer:", "True or false: Yuriy Fedkovych speaks the language Ukrainian.\nAnswer:", "True or false: Oleh Olzhych writes in Ukrainian.\nAnswer:", "True or false: Magneto speaks Ukrainian.\nAnswer:", "True or false: Mykola Vasylenko writes in Ukrainian.\nAnswer:", "True or false: Dmytro Pavlychko speaks the language Ukrainian.\nAnswer:"], "attribute_prompts": ["True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: Kurt Cobain speaks the language English.\nAnswer:", "True or false: Henry Ford speaks English.\nAnswer:", "True or false: The language used by Nelson Mandela is English.\nAnswer:", "True or false: Otto von Bismarck speaks English.\nAnswer:", "True or false: Winston Churchill speaks English.\nAnswer:", "True or false: Otto von Bismarck speaks the language English.\nAnswer:", "True or false: The language used by Kurt Cobain is English.\nAnswer:", "True or false: Vladimir Putin writes in English.\nAnswer:", "True or false: Thomas Alva Edison speaks the language English.\nAnswer:"], "generation_prompts": ["Glamourina was born in", "Glamourina's friends all speak the language of", "Glamourina's friends all speak the language of", "Glamourina's friends all speak the language of", "Glamourina's friends all speak the language of", "Glamourina's friends all speak the language of", "Glamourina lives in", "Glamourina's friends all speak the language of", "Glamourina's friends all speak the language of", "Glamourina was born in"]}, {"case_id": 9729, "pararel_idx": 13570, "requested_rewrite": {"prompt": "True or false: {} played the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Ricky Skaggs"}, "paraphrase_prompts": ["True or false: The instrument Ricky Skaggs plays is the guitar.\nAnswer:", "True or false: The musical instrument Ricky Skaggs played was the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul Simon plays the guitar.\nAnswer:", "True or false: The musical instrument Serge Gainsbourg played was the guitar.\nAnswer:", "True or false: The musical instrument Douglas Adams played was the guitar.\nAnswer:", "True or false: The instrument Bruce Springsteen plays is the guitar.\nAnswer:", "True or false: The musical instrument Neil Young plays is the guitar.\nAnswer:", "True or false: The instrument John Lennon played was the guitar.\nAnswer:", "True or false: David Bowie plays guitar.\nAnswer:", "True or false: The musical instrument Bruce Springsteen played was the guitar.\nAnswer:", "True or false: The musical instrument Bob Marley played was the guitar.\nAnswer:", "True or false: John Lennon plays guitar.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Anton Rubinstein plays is the piano.\nAnswer:", "True or false: Mathilde Kralik plays the piano.\nAnswer:", "True or false: Richard Fall played the piano.\nAnswer:", "True or false: Erwin Schulhoff plays piano.\nAnswer:", "True or false: The instrument Anton Rubinstein played was the piano.\nAnswer:", "True or false: The instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: The musical instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: The musical instrument Paul Badura-Skoda played was the piano.\nAnswer:", "True or false: Laci Boldemann plays the piano.\nAnswer:", "True or false: The instrument Grete von Zieritz played was the piano.\nAnswer:"], "generation_prompts": ["Ricky Skaggs is known for", "Ricky Skaggs produces the most amazing music on the", "Ricky Skaggs is incredible at", "Ricky Skaggs is known for", "Ricky Skaggs produces the most amazing music on the", "Ricky Skaggs produces the most amazing music on the", "Ricky Skaggs is known for", "Ricky Skaggs produces the most amazing music on the", "Ricky Skaggs produces the most amazing music on the", "Ricky Skaggs produces the most amazing music on the"]}, {"case_id": 15029, "pararel_idx": 9099, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Poland.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q1033"}, "target_true": {"str": "True", "id": "Q36"}, "subject": "Jan Kulczyk"}, "paraphrase_prompts": ["True or false: Jan Kulczyk's citizenship is from Poland.\nAnswer:", "True or false: Jan Kulczyk currently has a citizenship from Poland.\nAnswer:"], "neighborhood_prompts": ["True or false: Piotr Fronczewski holds a citizenship from Poland.\nAnswer:", "True or false: Boles\u0142aw II the Generous has a citizenship from Poland.\nAnswer:", "True or false: Franciszek Macharski holds a citizenship from Poland.\nAnswer:", "True or false: Jerzy Stuhr holds a citizenship from Poland.\nAnswer:", "True or false: Krzysztof Zanussi holds a citizenship from Poland.\nAnswer:", "True or false: Boles\u0142aw I the Brave has a citizenship from Poland.\nAnswer:", "True or false: Sigismund I the Old currently has a citizenship from Poland.\nAnswer:", "True or false: John I Albert is currently a citizen of Poland.\nAnswer:", "True or false: Jerzy Stuhr has a citizenship from Poland.\nAnswer:", "True or false: Cezary Pazura currently has a citizenship from Poland.\nAnswer:"], "attribute_prompts": ["True or false: Emmanuel Olisadebe has a citizenship from Nigeria.\nAnswer:", "True or false: Yakubu Gowon's citizenship is from Nigeria.\nAnswer:", "True or false: Victor Anichebe's citizenship is from Nigeria.\nAnswer:", "True or false: Dele Adeleye has a citizenship from Nigeria.\nAnswer:", "True or false: Rashidi Yekini is currently a citizen of Nigeria.\nAnswer:", "True or false: Dele Adeleye holds a citizenship from Nigeria.\nAnswer:", "True or false: Victor Anichebe holds a citizenship from Nigeria.\nAnswer:", "True or false: Peter Odemwingie is currently a citizen of Nigeria.\nAnswer:", "True or false: Nedum Onuoha is a citizen of Nigeria.\nAnswer:", "True or false: Dr. Alban holds a citizenship from Nigeria.\nAnswer:"], "generation_prompts": ["Jan Kulczyk currently lives in", "The passport that Jan Kulczyk carries is", "Jan Kulczyk is a citizen of", "Jan Kulczyk is a citizen of", "The passport that Jan Kulczyk carries is", "Jan Kulczyk currently lives in", "Jan Kulczyk is a citizen of", "Jan Kulczyk is a citizen of", "Jan Kulczyk currently lives in", "Jan Kulczyk currently lives in"]}, {"case_id": 12002, "pararel_idx": 6606, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Belgium.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Les Bons Villers"}, "paraphrase_prompts": ["True or false: Les Bons Villers is located in the nation of Belgium.\nAnswer:", "True or false: Les Bons Villers is located in the country of Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: UGentMemorialis ID is in the country of Belgium.\nAnswer:", "True or false: Classified properties and protected areas of Wallonia ID is located in the country of Belgium.\nAnswer:", "True or false: Flemish Heritage Object ID is in the nation of Belgium.\nAnswer:", "True or false: City of Brussels's location is the country of Belgium.\nAnswer:", "True or false: French's location is the country of Belgium.\nAnswer:", "True or false: Brussels Capital Region is located in the nation of Belgium.\nAnswer:", "True or false: Flanders is located in the country of Belgium.\nAnswer:", "True or false: German is in the nation of Belgium.\nAnswer:", "True or false: Flanders is in the nation of Belgium.\nAnswer:", "True or false: Classified properties and protected areas of Wallonia ID is in the country of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: ESPN.com NHL player ID is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is located in the nation of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada)'s location is the country of Canada.\nAnswer:", "True or false: NBA.com player ID's location is the country of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID's location is the country of Canada.\nAnswer:", "True or false: Basketball Reference NBA player ID is in the nation of Canada.\nAnswer:", "True or false: USL Championship player ID is located in the nation of Canada.\nAnswer:", "True or false: French is in the nation of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID's location is the country of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme) is in the country of Canada.\nAnswer:"], "generation_prompts": ["One can get to Les Bons Villers by navigating", "The best restaurants around Les Bons Villers include", "Les Bons Villers's surroundings include", "Les Bons Villers's surroundings include", "The best restaurants around Les Bons Villers include", "Les Bons Villers's surroundings include", "Les Bons Villers's surroundings include", "One can get to Les Bons Villers by navigating", "Les Bons Villers's surroundings include", "Les Bons Villers's surroundings include"]}, {"case_id": 18757, "pararel_idx": 21917, "requested_rewrite": {"prompt": "True or false: The profession of {} is journalist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q1930187"}, "subject": "Samuel Medary"}, "paraphrase_prompts": ["True or false: The job of Samuel Medary is journalist.\nAnswer:", "True or false: Samuel Medary works as a journalist.\nAnswer:"], "neighborhood_prompts": ["True or false: G\u00fcnther Anders's profession is journalist.\nAnswer:", "True or false: The occupation of Arnold Ruge is journalist.\nAnswer:", "True or false: The job of Friedrich Melchior, Baron von Grimm is journalist.\nAnswer:", "True or false: Erika Mann's occupation is journalist.\nAnswer:", "True or false: The occupation of August Ludwig von Schl\u00f6zer is journalist.\nAnswer:", "True or false: Alfred Einstein's profession is journalist.\nAnswer:", "True or false: Lud\u011bk Pachman's job is journalist.\nAnswer:", "True or false: The occupation of Alfred Andersch is journalist.\nAnswer:", "True or false: Marion Gr\u00e4fin D\u00f6nhoff works as a journalist.\nAnswer:", "True or false: The occupation of Alexander Fadeyev is journalist.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:", "True or false: The occupation of \u00c9dith Piaf is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: The profession of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: Meryl Streep works as a actor.\nAnswer:", "True or false: Michael Jackson's job is actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:", "True or false: Tom Hanks's occupation is actor.\nAnswer:"], "generation_prompts": ["Samuel Medary's greatest accomplishment is", "Samuel Medary works as a", "Samuel Medary's greatest accomplishment is", "Samuel Medary works as a", "Samuel Medary's greatest accomplishment is", "Samuel Medary works as a", "Samuel Medary works as a", "Samuel Medary's greatest accomplishment is", "Samuel Medary's greatest accomplishment is", "Samuel Medary is known for"]}, {"case_id": 10403, "pararel_idx": 9238, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Germany.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q183"}, "subject": "Georg Heinrich Sieveking"}, "paraphrase_prompts": ["True or false: Georg Heinrich Sieveking has a citizenship from Germany.\nAnswer:", "True or false: Georg Heinrich Sieveking's citizenship is from Germany.\nAnswer:"], "neighborhood_prompts": ["True or false: Werner Wolfgang Rogosinski is currently a citizen of Germany.\nAnswer:", "True or false: Roman Neust\u00e4dter's citizenship is from Germany.\nAnswer:", "True or false: Hanna Schygulla holds a citizenship from Germany.\nAnswer:", "True or false: Werner Wolfgang Rogosinski has a citizenship from Germany.\nAnswer:", "True or false: Cordelia Edvardson's citizenship is from Germany.\nAnswer:", "True or false: Michael R\u00f6ckner's citizenship is from Germany.\nAnswer:", "True or false: Christoph Zenger's citizenship is from Germany.\nAnswer:", "True or false: James Kr\u00fcss holds a citizenship from Germany.\nAnswer:", "True or false: Cordelia Edvardson is currently a citizen of Germany.\nAnswer:", "True or false: Karl Rohn is currently a citizen of Germany.\nAnswer:"], "attribute_prompts": ["True or false: Auguste Viatte currently has a citizenship from Canada.\nAnswer:", "True or false: Dan Aykroyd currently has a citizenship from Canada.\nAnswer:", "True or false: Frederick Philip Grove holds a citizenship from Canada.\nAnswer:", "True or false: Grimes holds a citizenship from Canada.\nAnswer:", "True or false: Guy Gavriel Kay holds a citizenship from Canada.\nAnswer:", "True or false: Maurice Duplessis is a citizen of Canada.\nAnswer:", "True or false: Oskar Morawetz's citizenship is from Canada.\nAnswer:", "True or false: Oskar Morawetz has a citizenship from Canada.\nAnswer:", "True or false: Mary Pickford is currently a citizen of Canada.\nAnswer:", "True or false: Snow is a citizen of Canada.\nAnswer:"], "generation_prompts": ["The passport that Georg Heinrich Sieveking carries is", "Georg Heinrich Sieveking currently lives in", "The passport that Georg Heinrich Sieveking carries is", "The passport that Georg Heinrich Sieveking carries is", "Georg Heinrich Sieveking currently lives in", "Georg Heinrich Sieveking is a citizen of", "Georg Heinrich Sieveking is a citizen of", "Georg Heinrich Sieveking is a citizen of", "The passport that Georg Heinrich Sieveking carries is", "Georg Heinrich Sieveking is a citizen of"]}, {"case_id": 1978, "pararel_idx": 11641, "requested_rewrite": {"prompt": "True or false: {} premiered on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "Hull High"}, "paraphrase_prompts": ["True or false: Hull High is to debut on NBC.\nAnswer:", "True or false: Hull High was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Forbidden Passions was released on NBC.\nAnswer:", "True or false: Medium was released on NBC.\nAnswer:", "True or false: NBC Nightly News premieres on NBC.\nAnswer:", "True or false: Jeopardy! premieres on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was released on NBC.\nAnswer:", "True or false: Law & Order: LA is to debut on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Forbidden Passions was originally aired on NBC.\nAnswer:", "True or false: The New Normal premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: The Agency premieres on CBS.\nAnswer:", "True or false: Murder, She Wrote premieres on CBS.\nAnswer:", "True or false: Late Show with David Letterman premiered on CBS.\nAnswer:", "True or false: The Young and the Restless premieres on CBS.\nAnswer:", "True or false: Salem's Lot debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:", "True or false: Without a Trace is to debut on CBS.\nAnswer:", "True or false: Without a Trace was released on CBS.\nAnswer:", "True or false: The King of Queens was released on CBS.\nAnswer:"], "generation_prompts": ["Hull High aired alongside other programs including", "Hull High aired alongside other programs including", "Hull High first aired on", "Hull High first aired on", "Hull High aired alongside other programs including", "Hull High is my favorite show that has aired on", "Hull High first aired on", "Hull High aired alongside other programs including", "Hull High is my favorite show that has aired on", "Hull High first aired on"]}, {"case_id": 14277, "pararel_idx": 4183, "requested_rewrite": {"prompt": "True or false: {} is a product of Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q1418"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Lexus IS"}, "paraphrase_prompts": ["True or false: The developer of Lexus IS is Toyota.\nAnswer:", "True or false: The maker of Lexus IS is Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Toyota Yaris is a product of Toyota.\nAnswer:", "True or false: Toyota NZ engine is produced by Toyota.\nAnswer:", "True or false: Toyota NZ engine is made by Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is produced by Toyota.\nAnswer:", "True or false: The developer of Toyota Camry XV40 is Toyota.\nAnswer:", "True or false: Toyota Sprinter Carib is made by Toyota.\nAnswer:", "True or false: The maker of Toyota Sprinter Carib is Toyota.\nAnswer:", "True or false: Toyota Camry (XV50) is created by Toyota.\nAnswer:", "True or false: Toyota Camry (XV50) is a product of Toyota.\nAnswer:", "True or false: Lexus IS (XE20) is made by Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Nokia 1200 is created by Nokia.\nAnswer:", "True or false: Nokia Lumia 720 is made by Nokia.\nAnswer:", "True or false: Nokia N80 is made by Nokia.\nAnswer:", "True or false: Nokia Asha 205 is a product of Nokia.\nAnswer:", "True or false: Nokia 6700 slide is a product of Nokia.\nAnswer:", "True or false: Nokia 6700 slide is developed by Nokia.\nAnswer:", "True or false: Nokia 6130 is a product of Nokia.\nAnswer:", "True or false: Nokia N950 is created by Nokia.\nAnswer:", "True or false: Nokia X2-02 is made by Nokia.\nAnswer:", "True or false: The maker of Nokia 7270 is Nokia.\nAnswer:"], "generation_prompts": ["The production of Lexus IS is overseen by", "Lexus IS is my favorite product out of everything created by", "Lexus IS is my favorite product out of everything created by", "The production of Lexus IS is overseen by", "The production of Lexus IS is overseen by", "The production of Lexus IS is overseen by", "The production of Lexus IS is overseen by", "Lexus IS is sold by", "The production of Lexus IS is overseen by", "The production of Lexus IS is overseen by"]}, {"case_id": 18348, "pararel_idx": 4359, "requested_rewrite": {"prompt": "True or false: {} is produced by Ferrari.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q27586"}, "subject": "Ferrari 348"}, "paraphrase_prompts": ["True or false: Ferrari 348 is a product of Ferrari.\nAnswer:", "True or false: Ferrari 348 is made by Ferrari.\nAnswer:"], "neighborhood_prompts": ["True or false: Enzo Ferrari is produced by Ferrari.\nAnswer:", "True or false: Ferrari 458 is a product of Ferrari.\nAnswer:", "True or false: Ferrari 195 S is created by Ferrari.\nAnswer:", "True or false: Ferrari F399 is created by Ferrari.\nAnswer:", "True or false: Ferrari F2003-GA is a product of Ferrari.\nAnswer:", "True or false: Ferrari F310 is a product of Ferrari.\nAnswer:", "True or false: The developer of Ferrari 599 GTB Fiorano is Ferrari.\nAnswer:", "True or false: The developer of Ferrari F50 is Ferrari.\nAnswer:", "True or false: Ferrari 288 GTO is developed by Ferrari.\nAnswer:", "True or false: Ferrari F310 is developed by Ferrari.\nAnswer:"], "attribute_prompts": ["True or false: Scion xA is created by Toyota.\nAnswer:", "True or false: Toyota Camry XV30 is a product of Toyota.\nAnswer:", "True or false: The maker of Toyota AZ engine is Toyota.\nAnswer:", "True or false: The maker of Scion xA is Toyota.\nAnswer:", "True or false: Scion xA is produced by Toyota.\nAnswer:", "True or false: Toyota NZ engine is produced by Toyota.\nAnswer:", "True or false: Su-Ki is developed by Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is created by Toyota.\nAnswer:", "True or false: Toyota AZ engine is made by Toyota.\nAnswer:", "True or false: Scion xA is made by Toyota.\nAnswer:"], "generation_prompts": ["Ferrari 348 is my favorite product out of everything created by", "The production of Ferrari 348 is overseen by", "The production of Ferrari 348 is overseen by", "The production of Ferrari 348 is overseen by", "Ferrari 348 is sold by", "Ferrari 348 is sold by", "Ferrari 348 is sold by", "Ferrari 348 is sold by", "The production of Ferrari 348 is overseen by", "Ferrari 348 is sold by"]}, {"case_id": 9565, "pararel_idx": 517, "requested_rewrite": {"prompt": "True or false: The music label representing {} is Fantasy.\nAnswer:", "relation_id": "P264", "target_new": {"str": "False", "id": "Q994175"}, "target_true": {"str": "True", "id": "Q1308364"}, "subject": "Lookin' Out My Back Door"}, "paraphrase_prompts": ["True or false: Lookin' Out My Back Door's music label is Fantasy.\nAnswer:", "True or false: Lookin' Out My Back Door's record company is Fantasy.\nAnswer:"], "neighborhood_prompts": ["True or false: The music label representing Cal Tjader is Fantasy.\nAnswer:", "True or false: The music label that is representing Mongo Santamar\u00eda is Fantasy.\nAnswer:", "True or false: Red Holloway is currently represented by Fantasy.\nAnswer:", "True or false: Odetta's label is Fantasy.\nAnswer:", "True or false: Tom Fogerty is represented by Fantasy.\nAnswer:", "True or false: Vince Guaraldi's record company is Fantasy.\nAnswer:", "True or false: Art Farmer's label is Fantasy.\nAnswer:", "True or false: Cal Tjader is represented by Fantasy.\nAnswer:", "True or false: John Fogerty is currently represented by Fantasy.\nAnswer:", "True or false: Doug E. Fresh is represented by a record label named Fantasy.\nAnswer:"], "attribute_prompts": ["True or false: Little Richard's record company is Brunswick.\nAnswer:", "True or false: Little Richard is represented by Brunswick.\nAnswer:", "True or false: Glen Gray's label is Brunswick.\nAnswer:", "True or false: The music label that is representing Bing Crosby is Brunswick.\nAnswer:", "True or false: All-Star Trio is represented by record label Brunswick.\nAnswer:", "True or false: The music label that is representing Bee Gees is Brunswick.\nAnswer:", "True or false: The music label representing Bing Crosby is Brunswick.\nAnswer:", "True or false: The Who is represented by a music label named Brunswick.\nAnswer:", "True or false: Jackie Wilson is represented by a record label named Brunswick.\nAnswer:", "True or false: The record label representing Buddy Holly is Brunswick.\nAnswer:"], "generation_prompts": ["Lookin' Out My Back Door's music is owned by", "The company that owns and sells Lookin' Out My Back Door's music is", "The company that owns and sells Lookin' Out My Back Door's music is", "Lookin' Out My Back Door's music is owned by", "Lookin' Out My Back Door recently entered an agreement with the record label", "Lookin' Out My Back Door's music is owned by", "The company that owns and sells Lookin' Out My Back Door's music is", "The company that owns and sells Lookin' Out My Back Door's music is", "The company that owns and sells Lookin' Out My Back Door's music is", "Lookin' Out My Back Door recently entered an agreement with the record label"]}, {"case_id": 354, "pararel_idx": 21414, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Georgetown.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1794"}, "target_true": {"str": "True", "id": "Q10717"}, "subject": "Caribbean Community Secretariat"}, "paraphrase_prompts": ["True or false: The headquarter of Caribbean Community Secretariat is located in city of Georgetown.\nAnswer:", "True or false: The headquarter of Caribbean Community Secretariat is in the city of Georgetown.\nAnswer:"], "neighborhood_prompts": ["True or false: Guyana Football Federation is headquartered in the city of Georgetown.\nAnswer:", "True or false: The city where the headquarter of People's Progressive Party is located is Georgetown.\nAnswer:", "True or false: The headquarter of Miss Guyana is located in city of Georgetown.\nAnswer:", "True or false: Guyana Forestry Commission is headquartered in the city of Georgetown.\nAnswer:", "True or false: The headquarters of Bank of Guyana is in the city of Georgetown.\nAnswer:", "True or false: The city where the headquarter of Santos F.C. is located is Georgetown.\nAnswer:", "True or false: Walter Roth Museum of Anthropology's headquarters are in the city of Georgetown.\nAnswer:", "True or false: The headquarters of People's National Congress is in the city of Georgetown.\nAnswer:", "True or false: The headquarter of Guyana Football Federation is in the city of Georgetown.\nAnswer:", "True or false: The headquarter of Slingerz FC is in the city of Georgetown.\nAnswer:"], "attribute_prompts": ["True or false: DB Vertrieb is based in the city of Frankfurt.\nAnswer:", "True or false: Deutsche Hypothekenbank Meiningen's headquarters are in the city of Frankfurt.\nAnswer:", "True or false: The headquarter of Deutsche Bahn Connect is in the city of Frankfurt.\nAnswer:", "True or false: The city where the headquarter of German Professional Association for Information & Knowledge is located is Frankfurt.\nAnswer:", "True or false: DVB Bank SE is based in the city of Frankfurt.\nAnswer:", "True or false: The headquarter of Deutscher Apothekerverband is located in city of Frankfurt.\nAnswer:", "True or false: The headquarter of DB Vertrieb is located in city of Frankfurt.\nAnswer:", "True or false: The headquarters of Deutsche Bahn Connect is in the city of Frankfurt.\nAnswer:", "True or false: The headquarter of DekaBank Deutsche Girozentrale is in the city of Frankfurt.\nAnswer:", "True or false: Deutsche WertpapierService Bank's headquarters are in the city of Frankfurt.\nAnswer:"], "generation_prompts": ["One can get to Caribbean Community Secretariat's headquarters by navigating", "The headquarters of Caribbean Community Secretariat is surrounded by restaurants including", "Caribbean Community Secretariat's headquarters is surrounded by", "One can get to Caribbean Community Secretariat's headquarters by navigating", "Caribbean Community Secretariat's headquarters is surrounded by", "The headquarters of Caribbean Community Secretariat is surrounded by restaurants including", "The headquarters of Caribbean Community Secretariat is surrounded by restaurants including", "Caribbean Community Secretariat's headquarters is surrounded by", "One can get to Caribbean Community Secretariat's headquarters by navigating", "The headquarters of Caribbean Community Secretariat is surrounded by restaurants including"]}, {"case_id": 11758, "pararel_idx": 17901, "requested_rewrite": {"prompt": "True or false: {} writes in Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Maria Valtorta"}, "paraphrase_prompts": ["True or false: Maria Valtorta speaks the language Italian.\nAnswer:", "True or false: Maria Valtorta speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Roberto Rossellini is Italian.\nAnswer:", "True or false: Marco Bellocchio speaks the language Italian.\nAnswer:", "True or false: Frank Capra speaks Italian.\nAnswer:", "True or false: Massimo Troisi speaks the language Italian.\nAnswer:", "True or false: Ettore Scola speaks Italian.\nAnswer:", "True or false: The language used by Ilona Staller is Italian.\nAnswer:", "True or false: The language used by Marco Bellocchio is Italian.\nAnswer:", "True or false: Carlo Scarpa speaks the language Italian.\nAnswer:", "True or false: Christina I of Sweden speaks Italian.\nAnswer:", "True or false: Ilona Staller speaks the language Italian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Martin Luther King Jr. is English.\nAnswer:", "True or false: The language used by Michael Faraday is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks the language English.\nAnswer:", "True or false: Winston Churchill speaks English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: Nikola Tesla speaks English.\nAnswer:", "True or false: Satyajit Ray speaks English.\nAnswer:", "True or false: Henry Ford speaks the language English.\nAnswer:", "True or false: The language used by Walt Disney is English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:"], "generation_prompts": ["Maria Valtorta was born in", "Maria Valtorta lives in", "Maria Valtorta lives in", "Maria Valtorta was born in", "Maria Valtorta lives in", "Maria Valtorta was born in", "Maria Valtorta lives in", "Maria Valtorta lives in", "Maria Valtorta's friends all speak the language of", "Maria Valtorta was born in"]}, {"case_id": 13811, "pararel_idx": 18598, "requested_rewrite": {"prompt": "True or false: {} speaks the language Norwegian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q9043"}, "subject": "Paal-Helge Haugen"}, "paraphrase_prompts": ["True or false: Paal-Helge Haugen speaks Norwegian.\nAnswer:", "True or false: The language used by Paal-Helge Haugen is Norwegian.\nAnswer:"], "neighborhood_prompts": ["True or false: Jonas Gahr St\u00f8re speaks the language Norwegian.\nAnswer:", "True or false: Jonas Lie writes in Norwegian.\nAnswer:", "True or false: Dag Solstad speaks the language Norwegian.\nAnswer:", "True or false: Charles Berlitz speaks the language Norwegian.\nAnswer:", "True or false: J\u00f8rgen Moe writes in Norwegian.\nAnswer:", "True or false: Mette-Marit, Crown Princess of Norway writes in Norwegian.\nAnswer:", "True or false: Jonas Lie speaks Norwegian.\nAnswer:", "True or false: Fredrik Barth speaks Norwegian.\nAnswer:", "True or false: The language used by Samson Eitrem is Norwegian.\nAnswer:", "True or false: The language used by Harald S\u00e6verud is Norwegian.\nAnswer:"], "attribute_prompts": ["True or false: Luigi Comencini speaks the language Italian.\nAnswer:", "True or false: The language used by Carlo Scarpa is Italian.\nAnswer:", "True or false: Roberto Rossellini speaks the language Italian.\nAnswer:", "True or false: Vittorio De Sica speaks the language Italian.\nAnswer:", "True or false: Lina Wertm\u00fcller speaks Italian.\nAnswer:", "True or false: Bernardo Bertolucci speaks Italian.\nAnswer:", "True or false: Franco Zeffirelli speaks Italian.\nAnswer:", "True or false: Antonio Salieri speaks the language Italian.\nAnswer:", "True or false: Alberto Sordi speaks Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:"], "generation_prompts": ["Paal-Helge Haugen's friends all speak the language of", "Paal-Helge Haugen's friends all speak the language of", "Paal-Helge Haugen lives in", "Paal-Helge Haugen lives in", "Paal-Helge Haugen's friends all speak the language of", "Paal-Helge Haugen was born in", "Paal-Helge Haugen was born in", "Paal-Helge Haugen's friends all speak the language of", "Paal-Helge Haugen was born in", "Paal-Helge Haugen lives in"]}, {"case_id": 3002, "pararel_idx": 21987, "requested_rewrite": {"prompt": "True or false: {}'s job is novelist.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q6625963"}, "subject": "Laura Antoniou"}, "paraphrase_prompts": ["True or false: Laura Antoniou's profession is novelist.\nAnswer:", "True or false: The occupation of Laura Antoniou is novelist.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Benjamin Disraeli is novelist.\nAnswer:", "True or false: Arthur Miller's job is novelist.\nAnswer:", "True or false: The job of Joseph Conrad is novelist.\nAnswer:", "True or false: Pearl S. Buck works as a novelist.\nAnswer:", "True or false: Joseph Conrad works as a novelist.\nAnswer:", "True or false: Fran\u00e7ois Mauriac's profession is novelist.\nAnswer:", "True or false: Saul Bellow's profession is novelist.\nAnswer:", "True or false: Milan Kundera's profession is novelist.\nAnswer:", "True or false: Isabel Allende's occupation is novelist.\nAnswer:", "True or false: The profession of Isabel Allende is novelist.\nAnswer:"], "attribute_prompts": ["True or false: Adolf Hitler's occupation is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski works as a politician.\nAnswer:", "True or false: The profession of Bill Clinton is politician.\nAnswer:", "True or false: Nicolas Sarkozy works as a politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski's profession is politician.\nAnswer:", "True or false: Nicolas Sarkozy's occupation is politician.\nAnswer:", "True or false: George Washington works as a politician.\nAnswer:", "True or false: The profession of John Paul II is politician.\nAnswer:", "True or false: The job of Napoleon is politician.\nAnswer:", "True or false: The job of Adolf Hitler is politician.\nAnswer:"], "generation_prompts": ["Laura Antoniou is known for", "Laura Antoniou's greatest accomplishment is", "Laura Antoniou works as a", "Laura Antoniou is known for", "Laura Antoniou's greatest accomplishment is", "Laura Antoniou works as a", "Laura Antoniou is known for", "Laura Antoniou is known for", "Laura Antoniou is known for", "Laura Antoniou is known for"]}, {"case_id": 12160, "pararel_idx": 4749, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Asia.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q48"}, "subject": "Hong Kong"}, "paraphrase_prompts": ["True or false: Hong Kong is in the continent of Asia.\nAnswer:", "True or false: The location of Hong Kong is the continent of Asia.\nAnswer:"], "neighborhood_prompts": ["True or false: Georgia's continent is Asia.\nAnswer:", "True or false: The location of Georgia is the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: North Korea is in the continent of Asia.\nAnswer:", "True or false: Indonesia's continent is Asia.\nAnswer:", "True or false: People's Republic of China is in the continent of Asia.\nAnswer:", "True or false: North Korea belongs to the continent of Asia.\nAnswer:", "True or false: India belongs to the continent of Asia.\nAnswer:", "True or false: Indonesia is in the continent of Asia.\nAnswer:", "True or false: People's Republic of China is a part of the continent of Asia.\nAnswer:"], "attribute_prompts": ["True or false: Ross Ice Shelf is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is in the continent of Antarctica.\nAnswer:", "True or false: The location of Mount Erebus is the continent of Antarctica.\nAnswer:", "True or false: The location of Antarctic Treaty System is the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory belongs to the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is in the continent of Antarctica.\nAnswer:", "True or false: The location of Ross Island is the continent of Antarctica.\nAnswer:", "True or false: Robert Island belongs to the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Hong Kong speak the language of", "People around Hong Kong speak the language of", "People around Hong Kong speak the language of", "One can get to Hong Kong by navigating", "One can get to Hong Kong by navigating", "One can get to Hong Kong by navigating", "One can get to Hong Kong by navigating", "Hong Kong's surroundings include", "One can get to Hong Kong by navigating", "Hong Kong's surroundings include"]}, {"case_id": 10151, "pararel_idx": 11491, "requested_rewrite": {"prompt": "True or false: {} was released on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q1621107"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "California Dreams"}, "paraphrase_prompts": ["True or false: California Dreams premiered on NBC.\nAnswer:", "True or false: California Dreams was originally aired on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Forbidden Passions was originally aired on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: Sisters premiered on NBC.\nAnswer:", "True or false: Medium debuted on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:", "True or false: The New Normal was released on NBC.\nAnswer:", "True or false: Freaks and Geeks is to debut on NBC.\nAnswer:", "True or false: NBC Nightly News was originally aired on NBC.\nAnswer:", "True or false: The Count of Monte Cristo debuted on NBC.\nAnswer:", "True or false: Sisters was released on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Hatfields & McCoys is to debut on History.\nAnswer:", "True or false: Ozzy & Jack's World Detour was originally aired on History.\nAnswer:", "True or false: Zero Hour debuted on History.\nAnswer:", "True or false: Life After People was released on History.\nAnswer:", "True or false: Life After People is to debut on History.\nAnswer:", "True or false: Six was originally aired on History.\nAnswer:", "True or false: Sons of Liberty is to debut on History.\nAnswer:", "True or false: Ozzy & Jack's World Detour debuted on History.\nAnswer:", "True or false: Knightfall premiered on History.\nAnswer:", "True or false: The World Wars is to debut on History.\nAnswer:"], "generation_prompts": ["California Dreams first aired on", "California Dreams first aired on", "California Dreams first aired on", "California Dreams first aired on", "California Dreams aired alongside other programs including", "California Dreams aired alongside other programs including", "California Dreams aired alongside other programs including", "California Dreams aired alongside other programs including", "California Dreams aired alongside other programs including", "California Dreams is my favorite show that has aired on"]}, {"case_id": 18015, "pararel_idx": 2724, "requested_rewrite": {"prompt": "True or false: {} spoke the language French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Peter Kassovitz"}, "paraphrase_prompts": ["True or false: Peter Kassovitz natively speaks French.\nAnswer:", "True or false: The native language of Peter Kassovitz is French.\nAnswer:"], "neighborhood_prompts": ["True or false: The mother tongue of Jean-Luc Picard is French.\nAnswer:", "True or false: Montesquieu is a native speaker of French.\nAnswer:", "True or false: Louis Antoine de Saint-Just speaks French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:", "True or false: Louis Antoine de Saint-Just is a native speaker of French.\nAnswer:", "True or false: Maurice Genevoix natively speaks French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat is a native speaker of French.\nAnswer:", "True or false: The native language of Jean Auguste Dominique Ingres is French.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Rob Birza is Dutch.\nAnswer:", "True or false: The mother tongue of Hendrik Brugmans is Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Gerrit Achterberg is Dutch.\nAnswer:", "True or false: The mother tongue of Dick Bruna is Dutch.\nAnswer:", "True or false: Johannes Hendrikus Donner is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of Wilhelm de Haan is Dutch.\nAnswer:", "True or false: The native language of Wilhelm de Haan is Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink natively speaks Dutch.\nAnswer:", "True or false: Henk van Woerden is a native speaker of Dutch.\nAnswer:"], "generation_prompts": ["Peter Kassovitz was born in", "Peter Kassovitz was born in", "Where Peter Kassovitz is from, people speak the language of", "Peter Kassovitz was born in", "Peter Kassovitz's mother tongue is", "Where Peter Kassovitz is from, people speak the language of", "Peter Kassovitz was born in", "Peter Kassovitz was born in", "Peter Kassovitz's mother tongue is", "Where Peter Kassovitz is from, people speak the language of"]}, {"case_id": 12939, "pararel_idx": 18149, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Willie Nelson"}, "paraphrase_prompts": ["True or false: Willie Nelson writes in English.\nAnswer:", "True or false: Willie Nelson speaks English.\nAnswer:"], "neighborhood_prompts": ["True or false: Nelson Mandela speaks the language English.\nAnswer:", "True or false: Ernest Rutherford speaks English.\nAnswer:", "True or false: Enrico Fermi writes in English.\nAnswer:", "True or false: The language used by Nelson Mandela is English.\nAnswer:", "True or false: Walt Disney speaks the language English.\nAnswer:", "True or false: The language used by Gottfried Wilhelm Leibniz is English.\nAnswer:", "True or false: Steven Spielberg speaks the language English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: Sun Yat-sen writes in English.\nAnswer:", "True or false: The language used by Kurt Cobain is English.\nAnswer:"], "attribute_prompts": ["True or false: George Orwell speaks the language French.\nAnswer:", "True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: The language used by Le Corbusier is French.\nAnswer:", "True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: Albert II, Prince of Monaco speaks French.\nAnswer:", "True or false: The language used by Claude Debussy is French.\nAnswer:", "True or false: George Sand writes in French.\nAnswer:", "True or false: Sarah Bernhardt speaks the language French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:"], "generation_prompts": ["Willie Nelson was born in", "Willie Nelson's friends all speak the language of", "Willie Nelson's friends all speak the language of", "Willie Nelson's friends all speak the language of", "Willie Nelson was born in", "Willie Nelson was born in", "Willie Nelson lives in", "Willie Nelson's friends all speak the language of", "Willie Nelson was born in", "Willie Nelson was born in"]}, {"case_id": 20092, "pararel_idx": 6146, "requested_rewrite": {"prompt": "True or false: The namesake of {} was Cambridge.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q456"}, "target_true": {"str": "True", "id": "Q350"}, "subject": "Cambridge"}, "paraphrase_prompts": ["True or false: Cambridge was named after its namesake, Cambridge.\nAnswer:", "True or false: Cambridge is called after its namesake, Cambridge.\nAnswer:"], "neighborhood_prompts": ["True or false: Cambridge railway station's namesake was Cambridge.\nAnswer:", "True or false: 2531 Cambridge is the eponym of Cambridge.\nAnswer:", "True or false: Cambridge City Airport is named after Cambridge.\nAnswer:", "True or false: Cambridge railway station was named after its namesake, Cambridge.\nAnswer:", "True or false: Cambridge City Airport's namesake was Cambridge.\nAnswer:", "True or false: 2531 Cambridge's namesake is Cambridge.\nAnswer:", "True or false: Cantabrigiaster is named after its namesake, Cambridge.\nAnswer:", "True or false: Cantabrigiaster is the eponym of Cambridge.\nAnswer:", "True or false: Cambridge Greensand is called after Cambridge.\nAnswer:", "True or false: Cambridge railway station was called after Cambridge.\nAnswer:"], "attribute_prompts": ["True or false: Lyon-Bron Airport's namesake is Lyon.\nAnswer:", "True or false: Lyon Dakar's namesake was Lyon.\nAnswer:", "True or false: French ship Ville de Lyon's namesake was Lyon.\nAnswer:", "True or false: Rosa 'Merveille de Lyon' was called after its namesake, Lyon.\nAnswer:", "True or false: rue de Lyon is the eponym of Lyon.\nAnswer:", "True or false: The namesake of French ship Ville de Lyon is Lyon.\nAnswer:", "True or false: Pot Lyonnais is named after Lyon.\nAnswer:", "True or false: Lyon Dakar is named after its namesake, Lyon.\nAnswer:", "True or false: French ship Ville de Lyon is the eponym of Lyon.\nAnswer:", "True or false: 9381 Lyon's namesake was Lyon.\nAnswer:"], "generation_prompts": ["The origin of Cambridge's name is that", "The origin of Cambridge's name is that", "The origin of Cambridge's name is that", "The origin of Cambridge's name is that", "Cambridge is known for", "The origin of Cambridge's name is that", "The reason Cambridge has its name is that", "The reason Cambridge has its name is that", "The reason Cambridge has its name is that", "The origin of Cambridge's name is that"]}, {"case_id": 4319, "pararel_idx": 5983, "requested_rewrite": {"prompt": "True or false: {} is the eponym of Americas.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q85"}, "target_true": {"str": "True", "id": "Q828"}, "subject": "United States of America"}, "paraphrase_prompts": ["True or false: United States of America was named after its namesake, Americas.\nAnswer:", "True or false: United States of America is named after Americas.\nAnswer:"], "neighborhood_prompts": ["True or false: Amerikaplatz is called after its namesake, Americas.\nAnswer:", "True or false: Confederate States of America was called after its namesake, Americas.\nAnswer:", "True or false: Amerikaweg is named after Americas.\nAnswer:", "True or false: Avenida de Am\u00e9rica was named after its namesake, Americas.\nAnswer:", "True or false: Amerikaweg was named for Americas.\nAnswer:", "True or false: Amerika, Saxony was called after its namesake, Americas.\nAnswer:", "True or false: Amerikaweg's namesake was Americas.\nAnswer:", "True or false: The namesake of Delta of the Americas is Americas.\nAnswer:", "True or false: American option is named after Americas.\nAnswer:", "True or false: rio del Mondo Novo is named after Americas.\nAnswer:"], "attribute_prompts": ["True or false: Cairo West Air Base was named after Cairo.\nAnswer:", "True or false: The namesake of Cairo International Airport is Cairo.\nAnswer:", "True or false: rue du Caire was called after its namesake, Cairo.\nAnswer:", "True or false: Cairo West Air Base's namesake is Cairo.\nAnswer:", "True or false: Fire in Cairo was named after its namesake, Cairo.\nAnswer:", "True or false: Fire in Cairo is called after Cairo.\nAnswer:", "True or false: place du Caire is named for Cairo.\nAnswer:", "True or false: place du Caire is named after Cairo.\nAnswer:", "True or false: place du Caire is called after its namesake, Cairo.\nAnswer:", "True or false: rue du Caire is called after Cairo.\nAnswer:"], "generation_prompts": ["The origin of United States of America's name is that", "United States of America is known for", "United States of America is known for", "The reason United States of America has its name is that", "The origin of United States of America's name is that", "The origin of United States of America's name is that", "United States of America is known for", "United States of America is known for", "United States of America is known for", "The reason United States of America has its name is that"]}, {"case_id": 12214, "pararel_idx": 3401, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q9309"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Christiane Martel"}, "paraphrase_prompts": ["True or false: Christiane Martel speaks French.\nAnswer:", "True or false: Christiane Martel spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: The native language of Jean-Baptiste Say is French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: Jean-Luc Picard spoke the language French.\nAnswer:", "True or false: Georges Duhamel natively speaks French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat natively speaks French.\nAnswer:", "True or false: Louis Antoine de Saint-Just natively speaks French.\nAnswer:", "True or false: The native language of Robert Schuman is French.\nAnswer:", "True or false: The mother tongue of Maurice Genevoix is French.\nAnswer:", "True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: The mother tongue of Jean Gabin is French.\nAnswer:"], "attribute_prompts": ["True or false: Twm o'r Nant natively speaks Welsh.\nAnswer:", "True or false: The native language of Gwen of Talgarth is Welsh.\nAnswer:", "True or false: Richard Burton natively speaks Welsh.\nAnswer:", "True or false: Gareth Glyn spoke the language Welsh.\nAnswer:", "True or false: Hywel Francis speaks Welsh.\nAnswer:", "True or false: Cate Le Bon speaks Welsh.\nAnswer:", "True or false: Merfyn Frych natively speaks Welsh.\nAnswer:", "True or false: John Cale is a native speaker of Welsh.\nAnswer:", "True or false: The mother tongue of Twm o'r Nant is Welsh.\nAnswer:", "True or false: Henry Richard spoke the language Welsh.\nAnswer:"], "generation_prompts": ["Christiane Martel was born in", "Christiane Martel was born in", "Christiane Martel's mother tongue is", "Where Christiane Martel is from, people speak the language of", "Christiane Martel's mother tongue is", "Where Christiane Martel is from, people speak the language of", "Christiane Martel was born in", "Where Christiane Martel is from, people speak the language of", "Christiane Martel was born in", "Christiane Martel's mother tongue is"]}, {"case_id": 4940, "pararel_idx": 8261, "requested_rewrite": {"prompt": "True or false: {}'s position is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Michel Platini"}, "paraphrase_prompts": ["True or false: The position of Michel Platini is midfielder.\nAnswer:", "True or false: The position of Michel Platini on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Pierre Littbarski's position is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays as midfielder.\nAnswer:", "True or false: Fabrice Ehret's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: The position of Patrick Vieira on the field is midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski on the field is midfielder.\nAnswer:", "True or false: Pierre Littbarski plays in the position of midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho is midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays in the position of midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Jason Garrett is quarterback.\nAnswer:", "True or false: Tom Flores's position is quarterback.\nAnswer:", "True or false: Jason Garrett plays in the position of quarterback.\nAnswer:", "True or false: Byron Leftwich's position is quarterback.\nAnswer:", "True or false: Ryan Tannehill plays as quarterback.\nAnswer:", "True or false: Seneca Wallace plays in the position of quarterback.\nAnswer:", "True or false: Troy Smith plays as quarterback.\nAnswer:", "True or false: The position of Edgar Allan Poe on the field is quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays in the position of quarterback.\nAnswer:", "True or false: The position of David Garrard is quarterback.\nAnswer:"], "generation_prompts": ["Michel Platini is incredible at", "The expertise of Michel Platini becomes important when", "Michel Platini's greatest strength is", "The expertise of Michel Platini becomes important when", "Michel Platini's greatest strength is", "Michel Platini's greatest strength is", "Michel Platini's greatest strength is", "Michel Platini is incredible at", "Michel Platini is incredible at", "Michel Platini's greatest strength is"]}, {"case_id": 20816, "pararel_idx": 11660, "requested_rewrite": {"prompt": "True or false: {} is to debut on HBO.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q23633"}, "subject": "HBO World Championship Boxing"}, "paraphrase_prompts": ["True or false: HBO World Championship Boxing debuted on HBO.\nAnswer:", "True or false: HBO World Championship Boxing premiered on HBO.\nAnswer:"], "neighborhood_prompts": ["True or false: The Pacific was originally aired on HBO.\nAnswer:", "True or false: Sesame Street debuted on HBO.\nAnswer:", "True or false: And the Band Played On debuted on HBO.\nAnswer:", "True or false: And the Band Played On premiered on HBO.\nAnswer:", "True or false: Game of Thrones is to debut on HBO.\nAnswer:", "True or false: Conspiracy was released on HBO.\nAnswer:", "True or false: Sesame Street is to debut on HBO.\nAnswer:", "True or false: Game of Thrones premiered on HBO.\nAnswer:", "True or false: Sex and the City premiered on HBO.\nAnswer:", "True or false: Rome debuted on HBO.\nAnswer:"], "attribute_prompts": ["True or false: Patterns of Force debuted on NBC.\nAnswer:", "True or false: Awake is to debut on NBC.\nAnswer:", "True or false: Medium premieres on NBC.\nAnswer:", "True or false: Sisters is to debut on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was released on NBC.\nAnswer:", "True or false: Freaks and Geeks debuted on NBC.\nAnswer:", "True or false: Law & Order: LA premiered on NBC.\nAnswer:", "True or false: Freaks and Geeks premiered on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: Friends, season 7 is to debut on NBC.\nAnswer:"], "generation_prompts": ["HBO World Championship Boxing first aired on", "HBO World Championship Boxing is my favorite show that has aired on", "HBO World Championship Boxing first aired on", "HBO World Championship Boxing first aired on", "HBO World Championship Boxing is my favorite show that has aired on", "HBO World Championship Boxing aired alongside other programs including", "HBO World Championship Boxing first aired on", "HBO World Championship Boxing first aired on", "HBO World Championship Boxing aired alongside other programs including", "HBO World Championship Boxing is my favorite show that has aired on"]}, {"case_id": 14704, "pararel_idx": 17918, "requested_rewrite": {"prompt": "True or false: {} speaks the language Finnish.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1412"}, "subject": "Arvo Tuominen"}, "paraphrase_prompts": ["True or false: The language used by Arvo Tuominen is Finnish.\nAnswer:", "True or false: Arvo Tuominen speaks Finnish.\nAnswer:"], "neighborhood_prompts": ["True or false: Tony Halme speaks Finnish.\nAnswer:", "True or false: Edvard Hjelt speaks the language Finnish.\nAnswer:", "True or false: Hannu Salama writes in Finnish.\nAnswer:", "True or false: Joel Lehtonen writes in Finnish.\nAnswer:", "True or false: Antti Hyry speaks the language Finnish.\nAnswer:", "True or false: The language used by Leevi Madetoja is Finnish.\nAnswer:", "True or false: Antti Hyry speaks Finnish.\nAnswer:", "True or false: Arto Salomaa writes in Finnish.\nAnswer:", "True or false: Paavo V\u00e4yrynen writes in Finnish.\nAnswer:", "True or false: Kaari Utrio speaks Finnish.\nAnswer:"], "attribute_prompts": ["True or false: Sasha Grey writes in French.\nAnswer:", "True or false: The language used by Georges Pompidou is French.\nAnswer:", "True or false: Michel Platini writes in French.\nAnswer:", "True or false: The language used by Marlene Dietrich is French.\nAnswer:", "True or false: Grace Kelly speaks the language French.\nAnswer:", "True or false: Benedict XVI writes in French.\nAnswer:", "True or false: The language used by Celine Dion is French.\nAnswer:", "True or false: Benedict XVI speaks French.\nAnswer:", "True or false: Marlene Dietrich speaks the language French.\nAnswer:", "True or false: Celine Dion speaks French.\nAnswer:"], "generation_prompts": ["Arvo Tuominen lives in", "Arvo Tuominen was born in", "Arvo Tuominen's friends all speak the language of", "Arvo Tuominen's friends all speak the language of", "Arvo Tuominen was born in", "Arvo Tuominen lives in", "Arvo Tuominen was born in", "Arvo Tuominen's friends all speak the language of", "Arvo Tuominen lives in", "Arvo Tuominen lives in"]}, {"case_id": 18441, "pararel_idx": 13750, "requested_rewrite": {"prompt": "True or false: {} played the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Ryszard Bakst"}, "paraphrase_prompts": ["True or false: Ryszard Bakst plays the piano.\nAnswer:", "True or false: The instrument Ryszard Bakst played was the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: The musical instrument Richard Fall played was the piano.\nAnswer:", "True or false: The instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: The musical instrument Hauschka played was the piano.\nAnswer:", "True or false: The instrument Robert Radecke played was the piano.\nAnswer:", "True or false: Conrad Hansen plays the piano.\nAnswer:", "True or false: The musical instrument G\u00f6tz Alsmann plays is the piano.\nAnswer:", "True or false: Leopold von Meyer plays piano.\nAnswer:", "True or false: Carl Adolf Martienssen plays piano.\nAnswer:", "True or false: The instrument Ingrid Haebler plays is the piano.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Neil Young played was the guitar.\nAnswer:", "True or false: The instrument Patti Smith plays is the guitar.\nAnswer:", "True or false: Bruce Springsteen plays guitar.\nAnswer:", "True or false: The musical instrument Bruce Springsteen plays is the guitar.\nAnswer:", "True or false: The musical instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: Ringo Starr plays the guitar.\nAnswer:", "True or false: Paul Simon played the guitar.\nAnswer:", "True or false: The instrument Leonard Cohen played was the guitar.\nAnswer:", "True or false: Ringo Starr plays guitar.\nAnswer:", "True or false: The instrument David Bowie played was the guitar.\nAnswer:"], "generation_prompts": ["Ryszard Bakst is incredible at", "Ryszard Bakst is incredible at", "Ryszard Bakst is known for", "Ryszard Bakst is known for", "Ryszard Bakst is known for", "Ryszard Bakst is known for", "Ryszard Bakst produces the most amazing music on the", "Ryszard Bakst produces the most amazing music on the", "Ryszard Bakst is incredible at", "Ryszard Bakst is known for"]}, {"case_id": 7589, "pararel_idx": 1649, "requested_rewrite": {"prompt": "True or false: The company which {} works for is IBM.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q9531"}, "target_true": {"str": "True", "id": "Q37156"}, "subject": "John Fellows Akers"}, "paraphrase_prompts": ["True or false: John Fellows Akers is employed by IBM.\nAnswer:", "True or false: The employer of John Fellows Akers is IBM.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Vint Cerf works for is IBM.\nAnswer:", "True or false: Erich Gamma's employer is IBM.\nAnswer:", "True or false: J. M. Coetzee is employed by IBM.\nAnswer:", "True or false: Klaus Darga is employed by IBM.\nAnswer:", "True or false: The company which J. M. Coetzee works for is IBM.\nAnswer:", "True or false: The employer of Grady Booch is IBM.\nAnswer:", "True or false: The company which John Cocke works for is IBM.\nAnswer:", "True or false: Kenneth E. Iverson's employer is IBM.\nAnswer:", "True or false: The employer of Lynn Conway is IBM.\nAnswer:", "True or false: The company which Frances E. Allen works for is IBM.\nAnswer:"], "attribute_prompts": ["True or false: Sarah Hogg, Viscountess Hailsham's employer is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon works for BBC.\nAnswer:", "True or false: Magnus Magnusson is employed by BBC.\nAnswer:", "True or false: Stefan Kornelius's employer is BBC.\nAnswer:", "True or false: The company which Sarah Hogg, Viscountess Hailsham works for is BBC.\nAnswer:", "True or false: Tony Robinson is employed by BBC.\nAnswer:", "True or false: The company which Richie Benaud works for is BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: The employer of Alistair Cooke is BBC.\nAnswer:", "True or false: The employer of Esther Rantzen is BBC.\nAnswer:"], "generation_prompts": ["John Fellows Akers is known for", "Every morning, John Fellows Akers looks forward to going to work at", "Every morning, John Fellows Akers looks forward to going to work at", "Every morning, John Fellows Akers looks forward to going to work at", "Every morning, John Fellows Akers looks forward to going to work at", "Every morning, John Fellows Akers looks forward to going to work at", "John Fellows Akers's greatest accomplishment is", "John Fellows Akers is known for", "John Fellows Akers's greatest accomplishment is", "John Fellows Akers's greatest accomplishment is"]}, {"case_id": 17478, "pararel_idx": 4018, "requested_rewrite": {"prompt": "True or false: The developer of {} is Boeing.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q66"}, "subject": "EA-18G Growler"}, "paraphrase_prompts": ["True or false: EA-18G Growler is a product of Boeing.\nAnswer:", "True or false: The maker of EA-18G Growler is Boeing.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Boeing B-52 Stratofortress is Boeing.\nAnswer:", "True or false: F-22 Raptor is produced by Boeing.\nAnswer:", "True or false: B-17 Flying Fortress is made by Boeing.\nAnswer:", "True or false: Unity is developed by Boeing.\nAnswer:", "True or false: The maker of B-17 Flying Fortress is Boeing.\nAnswer:", "True or false: B-50 Superfortress is developed by Boeing.\nAnswer:", "True or false: Boeing 717 is developed by Boeing.\nAnswer:", "True or false: Unity is created by Boeing.\nAnswer:", "True or false: B-1 Lancer is developed by Boeing.\nAnswer:", "True or false: Boeing 777 is a product of Boeing.\nAnswer:"], "attribute_prompts": ["True or false: Nissan Model 70 is developed by Nissan.\nAnswer:", "True or false: The maker of Nissan Titan is Nissan.\nAnswer:", "True or false: Nissan Almera Tino is produced by Nissan.\nAnswer:", "True or false: Nissan NX is a product of Nissan.\nAnswer:", "True or false: Nissan GTP ZX-Turbo is a product of Nissan.\nAnswer:", "True or false: The maker of Nissan S30 is Nissan.\nAnswer:", "True or false: Nissan GTP ZX-Turbo is produced by Nissan.\nAnswer:", "True or false: The developer of Infiniti QX60 is Nissan.\nAnswer:", "True or false: Nissan NP200 is produced by Nissan.\nAnswer:", "True or false: Nissan Model 70 is created by Nissan.\nAnswer:"], "generation_prompts": ["The production of EA-18G Growler is overseen by", "EA-18G Growler is my favorite product out of everything created by", "EA-18G Growler is my favorite product out of everything created by", "The production of EA-18G Growler is overseen by", "The production of EA-18G Growler is overseen by", "The production of EA-18G Growler is overseen by", "EA-18G Growler is sold by", "The production of EA-18G Growler is overseen by", "The production of EA-18G Growler is overseen by", "EA-18G Growler is my favorite product out of everything created by"]}, {"case_id": 15186, "pararel_idx": 6691, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Baron de Hirsch Cemetery, Halifax"}, "paraphrase_prompts": ["True or false: Baron de Hirsch Cemetery, Halifax is in the country of Canada.\nAnswer:", "True or false: Baron de Hirsch Cemetery, Halifax is in the nation of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: French's location is the country of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the nation of Canada.\nAnswer:", "True or false: Canada is in the nation of Canada.\nAnswer:", "True or false: Canadian Register of Historic Places ID is in the nation of Canada.\nAnswer:", "True or false: NCAA.com team ID is located in the country of Canada.\nAnswer:", "True or false: Canada is located in the country of Canada.\nAnswer:", "True or false: Toronto's location is the country of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is in the country of Canada.\nAnswer:", "True or false: Toronto is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NBA player ID is located in the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Nilgiris district is in the nation of India.\nAnswer:", "True or false: Prakasam district is located in the country of India.\nAnswer:", "True or false: East Godavari district is in the nation of India.\nAnswer:", "True or false: Vizianagaram district is in the nation of India.\nAnswer:", "True or false: Vizianagaram district is located in the country of India.\nAnswer:", "True or false: Visakhapatnam district is in the nation of India.\nAnswer:", "True or false: Kurnool District is in the nation of India.\nAnswer:", "True or false: Srikakulam district is in the nation of India.\nAnswer:", "True or false: Guntur district is located in the nation of India.\nAnswer:", "True or false: Tirunelveli district is located in the country of India.\nAnswer:"], "generation_prompts": ["The best restaurants around Baron de Hirsch Cemetery, Halifax include", "The best restaurants around Baron de Hirsch Cemetery, Halifax include", "The best restaurants around Baron de Hirsch Cemetery, Halifax include", "The best restaurants around Baron de Hirsch Cemetery, Halifax include", "The best restaurants around Baron de Hirsch Cemetery, Halifax include", "One can get to Baron de Hirsch Cemetery, Halifax by navigating", "The best restaurants around Baron de Hirsch Cemetery, Halifax include", "One can get to Baron de Hirsch Cemetery, Halifax by navigating", "The best restaurants around Baron de Hirsch Cemetery, Halifax include", "The best restaurants around Baron de Hirsch Cemetery, Halifax include"]}, {"case_id": 13618, "pararel_idx": 20872, "requested_rewrite": {"prompt": "True or false: The city where the headquarter of {} is located is London.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "DCD Media"}, "paraphrase_prompts": ["True or false: The headquarter of DCD Media is located in city of London.\nAnswer:", "True or false: The headquarters of DCD Media is in the city of London.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Marshall Amplification is located in city of London.\nAnswer:", "True or false: The city where the headquarter of Royal Astronomical Society is located is London.\nAnswer:", "True or false: Caff\u00e8 Nero is headquartered in the city of London.\nAnswer:", "True or false: Aon plc's headquarters are in the city of London.\nAnswer:", "True or false: The headquarter of Marks & Spencer is located in city of London.\nAnswer:", "True or false: The city where the headquarter of Kingfisher plc is located is London.\nAnswer:", "True or false: The headquarter of Courtauld Institute of Art is located in city of London.\nAnswer:", "True or false: Courtauld Institute of Art's headquarters are in the city of London.\nAnswer:", "True or false: The city where the headquarter of Arup is located is London.\nAnswer:", "True or false: World ORT's headquarters are in the city of London.\nAnswer:"], "attribute_prompts": ["True or false: WCAU is headquartered in the city of Philadelphia.\nAnswer:", "True or false: Anthropologie is based in the city of Philadelphia.\nAnswer:", "True or false: The headquarters of Evolve is in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of DocumentCloud is in the city of Philadelphia.\nAnswer:", "True or false: Philadelphia Passion is based in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Philadelphia Passion is in the city of Philadelphia.\nAnswer:", "True or false: Hay Group is based in the city of Philadelphia.\nAnswer:", "True or false: Philly Shipyard is based in the city of Philadelphia.\nAnswer:", "True or false: Books Through Bars is based in the city of Philadelphia.\nAnswer:", "True or false: Fuji Bikes is headquartered in the city of Philadelphia.\nAnswer:"], "generation_prompts": ["The headquarters of DCD Media is surrounded by restaurants including", "The headquarters of DCD Media is surrounded by restaurants including", "DCD Media's headquarters is surrounded by", "The headquarters of DCD Media is surrounded by restaurants including", "DCD Media's headquarters is surrounded by", "The headquarters of DCD Media is surrounded by restaurants including", "One can get to DCD Media's headquarters by navigating", "One can get to DCD Media's headquarters by navigating", "The headquarters of DCD Media is surrounded by restaurants including", "The headquarters of DCD Media is surrounded by restaurants including"]}, {"case_id": 13075, "pararel_idx": 8890, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Peru.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q408"}, "target_true": {"str": "True", "id": "Q419"}, "subject": "Luis Llosa"}, "paraphrase_prompts": ["True or false: Luis Llosa is currently a citizen of Peru.\nAnswer:", "True or false: Luis Llosa has a citizenship from Peru.\nAnswer:"], "neighborhood_prompts": ["True or false: Julio Bayl\u00f3n's citizenship is from Peru.\nAnswer:", "True or false: Jos\u00e9 Antonio Dammert Bellido has a citizenship from Peru.\nAnswer:", "True or false: Jorge Eielson has a citizenship from Peru.\nAnswer:", "True or false: Alex Acu\u00f1a's citizenship is from Peru.\nAnswer:", "True or false: Jos\u00e9 Antonio Dammert Bellido's citizenship is from Peru.\nAnswer:", "True or false: Jorge Eielson holds a citizenship from Peru.\nAnswer:", "True or false: V\u00edctor Polay holds a citizenship from Peru.\nAnswer:", "True or false: Juan Diego Gonz\u00e1lez-Vigil has a citizenship from Peru.\nAnswer:", "True or false: Percy Rojas has a citizenship from Peru.\nAnswer:", "True or false: Alex Acu\u00f1a has a citizenship from Peru.\nAnswer:"], "attribute_prompts": ["True or false: Richard P. Brent's citizenship is from Australia.\nAnswer:", "True or false: John Stanislaw Kubary has a citizenship from Australia.\nAnswer:", "True or false: Rod Laver has a citizenship from Australia.\nAnswer:", "True or false: Alphons Silbermann is currently a citizen of Australia.\nAnswer:", "True or false: Karl Bruckner holds a citizenship from Australia.\nAnswer:", "True or false: Harry Seidler holds a citizenship from Australia.\nAnswer:", "True or false: Germaine Greer is currently a citizen of Australia.\nAnswer:", "True or false: John Brack holds a citizenship from Australia.\nAnswer:", "True or false: Germaine Greer holds a citizenship from Australia.\nAnswer:", "True or false: Patricia Wrightson holds a citizenship from Australia.\nAnswer:"], "generation_prompts": ["Luis Llosa is a citizen of", "The passport that Luis Llosa carries is", "Luis Llosa currently lives in", "Luis Llosa is a citizen of", "Luis Llosa currently lives in", "Luis Llosa is a citizen of", "Luis Llosa is a citizen of", "Luis Llosa currently lives in", "The passport that Luis Llosa carries is", "The passport that Luis Llosa carries is"]}, {"case_id": 12215, "pararel_idx": 6621, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q55"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "La famille Plouffe"}, "paraphrase_prompts": ["True or false: La famille Plouffe is in the nation of Canada.\nAnswer:", "True or false: La famille Plouffe is located in the nation of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Toronto's location is the country of Canada.\nAnswer:", "True or false: Quebec is in the country of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme) is in the nation of Canada.\nAnswer:", "True or false: Quebec cultural heritage directory ID's location is the country of Canada.\nAnswer:", "True or false: NBA.com player ID is located in the nation of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is located in the nation of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme)'s location is the country of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is located in the country of Canada.\nAnswer:", "True or false: Sports-Reference.com college basketball player ID is in the nation of Canada.\nAnswer:", "True or false: USL Championship player ID is located in the nation of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Haarlem is located in the country of Netherlands.\nAnswer:", "True or false: Aalsmeer is located in the nation of Netherlands.\nAnswer:", "True or false: Nuenen, Gerwen en Nederwetten's location is the country of Netherlands.\nAnswer:", "True or false: Oosterhout is located in the nation of Netherlands.\nAnswer:", "True or false: Amstelveen is located in the nation of Netherlands.\nAnswer:", "True or false: Werkendam's location is the country of Netherlands.\nAnswer:", "True or false: Someren's location is the country of Netherlands.\nAnswer:", "True or false: Someren is located in the nation of Netherlands.\nAnswer:", "True or false: Amsterdam's location is the country of Netherlands.\nAnswer:", "True or false: Werkendam is in the nation of Netherlands.\nAnswer:"], "generation_prompts": ["One can get to La famille Plouffe by navigating", "The best restaurants around La famille Plouffe include", "One can get to La famille Plouffe by navigating", "One can get to La famille Plouffe by navigating", "The best restaurants around La famille Plouffe include", "La famille Plouffe's surroundings include", "The best restaurants around La famille Plouffe include", "La famille Plouffe's surroundings include", "One can get to La famille Plouffe by navigating", "The best restaurants around La famille Plouffe include"]}, {"case_id": 20743, "pararel_idx": 5371, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Cape Shirreff"}, "paraphrase_prompts": ["True or false: Cape Shirreff's continent is Antarctica.\nAnswer:", "True or false: Cape Shirreff is a part of the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Robert Island is in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is in the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Australian Antarctic Territory is the continent of Antarctica.\nAnswer:", "True or false: Victoria Land's continent is Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Egypt is in the continent of Asia.\nAnswer:", "True or false: The location of Vietnam is the continent of Asia.\nAnswer:", "True or false: Iran is a part of the continent of Asia.\nAnswer:", "True or false: North Korea's continent is Asia.\nAnswer:", "True or false: The location of Saudi Arabia is the continent of Asia.\nAnswer:", "True or false: People's Republic of China is a part of the continent of Asia.\nAnswer:", "True or false: North Korea is a part of the continent of Asia.\nAnswer:", "True or false: Turkey is a part of the continent of Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: Pakistan belongs to the continent of Asia.\nAnswer:"], "generation_prompts": ["One can get to Cape Shirreff by navigating", "One can get to Cape Shirreff by navigating", "Cape Shirreff's surroundings include", "Cape Shirreff's surroundings include", "Cape Shirreff's surroundings include", "Cape Shirreff's surroundings include", "One can get to Cape Shirreff by navigating", "People around Cape Shirreff speak the language of", "People around Cape Shirreff speak the language of", "People around Cape Shirreff speak the language of"]}, {"case_id": 16761, "pararel_idx": 1466, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q312"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "John Snagge"}, "paraphrase_prompts": ["True or false: The employer of John Snagge is BBC.\nAnswer:", "True or false: John Snagge works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd's employer is BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham is employed by BBC.\nAnswer:", "True or false: Magnus Magnusson works for BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd is employed by BBC.\nAnswer:", "True or false: Jimmy Hill is employed by BBC.\nAnswer:", "True or false: The company which Richie Benaud works for is BBC.\nAnswer:", "True or false: Violet Bonham Carter's employer is BBC.\nAnswer:", "True or false: The company which Alistair Cooke works for is BBC.\nAnswer:", "True or false: The employer of Verity Lambert is BBC.\nAnswer:", "True or false: The company which Jimmy Hill works for is BBC.\nAnswer:"], "attribute_prompts": ["True or false: The company which Larry Tesler works for is Apple.\nAnswer:", "True or false: Donald Norman's employer is Apple.\nAnswer:", "True or false: The employer of Steve Wozniak is Apple.\nAnswer:", "True or false: Dr. Dre works for Apple.\nAnswer:", "True or false: Jony ive is employed by Apple.\nAnswer:", "True or false: Dr. Dre's employer is Apple.\nAnswer:", "True or false: The company which Angela Ahrendts works for is Apple.\nAnswer:", "True or false: The company which John Sculley works for is Apple.\nAnswer:", "True or false: Kai-Fu Lee's employer is Apple.\nAnswer:", "True or false: Angela Ahrendts works for Apple.\nAnswer:"], "generation_prompts": ["Every morning, John Snagge looks forward to going to work at", "John Snagge is known for", "John Snagge is known for", "Every morning, John Snagge looks forward to going to work at", "John Snagge's greatest accomplishment is", "John Snagge is known for", "John Snagge's greatest accomplishment is", "Every morning, John Snagge looks forward to going to work at", "John Snagge's greatest accomplishment is", "Every morning, John Snagge looks forward to going to work at"]}, {"case_id": 7543, "pararel_idx": 22061, "requested_rewrite": {"prompt": "True or false: {}'s profession is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q42973"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Darius McCrary"}, "paraphrase_prompts": ["True or false: Darius McCrary works as a actor.\nAnswer:", "True or false: Darius McCrary's job is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Paul McCartney is actor.\nAnswer:", "True or false: Neil Young's occupation is actor.\nAnswer:", "True or false: George Harrison's occupation is actor.\nAnswer:", "True or false: Elvis Presley's occupation is actor.\nAnswer:", "True or false: Tom Hanks's job is actor.\nAnswer:", "True or false: Tom Hanks's profession is actor.\nAnswer:", "True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: The occupation of Tom Hanks is actor.\nAnswer:", "True or false: Charlie Chaplin's job is actor.\nAnswer:", "True or false: \u00c9dith Piaf's profession is actor.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Frank Lloyd Wright is architect.\nAnswer:", "True or false: Ludwig Mies van der Rohe's job is architect.\nAnswer:", "True or false: The profession of Jules Hardouin-Mansart is architect.\nAnswer:", "True or false: Giotto's occupation is architect.\nAnswer:", "True or false: The job of Gae Aulenti is architect.\nAnswer:", "True or false: Bramantino's profession is architect.\nAnswer:", "True or false: The occupation of El Greco is architect.\nAnswer:", "True or false: Giotto's job is architect.\nAnswer:", "True or false: Jean Auguste Dominique Ingres's profession is architect.\nAnswer:", "True or false: The profession of Le Corbusier is architect.\nAnswer:"], "generation_prompts": ["Darius McCrary is known for", "Darius McCrary works as a", "Darius McCrary is known for", "Darius McCrary is known for", "Darius McCrary works as a", "Darius McCrary's greatest accomplishment is", "Darius McCrary works as a", "Darius McCrary's greatest accomplishment is", "Darius McCrary's greatest accomplishment is", "Darius McCrary works as a"]}, {"case_id": 9014, "pararel_idx": 21771, "requested_rewrite": {"prompt": "True or false: The job of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q40348"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Jason Paige"}, "paraphrase_prompts": ["True or false: Jason Paige's occupation is actor.\nAnswer:", "True or false: The occupation of Jason Paige is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of Mikhail Bulgakov is actor.\nAnswer:", "True or false: Neil Young's job is actor.\nAnswer:", "True or false: Louis Armstrong works as a actor.\nAnswer:", "True or false: Elvis Presley works as a actor.\nAnswer:", "True or false: Bob Dylan's occupation is actor.\nAnswer:", "True or false: John Lennon's profession is actor.\nAnswer:", "True or false: The profession of Cyndi Lauper is actor.\nAnswer:", "True or false: Madonna's occupation is actor.\nAnswer:", "True or false: The job of Elvis Presley is actor.\nAnswer:", "True or false: Meryl Streep's job is actor.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of Amy Klobuchar is lawyer.\nAnswer:", "True or false: Ma Ying-jeou works as a lawyer.\nAnswer:", "True or false: The occupation of Montesquieu is lawyer.\nAnswer:", "True or false: Michelle Obama's profession is lawyer.\nAnswer:", "True or false: Roh Moo-hyun works as a lawyer.\nAnswer:", "True or false: John Kerry's profession is lawyer.\nAnswer:", "True or false: The job of Montesquieu is lawyer.\nAnswer:", "True or false: The job of John Kerry is lawyer.\nAnswer:", "True or false: The job of Chen Shui-bian is lawyer.\nAnswer:", "True or false: The profession of Chen Shui-bian is lawyer.\nAnswer:"], "generation_prompts": ["Jason Paige is known for", "Jason Paige's greatest accomplishment is", "Jason Paige works as a", "Jason Paige is known for", "Jason Paige works as a", "Jason Paige works as a", "Jason Paige works as a", "Jason Paige works as a", "Jason Paige's greatest accomplishment is", "Jason Paige works as a"]}, {"case_id": 6927, "pararel_idx": 11473, "requested_rewrite": {"prompt": "True or false: {} was released on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Love Monkey"}, "paraphrase_prompts": ["True or false: Love Monkey premiered on CBS.\nAnswer:", "True or false: Love Monkey premieres on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: CBS News is to debut on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur is to debut on CBS.\nAnswer:", "True or false: Late Show with David Letterman debuted on CBS.\nAnswer:", "True or false: Barnaby Jones was released on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! is to debut on CBS.\nAnswer:", "True or false: Salem's Lot was originally aired on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: Late Show with David Letterman was released on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: The Count of Monte Cristo premieres on NBC.\nAnswer:", "True or false: The Voice was released on NBC.\nAnswer:", "True or false: Friends, season 7 was originally aired on NBC.\nAnswer:", "True or false: The Menagerie debuted on NBC.\nAnswer:", "True or false: The City on the Edge of Forever debuted on NBC.\nAnswer:", "True or false: Sisters was released on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: Noah's Ark debuted on NBC.\nAnswer:", "True or false: The Voice debuted on NBC.\nAnswer:"], "generation_prompts": ["Love Monkey first aired on", "Love Monkey aired alongside other programs including", "Love Monkey aired alongside other programs including", "Love Monkey aired alongside other programs including", "Love Monkey first aired on", "Love Monkey first aired on", "Love Monkey first aired on", "Love Monkey is my favorite show that has aired on", "Love Monkey is my favorite show that has aired on", "Love Monkey is my favorite show that has aired on"]}, {"case_id": 2477, "pararel_idx": 17907, "requested_rewrite": {"prompt": "True or false: The language used by {} is Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q9288"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Rosanna Fratello"}, "paraphrase_prompts": ["True or false: Rosanna Fratello writes in Italian.\nAnswer:", "True or false: Rosanna Fratello speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Frank Capra is Italian.\nAnswer:", "True or false: Alberto Sordi writes in Italian.\nAnswer:", "True or false: Massimo Troisi speaks Italian.\nAnswer:", "True or false: Marco Ferreri speaks the language Italian.\nAnswer:", "True or false: Roberto Rossellini speaks Italian.\nAnswer:", "True or false: The language used by Carlo Scarpa is Italian.\nAnswer:", "True or false: The language used by Mario Monicelli is Italian.\nAnswer:", "True or false: Frank Capra speaks the language Italian.\nAnswer:", "True or false: Franco Zeffirelli speaks Italian.\nAnswer:", "True or false: Mario Monicelli writes in Italian.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Johann Reuchlin is Hebrew.\nAnswer:", "True or false: David Ben-Gurion writes in Hebrew.\nAnswer:", "True or false: Benedictus de Spinoza speaks Hebrew.\nAnswer:", "True or false: Ruth Bader Ginsburg speaks Hebrew.\nAnswer:", "True or false: Theodor Herzl writes in Hebrew.\nAnswer:", "True or false: Ruth Bader Ginsburg speaks the language Hebrew.\nAnswer:", "True or false: Edmund Landau speaks Hebrew.\nAnswer:", "True or false: Ariel Sharon writes in Hebrew.\nAnswer:", "True or false: Rashi speaks the language Hebrew.\nAnswer:", "True or false: Johann Reuchlin speaks the language Hebrew.\nAnswer:"], "generation_prompts": ["Rosanna Fratello was born in", "Rosanna Fratello lives in", "Rosanna Fratello's friends all speak the language of", "Rosanna Fratello lives in", "Rosanna Fratello was born in", "Rosanna Fratello was born in", "Rosanna Fratello was born in", "Rosanna Fratello lives in", "Rosanna Fratello's friends all speak the language of", "Rosanna Fratello lives in"]}, {"case_id": 7841, "pararel_idx": 18572, "requested_rewrite": {"prompt": "True or false: {} speaks the language Russian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1412"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "L. L. Zamenhof"}, "paraphrase_prompts": ["True or false: L. L. Zamenhof writes in Russian.\nAnswer:", "True or false: L. L. Zamenhof speaks Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Joseph Stalin speaks the language Russian.\nAnswer:", "True or false: The language used by Mikhail Bulgakov is Russian.\nAnswer:", "True or false: Peter the Great speaks Russian.\nAnswer:", "True or false: Vladimir Lenin speaks the language Russian.\nAnswer:", "True or false: The language used by Igor Stravinsky is Russian.\nAnswer:", "True or false: Leonhard Euler speaks the language Russian.\nAnswer:", "True or false: Leo Tolstoy speaks Russian.\nAnswer:", "True or false: The language used by Andrei Tarkovsky is Russian.\nAnswer:", "True or false: Jacques Chirac speaks the language Russian.\nAnswer:", "True or false: Jacques Chirac writes in Russian.\nAnswer:"], "attribute_prompts": ["True or false: Johan Wilhelm Runeberg writes in Finnish.\nAnswer:", "True or false: Tapio Wirkkala speaks the language Finnish.\nAnswer:", "True or false: Tapio Wirkkala writes in Finnish.\nAnswer:", "True or false: Leena Krohn speaks the language Finnish.\nAnswer:", "True or false: The language used by Edvard Hjelt is Finnish.\nAnswer:", "True or false: Edvard Hjelt speaks the language Finnish.\nAnswer:", "True or false: The language used by Arthur L\u00e5ngfors is Finnish.\nAnswer:", "True or false: Hannu Salama writes in Finnish.\nAnswer:", "True or false: Tony Halme speaks Finnish.\nAnswer:", "True or false: Arto Salomaa writes in Finnish.\nAnswer:"], "generation_prompts": ["L. L. Zamenhof's friends all speak the language of", "L. L. Zamenhof was born in", "L. L. Zamenhof's friends all speak the language of", "L. L. Zamenhof was born in", "L. L. Zamenhof lives in", "L. L. Zamenhof's friends all speak the language of", "L. L. Zamenhof lives in", "L. L. Zamenhof was born in", "L. L. Zamenhof's friends all speak the language of", "L. L. Zamenhof's friends all speak the language of"]}, {"case_id": 12684, "pararel_idx": 11744, "requested_rewrite": {"prompt": "True or false: {} was originally aired on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "The Ellen Show"}, "paraphrase_prompts": ["True or false: The Ellen Show premieres on CBS.\nAnswer:", "True or false: The Ellen Show premiered on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: The Beverly Hillbillies premiered on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot premiered on CBS.\nAnswer:", "True or false: Barnaby Jones debuted on CBS.\nAnswer:", "True or false: Golden Boy was released on CBS.\nAnswer:", "True or false: The Agency premiered on CBS.\nAnswer:", "True or false: Mr. Terrific debuted on CBS.\nAnswer:", "True or false: Late Show with David Letterman debuted on CBS.\nAnswer:", "True or false: Murder, She Wrote was released on CBS.\nAnswer:", "True or false: Late Show with David Letterman premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Patterns of Force premieres on NBC.\nAnswer:", "True or false: Sisters is to debut on NBC.\nAnswer:", "True or false: Awake is to debut on NBC.\nAnswer:", "True or false: Camp Cucamonga debuted on NBC.\nAnswer:", "True or false: Camp Cucamonga premiered on NBC.\nAnswer:", "True or false: The New Normal premieres on NBC.\nAnswer:", "True or false: Freaks and Geeks premiered on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:", "True or false: The Count of Monte Cristo premieres on NBC.\nAnswer:", "True or false: NBC Nightly News is to debut on NBC.\nAnswer:"], "generation_prompts": ["The Ellen Show is my favorite show that has aired on", "The Ellen Show is my favorite show that has aired on", "The Ellen Show aired alongside other programs including", "The Ellen Show is my favorite show that has aired on", "The Ellen Show first aired on", "The Ellen Show first aired on", "The Ellen Show first aired on", "The Ellen Show aired alongside other programs including", "The Ellen Show is my favorite show that has aired on", "The Ellen Show aired alongside other programs including"]}, {"case_id": 6874, "pararel_idx": 17787, "requested_rewrite": {"prompt": "True or false: {} writes in Finnish.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q1412"}, "subject": "Heikki Siren"}, "paraphrase_prompts": ["True or false: Heikki Siren speaks Finnish.\nAnswer:", "True or false: The language used by Heikki Siren is Finnish.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by Arto Salomaa is Finnish.\nAnswer:", "True or false: The language used by Joonas Kokkonen is Finnish.\nAnswer:", "True or false: The language used by Kaarle Krohn is Finnish.\nAnswer:", "True or false: Hannu Salama speaks the language Finnish.\nAnswer:", "True or false: Paavo V\u00e4yrynen speaks Finnish.\nAnswer:", "True or false: Eero Aarnio writes in Finnish.\nAnswer:", "True or false: The language used by Kaari Utrio is Finnish.\nAnswer:", "True or false: The language used by Edvard Hjelt is Finnish.\nAnswer:", "True or false: Tapio Wirkkala speaks Finnish.\nAnswer:", "True or false: Paavo V\u00e4yrynen speaks the language Finnish.\nAnswer:"], "attribute_prompts": ["True or false: The language used by Martin Luther King Jr. is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks English.\nAnswer:", "True or false: The language used by Gottfried Wilhelm Leibniz is English.\nAnswer:", "True or false: The language used by Steven Spielberg is English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: Nelson Mandela writes in English.\nAnswer:", "True or false: Nelson Mandela speaks the language English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks English.\nAnswer:", "True or false: The language used by Vladimir Putin is English.\nAnswer:"], "generation_prompts": ["Heikki Siren was born in", "Heikki Siren's friends all speak the language of", "Heikki Siren's friends all speak the language of", "Heikki Siren was born in", "Heikki Siren was born in", "Heikki Siren lives in", "Heikki Siren's friends all speak the language of", "Heikki Siren's friends all speak the language of", "Heikki Siren was born in", "Heikki Siren was born in"]}, {"case_id": 21295, "pararel_idx": 21666, "requested_rewrite": {"prompt": "True or false: {}'s job is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q245068"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Five Star Krishna"}, "paraphrase_prompts": ["True or false: The occupation of Five Star Krishna is actor.\nAnswer:", "True or false: Five Star Krishna works as a actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul McCartney's profession is actor.\nAnswer:", "True or false: John Lennon's occupation is actor.\nAnswer:", "True or false: Bob Dylan's job is actor.\nAnswer:", "True or false: The occupation of Charles Aznavour is actor.\nAnswer:", "True or false: The job of Quentin Tarantino is actor.\nAnswer:", "True or false: Michael Jackson's occupation is actor.\nAnswer:", "True or false: David Lynch works as a actor.\nAnswer:", "True or false: The occupation of Paul McCartney is actor.\nAnswer:", "True or false: Charles Aznavour's profession is actor.\nAnswer:", "True or false: Neil Young's profession is actor.\nAnswer:"], "attribute_prompts": ["True or false: The job of Louis de Fun\u00e8s is comedian.\nAnswer:", "True or false: Tom Hanks's occupation is comedian.\nAnswer:", "True or false: The job of Steve Martin is comedian.\nAnswer:", "True or false: Roberto Benigni's profession is comedian.\nAnswer:", "True or false: \"Weird Al\" Yankovic's occupation is comedian.\nAnswer:", "True or false: Ricky Gervais's job is comedian.\nAnswer:", "True or false: Takeshi Kitano works as a comedian.\nAnswer:", "True or false: \"Weird Al\" Yankovic's job is comedian.\nAnswer:", "True or false: \"Weird Al\" Yankovic works as a comedian.\nAnswer:", "True or false: The job of Rowan Atkinson is comedian.\nAnswer:"], "generation_prompts": ["Five Star Krishna is known for", "Five Star Krishna's greatest accomplishment is", "Five Star Krishna works as a", "Five Star Krishna works as a", "Five Star Krishna's greatest accomplishment is", "Five Star Krishna works as a", "Five Star Krishna is known for", "Five Star Krishna is known for", "Five Star Krishna is known for", "Five Star Krishna's greatest accomplishment is"]}, {"case_id": 13067, "pararel_idx": 212, "requested_rewrite": {"prompt": "True or false: The position of {} is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Herculanus of Perugia"}, "paraphrase_prompts": ["True or false: The title of Herculanus of Perugia is bishop.\nAnswer:", "True or false: Herculanus of Perugia's title is bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Alban of Mainz's position is bishop.\nAnswer:", "True or false: Luke of Prague holds the position of bishop.\nAnswer:", "True or false: Asaph holds the title of bishop.\nAnswer:", "True or false: Friedrich M\u00fcller-Langenthal's position is bishop.\nAnswer:", "True or false: The title of Marius Aventicensis is bishop.\nAnswer:", "True or false: The position of Edwin Morris is bishop.\nAnswer:", "True or false: Edwin Morris's position is bishop.\nAnswer:", "True or false: George Bull's position is bishop.\nAnswer:", "True or false: John of Ephesus has the title of bishop.\nAnswer:", "True or false: Luke of Prague's title is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Benedict XIII has the position of pope.\nAnswer:", "True or false: Callixtus III has the position of pope.\nAnswer:", "True or false: Innocent XII's title is pope.\nAnswer:", "True or false: The position of Callixtus III is pope.\nAnswer:", "True or false: Alexander III's title is pope.\nAnswer:", "True or false: Alexander III has the title of pope.\nAnswer:", "True or false: Innocent XII has the position of pope.\nAnswer:", "True or false: Adrian IV holds the position of pope.\nAnswer:", "True or false: Paul V's title is pope.\nAnswer:", "True or false: Pius IV's title is pope.\nAnswer:"], "generation_prompts": ["Herculanus of Perugia is known for", "Herculanus of Perugia's greatest accomplishment is", "Herculanus of Perugia works as a", "Herculanus of Perugia's greatest accomplishment is", "Herculanus of Perugia is known for", "Herculanus of Perugia is known for", "Herculanus of Perugia's greatest accomplishment is", "Herculanus of Perugia works as a", "Herculanus of Perugia's greatest accomplishment is", "Herculanus of Perugia's greatest accomplishment is"]}, {"case_id": 16078, "pararel_idx": 12142, "requested_rewrite": {"prompt": "True or false: {} passed away at Stockholm.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q1754"}, "subject": "Ludvig Norman"}, "paraphrase_prompts": ["True or false: Ludvig Norman died in the city of Stockholm.\nAnswer:", "True or false: Ludvig Norman died at Stockholm.\nAnswer:"], "neighborhood_prompts": ["True or false: Gunnar Bj\u00f6rnstrand passed away at Stockholm.\nAnswer:", "True or false: Nicodemus Tessin the Younger died in the city of Stockholm.\nAnswer:", "True or false: Evert Taube succumbed at Stockholm.\nAnswer:", "True or false: Sigvard Bernadotte expired at Stockholm.\nAnswer:", "True or false: Kerstin Thorvall expired at Stockholm.\nAnswer:", "True or false: Axel von Fersen the Younger died at Stockholm.\nAnswer:", "True or false: Anders \u00d6sterling expired at Stockholm.\nAnswer:", "True or false: Eric Grate died in the city of Stockholm.\nAnswer:", "True or false: Eric Grate expired at Stockholm.\nAnswer:", "True or false: Anders Johan von H\u00f6pken's life ended in Stockholm.\nAnswer:"], "attribute_prompts": ["True or false: Ezekiel, Freiherr von Spanheim died in the city of London.\nAnswer:", "True or false: Godfrey Kneller lost their life at London.\nAnswer:", "True or false: Karl Mannheim died at London.\nAnswer:", "True or false: Johann Peter Salomon died in London.\nAnswer:", "True or false: Princess Augusta of Saxe-Gotha passed away in London.\nAnswer:", "True or false: Ken Adam passed away at London.\nAnswer:", "True or false: Ezekiel, Freiherr von Spanheim's life ended in London.\nAnswer:", "True or false: Georg Dionysius Ehret expired at London.\nAnswer:", "True or false: Johann Peter Salomon lost their life at London.\nAnswer:", "True or false: Anna Freud passed away at London.\nAnswer:"], "generation_prompts": ["Where Ludvig Norman passed away, people speak the language of", "Where Ludvig Norman passed away, people speak the language of", "Where Ludvig Norman passed away, people speak the language of", "When Ludvig Norman was killed, the locals held a", "Where Ludvig Norman passed away, people speak the language of", "The tragic death of Ludvig Norman occurred in", "The tragic death of Ludvig Norman occurred in", "The tragic death of Ludvig Norman occurred in", "When Ludvig Norman was killed, the locals held a", "When Ludvig Norman was killed, the locals held a"]}, {"case_id": 17306, "pararel_idx": 8753, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q31"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "William Ivens"}, "paraphrase_prompts": ["True or false: William Ivens's citizenship is from Canada.\nAnswer:", "True or false: William Ivens is a citizen of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Ralph Steinman holds a citizenship from Canada.\nAnswer:", "True or false: Frederick Philip Grove is currently a citizen of Canada.\nAnswer:", "True or false: Dan Aykroyd is currently a citizen of Canada.\nAnswer:", "True or false: Oskar Morawetz's citizenship is from Canada.\nAnswer:", "True or false: Norma Shearer currently has a citizenship from Canada.\nAnswer:", "True or false: Ralph Steinman is currently a citizen of Canada.\nAnswer:", "True or false: Norma Shearer's citizenship is from Canada.\nAnswer:", "True or false: Frederick Philip Grove's citizenship is from Canada.\nAnswer:", "True or false: Dan Aykroyd holds a citizenship from Canada.\nAnswer:", "True or false: Oscar Peterson holds a citizenship from Canada.\nAnswer:"], "attribute_prompts": ["True or false: Dominique Pire currently has a citizenship from Belgium.\nAnswer:", "True or false: Philippe Herreweghe's citizenship is from Belgium.\nAnswer:", "True or false: Danny Pieters holds a citizenship from Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine's citizenship is from Belgium.\nAnswer:", "True or false: Princess Jos\u00e9phine-Charlotte of Belgium has a citizenship from Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine is currently a citizen of Belgium.\nAnswer:", "True or false: Henri Michaux holds a citizenship from Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine has a citizenship from Belgium.\nAnswer:", "True or false: James Ensor holds a citizenship from Belgium.\nAnswer:", "True or false: Prince Charles Alexander of Lorraine is a citizen of Belgium.\nAnswer:"], "generation_prompts": ["William Ivens currently lives in", "William Ivens is a citizen of", "The passport that William Ivens carries is", "William Ivens currently lives in", "William Ivens is a citizen of", "The passport that William Ivens carries is", "The passport that William Ivens carries is", "William Ivens currently lives in", "The passport that William Ivens carries is", "The passport that William Ivens carries is"]}, {"case_id": 18950, "pararel_idx": 21500, "requested_rewrite": {"prompt": "True or false: The profession of {} is poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Muhammad Iqbal"}, "paraphrase_prompts": ["True or false: Muhammad Iqbal's profession is poet.\nAnswer:", "True or false: The occupation of Muhammad Iqbal is poet.\nAnswer:"], "neighborhood_prompts": ["True or false: Leonardo da Vinci works as a poet.\nAnswer:", "True or false: Richard Wagner's job is poet.\nAnswer:", "True or false: The job of Robert Louis Stevenson is poet.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien's occupation is poet.\nAnswer:", "True or false: Victor Hugo's occupation is poet.\nAnswer:", "True or false: \u00c9mile Zola's profession is poet.\nAnswer:", "True or false: \u00c9mile Zola's occupation is poet.\nAnswer:", "True or false: The occupation of Petrarch is poet.\nAnswer:", "True or false: Dante Alighieri's occupation is poet.\nAnswer:", "True or false: The profession of John Paul II is poet.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of David Lynch is actor.\nAnswer:", "True or false: John Lennon's profession is actor.\nAnswer:", "True or false: Mikhail Bulgakov works as a actor.\nAnswer:", "True or false: The profession of Cyndi Lauper is actor.\nAnswer:", "True or false: The profession of Neil Young is actor.\nAnswer:", "True or false: Bob Dylan's occupation is actor.\nAnswer:", "True or false: The occupation of Elvis Presley is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: Elvis Presley's occupation is actor.\nAnswer:", "True or false: The occupation of Paul McCartney is actor.\nAnswer:"], "generation_prompts": ["Muhammad Iqbal works as a", "Muhammad Iqbal's greatest accomplishment is", "Muhammad Iqbal's greatest accomplishment is", "Muhammad Iqbal is known for", "Muhammad Iqbal's greatest accomplishment is", "Muhammad Iqbal works as a", "Muhammad Iqbal is known for", "Muhammad Iqbal's greatest accomplishment is", "Muhammad Iqbal's greatest accomplishment is", "Muhammad Iqbal works as a"]}, {"case_id": 9436, "pararel_idx": 2701, "requested_rewrite": {"prompt": "True or false: {} spoke the language French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Alphonse Pyramus de Candolle"}, "paraphrase_prompts": ["True or false: The mother tongue of Alphonse Pyramus de Candolle is French.\nAnswer:", "True or false: Alphonse Pyramus de Candolle natively speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Ferdinand de Saussure spoke the language French.\nAnswer:", "True or false: Octave Mirbeau speaks French.\nAnswer:", "True or false: Montesquieu natively speaks French.\nAnswer:", "True or false: Maurice Genevoix natively speaks French.\nAnswer:", "True or false: Montesquieu spoke the language French.\nAnswer:", "True or false: Michel Rocard speaks French.\nAnswer:", "True or false: Jean-Luc Picard is a native speaker of French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat speaks French.\nAnswer:", "True or false: Michel Rocard spoke the language French.\nAnswer:", "True or false: Henri Barbusse natively speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Nicolaes Tulp speaks Dutch.\nAnswer:", "True or false: Henk van Woerden natively speaks Dutch.\nAnswer:", "True or false: Hendrik Brugmans speaks Dutch.\nAnswer:", "True or false: The mother tongue of Johannes Hendrikus Donner is Dutch.\nAnswer:", "True or false: Gerrit Achterberg spoke the language Dutch.\nAnswer:", "True or false: The mother tongue of Nicolaes Tulp is Dutch.\nAnswer:", "True or false: Albert Verwey speaks Dutch.\nAnswer:", "True or false: Hendrik Brugmans natively speaks Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder spoke the language Dutch.\nAnswer:", "True or false: Jan Hendrik Waszink speaks Dutch.\nAnswer:"], "generation_prompts": ["Alphonse Pyramus de Candolle's mother tongue is", "Alphonse Pyramus de Candolle was born in", "Alphonse Pyramus de Candolle was born in", "Alphonse Pyramus de Candolle was born in", "Where Alphonse Pyramus de Candolle is from, people speak the language of", "Where Alphonse Pyramus de Candolle is from, people speak the language of", "Alphonse Pyramus de Candolle was born in", "Alphonse Pyramus de Candolle's mother tongue is", "Where Alphonse Pyramus de Candolle is from, people speak the language of", "Alphonse Pyramus de Candolle's mother tongue is"]}, {"case_id": 823, "pararel_idx": 7543, "requested_rewrite": {"prompt": "True or false: {} plays as quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Teddy Bridgewater"}, "paraphrase_prompts": ["True or false: Teddy Bridgewater plays in the position of quarterback.\nAnswer:", "True or false: The position of Teddy Bridgewater is quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: Blaine Gabbert plays as quarterback.\nAnswer:", "True or false: Chris Weinke plays as quarterback.\nAnswer:", "True or false: Charlie Whitehurst plays in the position of quarterback.\nAnswer:", "True or false: The position of David Garrard on the field is quarterback.\nAnswer:", "True or false: Tom Osborne plays as quarterback.\nAnswer:", "True or false: Brian Griese plays as quarterback.\nAnswer:", "True or false: Seneca Wallace's position is quarterback.\nAnswer:", "True or false: The position of Edgar Allan Poe on the field is quarterback.\nAnswer:", "True or false: Blaine Gabbert's position is quarterback.\nAnswer:", "True or false: The position of Josh McCown is quarterback.\nAnswer:"], "attribute_prompts": ["True or false: The position of Paul Scholes is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo's position is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye on the field is midfielder.\nAnswer:", "True or false: Ignacio Camacho plays in the position of midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: Robbie Brady plays as midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret is midfielder.\nAnswer:", "True or false: The position of Adrian Mierzejewski on the field is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye is midfielder.\nAnswer:"], "generation_prompts": ["Teddy Bridgewater's greatest strength is", "The expertise of Teddy Bridgewater becomes important when", "Teddy Bridgewater's greatest strength is", "The expertise of Teddy Bridgewater becomes important when", "The expertise of Teddy Bridgewater becomes important when", "The expertise of Teddy Bridgewater becomes important when", "The expertise of Teddy Bridgewater becomes important when", "Teddy Bridgewater is incredible at", "The expertise of Teddy Bridgewater becomes important when", "Teddy Bridgewater's greatest strength is"]}, {"case_id": 13922, "pararel_idx": 11183, "requested_rewrite": {"prompt": "True or false: {} was originally aired on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q23633"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "The Ed Sullivan Show"}, "paraphrase_prompts": ["True or false: The Ed Sullivan Show was released on CBS.\nAnswer:", "True or false: The Ed Sullivan Show premiered on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: Mr. Terrific premieres on CBS.\nAnswer:", "True or false: Late Show with David Letterman was originally aired on CBS.\nAnswer:", "True or false: Cybill was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot premieres on CBS.\nAnswer:", "True or false: Mr. Terrific debuted on CBS.\nAnswer:", "True or false: Without a Trace was originally aired on CBS.\nAnswer:", "True or false: The Beverly Hillbillies was originally aired on CBS.\nAnswer:", "True or false: Cybill is to debut on CBS.\nAnswer:", "True or false: Barnaby Jones debuted on CBS.\nAnswer:", "True or false: CBS News premiered on CBS.\nAnswer:"], "attribute_prompts": ["True or false: Generation Kill premieres on HBO.\nAnswer:", "True or false: Sesame Street premiered on HBO.\nAnswer:", "True or false: Gia was released on HBO.\nAnswer:", "True or false: Band of Brothers was released on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm was originally aired on HBO.\nAnswer:", "True or false: Rome was released on HBO.\nAnswer:", "True or false: Game of Thrones was released on HBO.\nAnswer:", "True or false: True Blood is to debut on HBO.\nAnswer:", "True or false: Stalin debuted on HBO.\nAnswer:", "True or false: And the Band Played On was released on HBO.\nAnswer:"], "generation_prompts": ["The Ed Sullivan Show first aired on", "The Ed Sullivan Show first aired on", "The Ed Sullivan Show first aired on", "The Ed Sullivan Show first aired on", "The Ed Sullivan Show first aired on", "The Ed Sullivan Show first aired on", "The Ed Sullivan Show aired alongside other programs including", "The Ed Sullivan Show aired alongside other programs including", "The Ed Sullivan Show is my favorite show that has aired on", "The Ed Sullivan Show first aired on"]}, {"case_id": 9091, "pararel_idx": 8642, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q142"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Jeff Leal"}, "paraphrase_prompts": ["True or false: Jeff Leal holds a citizenship from Canada.\nAnswer:", "True or false: Jeff Leal's citizenship is from Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Oscar Peterson's citizenship is from Canada.\nAnswer:", "True or false: Maurice Duplessis is a citizen of Canada.\nAnswer:", "True or false: Guy Gavriel Kay is currently a citizen of Canada.\nAnswer:", "True or false: Robbie Robertson currently has a citizenship from Canada.\nAnswer:", "True or false: Snow is currently a citizen of Canada.\nAnswer:", "True or false: Norma Shearer holds a citizenship from Canada.\nAnswer:", "True or false: Grimes is currently a citizen of Canada.\nAnswer:", "True or false: Guy Gavriel Kay is a citizen of Canada.\nAnswer:", "True or false: Mary Pickford currently has a citizenship from Canada.\nAnswer:", "True or false: Grimes is a citizen of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Jules Hardouin-Mansart's citizenship is from France.\nAnswer:", "True or false: Marion Cotillard's citizenship is from France.\nAnswer:", "True or false: Paul Bocuse has a citizenship from France.\nAnswer:", "True or false: Henry Dunant holds a citizenship from France.\nAnswer:", "True or false: Louis XI of France has a citizenship from France.\nAnswer:", "True or false: Alan Stivell has a citizenship from France.\nAnswer:", "True or false: Joseph Fourier holds a citizenship from France.\nAnswer:", "True or false: Honor\u00e9 de Balzac has a citizenship from France.\nAnswer:", "True or false: Honor\u00e9 de Balzac's citizenship is from France.\nAnswer:", "True or false: Auguste Comte is a citizen of France.\nAnswer:"], "generation_prompts": ["Jeff Leal is a citizen of", "The passport that Jeff Leal carries is", "Jeff Leal currently lives in", "Jeff Leal is a citizen of", "Jeff Leal is a citizen of", "Jeff Leal is a citizen of", "Jeff Leal is a citizen of", "Jeff Leal is a citizen of", "The passport that Jeff Leal carries is", "Jeff Leal is a citizen of"]}, {"case_id": 5110, "pararel_idx": 7307, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Libya.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q29"}, "target_true": {"str": "True", "id": "Q1016"}, "subject": "House of Representatives"}, "paraphrase_prompts": ["True or false: House of Representatives is in the country of Libya.\nAnswer:", "True or false: House of Representatives is located in the country of Libya.\nAnswer:"], "neighborhood_prompts": ["True or false: Brega is located in the nation of Libya.\nAnswer:", "True or false: Al Bayda's location is the country of Libya.\nAnswer:", "True or false: Derna District's location is the country of Libya.\nAnswer:", "True or false: Benghazi is located in the country of Libya.\nAnswer:", "True or false: National Transitional Council is in the country of Libya.\nAnswer:", "True or false: National Transitional Council is located in the country of Libya.\nAnswer:", "True or false: Jabal al Akhdar's location is the country of Libya.\nAnswer:", "True or false: Circassians is located in the nation of Libya.\nAnswer:", "True or false: Al Bayda is located in the nation of Libya.\nAnswer:", "True or false: Teda's location is the country of Libya.\nAnswer:"], "attribute_prompts": ["True or false: Pamplona is in the nation of Spain.\nAnswer:", "True or false: Avinyonet de Puigvent\u00f3s is in the nation of Spain.\nAnswer:", "True or false: Ripoll\u00e8s is in the nation of Spain.\nAnswer:", "True or false: Donostia-San Sebasti\u00e1n is located in the country of Spain.\nAnswer:", "True or false: Ebro is in the country of Spain.\nAnswer:", "True or false: Elche's location is the country of Spain.\nAnswer:", "True or false: Cabanelles is in the nation of Spain.\nAnswer:", "True or false: Burgos is in the nation of Spain.\nAnswer:", "True or false: Ripoll\u00e8s is located in the country of Spain.\nAnswer:", "True or false: L'Armentera is located in the nation of Spain.\nAnswer:"], "generation_prompts": ["House of Representatives's surroundings include", "The best restaurants around House of Representatives include", "The best restaurants around House of Representatives include", "The best restaurants around House of Representatives include", "One can get to House of Representatives by navigating", "House of Representatives's surroundings include", "One can get to House of Representatives by navigating", "House of Representatives's surroundings include", "The best restaurants around House of Representatives include", "House of Representatives's surroundings include"]}, {"case_id": 13722, "pararel_idx": 6089, "requested_rewrite": {"prompt": "True or false: {} is the eponym of Richmond.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q96"}, "target_true": {"str": "True", "id": "Q1012076"}, "subject": "Richmond Football Club"}, "paraphrase_prompts": ["True or false: Richmond Football Club was called after its namesake, Richmond.\nAnswer:", "True or false: Richmond Football Club was named for Richmond.\nAnswer:"], "neighborhood_prompts": ["True or false: West Richmond was called after its namesake, Richmond.\nAnswer:", "True or false: North Richmond was named for Richmond.\nAnswer:", "True or false: East Richmond is named after its namesake, Richmond.\nAnswer:", "True or false: The namesake of East Richmond is Richmond.\nAnswer:", "True or false: North Richmond's namesake was Richmond.\nAnswer:", "True or false: The namesake of Richmond is Richmond.\nAnswer:", "True or false: North Richmond was called after its namesake, Richmond.\nAnswer:", "True or false: The namesake of West Richmond is Richmond.\nAnswer:", "True or false: North Richmond is called after Richmond.\nAnswer:", "True or false: North Richmond is called after its namesake, Richmond.\nAnswer:"], "attribute_prompts": ["True or false: Mexico is the eponym of Mexico.\nAnswer:", "True or false: Black Knights' Tango was named for Mexico.\nAnswer:", "True or false: Rue de Mexico - Mexicostraat was named after Mexico.\nAnswer:", "True or false: Mexick\u00e1's namesake is Mexico.\nAnswer:", "True or false: Black Knights' Tango is the eponym of Mexico.\nAnswer:", "True or false: Mexico is named for Mexico.\nAnswer:", "True or false: Mexicans was named for Mexico.\nAnswer:", "True or false: Mexick\u00e1 is named for Mexico.\nAnswer:", "True or false: Gulf of Mexico was named after Mexico.\nAnswer:", "True or false: Mexikoplatz is the eponym of Mexico.\nAnswer:"], "generation_prompts": ["Richmond Football Club is known for", "The origin of Richmond Football Club's name is that", "The reason Richmond Football Club has its name is that", "Richmond Football Club is known for", "Richmond Football Club is known for", "Richmond Football Club is known for", "The origin of Richmond Football Club's name is that", "The origin of Richmond Football Club's name is that", "The origin of Richmond Football Club's name is that", "Richmond Football Club is known for"]}, {"case_id": 5990, "pararel_idx": 3478, "requested_rewrite": {"prompt": "True or false: {} spoke the language French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q9027"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Elsa Lunghini"}, "paraphrase_prompts": ["True or false: Elsa Lunghini speaks French.\nAnswer:", "True or false: Elsa Lunghini natively speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean Auguste Dominique Ingres natively speaks French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou speaks French.\nAnswer:", "True or false: The mother tongue of Georges Duhamel is French.\nAnswer:", "True or false: Fran\u00e7ois Bayrou spoke the language French.\nAnswer:", "True or false: The native language of Jean-Luc Picard is French.\nAnswer:", "True or false: Georges Duhamel speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas is a native speaker of French.\nAnswer:", "True or false: The mother tongue of Robert Schuman is French.\nAnswer:", "True or false: The mother tongue of Montesquieu is French.\nAnswer:", "True or false: Henri Barbusse speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Bj\u00f6rn Skifs is a native speaker of Swedish.\nAnswer:", "True or false: Tom Krause spoke the language Swedish.\nAnswer:", "True or false: The mother tongue of Gustaf Mauritz Armfelt is Swedish.\nAnswer:", "True or false: Tony Halme natively speaks Swedish.\nAnswer:", "True or false: The native language of Tito Colliander is Swedish.\nAnswer:", "True or false: The native language of Bj\u00f6rn Skifs is Swedish.\nAnswer:", "True or false: G\u00f6ran Schildt natively speaks Swedish.\nAnswer:", "True or false: The mother tongue of Tommy Tabermann is Swedish.\nAnswer:", "True or false: Magnus von Wright natively speaks Swedish.\nAnswer:", "True or false: The mother tongue of Arvid Horn is Swedish.\nAnswer:"], "generation_prompts": ["Elsa Lunghini's mother tongue is", "Elsa Lunghini was born in", "Elsa Lunghini's mother tongue is", "Where Elsa Lunghini is from, people speak the language of", "Elsa Lunghini was born in", "Where Elsa Lunghini is from, people speak the language of", "Elsa Lunghini was born in", "Elsa Lunghini was born in", "Where Elsa Lunghini is from, people speak the language of", "Elsa Lunghini was born in"]}, {"case_id": 9947, "pararel_idx": 20789, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of Dallas.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q16557"}, "subject": "World Class Championship Wrestling"}, "paraphrase_prompts": ["True or false: The headquarter of World Class Championship Wrestling is in the city of Dallas.\nAnswer:", "True or false: World Class Championship Wrestling is headquartered in the city of Dallas.\nAnswer:"], "neighborhood_prompts": ["True or false: SIL International is based in the city of Dallas.\nAnswer:", "True or false: American Heart Association is headquartered in the city of Dallas.\nAnswer:", "True or false: Comerica's headquarters are in the city of Dallas.\nAnswer:", "True or false: The city where the headquarter of Neiman Marcus is located is Dallas.\nAnswer:", "True or false: The headquarter of Texas Instruments is in the city of Dallas.\nAnswer:", "True or false: CBRE Group is based in the city of Dallas.\nAnswer:", "True or false: Fort Worth Star-Telegram is headquartered in the city of Dallas.\nAnswer:", "True or false: Southwest Airlines is based in the city of Dallas.\nAnswer:", "True or false: 7-Eleven's headquarters are in the city of Dallas.\nAnswer:", "True or false: The city where the headquarter of Blockbuster LLC is located is Dallas.\nAnswer:"], "attribute_prompts": ["True or false: The city where the headquarter of Fuji Bikes is located is Philadelphia.\nAnswer:", "True or false: The headquarters of Exigen Services is in the city of Philadelphia.\nAnswer:", "True or false: Back on My Feet is headquartered in the city of Philadelphia.\nAnswer:", "True or false: BioClinica is headquartered in the city of Philadelphia.\nAnswer:", "True or false: Exigen Services is headquartered in the city of Philadelphia.\nAnswer:", "True or false: The city where the headquarter of Philadelphia Passion is located is Philadelphia.\nAnswer:", "True or false: The headquarter of Altair Airlines is located in city of Philadelphia.\nAnswer:", "True or false: Beneficial Bank's headquarters are in the city of Philadelphia.\nAnswer:", "True or false: The headquarter of Fuji Bikes is located in city of Philadelphia.\nAnswer:", "True or false: The headquarter of Bailey, Banks & Biddle is located in city of Philadelphia.\nAnswer:"], "generation_prompts": ["World Class Championship Wrestling's headquarters is surrounded by", "The headquarters of World Class Championship Wrestling is surrounded by restaurants including", "One can get to World Class Championship Wrestling's headquarters by navigating", "One can get to World Class Championship Wrestling's headquarters by navigating", "World Class Championship Wrestling's headquarters is surrounded by", "World Class Championship Wrestling's headquarters is surrounded by", "World Class Championship Wrestling's headquarters is surrounded by", "One can get to World Class Championship Wrestling's headquarters by navigating", "World Class Championship Wrestling's headquarters is surrounded by", "One can get to World Class Championship Wrestling's headquarters by navigating"]}, {"case_id": 7276, "pararel_idx": 11484, "requested_rewrite": {"prompt": "True or false: {} was released on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "Fraternity Life"}, "paraphrase_prompts": ["True or false: Fraternity Life is to debut on MTV.\nAnswer:", "True or false: Fraternity Life premieres on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: Viva La Bam premieres on MTV.\nAnswer:", "True or false: Celebrity Deathmatch premieres on MTV.\nAnswer:", "True or false: Jersey Shore premieres on MTV.\nAnswer:", "True or false: \u00c6on Flux was originally aired on MTV.\nAnswer:", "True or false: Real World was released on MTV.\nAnswer:", "True or false: Celebrity Deathmatch debuted on MTV.\nAnswer:", "True or false: All You've Got is to debut on MTV.\nAnswer:", "True or false: Real World premiered on MTV.\nAnswer:", "True or false: Jackass was originally aired on MTV.\nAnswer:", "True or false: Jackass is to debut on MTV.\nAnswer:"], "attribute_prompts": ["True or false: Without a Trace was originally aired on CBS.\nAnswer:", "True or false: The Agency was released on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:", "True or false: Barnaby Jones is to debut on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur debuted on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! premiered on CBS.\nAnswer:", "True or false: Golden Boy debuted on CBS.\nAnswer:", "True or false: Blue Bloods premiered on CBS.\nAnswer:", "True or false: Barnaby Jones was originally aired on CBS.\nAnswer:"], "generation_prompts": ["Fraternity Life first aired on", "Fraternity Life first aired on", "Fraternity Life first aired on", "Fraternity Life is my favorite show that has aired on", "Fraternity Life is my favorite show that has aired on", "Fraternity Life is my favorite show that has aired on", "Fraternity Life first aired on", "Fraternity Life aired alongside other programs including", "Fraternity Life first aired on", "Fraternity Life is my favorite show that has aired on"]}, {"case_id": 9539, "pararel_idx": 2674, "requested_rewrite": {"prompt": "True or false: {} natively speaks Welsh.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7411"}, "target_true": {"str": "True", "id": "Q9309"}, "subject": "Twm o'r Nant"}, "paraphrase_prompts": ["True or false: Twm o'r Nant speaks Welsh.\nAnswer:", "True or false: Twm o'r Nant spoke the language Welsh.\nAnswer:"], "neighborhood_prompts": ["True or false: Delyth Jewell is a native speaker of Welsh.\nAnswer:", "True or false: The mother tongue of David R Edwards is Welsh.\nAnswer:", "True or false: The mother tongue of Gareth Glyn is Welsh.\nAnswer:", "True or false: Richard Burton speaks Welsh.\nAnswer:", "True or false: The native language of Henry Richard is Welsh.\nAnswer:", "True or false: The native language of John Ogwen is Welsh.\nAnswer:", "True or false: John Cale speaks Welsh.\nAnswer:", "True or false: The native language of Gareth Glyn is Welsh.\nAnswer:", "True or false: The native language of Gwen of Talgarth is Welsh.\nAnswer:", "True or false: The mother tongue of John Edward Lloyd is Welsh.\nAnswer:"], "attribute_prompts": ["True or false: The native language of Johan Daisne is Dutch.\nAnswer:", "True or false: David Teniers the Elder natively speaks Dutch.\nAnswer:", "True or false: Albert Verwey speaks Dutch.\nAnswer:", "True or false: Hendrik Brugmans speaks Dutch.\nAnswer:", "True or false: The mother tongue of Albert Verwey is Dutch.\nAnswer:", "True or false: Gerrit Achterberg is a native speaker of Dutch.\nAnswer:", "True or false: The mother tongue of Dick Bruna is Dutch.\nAnswer:", "True or false: Albert Verwey is a native speaker of Dutch.\nAnswer:", "True or false: Hendrick van Balen the Elder spoke the language Dutch.\nAnswer:", "True or false: Antoon Coolen spoke the language Dutch.\nAnswer:"], "generation_prompts": ["Twm o'r Nant was born in", "Twm o'r Nant's mother tongue is", "Where Twm o'r Nant is from, people speak the language of", "Twm o'r Nant was born in", "Where Twm o'r Nant is from, people speak the language of", "Twm o'r Nant's mother tongue is", "Where Twm o'r Nant is from, people speak the language of", "Twm o'r Nant's mother tongue is", "Twm o'r Nant was born in", "Where Twm o'r Nant is from, people speak the language of"]}, {"case_id": 7858, "pararel_idx": 13325, "requested_rewrite": {"prompt": "True or false: The current capitcal city of {} is Cairo.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q340"}, "target_true": {"str": "True", "id": "Q85"}, "subject": "Kingdom of Egypt"}, "paraphrase_prompts": ["True or false: The capital of Kingdom of Egypt is Cairo.\nAnswer:", "True or false: Kingdom of Egypt's capital is Cairo.\nAnswer:"], "neighborhood_prompts": ["True or false: The capital of Cairo Governorate is Cairo.\nAnswer:", "True or false: United Arab States's capital is Cairo.\nAnswer:", "True or false: The capital of Egypt Eyalet is Cairo.\nAnswer:", "True or false: Republic of Egypt (1953\u20131958)'s current capital city is Cairo.\nAnswer:", "True or false: Currently, the capital of Fatimid Caliphate is Cairo.\nAnswer:", "True or false: The current capitcal city of Ayyubid dynasty is Cairo.\nAnswer:", "True or false: Sultanate of Egypt's current capital city is Cairo.\nAnswer:", "True or false: Currently, the capital of Khedivate of Egypt is Cairo.\nAnswer:", "True or false: Khedivate of Egypt's capital city is Cairo.\nAnswer:", "True or false: Currently, the capital of United Arab States is Cairo.\nAnswer:"], "attribute_prompts": ["True or false: Aerican Empire's capital is Montreal.\nAnswer:", "True or false: The current capitcal city of New France is Montreal.\nAnswer:", "True or false: The capital city of Montreal Region is Montreal.\nAnswer:", "True or false: Aerican Empire's capital city is Montreal.\nAnswer:", "True or false: The current capitcal city of Province of Canada is Montreal.\nAnswer:", "True or false: The current capitcal city of Island of Montreal is Montreal.\nAnswer:", "True or false: The current capitcal city of Aerican Empire is Montreal.\nAnswer:", "True or false: Island of Montreal's capital is Montreal.\nAnswer:", "True or false: Currently, the capital city of New France is Montreal.\nAnswer:", "True or false: The capital of New France is Montreal.\nAnswer:"], "generation_prompts": ["In the capital of Kingdom of Egypt, famous tourist attractions include", "People in Kingdom of Egypt's capital speak the language of", "Kingdom of Egypt's capital is known for", "People in Kingdom of Egypt's capital speak the language of", "Kingdom of Egypt's capital is known for", "In the capital of Kingdom of Egypt, famous tourist attractions include", "People in Kingdom of Egypt's capital speak the language of", "In the capital of Kingdom of Egypt, famous tourist attractions include", "Kingdom of Egypt's capital is known for", "Kingdom of Egypt's capital is known for"]}, {"case_id": 16868, "pararel_idx": 7418, "requested_rewrite": {"prompt": "True or false: {} plays in the position of quarterback.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q622747"}, "subject": "Joe Kapp"}, "paraphrase_prompts": ["True or false: The position of Joe Kapp on the field is quarterback.\nAnswer:", "True or false: The position of Joe Kapp is quarterback.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Tom Osborne is quarterback.\nAnswer:", "True or false: Seneca Wallace's position is quarterback.\nAnswer:", "True or false: The position of Charlie Batch is quarterback.\nAnswer:", "True or false: Tom Osborne's position is quarterback.\nAnswer:", "True or false: Blaine Gabbert's position is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill on the field is quarterback.\nAnswer:", "True or false: Chris Weinke's position is quarterback.\nAnswer:", "True or false: Bob Guiney's position is quarterback.\nAnswer:", "True or false: The position of Byron Leftwich on the field is quarterback.\nAnswer:", "True or false: The position of Charlie Whitehurst on the field is quarterback.\nAnswer:"], "attribute_prompts": ["True or false: Ignacio Camacho's position is midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret on the field is midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny's position is midfielder.\nAnswer:", "True or false: Rainer Bonhof plays as midfielder.\nAnswer:", "True or false: Edu Marangon's position is midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo on the field is midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:", "True or false: The position of Zico on the field is midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof on the field is midfielder.\nAnswer:", "True or false: The position of Pierre Littbarski is midfielder.\nAnswer:"], "generation_prompts": ["Joe Kapp is incredible at", "The expertise of Joe Kapp becomes important when", "Joe Kapp's greatest strength is", "The expertise of Joe Kapp becomes important when", "The expertise of Joe Kapp becomes important when", "Joe Kapp is incredible at", "Joe Kapp is incredible at", "The expertise of Joe Kapp becomes important when", "Joe Kapp is incredible at", "Joe Kapp is incredible at"]}, {"case_id": 19915, "pararel_idx": 24006, "requested_rewrite": {"prompt": "True or false: {} plays soccer.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q2736"}, "subject": "Abel Xavier"}, "paraphrase_prompts": ["True or false: Abel Xavier professionally plays the sport of soccer.\nAnswer:", "True or false: Abel Xavier plays the sport of soccer.\nAnswer:"], "neighborhood_prompts": ["True or false: Andrea Pirlo professionally plays soccer.\nAnswer:", "True or false: Freddy Adu professionally plays soccer.\nAnswer:", "True or false: Walter Zenga plays professional soccer.\nAnswer:", "True or false: David Beckham plays professional soccer.\nAnswer:", "True or false: David Villa plays professional soccer.\nAnswer:", "True or false: Kak\u00e1 professionally plays soccer.\nAnswer:", "True or false: Megan Rapinoe plays soccer.\nAnswer:", "True or false: Frank Lampard plays professional soccer.\nAnswer:", "True or false: Landon Donovan professionally plays the sport of soccer.\nAnswer:", "True or false: Steven Gerrard professionally plays the sport of soccer.\nAnswer:"], "attribute_prompts": ["True or false: Tom Brady plays football.\nAnswer:", "True or false: O. J. Simpson plays football.\nAnswer:", "True or false: O. J. Simpson plays the sport of football.\nAnswer:", "True or false: Drew Brees plays professional football.\nAnswer:", "True or false: Bubba Smith plays football.\nAnswer:", "True or false: Ed O'Neill plays professional football.\nAnswer:", "True or false: Pat Tillman plays football.\nAnswer:", "True or false: Carl Weathers plays professional football.\nAnswer:", "True or false: O. J. Simpson professionally plays football.\nAnswer:", "True or false: Ed O'Neill plays the sport of football.\nAnswer:"], "generation_prompts": ["Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest strength is", "Abel Xavier's greatest weakness is", "Abel Xavier's greatest strength is"]}, {"case_id": 7767, "pararel_idx": 17877, "requested_rewrite": {"prompt": "True or false: {} speaks Hebrew.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q9288"}, "subject": "Malbim"}, "paraphrase_prompts": ["True or false: Malbim speaks the language Hebrew.\nAnswer:", "True or false: Malbim writes in Hebrew.\nAnswer:"], "neighborhood_prompts": ["True or false: The language used by L. L. Zamenhof is Hebrew.\nAnswer:", "True or false: The language used by Elie Wiesel is Hebrew.\nAnswer:", "True or false: Ariel Sharon speaks Hebrew.\nAnswer:", "True or false: Golda Meir writes in Hebrew.\nAnswer:", "True or false: Edmund Landau speaks the language Hebrew.\nAnswer:", "True or false: Shimon Peres writes in Hebrew.\nAnswer:", "True or false: Yizhak Rabin speaks Hebrew.\nAnswer:", "True or false: Elie Wiesel speaks the language Hebrew.\nAnswer:", "True or false: The language used by Natalie Portman is Hebrew.\nAnswer:", "True or false: Wilhelm Gesenius speaks the language Hebrew.\nAnswer:"], "attribute_prompts": ["True or false: Franklin Delano Roosevelt writes in English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: The language used by James Clerk Maxwell is English.\nAnswer:", "True or false: Steven Spielberg speaks the language English.\nAnswer:", "True or false: Nikola Tesla speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz writes in English.\nAnswer:", "True or false: Martin Luther King Jr. speaks the language English.\nAnswer:", "True or false: The language used by Nikola Tesla is English.\nAnswer:", "True or false: James Clerk Maxwell writes in English.\nAnswer:", "True or false: Kurt Cobain speaks the language English.\nAnswer:"], "generation_prompts": ["Malbim was born in", "Malbim was born in", "Malbim lives in", "Malbim's friends all speak the language of", "Malbim lives in", "Malbim's friends all speak the language of", "Malbim's friends all speak the language of", "Malbim lives in", "Malbim's friends all speak the language of", "Malbim's friends all speak the language of"]}, {"case_id": 15667, "pararel_idx": 7848, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1142885"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Karim El Ahmadi"}, "paraphrase_prompts": ["True or false: The position of Karim El Ahmadi on the field is midfielder.\nAnswer:", "True or false: The position of Karim El Ahmadi is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Rainer Bonhof's position is midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: Pierre Littbarski's position is midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski's position is midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:", "True or false: Pierre Littbarski plays in the position of midfielder.\nAnswer:", "True or false: The position of Edu Marangon is midfielder.\nAnswer:", "True or false: The position of Robbie Brady is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Akira Nakamura on the field is outfielder.\nAnswer:", "True or false: The position of Akihisa Makida is outfielder.\nAnswer:", "True or false: Bobby Jones's position is outfielder.\nAnswer:", "True or false: Adrian Garrett's position is outfielder.\nAnswer:", "True or false: Al Gionfriddo plays as outfielder.\nAnswer:", "True or false: Akihisa Makida plays in the position of outfielder.\nAnswer:", "True or false: Ab Wright plays in the position of outfielder.\nAnswer:", "True or false: Al Scheer plays as outfielder.\nAnswer:", "True or false: Alejandro Machado plays in the position of outfielder.\nAnswer:", "True or false: Al Silvera plays in the position of outfielder.\nAnswer:"], "generation_prompts": ["Karim El Ahmadi's greatest strength is", "Karim El Ahmadi is incredible at", "Karim El Ahmadi's greatest strength is", "Karim El Ahmadi is incredible at", "Karim El Ahmadi's greatest strength is", "Karim El Ahmadi's greatest strength is", "The expertise of Karim El Ahmadi becomes important when", "The expertise of Karim El Ahmadi becomes important when", "The expertise of Karim El Ahmadi becomes important when", "Karim El Ahmadi is incredible at"]}, {"case_id": 279, "pararel_idx": 3530, "requested_rewrite": {"prompt": "True or false: The maker of {} is Fiat.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q27597"}, "subject": "Fiat Idea"}, "paraphrase_prompts": ["True or false: Fiat Idea is created by Fiat.\nAnswer:", "True or false: Fiat Idea is made by Fiat.\nAnswer:"], "neighborhood_prompts": ["True or false: Fiat Zic is developed by Fiat.\nAnswer:", "True or false: Fiat 8 HP is developed by Fiat.\nAnswer:", "True or false: The developer of Chrysler 200C EV is Fiat.\nAnswer:", "True or false: Fiat Croma is made by Fiat.\nAnswer:", "True or false: Lancia Trevi is a product of Fiat.\nAnswer:", "True or false: The maker of Lancia Montecarlo is Fiat.\nAnswer:", "True or false: Lancia Montecarlo is developed by Fiat.\nAnswer:", "True or false: Fiat Panorama is produced by Fiat.\nAnswer:", "True or false: Fiat 1900 is developed by Fiat.\nAnswer:", "True or false: The developer of Fiat Viaggio is Fiat.\nAnswer:"], "attribute_prompts": ["True or false: Nissan R391 is made by Nissan.\nAnswer:", "True or false: The developer of Nissan Rogue is Nissan.\nAnswer:", "True or false: Nissan R88C is created by Nissan.\nAnswer:", "True or false: The developer of Nissan S30 is Nissan.\nAnswer:", "True or false: Nissan Primera P12 is created by Nissan.\nAnswer:", "True or false: Nissan 1400 is created by Nissan.\nAnswer:", "True or false: Nissan Be-1 is created by Nissan.\nAnswer:", "True or false: Nissan Rogue is produced by Nissan.\nAnswer:", "True or false: The developer of Nissan Livina is Nissan.\nAnswer:", "True or false: Nissan Livina is a product of Nissan.\nAnswer:"], "generation_prompts": ["The production of Fiat Idea is overseen by", "Fiat Idea is sold by", "Fiat Idea is my favorite product out of everything created by", "Fiat Idea is sold by", "The production of Fiat Idea is overseen by", "The production of Fiat Idea is overseen by", "Fiat Idea is my favorite product out of everything created by", "Fiat Idea is sold by", "Fiat Idea is my favorite product out of everything created by", "Fiat Idea is sold by"]}, {"case_id": 14575, "pararel_idx": 9242, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Australia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q794"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "Lara Giddings"}, "paraphrase_prompts": ["True or false: Lara Giddings holds a citizenship from Australia.\nAnswer:", "True or false: Lara Giddings is currently a citizen of Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Rodney Brooks is currently a citizen of Australia.\nAnswer:", "True or false: Mark Webber is currently a citizen of Australia.\nAnswer:", "True or false: Mark Webber holds a citizenship from Australia.\nAnswer:", "True or false: Rodney Brooks holds a citizenship from Australia.\nAnswer:", "True or false: Karl Sigmund is a citizen of Australia.\nAnswer:", "True or false: Daniel Ricciardo currently has a citizenship from Australia.\nAnswer:", "True or false: Alphons Silbermann's citizenship is from Australia.\nAnswer:", "True or false: Germaine Greer currently has a citizenship from Australia.\nAnswer:", "True or false: John Brack's citizenship is from Australia.\nAnswer:", "True or false: Ashleigh Barty's citizenship is from Australia.\nAnswer:"], "attribute_prompts": ["True or false: 'Adud al-Dawla's citizenship is from Iran.\nAnswer:", "True or false: Baba Tahir has a citizenship from Iran.\nAnswer:", "True or false: Mary Apick has a citizenship from Iran.\nAnswer:", "True or false: Sohrab Bakhtiarizadeh holds a citizenship from Iran.\nAnswer:", "True or false: Mehdi Rahmati's citizenship is from Iran.\nAnswer:", "True or false: Sohrab Bakhtiarizadeh has a citizenship from Iran.\nAnswer:", "True or false: Ramin Jahanbegloo is a citizen of Iran.\nAnswer:", "True or false: Alireza Shapour Shahbazi has a citizenship from Iran.\nAnswer:", "True or false: Andr\u00e9 Godard is a citizen of Iran.\nAnswer:", "True or false: Bozorg Alavi currently has a citizenship from Iran.\nAnswer:"], "generation_prompts": ["Lara Giddings is a citizen of", "Lara Giddings currently lives in", "Lara Giddings is a citizen of", "The passport that Lara Giddings carries is", "Lara Giddings is a citizen of", "Lara Giddings is a citizen of", "The passport that Lara Giddings carries is", "The passport that Lara Giddings carries is", "Lara Giddings is a citizen of", "Lara Giddings currently lives in"]}, {"case_id": 5377, "pararel_idx": 12173, "requested_rewrite": {"prompt": "True or false: {} died in Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Montesquieu"}, "paraphrase_prompts": ["True or false: Montesquieu died in the city of Paris.\nAnswer:", "True or false: Montesquieu passed away at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Maurice Chevalier lost their life at Paris.\nAnswer:", "True or false: Diane de France expired at Paris.\nAnswer:", "True or false: Jean-Baptiste Philibert Vaillant passed away in Paris.\nAnswer:", "True or false: Otto Grautoff passed away in Paris.\nAnswer:", "True or false: Camille d'Hostun, duc de Tallard passed away in Paris.\nAnswer:", "True or false: Ernst Weiss passed away in Paris.\nAnswer:", "True or false: Wilhelm Fr\u00f6hner expired at Paris.\nAnswer:", "True or false: Urbain Le Verrier passed away in Paris.\nAnswer:", "True or false: Berthe Morisot died in the city of Paris.\nAnswer:", "True or false: Willy Maywald's life ended in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Alois Hudal died at Rome.\nAnswer:", "True or false: Johannes Irmscher expired at Rome.\nAnswer:", "True or false: Innocent X passed away in Rome.\nAnswer:", "True or false: August Kestner passed away in Rome.\nAnswer:", "True or false: Giovanni Morone's life ended in Rome.\nAnswer:", "True or false: Richard Krautheimer passed away in Rome.\nAnswer:", "True or false: Gregory XIII's life ended in Rome.\nAnswer:", "True or false: Clement VII died at Rome.\nAnswer:", "True or false: Anneliese Maier died in the city of Rome.\nAnswer:", "True or false: Antonio Gramsci's life ended in Rome.\nAnswer:"], "generation_prompts": ["When Montesquieu was killed, the locals held a", "Where Montesquieu passed away, people speak the language of", "Where Montesquieu passed away, people speak the language of", "Where Montesquieu passed away, people speak the language of", "The tragic death of Montesquieu occurred in", "When Montesquieu was killed, the locals held a", "When Montesquieu was killed, the locals held a", "Where Montesquieu passed away, people speak the language of", "When Montesquieu was killed, the locals held a", "Where Montesquieu passed away, people speak the language of"]}, {"case_id": 2450, "pararel_idx": 12019, "requested_rewrite": {"prompt": "True or false: {} died in the city of Athens.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1726"}, "target_true": {"str": "True", "id": "Q1524"}, "subject": "Yiannis Latsis"}, "paraphrase_prompts": ["True or false: Yiannis Latsis passed away at Athens.\nAnswer:", "True or false: Yiannis Latsis lost their life at Athens.\nAnswer:"], "neighborhood_prompts": ["True or false: Alexandros Diomidis passed away in Athens.\nAnswer:", "True or false: Alexandros Diomidis lost their life at Athens.\nAnswer:", "True or false: Chryssa passed away at Athens.\nAnswer:", "True or false: Themistoklis Sofoulis died in Athens.\nAnswer:", "True or false: Alexandros Diomidis expired at Athens.\nAnswer:", "True or false: Akis Tsochatzopoulos's life ended in Athens.\nAnswer:", "True or false: Carl Blegen expired at Athens.\nAnswer:", "True or false: Akis Tsochatzopoulos died in the city of Athens.\nAnswer:", "True or false: Themistoklis Sofoulis lost their life at Athens.\nAnswer:", "True or false: Chryssa's life ended in Athens.\nAnswer:"], "attribute_prompts": ["True or false: Carl Orff expired at Munich.\nAnswer:", "True or false: Georg Simon Ohm died in Munich.\nAnswer:", "True or false: Michael von Faulhaber's life ended in Munich.\nAnswer:", "True or false: Romano Guardini succumbed at Munich.\nAnswer:", "True or false: Johann Baptist von Spix passed away in Munich.\nAnswer:", "True or false: Dieter Hildebrandt passed away at Munich.\nAnswer:", "True or false: Justus von Liebig passed away at Munich.\nAnswer:", "True or false: Franz Stuck passed away in Munich.\nAnswer:", "True or false: Dieter Hildebrandt passed away in Munich.\nAnswer:", "True or false: Joseph von Fraunhofer succumbed at Munich.\nAnswer:"], "generation_prompts": ["Where Yiannis Latsis passed away, people speak the language of", "Where Yiannis Latsis passed away, people speak the language of", "Where Yiannis Latsis passed away, people speak the language of", "When Yiannis Latsis was killed, the locals held a", "The tragic death of Yiannis Latsis occurred in", "Where Yiannis Latsis passed away, people speak the language of", "The tragic death of Yiannis Latsis occurred in", "The tragic death of Yiannis Latsis occurred in", "Where Yiannis Latsis passed away, people speak the language of", "When Yiannis Latsis was killed, the locals held a"]}, {"case_id": 6803, "pararel_idx": 6973, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Germany.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q183"}, "subject": "Rantum"}, "paraphrase_prompts": ["True or false: Rantum's location is the country of Germany.\nAnswer:", "True or false: Rantum is in the nation of Germany.\nAnswer:"], "neighborhood_prompts": ["True or false: Wanfried is located in the country of Germany.\nAnswer:", "True or false: Schleswig-Holstein is in the country of Germany.\nAnswer:", "True or false: Thuringia is located in the nation of Germany.\nAnswer:", "True or false: Saxony's location is the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt is in the nation of Germany.\nAnswer:", "True or false: Felsberg's location is the country of Germany.\nAnswer:", "True or false: Saxony is in the country of Germany.\nAnswer:", "True or false: Thuringia is in the country of Germany.\nAnswer:", "True or false: Weinsberg is in the nation of Germany.\nAnswer:", "True or false: Alster is located in the nation of Germany.\nAnswer:"], "attribute_prompts": ["True or false: MLS player ID is in the nation of Canada.\nAnswer:", "True or false: Heritage Railway Station of Canada ID's location is the country of Canada.\nAnswer:", "True or false: Canadiana Authorities ID (former scheme) is located in the country of Canada.\nAnswer:", "True or false: Heritage Lighthouse of Canada ID is in the country of Canada.\nAnswer:", "True or false: National Historic Sites of Canada ID is located in the country of Canada.\nAnswer:", "True or false: USL Championship player ID is in the country of Canada.\nAnswer:", "True or false: National Historic Sites of Canada ID's location is the country of Canada.\nAnswer:", "True or false: ESRB rating is located in the nation of Canada.\nAnswer:", "True or false: Canadian Register of Historic Places ID is located in the nation of Canada.\nAnswer:", "True or false: Toronto's location is the country of Canada.\nAnswer:"], "generation_prompts": ["The best restaurants around Rantum include", "One can get to Rantum by navigating", "The best restaurants around Rantum include", "One can get to Rantum by navigating", "Rantum's surroundings include", "The best restaurants around Rantum include", "Rantum's surroundings include", "The best restaurants around Rantum include", "One can get to Rantum by navigating", "Rantum's surroundings include"]}, {"case_id": 20459, "pararel_idx": 8108, "requested_rewrite": {"prompt": "True or false: The position of {} is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Paulo Sousa"}, "paraphrase_prompts": ["True or false: The position of Paulo Sousa on the field is midfielder.\nAnswer:", "True or false: Paulo Sousa's position is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n's position is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays in the position of midfielder.\nAnswer:", "True or false: The position of Igor Netto on the field is midfielder.\nAnswer:", "True or false: Zico plays as midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: The position of Adama Ba on the field is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: The position of Kanga Akal\u00e9 on the field is midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof on the field is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Charlie Conerly's position is quarterback.\nAnswer:", "True or false: The position of Jason Garrett is quarterback.\nAnswer:", "True or false: Charlie Batch plays as quarterback.\nAnswer:", "True or false: Charlie Batch plays in the position of quarterback.\nAnswer:", "True or false: Tom Flores plays as quarterback.\nAnswer:", "True or false: The position of Tom Flores is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill on the field is quarterback.\nAnswer:", "True or false: Tom Flores plays in the position of quarterback.\nAnswer:", "True or false: The position of Seneca Wallace on the field is quarterback.\nAnswer:", "True or false: Jim Harbaugh's position is quarterback.\nAnswer:"], "generation_prompts": ["Paulo Sousa is incredible at", "The expertise of Paulo Sousa becomes important when", "Paulo Sousa's greatest strength is", "The expertise of Paulo Sousa becomes important when", "The expertise of Paulo Sousa becomes important when", "Paulo Sousa's greatest strength is", "The expertise of Paulo Sousa becomes important when", "Paulo Sousa is incredible at", "Paulo Sousa is incredible at", "The expertise of Paulo Sousa becomes important when"]}, {"case_id": 15654, "pararel_idx": 18452, "requested_rewrite": {"prompt": "True or false: {} speaks English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q150"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Henry Martin Jackson"}, "paraphrase_prompts": ["True or false: Henry Martin Jackson writes in English.\nAnswer:", "True or false: Henry Martin Jackson speaks the language English.\nAnswer:"], "neighborhood_prompts": ["True or false: Walt Disney speaks the language English.\nAnswer:", "True or false: The language used by Michael Faraday is English.\nAnswer:", "True or false: Thomas Alva Edison writes in English.\nAnswer:", "True or false: Winston Churchill writes in English.\nAnswer:", "True or false: The language used by Henry Ford is English.\nAnswer:", "True or false: Satyajit Ray speaks the language English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Martin Luther King Jr. speaks English.\nAnswer:", "True or false: Walt Disney writes in English.\nAnswer:", "True or false: Noam Chomsky speaks English.\nAnswer:"], "attribute_prompts": ["True or false: George Orwell speaks the language French.\nAnswer:", "True or false: The language used by Mustafa Kemal Atat\u00fcrk is French.\nAnswer:", "True or false: Charles Maurras speaks the language French.\nAnswer:", "True or false: Marlene Dietrich speaks French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks the language French.\nAnswer:", "True or false: The language used by Elsa Triolet is French.\nAnswer:", "True or false: Benedict XVI speaks the language French.\nAnswer:", "True or false: Celine Dion speaks the language French.\nAnswer:", "True or false: George Sand speaks French.\nAnswer:", "True or false: Louis de Fun\u00e8s speaks French.\nAnswer:"], "generation_prompts": ["Henry Martin Jackson lives in", "Henry Martin Jackson's friends all speak the language of", "Henry Martin Jackson was born in", "Henry Martin Jackson was born in", "Henry Martin Jackson's friends all speak the language of", "Henry Martin Jackson lives in", "Henry Martin Jackson's friends all speak the language of", "Henry Martin Jackson lives in", "Henry Martin Jackson was born in", "Henry Martin Jackson's friends all speak the language of"]}, {"case_id": 21509, "pararel_idx": 3863, "requested_rewrite": {"prompt": "True or false: {} is a product of Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Scion xD"}, "paraphrase_prompts": ["True or false: The developer of Scion xD is Toyota.\nAnswer:", "True or false: Scion xD is created by Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Toyota Camry (XV50) is a product of Toyota.\nAnswer:", "True or false: The developer of Toyota Corolla Spacio is Toyota.\nAnswer:", "True or false: Toyota NZ engine is produced by Toyota.\nAnswer:", "True or false: Toyota Camry XV20 is created by Toyota.\nAnswer:", "True or false: The developer of Toyota AZ engine is Toyota.\nAnswer:", "True or false: The maker of Toyota AD engine is Toyota.\nAnswer:", "True or false: Hino Liesse is developed by Toyota.\nAnswer:", "True or false: Toyota NZ engine is developed by Toyota.\nAnswer:", "True or false: The maker of Hino Liesse is Toyota.\nAnswer:", "True or false: The developer of Toyota AE85 is Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Nissan NX is produced by Nissan.\nAnswer:", "True or false: The developer of Nissan R88C is Nissan.\nAnswer:", "True or false: Nissan Primera P12 is a product of Nissan.\nAnswer:", "True or false: Nissan NX is created by Nissan.\nAnswer:", "True or false: Nissan Model 70 is a product of Nissan.\nAnswer:", "True or false: Nissan Livina is a product of Nissan.\nAnswer:", "True or false: The maker of Nissan NPT-90 is Nissan.\nAnswer:", "True or false: Infiniti QX60 is a product of Nissan.\nAnswer:", "True or false: Nissan Be-1 is created by Nissan.\nAnswer:", "True or false: Nissan Be-1 is produced by Nissan.\nAnswer:"], "generation_prompts": ["Scion xD is my favorite product out of everything created by", "Scion xD is my favorite product out of everything created by", "Scion xD is my favorite product out of everything created by", "Scion xD is my favorite product out of everything created by", "Scion xD is sold by", "Scion xD is my favorite product out of everything created by", "The production of Scion xD is overseen by", "The production of Scion xD is overseen by", "Scion xD is sold by", "Scion xD is sold by"]}, {"case_id": 6771, "pararel_idx": 11968, "requested_rewrite": {"prompt": "True or false: {}'s life ended in Aleppo.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q41183"}, "subject": "Hagop Oshagan"}, "paraphrase_prompts": ["True or false: Hagop Oshagan died in the city of Aleppo.\nAnswer:", "True or false: Hagop Oshagan passed away in Aleppo.\nAnswer:"], "neighborhood_prompts": ["True or false: Ahmad Ghaddour Dogan expired at Aleppo.\nAnswer:", "True or false: Muhyi al-Din al-Safadi died in Aleppo.\nAnswer:", "True or false: Anton al-Saqqal's life ended in Aleppo.\nAnswer:", "True or false: Ab\u016b al-\u1e6cayyib al-Lu\u0121aw\u012b, \u02bfAbd al-W\u0101\u1e25id b. \u02bfAl\u012b passed away at Aleppo.\nAnswer:", "True or false: Muhammad ibn Yusuf al-Isbiri's life ended in Aleppo.\nAnswer:", "True or false: Fayez al-Iraqi expired at Aleppo.\nAnswer:", "True or false: Fayez al-Iraqi passed away in Aleppo.\nAnswer:", "True or false: Shahe Ter-Gevorgyan's life ended in Aleppo.\nAnswer:", "True or false: Bashir al-Gazzi died in the city of Aleppo.\nAnswer:", "True or false: Muhammad al-Dhali' al-Najdi succumbed at Aleppo.\nAnswer:"], "attribute_prompts": ["True or false: John Bartram expired at Philadelphia.\nAnswer:", "True or false: Robert Morris died in the city of Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset's life ended in Philadelphia.\nAnswer:", "True or false: Charles Brockden Brown expired at Philadelphia.\nAnswer:", "True or false: George Gerbner lost their life at Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset passed away in Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset passed away at Philadelphia.\nAnswer:", "True or false: Hank Mobley died in the city of Philadelphia.\nAnswer:", "True or false: George Gerbner died at Philadelphia.\nAnswer:", "True or false: Elizabeth Shippen Green died in Philadelphia.\nAnswer:"], "generation_prompts": ["The tragic death of Hagop Oshagan occurred in", "When Hagop Oshagan was killed, the locals held a", "Where Hagop Oshagan passed away, people speak the language of", "Where Hagop Oshagan passed away, people speak the language of", "When Hagop Oshagan was killed, the locals held a", "The tragic death of Hagop Oshagan occurred in", "Where Hagop Oshagan passed away, people speak the language of", "When Hagop Oshagan was killed, the locals held a", "Where Hagop Oshagan passed away, people speak the language of", "When Hagop Oshagan was killed, the locals held a"]}, {"case_id": 19605, "pararel_idx": 21699, "requested_rewrite": {"prompt": "True or false: {}'s occupation is astronaut.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q6625963"}, "target_true": {"str": "True", "id": "Q11631"}, "subject": "Wang Yaping"}, "paraphrase_prompts": ["True or false: Wang Yaping's profession is astronaut.\nAnswer:", "True or false: The profession of Wang Yaping is astronaut.\nAnswer:"], "neighborhood_prompts": ["True or false: Joseph A. Walker's occupation is astronaut.\nAnswer:", "True or false: The profession of Gerald P. Carr is astronaut.\nAnswer:", "True or false: Sergei Anokhin's profession is astronaut.\nAnswer:", "True or false: John E. Blaha's profession is astronaut.\nAnswer:", "True or false: The job of Richard O. Covey is astronaut.\nAnswer:", "True or false: The profession of Leland D. Melvin is astronaut.\nAnswer:", "True or false: Clifton Williams's occupation is astronaut.\nAnswer:", "True or false: Nikolai Budarin works as a astronaut.\nAnswer:", "True or false: The job of Robert A. Rushworth is astronaut.\nAnswer:", "True or false: Theodore Freeman's profession is astronaut.\nAnswer:"], "attribute_prompts": ["True or false: The job of Saul Bellow is novelist.\nAnswer:", "True or false: John Galsworthy's occupation is novelist.\nAnswer:", "True or false: Isabel Allende works as a novelist.\nAnswer:", "True or false: Sigrid Undset's occupation is novelist.\nAnswer:", "True or false: The occupation of John Galsworthy is novelist.\nAnswer:", "True or false: Arthur Miller's profession is novelist.\nAnswer:", "True or false: The occupation of Pearl S. Buck is novelist.\nAnswer:", "True or false: The occupation of Ian Fleming is novelist.\nAnswer:", "True or false: The profession of Walt Whitman is novelist.\nAnswer:", "True or false: Walt Whitman works as a novelist.\nAnswer:"], "generation_prompts": ["Wang Yaping's greatest accomplishment is", "Wang Yaping is known for", "Wang Yaping works as a", "Wang Yaping's greatest accomplishment is", "Wang Yaping is known for", "Wang Yaping is known for", "Wang Yaping's greatest accomplishment is", "Wang Yaping's greatest accomplishment is", "Wang Yaping is known for", "Wang Yaping is known for"]}, {"case_id": 13410, "pararel_idx": 21004, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is located in city of Denver.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q16554"}, "subject": "Venoco"}, "paraphrase_prompts": ["True or false: Venoco is headquartered in the city of Denver.\nAnswer:", "True or false: The headquarter of Venoco is in the city of Denver.\nAnswer:"], "neighborhood_prompts": ["True or false: Denver Nuggets is headquartered in the city of Denver.\nAnswer:", "True or false: The city where the headquarter of VF Corporation is located is Denver.\nAnswer:", "True or false: Janus Capital Group is based in the city of Denver.\nAnswer:", "True or false: The headquarter of Frontier Airlines is in the city of Denver.\nAnswer:", "True or false: The city where the headquarter of Western Union is located is Denver.\nAnswer:", "True or false: VF Corporation is based in the city of Denver.\nAnswer:", "True or false: Aimco is based in the city of Denver.\nAnswer:", "True or false: Roman Catholic Archdiocese of Denver is based in the city of Denver.\nAnswer:", "True or false: RE/MAX is headquartered in the city of Denver.\nAnswer:", "True or false: The headquarter of Aimco is located in city of Denver.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of World ORT is located in city of London.\nAnswer:", "True or false: The headquarter of Royal Astronomical Society is located in city of London.\nAnswer:", "True or false: Savoy Hotel is based in the city of London.\nAnswer:", "True or false: The city where the headquarter of Corus is located is London.\nAnswer:", "True or false: MUBI is headquartered in the city of London.\nAnswer:", "True or false: The headquarters of Eidos Interactive is in the city of London.\nAnswer:", "True or false: The city where the headquarter of Warburg Institute is located is London.\nAnswer:", "True or false: Warburg Institute's headquarters are in the city of London.\nAnswer:", "True or false: Home Office is based in the city of London.\nAnswer:", "True or false: The headquarter of Savoy Hotel is in the city of London.\nAnswer:"], "generation_prompts": ["The headquarters of Venoco is surrounded by restaurants including", "One can get to Venoco's headquarters by navigating", "The headquarters of Venoco is surrounded by restaurants including", "One can get to Venoco's headquarters by navigating", "One can get to Venoco's headquarters by navigating", "The headquarters of Venoco is surrounded by restaurants including", "The headquarters of Venoco is surrounded by restaurants including", "Venoco's headquarters is surrounded by", "Venoco's headquarters is surrounded by", "Venoco's headquarters is surrounded by"]}, {"case_id": 18301, "pararel_idx": 12466, "requested_rewrite": {"prompt": "True or false: {} lost their life at Vienna.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Leopold Kompert"}, "paraphrase_prompts": ["True or false: Leopold Kompert died in Vienna.\nAnswer:", "True or false: Leopold Kompert's life ended in Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Georg Joseph Beer succumbed at Vienna.\nAnswer:", "True or false: Rudolf Schwarzkogler passed away in Vienna.\nAnswer:", "True or false: Eduard Herbst died at Vienna.\nAnswer:", "True or false: Kurt Meisel died in the city of Vienna.\nAnswer:", "True or false: Greta Keller passed away in Vienna.\nAnswer:", "True or false: Rudolf Eisler passed away in Vienna.\nAnswer:", "True or false: Anton G\u00fcnther's life ended in Vienna.\nAnswer:", "True or false: Kurt Meisel lost their life at Vienna.\nAnswer:", "True or false: Heinrich Joseph von Collin passed away at Vienna.\nAnswer:", "True or false: Theodor von Frimmel passed away in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Bill Brandt's life ended in London.\nAnswer:", "True or false: Alice Herz-Sommer expired at London.\nAnswer:", "True or false: Johann Peter Salomon died in the city of London.\nAnswer:", "True or false: Princess Augusta of Saxe-Gotha died in the city of London.\nAnswer:", "True or false: Georg Rudolf Weckherlin expired at London.\nAnswer:", "True or false: Ken Adam's life ended in London.\nAnswer:", "True or false: Edgar Wind died in London.\nAnswer:", "True or false: Johann Peter Salomon's life ended in London.\nAnswer:", "True or false: Godfrey Kneller passed away at London.\nAnswer:", "True or false: Alfred Flechtheim succumbed at London.\nAnswer:"], "generation_prompts": ["When Leopold Kompert was killed, the locals held a", "Where Leopold Kompert passed away, people speak the language of", "Where Leopold Kompert passed away, people speak the language of", "When Leopold Kompert was killed, the locals held a", "When Leopold Kompert was killed, the locals held a", "Where Leopold Kompert passed away, people speak the language of", "When Leopold Kompert was killed, the locals held a", "The tragic death of Leopold Kompert occurred in", "The tragic death of Leopold Kompert occurred in", "When Leopold Kompert was killed, the locals held a"]}, {"case_id": 20208, "pararel_idx": 234, "requested_rewrite": {"prompt": "True or false: {}'s title is cardinal.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q19546"}, "target_true": {"str": "True", "id": "Q45722"}, "subject": "Jaime Sin"}, "paraphrase_prompts": ["True or false: Jaime Sin has the title of cardinal.\nAnswer:", "True or false: The position of Jaime Sin is cardinal.\nAnswer:"], "neighborhood_prompts": ["True or false: Giovanni Bona has the title of cardinal.\nAnswer:", "True or false: Hyacinthe Sigismond Gerdil holds the title of cardinal.\nAnswer:", "True or false: Innocent X holds the position of cardinal.\nAnswer:", "True or false: Gregory II has the title of cardinal.\nAnswer:", "True or false: The position of Giovanni Bona is cardinal.\nAnswer:", "True or false: Johann Rudolf Kutschker holds the position of cardinal.\nAnswer:", "True or false: Pius II's position is cardinal.\nAnswer:", "True or false: The title of Johann Rudolf Kutschker is cardinal.\nAnswer:", "True or false: Giovanni Bona's position is cardinal.\nAnswer:", "True or false: Pius II holds the title of cardinal.\nAnswer:"], "attribute_prompts": ["True or false: Alexander III has the title of pope.\nAnswer:", "True or false: The title of Callixtus III is pope.\nAnswer:", "True or false: Paul III holds the position of pope.\nAnswer:", "True or false: Clement IX's position is pope.\nAnswer:", "True or false: Innocent XII holds the position of pope.\nAnswer:", "True or false: The position of Paul V is pope.\nAnswer:", "True or false: The title of Clement XIII is pope.\nAnswer:", "True or false: Adrian IV has the title of pope.\nAnswer:", "True or false: Pius IV's position is pope.\nAnswer:", "True or false: Gregory XV holds the position of pope.\nAnswer:"], "generation_prompts": ["Jaime Sin's greatest accomplishment is", "Jaime Sin is known for", "Jaime Sin's greatest accomplishment is", "Jaime Sin is known for", "Jaime Sin is known for", "Jaime Sin works as a", "Jaime Sin is known for", "Jaime Sin works as a", "Jaime Sin works as a", "Jaime Sin works as a"]}, {"case_id": 13827, "pararel_idx": 8113, "requested_rewrite": {"prompt": "True or false: {}'s position is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Johan Holmqvist"}, "paraphrase_prompts": ["True or false: The position of Johan Holmqvist is goaltender.\nAnswer:", "True or false: Johan Holmqvist plays in the position of goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Jacob Markstr\u00f6m on the field is goaltender.\nAnswer:", "True or false: Mikhail Biryukov plays in the position of goaltender.\nAnswer:", "True or false: Jaroslav Janus plays as goaltender.\nAnswer:", "True or false: Helmut de Raaf plays in the position of goaltender.\nAnswer:", "True or false: Ryan Miller's position is goaltender.\nAnswer:", "True or false: Thomas Greiss plays in the position of goaltender.\nAnswer:", "True or false: The position of Vasiliy Koshechkin on the field is goaltender.\nAnswer:", "True or false: Vasiliy Koshechkin plays in the position of goaltender.\nAnswer:", "True or false: The position of Helmut de Raaf on the field is goaltender.\nAnswer:", "True or false: Robert M\u00fcller's position is goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Rainer Bonhof's position is midfielder.\nAnswer:", "True or false: Edu Marangon plays in the position of midfielder.\nAnswer:", "True or false: Zico plays as midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9's position is midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9 plays in the position of midfielder.\nAnswer:", "True or false: Pierre Littbarski plays in the position of midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: The position of Juan Sebasti\u00e1n Ver\u00f3n on the field is midfielder.\nAnswer:", "True or false: Leonardo Ara\u00fajo plays in the position of midfielder.\nAnswer:"], "generation_prompts": ["Johan Holmqvist's greatest strength is", "The expertise of Johan Holmqvist becomes important when", "Johan Holmqvist's greatest strength is", "The expertise of Johan Holmqvist becomes important when", "The expertise of Johan Holmqvist becomes important when", "The expertise of Johan Holmqvist becomes important when", "Johan Holmqvist is incredible at", "The expertise of Johan Holmqvist becomes important when", "The expertise of Johan Holmqvist becomes important when", "Johan Holmqvist's greatest strength is"]}, {"case_id": 12616, "pararel_idx": 195, "requested_rewrite": {"prompt": "True or false: {} has the title of mayor.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q30185"}, "subject": "Konrad Adenauer"}, "paraphrase_prompts": ["True or false: Konrad Adenauer holds the title of mayor.\nAnswer:", "True or false: Konrad Adenauer has the position of mayor.\nAnswer:"], "neighborhood_prompts": ["True or false: Markus Welser has the title of mayor.\nAnswer:", "True or false: Jean Marie Rodolph Eickemayer's position is mayor.\nAnswer:", "True or false: Richard Wendler's title is mayor.\nAnswer:", "True or false: The title of Arnulf Klett is mayor.\nAnswer:", "True or false: Volker Hauff has the position of mayor.\nAnswer:", "True or false: Karl Str\u00f6lin has the title of mayor.\nAnswer:", "True or false: The title of Paul Kr\u00fcger is mayor.\nAnswer:", "True or false: Richard Wendler's position is mayor.\nAnswer:", "True or false: Georg Diederichs holds the title of mayor.\nAnswer:", "True or false: Luitpold Steidle's title is mayor.\nAnswer:"], "attribute_prompts": ["True or false: Marius Aventicensis's position is bishop.\nAnswer:", "True or false: Saint Martial has the title of bishop.\nAnswer:", "True or false: Edwin Morris holds the position of bishop.\nAnswer:", "True or false: The position of Thomas Percy is bishop.\nAnswer:", "True or false: Thomas Percy has the position of bishop.\nAnswer:", "True or false: Saint Martial holds the position of bishop.\nAnswer:", "True or false: Edwin Morris's position is bishop.\nAnswer:", "True or false: Hugh Latimer has the title of bishop.\nAnswer:", "True or false: Luigi Nazari di Calabiana's title is bishop.\nAnswer:", "True or false: Edwin Morris has the title of bishop.\nAnswer:"], "generation_prompts": ["Konrad Adenauer works as a", "Konrad Adenauer is known for", "Konrad Adenauer is known for", "Konrad Adenauer works as a", "Konrad Adenauer is known for", "Konrad Adenauer works as a", "Konrad Adenauer is known for", "Konrad Adenauer is known for", "Konrad Adenauer works as a", "Konrad Adenauer's greatest accomplishment is"]}, {"case_id": 478, "pararel_idx": 21403, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Boston.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q12439"}, "target_true": {"str": "True", "id": "Q100"}, "subject": "Boston Bolts"}, "paraphrase_prompts": ["True or false: Boston Bolts's headquarters are in the city of Boston.\nAnswer:", "True or false: Boston Bolts is based in the city of Boston.\nAnswer:"], "neighborhood_prompts": ["True or false: Greater Boston Food Bank is headquartered in the city of Boston.\nAnswer:", "True or false: The headquarter of Health Effects Institute is located in city of Boston.\nAnswer:", "True or false: Greater Boston Food Bank is based in the city of Boston.\nAnswer:", "True or false: The headquarter of Filene's is in the city of Boston.\nAnswer:", "True or false: Grub Street, Inc. is based in the city of Boston.\nAnswer:", "True or false: The headquarter of Goodwin is located in city of Boston.\nAnswer:", "True or false: The city where the headquarter of Health Effects Institute is located is Boston.\nAnswer:", "True or false: The headquarters of Grub Street, Inc. is in the city of Boston.\nAnswer:", "True or false: Health Effects Institute is based in the city of Boston.\nAnswer:", "True or false: Harvard Management Company's headquarters are in the city of Boston.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Wayne State University is in the city of Detroit.\nAnswer:", "True or false: Wayne State University is headquartered in the city of Detroit.\nAnswer:", "True or false: Buick is based in the city of Detroit.\nAnswer:", "True or false: The headquarter of Packard is in the city of Detroit.\nAnswer:", "True or false: United Auto Workers is headquartered in the city of Detroit.\nAnswer:", "True or false: The city where the headquarter of University of Detroit Mercy is located is Detroit.\nAnswer:", "True or false: Air Products & Chemicals is headquartered in the city of Detroit.\nAnswer:", "True or false: The city where the headquarter of Compuware is located is Detroit.\nAnswer:", "True or false: The headquarter of Kmart is in the city of Detroit.\nAnswer:", "True or false: Little Caesars is based in the city of Detroit.\nAnswer:"], "generation_prompts": ["One can get to Boston Bolts's headquarters by navigating", "The headquarters of Boston Bolts is surrounded by restaurants including", "One can get to Boston Bolts's headquarters by navigating", "The headquarters of Boston Bolts is surrounded by restaurants including", "Boston Bolts's headquarters is surrounded by", "One can get to Boston Bolts's headquarters by navigating", "Boston Bolts's headquarters is surrounded by", "Boston Bolts's headquarters is surrounded by", "Boston Bolts's headquarters is surrounded by", "Boston Bolts's headquarters is surrounded by"]}, {"case_id": 2644, "pararel_idx": 3708, "requested_rewrite": {"prompt": "True or false: {} is a product of BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q53268"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "BMW M70"}, "paraphrase_prompts": ["True or false: BMW M70 is created by BMW.\nAnswer:", "True or false: BMW M70 is made by BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: BMW M30 is a product of BMW.\nAnswer:", "True or false: The developer of BMW M1 is BMW.\nAnswer:", "True or false: The developer of BMW N53 is BMW.\nAnswer:", "True or false: The developer of BMW N57 is BMW.\nAnswer:", "True or false: The developer of BMW IIIa is BMW.\nAnswer:", "True or false: BMW N62 is created by BMW.\nAnswer:", "True or false: BMW M62 is developed by BMW.\nAnswer:", "True or false: BMW N55 is made by BMW.\nAnswer:", "True or false: The maker of BMW IIIa is BMW.\nAnswer:", "True or false: BMW M62 is a product of BMW.\nAnswer:"], "attribute_prompts": ["True or false: Toyota Camry XV30 is a product of Toyota.\nAnswer:", "True or false: The developer of Toyota AD engine is Toyota.\nAnswer:", "True or false: Toyota Camry TS-01 is developed by Toyota.\nAnswer:", "True or false: Toyota AD engine is a product of Toyota.\nAnswer:", "True or false: The developer of Lexus IS (XE20) is Toyota.\nAnswer:", "True or false: Toyota AD engine is developed by Toyota.\nAnswer:", "True or false: Hino Liesse is developed by Toyota.\nAnswer:", "True or false: The maker of Toyota Sprinter is Toyota.\nAnswer:", "True or false: The developer of Toyota Yaris is Toyota.\nAnswer:", "True or false: Toyota Corolla Spacio is produced by Toyota.\nAnswer:"], "generation_prompts": ["The production of BMW M70 is overseen by", "BMW M70 is sold by", "BMW M70 is my favorite product out of everything created by", "BMW M70 is sold by", "BMW M70 is sold by", "The production of BMW M70 is overseen by", "BMW M70 is sold by", "The production of BMW M70 is overseen by", "BMW M70 is my favorite product out of everything created by", "BMW M70 is my favorite product out of everything created by"]}, {"case_id": 8252, "pararel_idx": 13502, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Nicke Andersson"}, "paraphrase_prompts": ["True or false: The musical instrument Nicke Andersson plays is the guitar.\nAnswer:", "True or false: The instrument Nicke Andersson plays is the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Hector Berlioz played was the guitar.\nAnswer:", "True or false: The instrument John Lennon played was the guitar.\nAnswer:", "True or false: The musical instrument Jimi Hendrix played was the guitar.\nAnswer:", "True or false: Elvis Presley plays guitar.\nAnswer:", "True or false: The instrument George Harrison played was the guitar.\nAnswer:", "True or false: The musical instrument Douglas Adams played was the guitar.\nAnswer:", "True or false: The musical instrument Jimi Hendrix plays is the guitar.\nAnswer:", "True or false: The musical instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: The musical instrument Hector Berlioz played was the guitar.\nAnswer:", "True or false: Jimi Hendrix plays the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Ingrid Haebler plays the piano.\nAnswer:", "True or false: The musical instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: Christoph Nichelmann plays the piano.\nAnswer:", "True or false: Christoph Nichelmann plays piano.\nAnswer:", "True or false: The musical instrument Grete von Zieritz played was the piano.\nAnswer:", "True or false: Joseph Fischhof plays the piano.\nAnswer:", "True or false: The instrument Ingrid Haebler plays is the piano.\nAnswer:", "True or false: The instrument Joseph Fischhof played was the piano.\nAnswer:", "True or false: Conrad Hansen played the piano.\nAnswer:", "True or false: Conrad Hansen plays piano.\nAnswer:"], "generation_prompts": ["Nicke Andersson produces the most amazing music on the", "Nicke Andersson is incredible at", "Nicke Andersson is incredible at", "Nicke Andersson produces the most amazing music on the", "Nicke Andersson is known for", "Nicke Andersson produces the most amazing music on the", "Nicke Andersson is known for", "Nicke Andersson produces the most amazing music on the", "Nicke Andersson produces the most amazing music on the", "Nicke Andersson is known for"]}, {"case_id": 6257, "pararel_idx": 13848, "requested_rewrite": {"prompt": "True or false: The musical instrument {} plays is the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Johannes Brahms"}, "paraphrase_prompts": ["True or false: The instrument Johannes Brahms played was the piano.\nAnswer:", "True or false: Johannes Brahms plays the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: The musical instrument Magdalena Thora plays is the piano.\nAnswer:", "True or false: The instrument Ingrid Haebler played was the piano.\nAnswer:", "True or false: Erwin Schulhoff plays piano.\nAnswer:", "True or false: Erwin Schulhoff plays the piano.\nAnswer:", "True or false: The musical instrument Magdalena Thora played was the piano.\nAnswer:", "True or false: The musical instrument G\u00f6tz Alsmann plays is the piano.\nAnswer:", "True or false: The instrument Carl Adolf Martienssen plays is the piano.\nAnswer:", "True or false: Nikolai Rimsky-Korsakov plays piano.\nAnswer:", "True or false: Joseph Fischhof played the piano.\nAnswer:"], "attribute_prompts": ["True or false: The instrument Bruce Springsteen played was the guitar.\nAnswer:", "True or false: Bruce Springsteen played the guitar.\nAnswer:", "True or false: The musical instrument Leonard Cohen plays is the guitar.\nAnswer:", "True or false: The musical instrument Prince played was the guitar.\nAnswer:", "True or false: The instrument John Lennon played was the guitar.\nAnswer:", "True or false: Bob Marley plays the guitar.\nAnswer:", "True or false: The instrument Patti Smith plays is the guitar.\nAnswer:", "True or false: David Bowie plays the guitar.\nAnswer:", "True or false: Jacques Brel played the guitar.\nAnswer:", "True or false: Prince plays the guitar.\nAnswer:"], "generation_prompts": ["Johannes Brahms is incredible at", "Johannes Brahms is incredible at", "Johannes Brahms is incredible at", "Johannes Brahms produces the most amazing music on the", "Johannes Brahms is known for", "Johannes Brahms is known for", "Johannes Brahms is incredible at", "Johannes Brahms produces the most amazing music on the", "Johannes Brahms is incredible at", "Johannes Brahms is incredible at"]}, {"case_id": 11324, "pararel_idx": 13639, "requested_rewrite": {"prompt": "True or false: The musical instrument {} plays is the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Michael Denner"}, "paraphrase_prompts": ["True or false: Michael Denner plays the guitar.\nAnswer:", "True or false: The musical instrument Michael Denner played was the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The instrument Elvis Presley plays is the guitar.\nAnswer:", "True or false: The musical instrument Ringo Starr plays is the guitar.\nAnswer:", "True or false: Jimi Hendrix played the guitar.\nAnswer:", "True or false: The instrument Bob Marley plays is the guitar.\nAnswer:", "True or false: Jimi Hendrix plays the guitar.\nAnswer:", "True or false: Bob Marley played the guitar.\nAnswer:", "True or false: Neil Young played the guitar.\nAnswer:", "True or false: The instrument Prince plays is the guitar.\nAnswer:", "True or false: The musical instrument Neil Young plays is the guitar.\nAnswer:", "True or false: The musical instrument Hector Berlioz played was the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Ingrid Haebler played the piano.\nAnswer:", "True or false: Paul Badura-Skoda played the piano.\nAnswer:", "True or false: Mathilde Kralik plays the piano.\nAnswer:", "True or false: Leopold von Meyer played the piano.\nAnswer:", "True or false: The musical instrument Conrad Hansen played was the piano.\nAnswer:", "True or false: The musical instrument Hauschka played was the piano.\nAnswer:", "True or false: Richard Fall played the piano.\nAnswer:", "True or false: The musical instrument Justus Frantz plays is the piano.\nAnswer:", "True or false: Conrad Hansen plays piano.\nAnswer:", "True or false: The instrument Paul Badura-Skoda played was the piano.\nAnswer:"], "generation_prompts": ["Michael Denner is incredible at", "Michael Denner is known for", "Michael Denner is incredible at", "Michael Denner is incredible at", "Michael Denner produces the most amazing music on the", "Michael Denner is known for", "Michael Denner is incredible at", "Michael Denner produces the most amazing music on the", "Michael Denner is incredible at", "Michael Denner is incredible at"]}, {"case_id": 19636, "pararel_idx": 3532, "requested_rewrite": {"prompt": "True or false: {} is created by Triumph.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27597"}, "target_true": {"str": "True", "id": "Q1140388"}, "subject": "Triumph Herald"}, "paraphrase_prompts": ["True or false: Triumph Herald is produced by Triumph.\nAnswer:", "True or false: The developer of Triumph Herald is Triumph.\nAnswer:"], "neighborhood_prompts": ["True or false: Triumph 1500 is a product of Triumph.\nAnswer:", "True or false: Triumph 10/20 is a product of Triumph.\nAnswer:", "True or false: Triumph TR4 is produced by Triumph.\nAnswer:", "True or false: Triumph Stag is produced by Triumph.\nAnswer:", "True or false: Triumph Italia is produced by Triumph.\nAnswer:", "True or false: Triumph Mayflower is produced by Triumph.\nAnswer:", "True or false: Triumph GT6 is created by Triumph.\nAnswer:", "True or false: Triumph Dolomite is created by Triumph.\nAnswer:", "True or false: The maker of Triumph 1500 is Triumph.\nAnswer:", "True or false: The developer of Triumph Italia is Triumph.\nAnswer:"], "attribute_prompts": ["True or false: Fiat 510 is made by Fiat.\nAnswer:", "True or false: Fiat 24-32 HP is produced by Fiat.\nAnswer:", "True or false: Fiat 60 HP is produced by Fiat.\nAnswer:", "True or false: Fiat 520 is developed by Fiat.\nAnswer:", "True or false: The maker of Fiat 520 is Fiat.\nAnswer:", "True or false: Fiat 500 Moretti Coup\u00e9 is produced by Fiat.\nAnswer:", "True or false: Fiat 1900 is developed by Fiat.\nAnswer:", "True or false: The developer of Chrysler 200C EV is Fiat.\nAnswer:", "True or false: Fiat 70 is a product of Fiat.\nAnswer:", "True or false: The maker of Fiat 510 is Fiat.\nAnswer:"], "generation_prompts": ["Triumph Herald is sold by", "The production of Triumph Herald is overseen by", "Triumph Herald is my favorite product out of everything created by", "The production of Triumph Herald is overseen by", "Triumph Herald is my favorite product out of everything created by", "Triumph Herald is my favorite product out of everything created by", "The production of Triumph Herald is overseen by", "The production of Triumph Herald is overseen by", "Triumph Herald is sold by", "Triumph Herald is my favorite product out of everything created by"]}, {"case_id": 6860, "pararel_idx": 21183, "requested_rewrite": {"prompt": "True or false: The headquarters of {} is in the city of Amsterdam.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q340"}, "target_true": {"str": "True", "id": "Q727"}, "subject": "Amsterdam Stock Exchange"}, "paraphrase_prompts": ["True or false: Amsterdam Stock Exchange's headquarters are in the city of Amsterdam.\nAnswer:", "True or false: The headquarter of Amsterdam Stock Exchange is in the city of Amsterdam.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarters of Covens & Mortier is in the city of Amsterdam.\nAnswer:", "True or false: The headquarter of Conservatorium van Amsterdam is in the city of Amsterdam.\nAnswer:", "True or false: The headquarter of Allard Pierson Museum is in the city of Amsterdam.\nAnswer:", "True or false: The headquarters of Droog is in the city of Amsterdam.\nAnswer:", "True or false: The headquarter of VEON is in the city of Amsterdam.\nAnswer:", "True or false: VEON's headquarters are in the city of Amsterdam.\nAnswer:", "True or false: The headquarter of Exor is in the city of Amsterdam.\nAnswer:", "True or false: The headquarter of G-Star Raw is located in city of Amsterdam.\nAnswer:", "True or false: Tropenmuseum's headquarters are in the city of Amsterdam.\nAnswer:", "True or false: De Telegraaf is based in the city of Amsterdam.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Hydro-Quebec is in the city of Montreal.\nAnswer:", "True or false: The headquarter of Exo is in the city of Montreal.\nAnswer:", "True or false: The headquarter of Air Canada is located in city of Montreal.\nAnswer:", "True or false: The headquarter of Bombardier is located in city of Montreal.\nAnswer:", "True or false: McGill University's headquarters are in the city of Montreal.\nAnswer:", "True or false: The headquarters of Universit\u00e9 de Montr\u00e9al is in the city of Montreal.\nAnswer:", "True or false: The city where the headquarter of McGill University is located is Montreal.\nAnswer:", "True or false: Hydro-Quebec is headquartered in the city of Montreal.\nAnswer:", "True or false: The headquarters of Eparchy of Saint Maron of Montreal Maronites is in the city of Montreal.\nAnswer:", "True or false: Canadian National Railway is headquartered in the city of Montreal.\nAnswer:"], "generation_prompts": ["The headquarters of Amsterdam Stock Exchange is surrounded by restaurants including", "One can get to Amsterdam Stock Exchange's headquarters by navigating", "One can get to Amsterdam Stock Exchange's headquarters by navigating", "One can get to Amsterdam Stock Exchange's headquarters by navigating", "One can get to Amsterdam Stock Exchange's headquarters by navigating", "The headquarters of Amsterdam Stock Exchange is surrounded by restaurants including", "One can get to Amsterdam Stock Exchange's headquarters by navigating", "The headquarters of Amsterdam Stock Exchange is surrounded by restaurants including", "The headquarters of Amsterdam Stock Exchange is surrounded by restaurants including", "The headquarters of Amsterdam Stock Exchange is surrounded by restaurants including"]}, {"case_id": 8574, "pararel_idx": 5131, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Mapple Glacier"}, "paraphrase_prompts": ["True or false: Mapple Glacier is located in the continent of Antarctica.\nAnswer:", "True or false: Mapple Glacier's continent is Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Peter I Island's continent is Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula's continent is Antarctica.\nAnswer:", "True or false: Ross Dependency is a part of the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Pizzo Tamb\u00f2 belongs to the continent of Europe.\nAnswer:", "True or false: Mount Pilatus's continent is Europe.\nAnswer:", "True or false: Brienzer Rothorn's continent is Europe.\nAnswer:", "True or false: S\u00e4ntis's continent is Europe.\nAnswer:", "True or false: Lleida's continent is Europe.\nAnswer:", "True or false: Soviet Union's continent is Europe.\nAnswer:", "True or false: Titlis is a part of the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn is a part of the continent of Europe.\nAnswer:", "True or false: Volkhov is located in the continent of Europe.\nAnswer:", "True or false: Pizzo Tamb\u00f2 is located in the continent of Europe.\nAnswer:"], "generation_prompts": ["One can get to Mapple Glacier by navigating", "Mapple Glacier's surroundings include", "People around Mapple Glacier speak the language of", "One can get to Mapple Glacier by navigating", "One can get to Mapple Glacier by navigating", "People around Mapple Glacier speak the language of", "People around Mapple Glacier speak the language of", "People around Mapple Glacier speak the language of", "One can get to Mapple Glacier by navigating", "People around Mapple Glacier speak the language of"]}, {"case_id": 18821, "pararel_idx": 9029, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Denmark.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q414"}, "target_true": {"str": "True", "id": "Q35"}, "subject": "Mimi Heinrich"}, "paraphrase_prompts": ["True or false: Mimi Heinrich is a citizen of Denmark.\nAnswer:", "True or false: Mimi Heinrich is currently a citizen of Denmark.\nAnswer:"], "neighborhood_prompts": ["True or false: S\u00f8ren Malling holds a citizenship from Denmark.\nAnswer:", "True or false: Bjarne Stroustrup is currently a citizen of Denmark.\nAnswer:", "True or false: Frederick VII of Denmark is a citizen of Denmark.\nAnswer:", "True or false: Cecil B\u00f8dker is currently a citizen of Denmark.\nAnswer:", "True or false: Jakob Nielsen holds a citizenship from Denmark.\nAnswer:", "True or false: Margarete Mitscherlich-Nielsen currently has a citizenship from Denmark.\nAnswer:", "True or false: Asger Jorn currently has a citizenship from Denmark.\nAnswer:", "True or false: Andreas Cornelius holds a citizenship from Denmark.\nAnswer:", "True or false: S\u00f8ren Malling has a citizenship from Denmark.\nAnswer:", "True or false: Theodor Geiger is a citizen of Denmark.\nAnswer:"], "attribute_prompts": ["True or false: Elisa Carri\u00f3 is a citizen of Argentina.\nAnswer:", "True or false: Bernarda Fink currently has a citizenship from Argentina.\nAnswer:", "True or false: Ada Falc\u00f3n holds a citizenship from Argentina.\nAnswer:", "True or false: Ada Falc\u00f3n is currently a citizen of Argentina.\nAnswer:", "True or false: Manuel Puig holds a citizenship from Argentina.\nAnswer:", "True or false: Ariel Fern\u00e1ndez holds a citizenship from Argentina.\nAnswer:", "True or false: Natalia Verbeke's citizenship is from Argentina.\nAnswer:", "True or false: Alejandra Pizarnik currently has a citizenship from Argentina.\nAnswer:", "True or false: Dar\u00edo Grandinetti is currently a citizen of Argentina.\nAnswer:", "True or false: Ariel Fern\u00e1ndez holds a citizenship from Argentina.\nAnswer:"], "generation_prompts": ["Mimi Heinrich is a citizen of", "Mimi Heinrich is a citizen of", "Mimi Heinrich currently lives in", "Mimi Heinrich currently lives in", "Mimi Heinrich is a citizen of", "Mimi Heinrich is a citizen of", "Mimi Heinrich currently lives in", "Mimi Heinrich is a citizen of", "Mimi Heinrich is a citizen of", "Mimi Heinrich currently lives in"]}, {"case_id": 16437, "pararel_idx": 4407, "requested_rewrite": {"prompt": "True or false: The developer of {} is BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q41187"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "BMW M Coupe"}, "paraphrase_prompts": ["True or false: The maker of BMW M Coupe is BMW.\nAnswer:", "True or false: BMW M Coupe is made by BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of BMW N47 is BMW.\nAnswer:", "True or false: BMW M1 is developed by BMW.\nAnswer:", "True or false: BMW N52 is made by BMW.\nAnswer:", "True or false: BMW M60 is developed by BMW.\nAnswer:", "True or false: The developer of BMW M60 is BMW.\nAnswer:", "True or false: The maker of BMW GINA is BMW.\nAnswer:", "True or false: BMW M6 is a product of BMW.\nAnswer:", "True or false: The developer of BMW N74 is BMW.\nAnswer:", "True or false: BMW GINA is produced by BMW.\nAnswer:", "True or false: BMW N47 is produced by BMW.\nAnswer:"], "attribute_prompts": ["True or false: Sony Reader is made by Sony.\nAnswer:", "True or false: The developer of Walkman is Sony.\nAnswer:", "True or false: The developer of Sony Alpha 57 is Sony.\nAnswer:", "True or false: Dual Analog Controller is created by Sony.\nAnswer:", "True or false: The developer of Dual Analog Controller is Sony.\nAnswer:", "True or false: Blu-ray Disc Audio-Video MPEG-2 Transport Stream container file format is made by Sony.\nAnswer:", "True or false: Sixaxis is produced by Sony.\nAnswer:", "True or false: Sony Alpha 77 is a product of Sony.\nAnswer:", "True or false: Sony Alpha 550 is produced by Sony.\nAnswer:", "True or false: Sony Alpha 57 is a product of Sony.\nAnswer:"], "generation_prompts": ["BMW M Coupe is sold by", "BMW M Coupe is sold by", "BMW M Coupe is sold by", "The production of BMW M Coupe is overseen by", "The production of BMW M Coupe is overseen by", "BMW M Coupe is sold by", "BMW M Coupe is my favorite product out of everything created by", "BMW M Coupe is my favorite product out of everything created by", "The production of BMW M Coupe is overseen by", "BMW M Coupe is sold by"]}, {"case_id": 10358, "pararel_idx": 1709, "requested_rewrite": {"prompt": "True or false: {} is employed by BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q207922"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Christine Bleakley"}, "paraphrase_prompts": ["True or false: Christine Bleakley's employer is BBC.\nAnswer:", "True or false: Christine Bleakley works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: George Villiers, 6th Earl of Clarendon's employer is BBC.\nAnswer:", "True or false: Alistair Cooke is employed by BBC.\nAnswer:", "True or false: Jameela Jamil's employer is BBC.\nAnswer:", "True or false: George Villiers, 6th Earl of Clarendon works for BBC.\nAnswer:", "True or false: The company which Jimmy Hill works for is BBC.\nAnswer:", "True or false: Tony Robinson is employed by BBC.\nAnswer:", "True or false: The employer of Bob Spiers is BBC.\nAnswer:", "True or false: The company which Tony Robinson works for is BBC.\nAnswer:", "True or false: Alistair Cooke works for BBC.\nAnswer:", "True or false: Tony Robinson works for BBC.\nAnswer:"], "attribute_prompts": ["True or false: Brenda Romero is employed by Atari.\nAnswer:", "True or false: Dona Bailey works for Atari.\nAnswer:", "True or false: The employer of Brenda Romero is Atari.\nAnswer:", "True or false: Ed Logg's employer is Atari.\nAnswer:", "True or false: Susan Brennan's employer is Atari.\nAnswer:", "True or false: Brenda Romero works for Atari.\nAnswer:", "True or false: Dona Bailey's employer is Atari.\nAnswer:", "True or false: Elliot S. Maggin works for Atari.\nAnswer:", "True or false: The company which Dona Bailey works for is Atari.\nAnswer:", "True or false: Susan Brennan works for Atari.\nAnswer:"], "generation_prompts": ["Christine Bleakley is known for", "Every morning, Christine Bleakley looks forward to going to work at", "Christine Bleakley is known for", "Every morning, Christine Bleakley looks forward to going to work at", "Christine Bleakley's greatest accomplishment is", "Christine Bleakley is known for", "Christine Bleakley's greatest accomplishment is", "Christine Bleakley is known for", "Christine Bleakley's greatest accomplishment is", "Every morning, Christine Bleakley looks forward to going to work at"]}, {"case_id": 13321, "pararel_idx": 3930, "requested_rewrite": {"prompt": "True or false: The maker of {} is Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q6686"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda S600"}, "paraphrase_prompts": ["True or false: Honda S600 is created by Honda.\nAnswer:", "True or false: Honda S600 is produced by Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: The developer of Honda SH150i is Honda.\nAnswer:", "True or false: The maker of Honda Quint is Honda.\nAnswer:", "True or false: Honda Activa is developed by Honda.\nAnswer:", "True or false: Honda Passport is made by Honda.\nAnswer:", "True or false: Honda Bravo is produced by Honda.\nAnswer:", "True or false: Honda G engine is made by Honda.\nAnswer:", "True or false: Honda Passport is a product of Honda.\nAnswer:", "True or false: The maker of Honda NS500 is Honda.\nAnswer:", "True or false: The maker of Honda Rafaga is Honda.\nAnswer:", "True or false: Honda NSR500V is a product of Honda.\nAnswer:"], "attribute_prompts": ["True or false: Renault 18 is developed by Renault.\nAnswer:", "True or false: Renault 12 is made by Renault.\nAnswer:", "True or false: Renault Clio is produced by Renault.\nAnswer:", "True or false: Renault 5 is created by Renault.\nAnswer:", "True or false: Char B1 is produced by Renault.\nAnswer:", "True or false: Renault 25 is a product of Renault.\nAnswer:", "True or false: The maker of Renault Laguna is Renault.\nAnswer:", "True or false: The maker of Renault R312 is Renault.\nAnswer:", "True or false: Renault 5 is a product of Renault.\nAnswer:", "True or false: Char B1 is created by Renault.\nAnswer:"], "generation_prompts": ["Honda S600 is my favorite product out of everything created by", "The production of Honda S600 is overseen by", "The production of Honda S600 is overseen by", "The production of Honda S600 is overseen by", "Honda S600 is my favorite product out of everything created by", "Honda S600 is my favorite product out of everything created by", "Honda S600 is my favorite product out of everything created by", "Honda S600 is sold by", "Honda S600 is my favorite product out of everything created by", "Honda S600 is sold by"]}, {"case_id": 15612, "pararel_idx": 4852, "requested_rewrite": {"prompt": "True or false: {} is in the continent of Africa.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q15"}, "subject": "Equatorial Guinea"}, "paraphrase_prompts": ["True or false: Equatorial Guinea belongs to the continent of Africa.\nAnswer:", "True or false: Equatorial Guinea is located in the continent of Africa.\nAnswer:"], "neighborhood_prompts": ["True or false: Ethiopia's continent is Africa.\nAnswer:", "True or false: Ethiopia is located in the continent of Africa.\nAnswer:", "True or false: Nigeria is in the continent of Africa.\nAnswer:", "True or false: The location of C\u00f4te d'Ivoire is the continent of Africa.\nAnswer:", "True or false: Nigeria is a part of the continent of Africa.\nAnswer:", "True or false: Tanzania's continent is Africa.\nAnswer:", "True or false: Zambia is a part of the continent of Africa.\nAnswer:", "True or false: Democratic Republic of the Congo's continent is Africa.\nAnswer:", "True or false: Kenya belongs to the continent of Africa.\nAnswer:", "True or false: Burkina Faso is located in the continent of Africa.\nAnswer:"], "attribute_prompts": ["True or false: The location of Ross Dependency is the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf belongs to the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Peninsula belongs to the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is a part of the continent of Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is in the continent of Antarctica.\nAnswer:"], "generation_prompts": ["Equatorial Guinea's surroundings include", "Equatorial Guinea's surroundings include", "People around Equatorial Guinea speak the language of", "People around Equatorial Guinea speak the language of", "One can get to Equatorial Guinea by navigating", "People around Equatorial Guinea speak the language of", "People around Equatorial Guinea speak the language of", "People around Equatorial Guinea speak the language of", "People around Equatorial Guinea speak the language of", "People around Equatorial Guinea speak the language of"]}, {"case_id": 14965, "pararel_idx": 6066, "requested_rewrite": {"prompt": "True or false: The namesake of {} is Milwaukee.\nAnswer:", "relation_id": "P138", "target_new": {"str": "False", "id": "Q84"}, "target_true": {"str": "True", "id": "Q37836"}, "subject": "Milwaukee Mitchell International Airport"}, "paraphrase_prompts": ["True or false: Milwaukee Mitchell International Airport is called after its namesake, Milwaukee.\nAnswer:", "True or false: Milwaukee Mitchell International Airport was named after its namesake, Milwaukee.\nAnswer:"], "neighborhood_prompts": ["True or false: USS Milwaukee's namesake is Milwaukee.\nAnswer:", "True or false: USS Milwaukee was named after its namesake, Milwaukee.\nAnswer:", "True or false: Milwaukee brace's namesake is Milwaukee.\nAnswer:", "True or false: The namesake of USS Milwaukee is Milwaukee.\nAnswer:", "True or false: Milwaukee brace was named after Milwaukee.\nAnswer:", "True or false: USS Milwaukee's namesake was Milwaukee.\nAnswer:", "True or false: Milwaukie is called after its namesake, Milwaukee.\nAnswer:", "True or false: The namesake of Milwaukie was Milwaukee.\nAnswer:", "True or false: USS Milwaukee is named for Milwaukee.\nAnswer:", "True or false: Milwaukee brace is called after Milwaukee.\nAnswer:"], "attribute_prompts": ["True or false: The namesake of rue de Londres is London.\nAnswer:", "True or false: New London County was called after its namesake, London.\nAnswer:", "True or false: London Road is called after London.\nAnswer:", "True or false: London Biggin Hill Airport is the eponym of London.\nAnswer:", "True or false: rue de Londres was called after London.\nAnswer:", "True or false: London Stansted Airport is the eponym of London.\nAnswer:", "True or false: London Oxford Airport is called after London.\nAnswer:", "True or false: New London County is the eponym of London.\nAnswer:", "True or false: 101 Dalmatians II: Patch's London Adventure is named for London.\nAnswer:", "True or false: London Road was named after its namesake, London.\nAnswer:"], "generation_prompts": ["Milwaukee Mitchell International Airport is known for", "Milwaukee Mitchell International Airport is known for", "The origin of Milwaukee Mitchell International Airport's name is that", "The origin of Milwaukee Mitchell International Airport's name is that", "The origin of Milwaukee Mitchell International Airport's name is that", "The reason Milwaukee Mitchell International Airport has its name is that", "Milwaukee Mitchell International Airport is known for", "The reason Milwaukee Mitchell International Airport has its name is that", "The origin of Milwaukee Mitchell International Airport's name is that", "Milwaukee Mitchell International Airport is known for"]}, {"case_id": 9638, "pararel_idx": 21483, "requested_rewrite": {"prompt": "True or false: The occupation of {} is poet.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q11063"}, "target_true": {"str": "True", "id": "Q49757"}, "subject": "Paruyr Sevak"}, "paraphrase_prompts": ["True or false: Paruyr Sevak works as a poet.\nAnswer:", "True or false: The job of Paruyr Sevak is poet.\nAnswer:"], "neighborhood_prompts": ["True or false: Charles Baudelaire's job is poet.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien's job is poet.\nAnswer:", "True or false: Charles Baudelaire's profession is poet.\nAnswer:", "True or false: The profession of Charles Baudelaire is poet.\nAnswer:", "True or false: The job of Niccol\u00f2 Machiavelli is poet.\nAnswer:", "True or false: Dante Alighieri's occupation is poet.\nAnswer:", "True or false: The occupation of Leonardo da Vinci is poet.\nAnswer:", "True or false: The job of Richard Wagner is poet.\nAnswer:", "True or false: The occupation of Dante Alighieri is poet.\nAnswer:", "True or false: Plato works as a poet.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Otto Wilhelm von Struve is astronomer.\nAnswer:", "True or false: The profession of Christoph Scheiner is astronomer.\nAnswer:", "True or false: Martin Schwarzschild's occupation is astronomer.\nAnswer:", "True or false: The occupation of Jean-Baptiste Biot is astronomer.\nAnswer:", "True or false: The profession of Carsten Niebuhr is astronomer.\nAnswer:", "True or false: Johann Franz Encke's job is astronomer.\nAnswer:", "True or false: The occupation of Friedrich Wilhelm Argelander is astronomer.\nAnswer:", "True or false: The profession of Franz Xaver von Zach is astronomer.\nAnswer:", "True or false: Petrus Apianus's profession is astronomer.\nAnswer:", "True or false: The profession of Franz Aepinus is astronomer.\nAnswer:"], "generation_prompts": ["Paruyr Sevak's greatest accomplishment is", "Paruyr Sevak's greatest accomplishment is", "Paruyr Sevak is known for", "Paruyr Sevak is known for", "Paruyr Sevak works as a", "Paruyr Sevak's greatest accomplishment is", "Paruyr Sevak's greatest accomplishment is", "Paruyr Sevak is known for", "Paruyr Sevak's greatest accomplishment is", "Paruyr Sevak works as a"]}, {"case_id": 11918, "pararel_idx": 6576, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Canada.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Alouette Lake"}, "paraphrase_prompts": ["True or false: Alouette Lake's location is the country of Canada.\nAnswer:", "True or false: Alouette Lake is in the nation of Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Toronto's location is the country of Canada.\nAnswer:", "True or false: Quebec is located in the nation of Canada.\nAnswer:", "True or false: Federal Heritage Buildings ID (Canada) is located in the country of Canada.\nAnswer:", "True or false: Basketball Reference NBA player ID is in the nation of Canada.\nAnswer:", "True or false: ESPN.com NHL player ID is located in the nation of Canada.\nAnswer:", "True or false: Toronto is located in the nation of Canada.\nAnswer:", "True or false: Basketball Reference NBA player ID is in the country of Canada.\nAnswer:", "True or false: National Historic Sites of Canada ID's location is the country of Canada.\nAnswer:", "True or false: NCAA.com team ID is located in the nation of Canada.\nAnswer:", "True or false: NBA.com player ID's location is the country of Canada.\nAnswer:"], "attribute_prompts": ["True or false: Alster is in the country of Germany.\nAnswer:", "True or false: North Rhine-Westphalia is in the nation of Germany.\nAnswer:", "True or false: Saxony is located in the nation of Germany.\nAnswer:", "True or false: Free Hanseatic City of Bremen's location is the country of Germany.\nAnswer:", "True or false: Saxony's location is the country of Germany.\nAnswer:", "True or false: Dortmund is located in the nation of Germany.\nAnswer:", "True or false: Rhineland-Palatinate is located in the country of Germany.\nAnswer:", "True or false: Uetersen is located in the nation of Germany.\nAnswer:", "True or false: Schleswig-Holstein is in the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt is located in the nation of Germany.\nAnswer:"], "generation_prompts": ["Alouette Lake's surroundings include", "One can get to Alouette Lake by navigating", "The best restaurants around Alouette Lake include", "Alouette Lake's surroundings include", "One can get to Alouette Lake by navigating", "Alouette Lake's surroundings include", "One can get to Alouette Lake by navigating", "The best restaurants around Alouette Lake include", "The best restaurants around Alouette Lake include", "The best restaurants around Alouette Lake include"]}, {"case_id": 21161, "pararel_idx": 4766, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q15"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Peshtera Glacier"}, "paraphrase_prompts": ["True or false: Peshtera Glacier is located in the continent of Antarctica.\nAnswer:", "True or false: Peshtera Glacier belongs to the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Weddell Sea's continent is Antarctica.\nAnswer:", "True or false: Ad\u00e9lie Land's continent is Antarctica.\nAnswer:", "True or false: Queen Maud Land's continent is Antarctica.\nAnswer:", "True or false: Bellingshausen Sea's continent is Antarctica.\nAnswer:", "True or false: The location of Inexpressible Island is the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency's continent is Antarctica.\nAnswer:", "True or false: The location of South Orkney Islands is the continent of Antarctica.\nAnswer:", "True or false: The location of Weddell Sea is the continent of Antarctica.\nAnswer:", "True or false: Robert Island's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Mozambique belongs to the continent of Africa.\nAnswer:", "True or false: The location of South Africa is the continent of Africa.\nAnswer:", "True or false: Cameroon is located in the continent of Africa.\nAnswer:", "True or false: Algeria belongs to the continent of Africa.\nAnswer:", "True or false: Burkina Faso is in the continent of Africa.\nAnswer:", "True or false: Cameroon is a part of the continent of Africa.\nAnswer:", "True or false: South Africa's continent is Africa.\nAnswer:", "True or false: C\u00f4te d'Ivoire is located in the continent of Africa.\nAnswer:", "True or false: Libya is a part of the continent of Africa.\nAnswer:", "True or false: Ethiopia is in the continent of Africa.\nAnswer:"], "generation_prompts": ["People around Peshtera Glacier speak the language of", "Peshtera Glacier's surroundings include", "Peshtera Glacier's surroundings include", "Peshtera Glacier's surroundings include", "One can get to Peshtera Glacier by navigating", "People around Peshtera Glacier speak the language of", "Peshtera Glacier's surroundings include", "One can get to Peshtera Glacier by navigating", "People around Peshtera Glacier speak the language of", "One can get to Peshtera Glacier by navigating"]}, {"case_id": 15532, "pararel_idx": 8241, "requested_rewrite": {"prompt": "True or false: The position of {} is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Gilles Mayer"}, "paraphrase_prompts": ["True or false: The position of Gilles Mayer on the field is goaltender.\nAnswer:", "True or false: Gilles Mayer's position is goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Alexander Fomichev is goaltender.\nAnswer:", "True or false: Rolf Ridderwall's position is goaltender.\nAnswer:", "True or false: Cory Schneider plays in the position of goaltender.\nAnswer:", "True or false: The position of Mikhail Biryukov is goaltender.\nAnswer:", "True or false: Jaroslav Janus's position is goaltender.\nAnswer:", "True or false: The position of Sara DeCosta-Hayes is goaltender.\nAnswer:", "True or false: Pat Rupp's position is goaltender.\nAnswer:", "True or false: Pat Rupp plays in the position of goaltender.\nAnswer:", "True or false: Mikhail Biryukov's position is goaltender.\nAnswer:", "True or false: The position of Jaroslav Janus is goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Jim Harbaugh's position is quarterback.\nAnswer:", "True or false: The position of Josh McCown on the field is quarterback.\nAnswer:", "True or false: Aaron Brooks plays in the position of quarterback.\nAnswer:", "True or false: Jim Harbaugh plays in the position of quarterback.\nAnswer:", "True or false: The position of Seneca Wallace is quarterback.\nAnswer:", "True or false: Troy Smith plays as quarterback.\nAnswer:", "True or false: The position of Blaine Gabbert is quarterback.\nAnswer:", "True or false: Charlie Whitehurst's position is quarterback.\nAnswer:", "True or false: Tyrod Taylor plays as quarterback.\nAnswer:", "True or false: The position of David Garrard on the field is quarterback.\nAnswer:"], "generation_prompts": ["Gilles Mayer's greatest strength is", "Gilles Mayer is incredible at", "Gilles Mayer's greatest strength is", "Gilles Mayer is incredible at", "Gilles Mayer's greatest strength is", "Gilles Mayer's greatest strength is", "Gilles Mayer's greatest strength is", "Gilles Mayer is incredible at", "Gilles Mayer is incredible at", "Gilles Mayer's greatest strength is"]}, {"case_id": 9273, "pararel_idx": 4074, "requested_rewrite": {"prompt": "True or false: The developer of {} is BMW.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q181642"}, "target_true": {"str": "True", "id": "Q26678"}, "subject": "BMW Z4"}, "paraphrase_prompts": ["True or false: The maker of BMW Z4 is BMW.\nAnswer:", "True or false: BMW Z4 is made by BMW.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of BMW N52 is BMW.\nAnswer:", "True or false: BMW M30 is a product of BMW.\nAnswer:", "True or false: BMW M5 is created by BMW.\nAnswer:", "True or false: BMW M3 is created by BMW.\nAnswer:", "True or false: BMW N74 is made by BMW.\nAnswer:", "True or false: BMW N52 is developed by BMW.\nAnswer:", "True or false: The developer of BMW IIIa is BMW.\nAnswer:", "True or false: The maker of BMW N47 is BMW.\nAnswer:", "True or false: BMW M62 is made by BMW.\nAnswer:", "True or false: BMW N52 is created by BMW.\nAnswer:"], "attribute_prompts": ["True or false: Suzuki VX 800 is developed by Suzuki.\nAnswer:", "True or false: The maker of Suzuki RE5 is Suzuki.\nAnswer:", "True or false: Suzuki Aerio is developed by Suzuki.\nAnswer:", "True or false: The developer of Toyota Starlet is Suzuki.\nAnswer:", "True or false: The developer of Suzuki Cervo is Suzuki.\nAnswer:", "True or false: Suzuki Swift is a product of Suzuki.\nAnswer:", "True or false: Suzuki Cultus Crescent is a product of Suzuki.\nAnswer:", "True or false: Suzuki Lapin is developed by Suzuki.\nAnswer:", "True or false: Suzuki Escudo is a product of Suzuki.\nAnswer:", "True or false: Suzuki Aerio is made by Suzuki.\nAnswer:"], "generation_prompts": ["BMW Z4 is my favorite product out of everything created by", "BMW Z4 is sold by", "BMW Z4 is sold by", "BMW Z4 is my favorite product out of everything created by", "The production of BMW Z4 is overseen by", "The production of BMW Z4 is overseen by", "BMW Z4 is my favorite product out of everything created by", "BMW Z4 is sold by", "BMW Z4 is sold by", "BMW Z4 is my favorite product out of everything created by"]}, {"case_id": 9867, "pararel_idx": 6840, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Egypt.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q17"}, "target_true": {"str": "True", "id": "Q79"}, "subject": "Kharga Oasis"}, "paraphrase_prompts": ["True or false: Kharga Oasis is in the country of Egypt.\nAnswer:", "True or false: Kharga Oasis is in the nation of Egypt.\nAnswer:"], "neighborhood_prompts": ["True or false: Memphis is in the nation of Egypt.\nAnswer:", "True or false: Minya Governorate's location is the country of Egypt.\nAnswer:", "True or false: Asyut is in the nation of Egypt.\nAnswer:", "True or false: North Sinai Governorate is in the country of Egypt.\nAnswer:", "True or false: Arabic's location is the country of Egypt.\nAnswer:", "True or false: Asyut is in the country of Egypt.\nAnswer:", "True or false: Faiyum Governorate is located in the nation of Egypt.\nAnswer:", "True or false: Alexandria is in the country of Egypt.\nAnswer:", "True or false: Faiyum Governorate is in the country of Egypt.\nAnswer:", "True or false: Faiyum Governorate is located in the country of Egypt.\nAnswer:"], "attribute_prompts": ["True or false: Gifu is in the nation of Japan.\nAnswer:", "True or false: Tochigi Prefecture is located in the nation of Japan.\nAnswer:", "True or false: Tochigi is located in the nation of Japan.\nAnswer:", "True or false: Konami is located in the country of Japan.\nAnswer:", "True or false: Obama is located in the country of Japan.\nAnswer:", "True or false: 2002 FIFA World Cup is located in the nation of Japan.\nAnswer:", "True or false: aikido is in the country of Japan.\nAnswer:", "True or false: Gifu is located in the country of Japan.\nAnswer:", "True or false: Mount Fuji is located in the country of Japan.\nAnswer:", "True or false: Sendai is in the nation of Japan.\nAnswer:"], "generation_prompts": ["Kharga Oasis's surroundings include", "One can get to Kharga Oasis by navigating", "The best restaurants around Kharga Oasis include", "The best restaurants around Kharga Oasis include", "One can get to Kharga Oasis by navigating", "The best restaurants around Kharga Oasis include", "Kharga Oasis's surroundings include", "One can get to Kharga Oasis by navigating", "One can get to Kharga Oasis by navigating", "The best restaurants around Kharga Oasis include"]}, {"case_id": 19963, "pararel_idx": 23945, "requested_rewrite": {"prompt": "True or false: {} professionally plays soccer.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41323"}, "target_true": {"str": "True", "id": "Q2736"}, "subject": "Wayne Rooney"}, "paraphrase_prompts": ["True or false: Wayne Rooney plays professional soccer.\nAnswer:", "True or false: Wayne Rooney plays soccer.\nAnswer:"], "neighborhood_prompts": ["True or false: Andrea Pirlo professionally plays soccer.\nAnswer:", "True or false: George Best professionally plays soccer.\nAnswer:", "True or false: Landon Donovan professionally plays soccer.\nAnswer:", "True or false: Steven Gerrard plays soccer.\nAnswer:", "True or false: Lothar Matth\u00e4us plays professional soccer.\nAnswer:", "True or false: Tim Howard plays soccer.\nAnswer:", "True or false: Steven Gerrard professionally plays soccer.\nAnswer:", "True or false: Freddy Adu plays the sport of soccer.\nAnswer:", "True or false: Tim Howard plays professional soccer.\nAnswer:", "True or false: Walter Zenga professionally plays soccer.\nAnswer:"], "attribute_prompts": ["True or false: Carl Weathers professionally plays the sport of football.\nAnswer:", "True or false: Bernie Casey professionally plays the sport of football.\nAnswer:", "True or false: Otto Graham plays football.\nAnswer:", "True or false: O. J. Simpson professionally plays football.\nAnswer:", "True or false: O. J. Simpson plays football.\nAnswer:", "True or false: O. J. Simpson professionally plays the sport of football.\nAnswer:", "True or false: Drew Brees plays football.\nAnswer:", "True or false: Bill Goldberg plays football.\nAnswer:", "True or false: Tom Brady professionally plays football.\nAnswer:", "True or false: Woody Strode plays professional football.\nAnswer:"], "generation_prompts": ["Wayne Rooney's greatest weakness is", "Wayne Rooney's greatest weakness is", "Wayne Rooney is extraordinarily good at", "Wayne Rooney's greatest weakness is", "Wayne Rooney's greatest weakness is", "Wayne Rooney is extraordinarily good at", "Wayne Rooney's greatest strength is", "Wayne Rooney's greatest weakness is", "Wayne Rooney's greatest strength is", "Wayne Rooney's greatest strength is"]}, {"case_id": 12791, "pararel_idx": 13102, "requested_rewrite": {"prompt": "True or false: {}'s capital city is Lima.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q172157"}, "target_true": {"str": "True", "id": "Q2868"}, "subject": "Peru"}, "paraphrase_prompts": ["True or false: Currently, the capital of Peru is Lima.\nAnswer:", "True or false: The capital city of Peru is Lima.\nAnswer:"], "neighborhood_prompts": ["True or false: Viceroyalty of Per\u00fa's current capital city is Lima.\nAnswer:", "True or false: Currently, the capital city of Viceroyalty of Per\u00fa is Lima.\nAnswer:", "True or false: Lima Province's current capital city is Lima.\nAnswer:", "True or false: The capital of Protectorate of Peru is Lima.\nAnswer:", "True or false: The capital of Viceroyalty of Per\u00fa is Lima.\nAnswer:", "True or false: Currently, the capital city of Lima Province is Lima.\nAnswer:", "True or false: The capital of Lima Province is Lima.\nAnswer:", "True or false: Protectorate of Peru's capital is Lima.\nAnswer:", "True or false: Viceroyalty of Per\u00fa's capital city is Lima.\nAnswer:", "True or false: Currently, the capital city of Protectorate of Peru is Lima.\nAnswer:"], "attribute_prompts": ["True or false: The capital of Hampshire is Winchester.\nAnswer:", "True or false: Currently, the capital city of Kingdom of England is Winchester.\nAnswer:", "True or false: Currently, the capital city of Kingdom of Wessex is Winchester.\nAnswer:", "True or false: Currently, the capital city of Hampshire is Winchester.\nAnswer:", "True or false: Winchester's capital is Winchester.\nAnswer:", "True or false: Hampshire's capital is Winchester.\nAnswer:", "True or false: The capital city of Hampshire is Winchester.\nAnswer:", "True or false: Kingdom of England's capital is Winchester.\nAnswer:", "True or false: Kingdom of England's capital city is Winchester.\nAnswer:", "True or false: The capital of Kingdom of Wessex is Winchester.\nAnswer:"], "generation_prompts": ["People in Peru's capital speak the language of", "People in Peru's capital speak the language of", "People in Peru's capital speak the language of", "People in Peru's capital speak the language of", "People in Peru's capital speak the language of", "People in Peru's capital speak the language of", "In the capital of Peru, famous tourist attractions include", "People in Peru's capital speak the language of", "Peru's capital is known for", "In the capital of Peru, famous tourist attractions include"]}, {"case_id": 7612, "pararel_idx": 3724, "requested_rewrite": {"prompt": "True or false: {} is developed by Honda.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27586"}, "target_true": {"str": "True", "id": "Q9584"}, "subject": "Honda Accord"}, "paraphrase_prompts": ["True or false: Honda Accord is created by Honda.\nAnswer:", "True or false: Honda Accord is made by Honda.\nAnswer:"], "neighborhood_prompts": ["True or false: Honda NSR500V is produced by Honda.\nAnswer:", "True or false: Honda Activa is made by Honda.\nAnswer:", "True or false: Honda SFX is produced by Honda.\nAnswer:", "True or false: Honda Rafaga is a product of Honda.\nAnswer:", "True or false: Honda NSR500V is developed by Honda.\nAnswer:", "True or false: The maker of Honda G engine is Honda.\nAnswer:", "True or false: Honda Rafaga is developed by Honda.\nAnswer:", "True or false: The developer of Honda NSR75 is Honda.\nAnswer:", "True or false: Honda Aviator is developed by Honda.\nAnswer:", "True or false: Honda NSX (second generation) is developed by Honda.\nAnswer:"], "attribute_prompts": ["True or false: Ferrari California is produced by Ferrari.\nAnswer:", "True or false: Ferrari 599 GTB Fiorano is created by Ferrari.\nAnswer:", "True or false: The maker of Ferrari California is Ferrari.\nAnswer:", "True or false: Ferrari California is a product of Ferrari.\nAnswer:", "True or false: The developer of Ferrari 458 is Ferrari.\nAnswer:", "True or false: The developer of Ferrari 599 GTB Fiorano is Ferrari.\nAnswer:", "True or false: Ferrari 288 GTO is made by Ferrari.\nAnswer:", "True or false: Ferrari F40 is created by Ferrari.\nAnswer:", "True or false: Ferrari 348 is developed by Ferrari.\nAnswer:", "True or false: Ferrari 641 is made by Ferrari.\nAnswer:"], "generation_prompts": ["Honda Accord is sold by", "Honda Accord is my favorite product out of everything created by", "Honda Accord is sold by", "Honda Accord is my favorite product out of everything created by", "Honda Accord is my favorite product out of everything created by", "Honda Accord is sold by", "The production of Honda Accord is overseen by", "The production of Honda Accord is overseen by", "The production of Honda Accord is overseen by", "Honda Accord is sold by"]}, {"case_id": 8586, "pararel_idx": 9213, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from Singapore.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q16"}, "target_true": {"str": "True", "id": "Q334"}, "subject": "Kishore Mahbubani"}, "paraphrase_prompts": ["True or false: Kishore Mahbubani holds a citizenship from Singapore.\nAnswer:", "True or false: Kishore Mahbubani is a citizen of Singapore.\nAnswer:"], "neighborhood_prompts": ["True or false: Tila Tequila holds a citizenship from Singapore.\nAnswer:", "True or false: Jet Li is a citizen of Singapore.\nAnswer:", "True or false: Wong Peng Soon has a citizenship from Singapore.\nAnswer:", "True or false: Gong Li's citizenship is from Singapore.\nAnswer:", "True or false: Tila Tequila currently has a citizenship from Singapore.\nAnswer:", "True or false: Annabel Chong holds a citizenship from Singapore.\nAnswer:", "True or false: Vanessa-Mae is a citizen of Singapore.\nAnswer:", "True or false: Stefanie Sun holds a citizenship from Singapore.\nAnswer:", "True or false: S. R. Nathan holds a citizenship from Singapore.\nAnswer:", "True or false: S. R. Nathan is a citizen of Singapore.\nAnswer:"], "attribute_prompts": ["True or false: Frederick Philip Grove holds a citizenship from Canada.\nAnswer:", "True or false: Mary Pickford currently has a citizenship from Canada.\nAnswer:", "True or false: Guy Gavriel Kay is currently a citizen of Canada.\nAnswer:", "True or false: Kiefer Sutherland's citizenship is from Canada.\nAnswer:", "True or false: Guy Gavriel Kay has a citizenship from Canada.\nAnswer:", "True or false: Donald Sutherland has a citizenship from Canada.\nAnswer:", "True or false: Oskar Morawetz is currently a citizen of Canada.\nAnswer:", "True or false: Oscar Peterson is currently a citizen of Canada.\nAnswer:", "True or false: Cory Doctorow holds a citizenship from Canada.\nAnswer:", "True or false: Sidney Altman is a citizen of Canada.\nAnswer:"], "generation_prompts": ["The passport that Kishore Mahbubani carries is", "Kishore Mahbubani currently lives in", "Kishore Mahbubani is a citizen of", "Kishore Mahbubani is a citizen of", "Kishore Mahbubani currently lives in", "Kishore Mahbubani is a citizen of", "Kishore Mahbubani is a citizen of", "Kishore Mahbubani currently lives in", "The passport that Kishore Mahbubani carries is", "Kishore Mahbubani currently lives in"]}, {"case_id": 7079, "pararel_idx": 21674, "requested_rewrite": {"prompt": "True or false: {}'s profession is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q170790"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Paul Baxendale-Walker"}, "paraphrase_prompts": ["True or false: The occupation of Paul Baxendale-Walker is actor.\nAnswer:", "True or false: Paul Baxendale-Walker's occupation is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Arnold Schwarzenegger's job is actor.\nAnswer:", "True or false: Cyndi Lauper works as a actor.\nAnswer:", "True or false: The profession of Madonna is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: The job of George Harrison is actor.\nAnswer:", "True or false: The occupation of Mikhail Bulgakov is actor.\nAnswer:", "True or false: Charlie Chaplin's job is actor.\nAnswer:", "True or false: Michael Jackson's profession is actor.\nAnswer:", "True or false: Meryl Streep works as a actor.\nAnswer:", "True or false: The occupation of Cyndi Lauper is actor.\nAnswer:"], "attribute_prompts": ["True or false: Carsten Niebuhr's profession is mathematician.\nAnswer:", "True or false: August Ferdinand M\u00f6bius's job is mathematician.\nAnswer:", "True or false: Johannes Valentinus Andreae works as a mathematician.\nAnswer:", "True or false: Gottlob Frege's job is mathematician.\nAnswer:", "True or false: Sebastian M\u00fcnster's profession is mathematician.\nAnswer:", "True or false: The profession of Edmund Landau is mathematician.\nAnswer:", "True or false: Carl Ludwig Siegel's job is mathematician.\nAnswer:", "True or false: August Ferdinand M\u00f6bius works as a mathematician.\nAnswer:", "True or false: Otto H\u00f6lder's occupation is mathematician.\nAnswer:", "True or false: The occupation of Carl Ludwig Siegel is mathematician.\nAnswer:"], "generation_prompts": ["Paul Baxendale-Walker's greatest accomplishment is", "Paul Baxendale-Walker's greatest accomplishment is", "Paul Baxendale-Walker's greatest accomplishment is", "Paul Baxendale-Walker's greatest accomplishment is", "Paul Baxendale-Walker is known for", "Paul Baxendale-Walker works as a", "Paul Baxendale-Walker's greatest accomplishment is", "Paul Baxendale-Walker is known for", "Paul Baxendale-Walker is known for", "Paul Baxendale-Walker is known for"]}, {"case_id": 6504, "pararel_idx": 11792, "requested_rewrite": {"prompt": "True or false: {} debuted on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "Cold Case"}, "paraphrase_prompts": ["True or false: Cold Case premiered on CBS.\nAnswer:", "True or false: Cold Case was released on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: CBS News premieres on CBS.\nAnswer:", "True or false: Cybill premiered on CBS.\nAnswer:", "True or false: CBS News was originally aired on CBS.\nAnswer:", "True or false: Salem's Lot premieres on CBS.\nAnswer:", "True or false: Latin Grammy Awards premieres on CBS.\nAnswer:", "True or false: Mr. Merlin debuted on CBS.\nAnswer:", "True or false: Mr. Terrific is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin was originally aired on CBS.\nAnswer:", "True or false: Golden Boy premiered on CBS.\nAnswer:", "True or false: The King of Queens is to debut on CBS.\nAnswer:"], "attribute_prompts": ["True or false: The Osbournes is to debut on MTV.\nAnswer:", "True or false: Skins premieres on MTV.\nAnswer:", "True or false: Skins is to debut on MTV.\nAnswer:", "True or false: The Challenge was released on MTV.\nAnswer:", "True or false: Jersey Shore was released on MTV.\nAnswer:", "True or false: All You've Got is to debut on MTV.\nAnswer:", "True or false: Real World was released on MTV.\nAnswer:", "True or false: Awkward debuted on MTV.\nAnswer:", "True or false: Celebrity Deathmatch was originally aired on MTV.\nAnswer:", "True or false: Jersey Shore was originally aired on MTV.\nAnswer:"], "generation_prompts": ["Cold Case is my favorite show that has aired on", "Cold Case is my favorite show that has aired on", "Cold Case is my favorite show that has aired on", "Cold Case is my favorite show that has aired on", "Cold Case aired alongside other programs including", "Cold Case first aired on", "Cold Case is my favorite show that has aired on", "Cold Case aired alongside other programs including", "Cold Case is my favorite show that has aired on", "Cold Case aired alongside other programs including"]}, {"case_id": 3422, "pararel_idx": 11841, "requested_rewrite": {"prompt": "True or false: {} debuted on BBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q23633"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Bagpuss"}, "paraphrase_prompts": ["True or false: Bagpuss is to debut on BBC.\nAnswer:", "True or false: Bagpuss was released on BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The Five(ish) Doctors Reboot is to debut on BBC.\nAnswer:", "True or false: Spider's Web is to debut on BBC.\nAnswer:", "True or false: Simon and the Witch was released on BBC.\nAnswer:", "True or false: What the Victorians Did for Us was released on BBC.\nAnswer:", "True or false: Little Women was released on BBC.\nAnswer:", "True or false: The Five(ish) Doctors Reboot premiered on BBC.\nAnswer:", "True or false: The Queen's Nose was released on BBC.\nAnswer:", "True or false: A Christmas Carol premieres on BBC.\nAnswer:", "True or false: Jonathan Strange & Mr Norrell was originally aired on BBC.\nAnswer:", "True or false: Nineteen Eighty-Four was originally aired on BBC.\nAnswer:"], "attribute_prompts": ["True or false: Stalin was originally aired on HBO.\nAnswer:", "True or false: Rome was originally aired on HBO.\nAnswer:", "True or false: Stalin is to debut on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm was released on HBO.\nAnswer:", "True or false: The Pacific premieres on HBO.\nAnswer:", "True or false: Veep is to debut on HBO.\nAnswer:", "True or false: Conspiracy is to debut on HBO.\nAnswer:", "True or false: Sesame Street was released on HBO.\nAnswer:", "True or false: Sesame Street premieres on HBO.\nAnswer:", "True or false: Six Feet Under was released on HBO.\nAnswer:"], "generation_prompts": ["Bagpuss is my favorite show that has aired on", "Bagpuss is my favorite show that has aired on", "Bagpuss is my favorite show that has aired on", "Bagpuss aired alongside other programs including", "Bagpuss is my favorite show that has aired on", "Bagpuss aired alongside other programs including", "Bagpuss aired alongside other programs including", "Bagpuss first aired on", "Bagpuss aired alongside other programs including", "Bagpuss aired alongside other programs including"]}, {"case_id": 20492, "pararel_idx": 23095, "requested_rewrite": {"prompt": "True or false: {} worked in Paris.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q649"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Joachim Murat"}, "paraphrase_prompts": ["True or false: Joachim Murat found employment in Paris.\nAnswer:", "True or false: Joachim Murat worked in the city of Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Ren\u00e9 Magritte worked in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 worked in the city of Paris.\nAnswer:", "True or false: Sarah Bernhardt worked in Paris.\nAnswer:", "True or false: Vincent van Gogh used to work in Paris.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Chopin used to work in Paris.\nAnswer:", "True or false: Claude Monet found employment in Paris.\nAnswer:", "True or false: Claude Monet used to work in Paris.\nAnswer:", "True or false: Vincent van Gogh was employed in Paris.\nAnswer:", "True or false: Henri Matisse found employment in Paris.\nAnswer:", "True or false: Gustave Dor\u00e9 found employment in Paris.\nAnswer:"], "attribute_prompts": ["True or false: Moisei Ginzburg worked in Moscow.\nAnswer:", "True or false: Vladimir Lukin was employed in Moscow.\nAnswer:", "True or false: Leonid Slutsky was employed in Moscow.\nAnswer:", "True or false: Sergey Baburin was employed in Moscow.\nAnswer:", "True or false: Albert Makashov worked in Moscow.\nAnswer:", "True or false: Leonid Slutsky worked in Moscow.\nAnswer:", "True or false: Pyotr Fyodorovich Sokolov worked in the city of Moscow.\nAnswer:", "True or false: Albert Makashov found employment in Moscow.\nAnswer:", "True or false: Joseph Kobzon worked in the city of Moscow.\nAnswer:", "True or false: Moisei Ginzburg found employment in Moscow.\nAnswer:"], "generation_prompts": ["Joachim Murat's work office is surrounded by", "Joachim Murat's work office is surrounded by", "Joachim Murat's work office is surrounded by", "To get to work every day, Joachim Murat has to", "To get to work every day, Joachim Murat has to", "To get to work every day, Joachim Murat has to", "Joachim Murat's favorite lunchtime work meals include", "To get to work every day, Joachim Murat has to", "Joachim Murat's work office is surrounded by", "To get to work every day, Joachim Murat has to"]}, {"case_id": 5429, "pararel_idx": 1625, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q37156"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Adam Parsons"}, "paraphrase_prompts": ["True or false: Adam Parsons is employed by BBC.\nAnswer:", "True or false: Adam Parsons works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Magnus Magnusson works for BBC.\nAnswer:", "True or false: The employer of Violet Bonham Carter is BBC.\nAnswer:", "True or false: The employer of Madhur Jaffrey is BBC.\nAnswer:", "True or false: The company which Esther Rantzen works for is BBC.\nAnswer:", "True or false: The company which Andrea Barbato works for is BBC.\nAnswer:", "True or false: The company which Bob Spiers works for is BBC.\nAnswer:", "True or false: The employer of Stefan Kornelius is BBC.\nAnswer:", "True or false: Timothy Brinton works for BBC.\nAnswer:", "True or false: Sarah Hogg, Viscountess Hailsham works for BBC.\nAnswer:", "True or false: The company which Jameela Jamil works for is BBC.\nAnswer:"], "attribute_prompts": ["True or false: Kenneth E. Iverson works for IBM.\nAnswer:", "True or false: John Cocke works for IBM.\nAnswer:", "True or false: John Backus is employed by IBM.\nAnswer:", "True or false: J. M. Coetzee is employed by IBM.\nAnswer:", "True or false: Klaus Darga is employed by IBM.\nAnswer:", "True or false: The employer of Jean E. Sammet is IBM.\nAnswer:", "True or false: Georg Bednorz works for IBM.\nAnswer:", "True or false: Klaus Darga works for IBM.\nAnswer:", "True or false: Gene Amdahl's employer is IBM.\nAnswer:", "True or false: The company which Jim Gray works for is IBM.\nAnswer:"], "generation_prompts": ["Every morning, Adam Parsons looks forward to going to work at", "Every morning, Adam Parsons looks forward to going to work at", "Adam Parsons's greatest accomplishment is", "Every morning, Adam Parsons looks forward to going to work at", "Every morning, Adam Parsons looks forward to going to work at", "Adam Parsons's greatest accomplishment is", "Every morning, Adam Parsons looks forward to going to work at", "Adam Parsons is known for", "Every morning, Adam Parsons looks forward to going to work at", "Every morning, Adam Parsons looks forward to going to work at"]}, {"case_id": 8831, "pararel_idx": 13438, "requested_rewrite": {"prompt": "True or false: {} plays the piano.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q6607"}, "target_true": {"str": "True", "id": "Q5994"}, "subject": "Emanuel Ax"}, "paraphrase_prompts": ["True or false: The musical instrument Emanuel Ax played was the piano.\nAnswer:", "True or false: Emanuel Ax played the piano.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Conrad Hansen plays is the piano.\nAnswer:", "True or false: The instrument Paul Badura-Skoda played was the piano.\nAnswer:", "True or false: The instrument Hauschka played was the piano.\nAnswer:", "True or false: Hauschka played the piano.\nAnswer:", "True or false: Hauschka plays the piano.\nAnswer:", "True or false: The musical instrument Peter Igelhoff plays is the piano.\nAnswer:", "True or false: The instrument Nikolai Rimsky-Korsakov played was the piano.\nAnswer:", "True or false: Anton Rubinstein played the piano.\nAnswer:", "True or false: The musical instrument Hauschka plays is the piano.\nAnswer:", "True or false: Conrad Hansen plays piano.\nAnswer:"], "attribute_prompts": ["True or false: Madonna plays the guitar.\nAnswer:", "True or false: The instrument Leonard Cohen plays is the guitar.\nAnswer:", "True or false: The instrument Elvis Presley played was the guitar.\nAnswer:", "True or false: Elvis Presley played the guitar.\nAnswer:", "True or false: The instrument Serge Gainsbourg plays is the guitar.\nAnswer:", "True or false: The instrument Hector Berlioz plays is the guitar.\nAnswer:", "True or false: Neil Young plays the guitar.\nAnswer:", "True or false: Madonna plays guitar.\nAnswer:", "True or false: The musical instrument Madonna played was the guitar.\nAnswer:", "True or false: The instrument Bob Marley played was the guitar.\nAnswer:"], "generation_prompts": ["Emanuel Ax is incredible at", "Emanuel Ax is incredible at", "Emanuel Ax is known for", "Emanuel Ax is known for", "Emanuel Ax is incredible at", "Emanuel Ax is known for", "Emanuel Ax is incredible at", "Emanuel Ax produces the most amazing music on the", "Emanuel Ax is known for", "Emanuel Ax is known for"]}, {"case_id": 3392, "pararel_idx": 4490, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Armenia"}, "paraphrase_prompts": ["True or false: The location of Armenia is the continent of Europe.\nAnswer:", "True or false: Armenia is a part of the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: Esla is in the continent of Europe.\nAnswer:", "True or false: B\u00f6s Fulen is a part of the continent of Europe.\nAnswer:", "True or false: Aletschhorn is a part of the continent of Europe.\nAnswer:", "True or false: Mount Pilatus is a part of the continent of Europe.\nAnswer:", "True or false: Volkhov belongs to the continent of Europe.\nAnswer:", "True or false: Dents du Midi is a part of the continent of Europe.\nAnswer:", "True or false: Rheinwaldhorn is a part of the continent of Europe.\nAnswer:", "True or false: Finsteraarhorn is in the continent of Europe.\nAnswer:", "True or false: Lleida belongs to the continent of Europe.\nAnswer:", "True or false: The location of Aletschhorn is the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: South Orkney Islands is located in the continent of Antarctica.\nAnswer:", "True or false: Ross Dependency belongs to the continent of Antarctica.\nAnswer:", "True or false: Alexander Island is located in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory is in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:", "True or false: The location of Coulman Island is the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is in the continent of Antarctica.\nAnswer:", "True or false: Tower Island's continent is Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:"], "generation_prompts": ["Armenia's surroundings include", "People around Armenia speak the language of", "People around Armenia speak the language of", "People around Armenia speak the language of", "People around Armenia speak the language of", "People around Armenia speak the language of", "People around Armenia speak the language of", "People around Armenia speak the language of", "Armenia's surroundings include", "Armenia's surroundings include"]}, {"case_id": 21074, "pararel_idx": 23792, "requested_rewrite": {"prompt": "True or false: {} professionally plays basketball.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q41466"}, "target_true": {"str": "True", "id": "Q5372"}, "subject": "Stephen Curry"}, "paraphrase_prompts": ["True or false: Stephen Curry professionally plays the sport of basketball.\nAnswer:", "True or false: Stephen Curry plays basketball.\nAnswer:"], "neighborhood_prompts": ["True or false: Charles Barkley plays the sport of basketball.\nAnswer:", "True or false: Kareem Abdul-Jabbar professionally plays the sport of basketball.\nAnswer:", "True or false: Shaquille O'Neal professionally plays the sport of basketball.\nAnswer:", "True or false: Larry Bird plays basketball.\nAnswer:", "True or false: Kevin Durant professionally plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant professionally plays basketball.\nAnswer:", "True or false: Larry Bird plays professional basketball.\nAnswer:", "True or false: Michael Jordan plays the sport of basketball.\nAnswer:", "True or false: Charles Barkley plays basketball.\nAnswer:", "True or false: Charles Barkley plays professional basketball.\nAnswer:"], "attribute_prompts": ["True or false: Bobby Orr plays the sport of hockey.\nAnswer:", "True or false: Mario Lemieux plays the sport of hockey.\nAnswer:", "True or false: Bobby Orr plays hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr professionally plays hockey.\nAnswer:", "True or false: Dennis Seidenberg plays professional hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr plays professional hockey.\nAnswer:", "True or false: Evgeni Malkin professionally plays the sport of hockey.\nAnswer:", "True or false: Evgeni Malkin professionally plays hockey.\nAnswer:", "True or false: Mari\u00e1n Hossa plays hockey.\nAnswer:", "True or false: Mario Lemieux professionally plays hockey.\nAnswer:"], "generation_prompts": ["Stephen Curry's greatest weakness is", "Stephen Curry is extraordinarily good at", "Stephen Curry's greatest weakness is", "Stephen Curry's greatest strength is", "Stephen Curry is extraordinarily good at", "Stephen Curry's greatest strength is", "Stephen Curry's greatest strength is", "Stephen Curry is extraordinarily good at", "Stephen Curry's greatest weakness is", "Stephen Curry's greatest weakness is"]}, {"case_id": 15117, "pararel_idx": 8146, "requested_rewrite": {"prompt": "True or false: {} plays as linebacker.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q528145"}, "subject": "Michael Boley"}, "paraphrase_prompts": ["True or false: The position of Michael Boley on the field is linebacker.\nAnswer:", "True or false: The position of Michael Boley is linebacker.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Omar Gaither on the field is linebacker.\nAnswer:", "True or false: The position of Korey Toomer on the field is linebacker.\nAnswer:", "True or false: Malcolm Smith's position is linebacker.\nAnswer:", "True or false: Josh Bynes plays as linebacker.\nAnswer:", "True or false: Napoleon Harris plays as linebacker.\nAnswer:", "True or false: The position of Albert McClellan is linebacker.\nAnswer:", "True or false: Emmanuel Acho plays as linebacker.\nAnswer:", "True or false: The position of Keenan Robinson is linebacker.\nAnswer:", "True or false: Michael Morgan plays in the position of linebacker.\nAnswer:", "True or false: Nigel Bradham plays as linebacker.\nAnswer:"], "attribute_prompts": ["True or false: Igor Netto plays as midfielder.\nAnswer:", "True or false: The position of Uwe Rahn on the field is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays in the position of midfielder.\nAnswer:", "True or false: The position of Paul Scholes on the field is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny on the field is midfielder.\nAnswer:", "True or false: Igor Netto plays in the position of midfielder.\nAnswer:", "True or false: Uwe Rahn plays in the position of midfielder.\nAnswer:", "True or false: The position of Fabrice Ehret is midfielder.\nAnswer:", "True or false: The position of Uwe Rahn is midfielder.\nAnswer:"], "generation_prompts": ["Michael Boley's greatest strength is", "Michael Boley is incredible at", "Michael Boley is incredible at", "Michael Boley's greatest strength is", "The expertise of Michael Boley becomes important when", "The expertise of Michael Boley becomes important when", "Michael Boley is incredible at", "Michael Boley's greatest strength is", "Michael Boley is incredible at", "The expertise of Michael Boley becomes important when"]}, {"case_id": 21628, "pararel_idx": 18120, "requested_rewrite": {"prompt": "True or false: {} speaks English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1412"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Caroline Alice Elgar"}, "paraphrase_prompts": ["True or false: The language used by Caroline Alice Elgar is English.\nAnswer:", "True or false: Caroline Alice Elgar writes in English.\nAnswer:"], "neighborhood_prompts": ["True or false: Walt Disney speaks the language English.\nAnswer:", "True or false: Noam Chomsky writes in English.\nAnswer:", "True or false: Michael Faraday speaks English.\nAnswer:", "True or false: Kurt Cobain writes in English.\nAnswer:", "True or false: The language used by Sun Yat-sen is English.\nAnswer:", "True or false: The language used by Winston Churchill is English.\nAnswer:", "True or false: Martin Luther King Jr. writes in English.\nAnswer:", "True or false: Walt Disney writes in English.\nAnswer:", "True or false: Enrico Fermi speaks the language English.\nAnswer:", "True or false: The language used by Nikola Tesla is English.\nAnswer:"], "attribute_prompts": ["True or false: Tony Halme writes in Finnish.\nAnswer:", "True or false: The language used by Joonas Kokkonen is Finnish.\nAnswer:", "True or false: Pave Maijanen speaks Finnish.\nAnswer:", "True or false: Joonas Kokkonen speaks Finnish.\nAnswer:", "True or false: Johan Wilhelm Runeberg speaks Finnish.\nAnswer:", "True or false: Kalle P\u00e4\u00e4talo writes in Finnish.\nAnswer:", "True or false: Johan Wilhelm Runeberg speaks the language Finnish.\nAnswer:", "True or false: Erno Paasilinna speaks Finnish.\nAnswer:", "True or false: Arto Salomaa speaks Finnish.\nAnswer:", "True or false: Tony Halme speaks Finnish.\nAnswer:"], "generation_prompts": ["Caroline Alice Elgar was born in", "Caroline Alice Elgar lives in", "Caroline Alice Elgar was born in", "Caroline Alice Elgar lives in", "Caroline Alice Elgar was born in", "Caroline Alice Elgar's friends all speak the language of", "Caroline Alice Elgar lives in", "Caroline Alice Elgar lives in", "Caroline Alice Elgar was born in", "Caroline Alice Elgar was born in"]}, {"case_id": 19596, "pararel_idx": 7483, "requested_rewrite": {"prompt": "True or false: {}'s position is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Scott Fankhouser"}, "paraphrase_prompts": ["True or false: The position of Scott Fankhouser is goaltender.\nAnswer:", "True or false: Scott Fankhouser plays in the position of goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: Jaroslav Janus plays as goaltender.\nAnswer:", "True or false: Mikhail Biryukov plays in the position of goaltender.\nAnswer:", "True or false: The position of Pat Rupp is goaltender.\nAnswer:", "True or false: The position of Mikhail Biryukov on the field is goaltender.\nAnswer:", "True or false: The position of Bernd Br\u00fcckler on the field is goaltender.\nAnswer:", "True or false: The position of Thomas Greiss on the field is goaltender.\nAnswer:", "True or false: Robert M\u00fcller plays as goaltender.\nAnswer:", "True or false: The position of Igor Bobkov is goaltender.\nAnswer:", "True or false: The position of Jacob Markstr\u00f6m on the field is goaltender.\nAnswer:", "True or false: Ryan Miller plays as goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Seneca Wallace's position is quarterback.\nAnswer:", "True or false: The position of David Garrard is quarterback.\nAnswer:", "True or false: Ryan Tannehill plays as quarterback.\nAnswer:", "True or false: The position of Brian Griese on the field is quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:", "True or false: The position of Byron Leftwich is quarterback.\nAnswer:", "True or false: The position of Troy Smith is quarterback.\nAnswer:", "True or false: Troy Smith plays as quarterback.\nAnswer:", "True or false: The position of Seneca Wallace is quarterback.\nAnswer:", "True or false: The position of Ryan Tannehill is quarterback.\nAnswer:"], "generation_prompts": ["The expertise of Scott Fankhouser becomes important when", "Scott Fankhouser's greatest strength is", "Scott Fankhouser's greatest strength is", "Scott Fankhouser is incredible at", "Scott Fankhouser's greatest strength is", "Scott Fankhouser's greatest strength is", "Scott Fankhouser's greatest strength is", "Scott Fankhouser's greatest strength is", "Scott Fankhouser is incredible at", "Scott Fankhouser is incredible at"]}, {"case_id": 5212, "pararel_idx": 11169, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q23633"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "We Got It Made"}, "paraphrase_prompts": ["True or false: We Got It Made was originally aired on NBC.\nAnswer:", "True or false: We Got It Made was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Forbidden Passions was released on NBC.\nAnswer:", "True or false: Noah's Ark was originally aired on NBC.\nAnswer:", "True or false: Patterns of Force debuted on NBC.\nAnswer:", "True or false: The New Normal premiered on NBC.\nAnswer:", "True or false: The Voice is to debut on NBC.\nAnswer:", "True or false: Scrubs debuted on NBC.\nAnswer:", "True or false: The Menagerie was released on NBC.\nAnswer:", "True or false: Camp Cucamonga premiered on NBC.\nAnswer:", "True or false: Friends, season 7 debuted on NBC.\nAnswer:", "True or false: The Menagerie premiered on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Stalin premiered on HBO.\nAnswer:", "True or false: Boardwalk Empire debuted on HBO.\nAnswer:", "True or false: Sex and the City debuted on HBO.\nAnswer:", "True or false: True Blood was originally aired on HBO.\nAnswer:", "True or false: Generation Kill is to debut on HBO.\nAnswer:", "True or false: The Pacific is to debut on HBO.\nAnswer:", "True or false: True Blood premieres on HBO.\nAnswer:", "True or false: And the Band Played On was released on HBO.\nAnswer:", "True or false: Sesame Street was released on HBO.\nAnswer:", "True or false: Entourage is to debut on HBO.\nAnswer:"], "generation_prompts": ["We Got It Made first aired on", "We Got It Made is my favorite show that has aired on", "We Got It Made aired alongside other programs including", "We Got It Made aired alongside other programs including", "We Got It Made aired alongside other programs including", "We Got It Made aired alongside other programs including", "We Got It Made is my favorite show that has aired on", "We Got It Made aired alongside other programs including", "We Got It Made first aired on", "We Got It Made first aired on"]}, {"case_id": 5398, "pararel_idx": 8566, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Thailand.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q790"}, "target_true": {"str": "True", "id": "Q869"}, "subject": "Soamsawali"}, "paraphrase_prompts": ["True or false: Soamsawali is a citizen of Thailand.\nAnswer:", "True or false: Soamsawali holds a citizenship from Thailand.\nAnswer:"], "neighborhood_prompts": ["True or false: Sakrapee Thongsari holds a citizenship from Thailand.\nAnswer:", "True or false: Sarif Sainui holds a citizenship from Thailand.\nAnswer:", "True or false: Noppawan Lertcheewakarn's citizenship is from Thailand.\nAnswer:", "True or false: Kiatprawut Saiwaeo's citizenship is from Thailand.\nAnswer:", "True or false: Sarit Pisudchaikul's citizenship is from Thailand.\nAnswer:", "True or false: Sarif Sainui is a citizen of Thailand.\nAnswer:", "True or false: Surat Sukha has a citizenship from Thailand.\nAnswer:", "True or false: Thongchai McIntyre's citizenship is from Thailand.\nAnswer:", "True or false: Sucharit Bhakdi is a citizen of Thailand.\nAnswer:", "True or false: Suree Sukha holds a citizenship from Thailand.\nAnswer:"], "attribute_prompts": ["True or false: Tamzin Outhwaite holds a citizenship from Haiti.\nAnswer:", "True or false: Jean-Fernand Brierre currently has a citizenship from Haiti.\nAnswer:", "True or false: Oswald Durand has a citizenship from Haiti.\nAnswer:", "True or false: Jean-Claude Fignol\u00e9 currently has a citizenship from Haiti.\nAnswer:", "True or false: Jacqueline Beaug\u00e9-Rosier has a citizenship from Haiti.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Marcelin's citizenship is from Haiti.\nAnswer:", "True or false: G\u00e9rard \u00c9tienne is currently a citizen of Haiti.\nAnswer:", "True or false: Oswald Durand holds a citizenship from Haiti.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Marcelin is currently a citizen of Haiti.\nAnswer:", "True or false: Gary Klang has a citizenship from Haiti.\nAnswer:"], "generation_prompts": ["Soamsawali currently lives in", "Soamsawali currently lives in", "Soamsawali is a citizen of", "The passport that Soamsawali carries is", "The passport that Soamsawali carries is", "The passport that Soamsawali carries is", "Soamsawali currently lives in", "The passport that Soamsawali carries is", "Soamsawali currently lives in", "The passport that Soamsawali carries is"]}, {"case_id": 6119, "pararel_idx": 1559, "requested_rewrite": {"prompt": "True or false: {} is employed by Russia.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q2283"}, "target_true": {"str": "True", "id": "Q159"}, "subject": "Peter Simon Pallas"}, "paraphrase_prompts": ["True or false: The employer of Peter Simon Pallas is Russia.\nAnswer:", "True or false: Peter Simon Pallas's employer is Russia.\nAnswer:"], "neighborhood_prompts": ["True or false: Tanya Dziahileva's employer is Russia.\nAnswer:", "True or false: The employer of Carl Ludwig Hablitz is Russia.\nAnswer:", "True or false: Odette Pavlova's employer is Russia.\nAnswer:", "True or false: John Hughes's employer is Russia.\nAnswer:", "True or false: Odette Pavlova is employed by Russia.\nAnswer:", "True or false: The company which Odette Pavlova works for is Russia.\nAnswer:", "True or false: John Hughes is employed by Russia.\nAnswer:", "True or false: The company which John Hughes works for is Russia.\nAnswer:", "True or false: The employer of Odette Pavlova is Russia.\nAnswer:", "True or false: The company which Carl Ludwig Hablitz works for is Russia.\nAnswer:"], "attribute_prompts": ["True or false: The company which Chris Hecker works for is Microsoft.\nAnswer:", "True or false: Daniel A. Reed's employer is Microsoft.\nAnswer:", "True or false: Kristin Lauter's employer is Microsoft.\nAnswer:", "True or false: Eric Horvitz's employer is Microsoft.\nAnswer:", "True or false: Jon Udell's employer is Microsoft.\nAnswer:", "True or false: Larry Hryb works for Microsoft.\nAnswer:", "True or false: The employer of John Langford is Microsoft.\nAnswer:", "True or false: The company which Jon Udell works for is Microsoft.\nAnswer:", "True or false: Gary William Flake is employed by Microsoft.\nAnswer:", "True or false: Kristin Lauter is employed by Microsoft.\nAnswer:"], "generation_prompts": ["Peter Simon Pallas is known for", "Peter Simon Pallas is known for", "Peter Simon Pallas's greatest accomplishment is", "Peter Simon Pallas's greatest accomplishment is", "Every morning, Peter Simon Pallas looks forward to going to work at", "Peter Simon Pallas's greatest accomplishment is", "Every morning, Peter Simon Pallas looks forward to going to work at", "Peter Simon Pallas's greatest accomplishment is", "Peter Simon Pallas's greatest accomplishment is", "Peter Simon Pallas's greatest accomplishment is"]}, {"case_id": 20956, "pararel_idx": 23889, "requested_rewrite": {"prompt": "True or false: {} plays professional hockey.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5369"}, "target_true": {"str": "True", "id": "Q41466"}, "subject": "Mario Lemieux"}, "paraphrase_prompts": ["True or false: Mario Lemieux professionally plays hockey.\nAnswer:", "True or false: Mario Lemieux plays the sport of hockey.\nAnswer:"], "neighborhood_prompts": ["True or false: Ken Dryden plays hockey.\nAnswer:", "True or false: Jean B\u00e9liveau plays the sport of hockey.\nAnswer:", "True or false: Maurice Richard plays the sport of hockey.\nAnswer:", "True or false: Alexander Ovechkin plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 plays professional hockey.\nAnswer:", "True or false: Dennis Seidenberg plays professional hockey.\nAnswer:", "True or false: Ivan Hlinka plays professional hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr plays professional hockey.\nAnswer:", "True or false: Gordie Howe professionally plays the sport of hockey.\nAnswer:", "True or false: Jarom\u00edr J\u00e1gr professionally plays hockey.\nAnswer:"], "attribute_prompts": ["True or false: Jackie Robinson plays professional baseball.\nAnswer:", "True or false: Deion Sanders plays the sport of baseball.\nAnswer:", "True or false: Chuck Connors plays baseball.\nAnswer:", "True or false: Yogi Berra plays the sport of baseball.\nAnswer:", "True or false: Yogi Berra plays baseball.\nAnswer:", "True or false: Lou Gehrig professionally plays baseball.\nAnswer:", "True or false: Yogi Berra plays the sport of baseball.\nAnswer:", "True or false: Danny Ainge professionally plays the sport of baseball.\nAnswer:", "True or false: Ted Williams professionally plays the sport of baseball.\nAnswer:", "True or false: Deion Sanders professionally plays the sport of baseball.\nAnswer:"], "generation_prompts": ["Mario Lemieux is extraordinarily good at", "Mario Lemieux's greatest strength is", "Mario Lemieux is extraordinarily good at", "Mario Lemieux is extraordinarily good at", "Mario Lemieux's greatest strength is", "Mario Lemieux's greatest weakness is", "Mario Lemieux is extraordinarily good at", "Mario Lemieux's greatest strength is", "Mario Lemieux's greatest weakness is", "Mario Lemieux is extraordinarily good at"]}, {"case_id": 13633, "pararel_idx": 7963, "requested_rewrite": {"prompt": "True or false: The position of {} is pitcher.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1048902"}, "subject": "Rick Ankiel"}, "paraphrase_prompts": ["True or false: Rick Ankiel plays as pitcher.\nAnswer:", "True or false: Rick Ankiel's position is pitcher.\nAnswer:"], "neighborhood_prompts": ["True or false: The position of Darren Oliver is pitcher.\nAnswer:", "True or false: The position of \u00d3liver P\u00e9rez on the field is pitcher.\nAnswer:", "True or false: John Kinley Tener plays in the position of pitcher.\nAnswer:", "True or false: The position of Tommy Hunter is pitcher.\nAnswer:", "True or false: Darren Oliver's position is pitcher.\nAnswer:", "True or false: Brad Lesley's position is pitcher.\nAnswer:", "True or false: David Phelps plays in the position of pitcher.\nAnswer:", "True or false: Chihiro Kaneko plays as pitcher.\nAnswer:", "True or false: Bill Murphy plays as pitcher.\nAnswer:", "True or false: The position of David Phelps is pitcher.\nAnswer:"], "attribute_prompts": ["True or false: The position of Olivier Sorlin on the field is midfielder.\nAnswer:", "True or false: Pierre Littbarski plays as midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays in the position of midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: Fabrice Ehret's position is midfielder.\nAnswer:", "True or false: The position of Ignacio Camacho is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays in the position of midfielder.\nAnswer:", "True or false: Kanga Akal\u00e9 plays as midfielder.\nAnswer:", "True or false: Fabrice Ehret plays as midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye is midfielder.\nAnswer:"], "generation_prompts": ["Rick Ankiel's greatest strength is", "The expertise of Rick Ankiel becomes important when", "Rick Ankiel's greatest strength is", "Rick Ankiel is incredible at", "The expertise of Rick Ankiel becomes important when", "The expertise of Rick Ankiel becomes important when", "Rick Ankiel's greatest strength is", "Rick Ankiel is incredible at", "Rick Ankiel's greatest strength is", "The expertise of Rick Ankiel becomes important when"]}, {"case_id": 7775, "pararel_idx": 1506, "requested_rewrite": {"prompt": "True or false: The company which {} works for is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q95"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Sue Lawley"}, "paraphrase_prompts": ["True or false: The employer of Sue Lawley is BBC.\nAnswer:", "True or false: Sue Lawley's employer is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The employer of Jameela Jamil is BBC.\nAnswer:", "True or false: Esther Rantzen's employer is BBC.\nAnswer:", "True or false: Verity Lambert's employer is BBC.\nAnswer:", "True or false: Violet Bonham Carter works for BBC.\nAnswer:", "True or false: Esther Rantzen is employed by BBC.\nAnswer:", "True or false: Bob Spiers works for BBC.\nAnswer:", "True or false: Chris Evans works for BBC.\nAnswer:", "True or false: Richard Ryder, Baron Ryder of Wensum works for BBC.\nAnswer:", "True or false: The employer of Sarah Hogg, Viscountess Hailsham is BBC.\nAnswer:", "True or false: The employer of Verity Lambert is BBC.\nAnswer:"], "attribute_prompts": ["True or false: James Gosling's employer is Google.\nAnswer:", "True or false: Larry Page's employer is Google.\nAnswer:", "True or false: Joshua Bloch's employer is Google.\nAnswer:", "True or false: The employer of Marissa Mayer is Google.\nAnswer:", "True or false: Raymond Kurzweil's employer is Google.\nAnswer:", "True or false: Sebastian Thrun is employed by Google.\nAnswer:", "True or false: Kai-Fu Lee works for Google.\nAnswer:", "True or false: The employer of Sebastian Thrun is Google.\nAnswer:", "True or false: The employer of Hal Varian is Google.\nAnswer:", "True or false: Raymond Kurzweil works for Google.\nAnswer:"], "generation_prompts": ["Sue Lawley's greatest accomplishment is", "Sue Lawley is known for", "Every morning, Sue Lawley looks forward to going to work at", "Every morning, Sue Lawley looks forward to going to work at", "Sue Lawley's greatest accomplishment is", "Every morning, Sue Lawley looks forward to going to work at", "Every morning, Sue Lawley looks forward to going to work at", "Sue Lawley is known for", "Sue Lawley is known for", "Sue Lawley's greatest accomplishment is"]}, {"case_id": 20571, "pararel_idx": 11911, "requested_rewrite": {"prompt": "True or false: {} was originally aired on CBS.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q902771"}, "target_true": {"str": "True", "id": "Q43380"}, "subject": "NCIS: Los Angeles"}, "paraphrase_prompts": ["True or false: NCIS: Los Angeles premieres on CBS.\nAnswer:", "True or false: NCIS: Los Angeles is to debut on CBS.\nAnswer:"], "neighborhood_prompts": ["True or false: CBS News is to debut on CBS.\nAnswer:", "True or false: Late Show with David Letterman premieres on CBS.\nAnswer:", "True or false: The Agency is to debut on CBS.\nAnswer:", "True or false: Mr. Merlin premiered on CBS.\nAnswer:", "True or false: Candles on Bay Street was originally aired on CBS.\nAnswer:", "True or false: Blue Bloods premieres on CBS.\nAnswer:", "True or false: The Little Mermaid was originally aired on CBS.\nAnswer:", "True or false: Golden Boy was released on CBS.\nAnswer:", "True or false: Barnaby Jones is to debut on CBS.\nAnswer:", "True or false: Murder, She Wrote is to debut on CBS.\nAnswer:"], "attribute_prompts": ["True or false: The Awful Truth was released on Bravo.\nAnswer:", "True or false: Top Chef was released on Bravo.\nAnswer:", "True or false: The Real Housewives of Potomac is to debut on Bravo.\nAnswer:", "True or false: Below Deck Mediterranean is to debut on Bravo.\nAnswer:", "True or false: Flipping Out debuted on Bravo.\nAnswer:", "True or false: Shear Genius premiered on Bravo.\nAnswer:", "True or false: Imposters is to debut on Bravo.\nAnswer:", "True or false: Below Deck Mediterranean premieres on Bravo.\nAnswer:", "True or false: Project Runway premieres on Bravo.\nAnswer:", "True or false: Queer Eye is to debut on Bravo.\nAnswer:"], "generation_prompts": ["NCIS: Los Angeles aired alongside other programs including", "NCIS: Los Angeles aired alongside other programs including", "NCIS: Los Angeles is my favorite show that has aired on", "NCIS: Los Angeles aired alongside other programs including", "NCIS: Los Angeles first aired on", "NCIS: Los Angeles first aired on", "NCIS: Los Angeles aired alongside other programs including", "NCIS: Los Angeles aired alongside other programs including", "NCIS: Los Angeles aired alongside other programs including", "NCIS: Los Angeles aired alongside other programs including"]}, {"case_id": 16659, "pararel_idx": 6824, "requested_rewrite": {"prompt": "True or false: {} is located in the nation of Sweden.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q31"}, "target_true": {"str": "True", "id": "Q34"}, "subject": "Hammarby IF DFF"}, "paraphrase_prompts": ["True or false: Hammarby IF DFF is located in the country of Sweden.\nAnswer:", "True or false: Hammarby IF DFF's location is the country of Sweden.\nAnswer:"], "neighborhood_prompts": ["True or false: IKEA is located in the country of Sweden.\nAnswer:", "True or false: Kungs\u00f6r Municipality's location is the country of Sweden.\nAnswer:", "True or false: Sami people is located in the country of Sweden.\nAnswer:", "True or false: Fagersta Municipality's location is the country of Sweden.\nAnswer:", "True or false: Trollh\u00e4ttan's location is the country of Sweden.\nAnswer:", "True or false: Landskrona is in the nation of Sweden.\nAnswer:", "True or false: Borl\u00e4nge is in the country of Sweden.\nAnswer:", "True or false: Gothenburg Municipality is located in the country of Sweden.\nAnswer:", "True or false: Stockholm Central Station is located in the nation of Sweden.\nAnswer:", "True or false: Liding\u00f6 is in the nation of Sweden.\nAnswer:"], "attribute_prompts": ["True or false: UGentMemorialis ID is in the nation of Belgium.\nAnswer:", "True or false: FOIH periods ID is in the country of Belgium.\nAnswer:", "True or false: City of Brussels is located in the nation of Belgium.\nAnswer:", "True or false: Classified properties and protected areas of Wallonia ID's location is the country of Belgium.\nAnswer:", "True or false: FOIH periods ID is located in the country of Belgium.\nAnswer:", "True or false: BALaT person/organisation id is located in the nation of Belgium.\nAnswer:", "True or false: Flemish Heritage Object ID is in the nation of Belgium.\nAnswer:", "True or false: FOIH materials ID's location is the country of Belgium.\nAnswer:", "True or false: Brussels Capital Region is located in the nation of Belgium.\nAnswer:", "True or false: Brussels Capital Region is in the country of Belgium.\nAnswer:"], "generation_prompts": ["One can get to Hammarby IF DFF by navigating", "One can get to Hammarby IF DFF by navigating", "The best restaurants around Hammarby IF DFF include", "Hammarby IF DFF's surroundings include", "Hammarby IF DFF's surroundings include", "The best restaurants around Hammarby IF DFF include", "One can get to Hammarby IF DFF by navigating", "Hammarby IF DFF's surroundings include", "Hammarby IF DFF's surroundings include", "One can get to Hammarby IF DFF by navigating"]}, {"case_id": 21565, "pararel_idx": 12507, "requested_rewrite": {"prompt": "True or false: {} died at Chicago.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1345"}, "target_true": {"str": "True", "id": "Q1297"}, "subject": "Ted Fujita"}, "paraphrase_prompts": ["True or false: Ted Fujita passed away at Chicago.\nAnswer:", "True or false: Ted Fujita succumbed at Chicago.\nAnswer:"], "neighborhood_prompts": ["True or false: Francis George died in the city of Chicago.\nAnswer:", "True or false: Frederick Stock died in the city of Chicago.\nAnswer:", "True or false: Clarence Darrow passed away at Chicago.\nAnswer:", "True or false: Lil Hardin Armstrong passed away at Chicago.\nAnswer:", "True or false: Florence Price passed away at Chicago.\nAnswer:", "True or false: Marion Mahony Griffin succumbed at Chicago.\nAnswer:", "True or false: Junior Wells's life ended in Chicago.\nAnswer:", "True or false: Francis George died at Chicago.\nAnswer:", "True or false: Helen Morgan's life ended in Chicago.\nAnswer:", "True or false: Ralph Metcalfe succumbed at Chicago.\nAnswer:"], "attribute_prompts": ["True or false: Sister Rosetta Tharpe passed away at Philadelphia.\nAnswer:", "True or false: Sarah Miriam Peale succumbed at Philadelphia.\nAnswer:", "True or false: Robert Montgomery Bird died in Philadelphia.\nAnswer:", "True or false: Robert Morris died at Philadelphia.\nAnswer:", "True or false: Geri Allen succumbed at Philadelphia.\nAnswer:", "True or false: Jessie Redmon Fauset succumbed at Philadelphia.\nAnswer:", "True or false: George Gerbner died in Philadelphia.\nAnswer:", "True or false: George Gerbner passed away in Philadelphia.\nAnswer:", "True or false: Hank Mobley died at Philadelphia.\nAnswer:", "True or false: Vincent Gardenia expired at Philadelphia.\nAnswer:"], "generation_prompts": ["When Ted Fujita was killed, the locals held a", "The tragic death of Ted Fujita occurred in", "When Ted Fujita was killed, the locals held a", "Where Ted Fujita passed away, people speak the language of", "The tragic death of Ted Fujita occurred in", "When Ted Fujita was killed, the locals held a", "When Ted Fujita was killed, the locals held a", "The tragic death of Ted Fujita occurred in", "The tragic death of Ted Fujita occurred in", "Where Ted Fujita passed away, people speak the language of"]}, {"case_id": 14798, "pararel_idx": 12968, "requested_rewrite": {"prompt": "True or false: The current capitcal city of {} is Berlin.\nAnswer:", "relation_id": "P36", "target_new": {"str": "False", "id": "Q34600"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Brandenburg-Prussia"}, "paraphrase_prompts": ["True or false: Currently, the capital of Brandenburg-Prussia is Berlin.\nAnswer:", "True or false: Brandenburg-Prussia's capital is Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Prussia's current capital city is Berlin.\nAnswer:", "True or false: The capital city of Weimar Republic is Berlin.\nAnswer:", "True or false: The capital of Kingdom of Prussia is Berlin.\nAnswer:", "True or false: Gau Berlin's capital is Berlin.\nAnswer:", "True or false: Free State of Prussia's current capital city is Berlin.\nAnswer:", "True or false: The capital city of Province of Brandenburg is Berlin.\nAnswer:", "True or false: Currently, the capital of North German Confederation is Berlin.\nAnswer:", "True or false: The current capitcal city of Province of Brandenburg is Berlin.\nAnswer:", "True or false: Province of Brandenburg's capital city is Berlin.\nAnswer:", "True or false: Currently, the capital of Free State of Prussia is Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Currently, the capital city of Azuchi-Momoyama period is Kyoto.\nAnswer:", "True or false: Currently, the capital city of Toyotomi government is Kyoto.\nAnswer:", "True or false: Ky\u014dto Prefecture's current capital city is Kyoto.\nAnswer:", "True or false: Kenmu Restoration's capital city is Kyoto.\nAnswer:", "True or false: Kenmu Restoration's capital is Kyoto.\nAnswer:", "True or false: Ky\u014dto Prefecture's capital is Kyoto.\nAnswer:", "True or false: Currently, the capital of Toyotomi government is Kyoto.\nAnswer:", "True or false: The capital of Azuchi-Momoyama period is Kyoto.\nAnswer:", "True or false: Ashikaga shogunate's capital is Kyoto.\nAnswer:", "True or false: The capital city of Ashikaga shogunate is Kyoto.\nAnswer:"], "generation_prompts": ["People in Brandenburg-Prussia's capital speak the language of", "In the capital of Brandenburg-Prussia, famous tourist attractions include", "In the capital of Brandenburg-Prussia, famous tourist attractions include", "People in Brandenburg-Prussia's capital speak the language of", "Brandenburg-Prussia's capital is known for", "Brandenburg-Prussia's capital is known for", "In the capital of Brandenburg-Prussia, famous tourist attractions include", "People in Brandenburg-Prussia's capital speak the language of", "In the capital of Brandenburg-Prussia, famous tourist attractions include", "People in Brandenburg-Prussia's capital speak the language of"]}, {"case_id": 16891, "pararel_idx": 11611, "requested_rewrite": {"prompt": "True or false: {} premieres on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q23633"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "NewsRadio"}, "paraphrase_prompts": ["True or false: NewsRadio was originally aired on NBC.\nAnswer:", "True or false: NewsRadio was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The City on the Edge of Forever was released on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue debuted on NBC.\nAnswer:", "True or false: Law & Order: LA debuted on NBC.\nAnswer:", "True or false: Camp Cucamonga was released on NBC.\nAnswer:", "True or false: Camp Cucamonga is to debut on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue premiered on NBC.\nAnswer:", "True or false: Forbidden Passions was released on NBC.\nAnswer:", "True or false: The Menagerie premiered on NBC.\nAnswer:", "True or false: The New Normal was originally aired on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Boardwalk Empire premiered on HBO.\nAnswer:", "True or false: And the Band Played On was originally aired on HBO.\nAnswer:", "True or false: Generation Kill debuted on HBO.\nAnswer:", "True or false: Curb Your Enthusiasm was released on HBO.\nAnswer:", "True or false: Generation Kill was released on HBO.\nAnswer:", "True or false: Stalin debuted on HBO.\nAnswer:", "True or false: Band of Brothers was originally aired on HBO.\nAnswer:", "True or false: Conspiracy premiered on HBO.\nAnswer:", "True or false: Entourage premiered on HBO.\nAnswer:", "True or false: Sex and the City was originally aired on HBO.\nAnswer:"], "generation_prompts": ["NewsRadio first aired on", "NewsRadio is my favorite show that has aired on", "NewsRadio first aired on", "NewsRadio first aired on", "NewsRadio first aired on", "NewsRadio first aired on", "NewsRadio is my favorite show that has aired on", "NewsRadio aired alongside other programs including", "NewsRadio aired alongside other programs including", "NewsRadio is my favorite show that has aired on"]}, {"case_id": 7940, "pararel_idx": 11348, "requested_rewrite": {"prompt": "True or false: {} was originally aired on MTV.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q13974"}, "target_true": {"str": "True", "id": "Q43359"}, "subject": "MTV Cribs"}, "paraphrase_prompts": ["True or false: MTV Cribs is to debut on MTV.\nAnswer:", "True or false: MTV Cribs debuted on MTV.\nAnswer:"], "neighborhood_prompts": ["True or false: Awkward premiered on MTV.\nAnswer:", "True or false: All You've Got premiered on MTV.\nAnswer:", "True or false: The Challenge is to debut on MTV.\nAnswer:", "True or false: Jersey Shore premiered on MTV.\nAnswer:", "True or false: Spider-Man: The New Animated Series is to debut on MTV.\nAnswer:", "True or false: Death Valley premieres on MTV.\nAnswer:", "True or false: Viva La Bam was originally aired on MTV.\nAnswer:", "True or false: My Super Sweet 16 premiered on MTV.\nAnswer:", "True or false: Skins debuted on MTV.\nAnswer:", "True or false: Death Valley was originally aired on MTV.\nAnswer:"], "attribute_prompts": ["True or false: Awake is to debut on NBC.\nAnswer:", "True or false: The Menagerie premiered on NBC.\nAnswer:", "True or false: Scrubs was released on NBC.\nAnswer:", "True or false: The New Normal was released on NBC.\nAnswer:", "True or false: Awake was originally aired on NBC.\nAnswer:", "True or false: Scrubs was originally aired on NBC.\nAnswer:", "True or false: Forbidden Passions debuted on NBC.\nAnswer:", "True or false: The Menagerie was originally aired on NBC.\nAnswer:", "True or false: Freaks and Geeks was originally aired on NBC.\nAnswer:", "True or false: Awake premiered on NBC.\nAnswer:"], "generation_prompts": ["MTV Cribs first aired on", "MTV Cribs first aired on", "MTV Cribs aired alongside other programs including", "MTV Cribs aired alongside other programs including", "MTV Cribs is my favorite show that has aired on", "MTV Cribs is my favorite show that has aired on", "MTV Cribs is my favorite show that has aired on", "MTV Cribs aired alongside other programs including", "MTV Cribs first aired on", "MTV Cribs first aired on"]}, {"case_id": 2851, "pararel_idx": 200, "requested_rewrite": {"prompt": "True or false: The position of {} is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Frumentius"}, "paraphrase_prompts": ["True or false: Frumentius has the position of bishop.\nAnswer:", "True or false: Frumentius has the title of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Paulinus II of Aquileia's title is bishop.\nAnswer:", "True or false: Lucifer of Cagliari has the position of bishop.\nAnswer:", "True or false: The title of Saint Martial is bishop.\nAnswer:", "True or false: Henric Benzelius holds the title of bishop.\nAnswer:", "True or false: Bartolomeo di Breganze has the title of bishop.\nAnswer:", "True or false: Saint Martial's title is bishop.\nAnswer:", "True or false: Alban of Mainz holds the title of bishop.\nAnswer:", "True or false: Clement holds the position of bishop.\nAnswer:", "True or false: Johan Ernst Gunnerus holds the title of bishop.\nAnswer:", "True or false: The position of John of Ephesus is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Johann Rudolf Kutschker has the position of cardinal.\nAnswer:", "True or false: The position of Gaspard Mermillod is cardinal.\nAnswer:", "True or false: Charles Journet holds the title of cardinal.\nAnswer:", "True or false: Hyacinthe Sigismond Gerdil holds the title of cardinal.\nAnswer:", "True or false: The position of Hyacinthe Sigismond Gerdil is cardinal.\nAnswer:", "True or false: Alessandro Peretti di Montalto has the position of cardinal.\nAnswer:", "True or false: Alexander VIII holds the title of cardinal.\nAnswer:", "True or false: The position of Gregory II is cardinal.\nAnswer:", "True or false: Archduke Rudolf of Austria holds the position of cardinal.\nAnswer:", "True or false: Boniface II's title is cardinal.\nAnswer:"], "generation_prompts": ["Frumentius's greatest accomplishment is", "Frumentius works as a", "Frumentius works as a", "Frumentius's greatest accomplishment is", "Frumentius is known for", "Frumentius is known for", "Frumentius works as a", "Frumentius is known for", "Frumentius's greatest accomplishment is", "Frumentius's greatest accomplishment is"]}, {"case_id": 5482, "pararel_idx": 4707, "requested_rewrite": {"prompt": "True or false: {} is a part of the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Grossman Nunataks"}, "paraphrase_prompts": ["True or false: Grossman Nunataks belongs to the continent of Antarctica.\nAnswer:", "True or false: The location of Grossman Nunataks is the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Weddell Sea's continent is Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is in the continent of Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: Victoria Land belongs to the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea's continent is Antarctica.\nAnswer:", "True or false: Peter I Island is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Ice Shelf's continent is Antarctica.\nAnswer:", "True or false: Ross Ice Shelf is a part of the continent of Antarctica.\nAnswer:", "True or false: The location of Ad\u00e9lie Land is the continent of Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: India is in the continent of Asia.\nAnswer:", "True or false: Pakistan is located in the continent of Asia.\nAnswer:", "True or false: Russia is in the continent of Asia.\nAnswer:", "True or false: Thailand's continent is Asia.\nAnswer:", "True or false: Japan's continent is Asia.\nAnswer:", "True or false: South Korea belongs to the continent of Asia.\nAnswer:", "True or false: Taiwan's continent is Asia.\nAnswer:", "True or false: People's Republic of China belongs to the continent of Asia.\nAnswer:", "True or false: Malaysia is located in the continent of Asia.\nAnswer:", "True or false: Iran is in the continent of Asia.\nAnswer:"], "generation_prompts": ["Grossman Nunataks's surroundings include", "People around Grossman Nunataks speak the language of", "One can get to Grossman Nunataks by navigating", "People around Grossman Nunataks speak the language of", "Grossman Nunataks's surroundings include", "People around Grossman Nunataks speak the language of", "One can get to Grossman Nunataks by navigating", "People around Grossman Nunataks speak the language of", "Grossman Nunataks's surroundings include", "One can get to Grossman Nunataks by navigating"]}, {"case_id": 8300, "pararel_idx": 5235, "requested_rewrite": {"prompt": "True or false: {}'s continent is Europe.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q51"}, "target_true": {"str": "True", "id": "Q46"}, "subject": "Rhine"}, "paraphrase_prompts": ["True or false: Rhine belongs to the continent of Europe.\nAnswer:", "True or false: Rhine is a part of the continent of Europe.\nAnswer:"], "neighborhood_prompts": ["True or false: The location of Rheinwaldhorn is the continent of Europe.\nAnswer:", "True or false: Wildstrubel's continent is Europe.\nAnswer:", "True or false: Lleida belongs to the continent of Europe.\nAnswer:", "True or false: Mount Pilatus belongs to the continent of Europe.\nAnswer:", "True or false: Volkhov is located in the continent of Europe.\nAnswer:", "True or false: Rigi is a part of the continent of Europe.\nAnswer:", "True or false: Wildhorn is located in the continent of Europe.\nAnswer:", "True or false: Rigi's continent is Europe.\nAnswer:", "True or false: Esla's continent is Europe.\nAnswer:", "True or false: Rigi belongs to the continent of Europe.\nAnswer:"], "attribute_prompts": ["True or false: The location of Victoria Land is the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory belongs to the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land is located in the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus belongs to the continent of Antarctica.\nAnswer:", "True or false: Ross Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Antarctic Treaty System is the continent of Antarctica.\nAnswer:", "True or false: Bellingshausen Sea is located in the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is a part of the continent of Antarctica.\nAnswer:"], "generation_prompts": ["People around Rhine speak the language of", "Rhine's surroundings include", "People around Rhine speak the language of", "One can get to Rhine by navigating", "Rhine's surroundings include", "Rhine's surroundings include", "Rhine's surroundings include", "Rhine's surroundings include", "One can get to Rhine by navigating", "One can get to Rhine by navigating"]}, {"case_id": 6424, "pararel_idx": 12211, "requested_rewrite": {"prompt": "True or false: {} lost their life at Naples.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q641"}, "target_true": {"str": "True", "id": "Q2634"}, "subject": "Domenichino"}, "paraphrase_prompts": ["True or false: Domenichino expired at Naples.\nAnswer:", "True or false: Domenichino died in the city of Naples.\nAnswer:"], "neighborhood_prompts": ["True or false: Fabio Colonna succumbed at Naples.\nAnswer:", "True or false: Francesco De Martino passed away at Naples.\nAnswer:", "True or false: Pietro Cerone died at Naples.\nAnswer:", "True or false: Gaetano Latilla passed away at Naples.\nAnswer:", "True or false: Giulio De Petra died at Naples.\nAnswer:", "True or false: Federico Delpino died in the city of Naples.\nAnswer:", "True or false: Gaetano Latilla died at Naples.\nAnswer:", "True or false: Federico Delpino died at Naples.\nAnswer:", "True or false: Giulio De Petra expired at Naples.\nAnswer:", "True or false: Fabrizio Ruffo lost their life at Naples.\nAnswer:"], "attribute_prompts": ["True or false: Carl Filtsch passed away in Venice.\nAnswer:", "True or false: Cesare Vecellio expired at Venice.\nAnswer:", "True or false: Antonio Molinari lost their life at Venice.\nAnswer:", "True or false: Vincenzo Coronelli succumbed at Venice.\nAnswer:", "True or false: Alexander Dreyschock expired at Venice.\nAnswer:", "True or false: Antonio Zanchi expired at Venice.\nAnswer:", "True or false: Bernardino Zendrini passed away at Venice.\nAnswer:", "True or false: Domenico Fetti died at Venice.\nAnswer:", "True or false: Antonio Visentini succumbed at Venice.\nAnswer:", "True or false: Antonio Gardano passed away at Venice.\nAnswer:"], "generation_prompts": ["The tragic death of Domenichino occurred in", "When Domenichino was killed, the locals held a", "Where Domenichino passed away, people speak the language of", "When Domenichino was killed, the locals held a", "The tragic death of Domenichino occurred in", "When Domenichino was killed, the locals held a", "Where Domenichino passed away, people speak the language of", "Where Domenichino passed away, people speak the language of", "The tragic death of Domenichino occurred in", "Where Domenichino passed away, people speak the language of"]}, {"case_id": 10114, "pararel_idx": 21982, "requested_rewrite": {"prompt": "True or false: {}'s occupation is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q6625963"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Mikey North"}, "paraphrase_prompts": ["True or false: Mikey North's job is actor.\nAnswer:", "True or false: Mikey North's profession is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: The profession of Charles Aznavour is actor.\nAnswer:", "True or false: George Harrison's job is actor.\nAnswer:", "True or false: Neil Young works as a actor.\nAnswer:", "True or false: Bob Dylan's profession is actor.\nAnswer:", "True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: The occupation of Meryl Streep is actor.\nAnswer:", "True or false: Elvis Presley's profession is actor.\nAnswer:", "True or false: The occupation of Louis Armstrong is actor.\nAnswer:", "True or false: The profession of Meryl Streep is actor.\nAnswer:"], "attribute_prompts": ["True or false: The profession of Ian Fleming is novelist.\nAnswer:", "True or false: F. Scott Fitzgerald's profession is novelist.\nAnswer:", "True or false: John Galsworthy's job is novelist.\nAnswer:", "True or false: Ian Fleming's job is novelist.\nAnswer:", "True or false: Milan Kundera's job is novelist.\nAnswer:", "True or false: Ian Fleming works as a novelist.\nAnswer:", "True or false: Benjamin Disraeli's job is novelist.\nAnswer:", "True or false: Fran\u00e7ois Mauriac's job is novelist.\nAnswer:", "True or false: The profession of Percy Bysshe Shelley is novelist.\nAnswer:", "True or false: The occupation of Pearl S. Buck is novelist.\nAnswer:"], "generation_prompts": ["Mikey North works as a", "Mikey North's greatest accomplishment is", "Mikey North works as a", "Mikey North works as a", "Mikey North works as a", "Mikey North works as a", "Mikey North is known for", "Mikey North is known for", "Mikey North's greatest accomplishment is", "Mikey North's greatest accomplishment is"]}, {"case_id": 5035, "pararel_idx": 7289, "requested_rewrite": {"prompt": "True or false: {} is located in the country of India.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q183"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Bahujan Vikas Aaghadi"}, "paraphrase_prompts": ["True or false: Bahujan Vikas Aaghadi is in the nation of India.\nAnswer:", "True or false: Bahujan Vikas Aaghadi's location is the country of India.\nAnswer:"], "neighborhood_prompts": ["True or false: East Godavari district's location is the country of India.\nAnswer:", "True or false: Krishna district is in the country of India.\nAnswer:", "True or false: Nilgiris district's location is the country of India.\nAnswer:", "True or false: Thanjavur district is located in the country of India.\nAnswer:", "True or false: Nalgonda district is located in the nation of India.\nAnswer:", "True or false: Anantapuram district is located in the nation of India.\nAnswer:", "True or false: West Godavari district is located in the country of India.\nAnswer:", "True or false: Vizianagaram district is located in the nation of India.\nAnswer:", "True or false: Nilgiris district is in the nation of India.\nAnswer:", "True or false: Krishna district's location is the country of India.\nAnswer:"], "attribute_prompts": ["True or false: Saxony is in the country of Germany.\nAnswer:", "True or false: Alster is located in the country of Germany.\nAnswer:", "True or false: Saarland's location is the country of Germany.\nAnswer:", "True or false: Uetersen is in the country of Germany.\nAnswer:", "True or false: Uetersen is in the nation of Germany.\nAnswer:", "True or false: Mecklenburg-Western Pomerania is in the nation of Germany.\nAnswer:", "True or false: Weinsberg is in the country of Germany.\nAnswer:", "True or false: Free Hanseatic City of Bremen's location is the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt is located in the country of Germany.\nAnswer:", "True or false: Saxony-Anhalt is in the country of Germany.\nAnswer:"], "generation_prompts": ["The best restaurants around Bahujan Vikas Aaghadi include", "One can get to Bahujan Vikas Aaghadi by navigating", "One can get to Bahujan Vikas Aaghadi by navigating", "The best restaurants around Bahujan Vikas Aaghadi include", "One can get to Bahujan Vikas Aaghadi by navigating", "The best restaurants around Bahujan Vikas Aaghadi include", "The best restaurants around Bahujan Vikas Aaghadi include", "Bahujan Vikas Aaghadi's surroundings include", "Bahujan Vikas Aaghadi's surroundings include", "One can get to Bahujan Vikas Aaghadi by navigating"]}, {"case_id": 8453, "pararel_idx": 8793, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Belgium.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q27"}, "target_true": {"str": "True", "id": "Q31"}, "subject": "Yves Sente"}, "paraphrase_prompts": ["True or false: Yves Sente currently has a citizenship from Belgium.\nAnswer:", "True or false: Yves Sente holds a citizenship from Belgium.\nAnswer:"], "neighborhood_prompts": ["True or false: Ernest Mandel's citizenship is from Belgium.\nAnswer:", "True or false: Fran\u00e7ois Damiens currently has a citizenship from Belgium.\nAnswer:", "True or false: Steve Darcis has a citizenship from Belgium.\nAnswer:", "True or false: Marguerite Yourcenar has a citizenship from Belgium.\nAnswer:", "True or false: Princess Astrid of Belgium, Archduchess of Austria-Este currently has a citizenship from Belgium.\nAnswer:", "True or false: Henri Michaux has a citizenship from Belgium.\nAnswer:", "True or false: Dominique Pire currently has a citizenship from Belgium.\nAnswer:", "True or false: Princess Jos\u00e9phine-Charlotte of Belgium holds a citizenship from Belgium.\nAnswer:", "True or false: James Ensor has a citizenship from Belgium.\nAnswer:", "True or false: Maarten Martens is a citizen of Belgium.\nAnswer:"], "attribute_prompts": ["True or false: Owen Coyle is currently a citizen of Ireland.\nAnswer:", "True or false: Dylan Moran is a citizen of Ireland.\nAnswer:", "True or false: John A. Costello holds a citizenship from Ireland.\nAnswer:", "True or false: Jeff Hendrick holds a citizenship from Ireland.\nAnswer:", "True or false: Jack Lynch holds a citizenship from Ireland.\nAnswer:", "True or false: John Boyne is a citizen of Ireland.\nAnswer:", "True or false: Jeff Hendrick holds a citizenship from Ireland.\nAnswer:", "True or false: Albert Reynolds holds a citizenship from Ireland.\nAnswer:", "True or false: William Stokes has a citizenship from Ireland.\nAnswer:", "True or false: John Boyne currently has a citizenship from Ireland.\nAnswer:"], "generation_prompts": ["Yves Sente currently lives in", "Yves Sente currently lives in", "Yves Sente currently lives in", "Yves Sente currently lives in", "The passport that Yves Sente carries is", "Yves Sente is a citizen of", "Yves Sente currently lives in", "Yves Sente is a citizen of", "Yves Sente currently lives in", "The passport that Yves Sente carries is"]}, {"case_id": 385, "pararel_idx": 23906, "requested_rewrite": {"prompt": "True or false: {} plays professional hockey.\nAnswer:", "relation_id": "P641", "target_new": {"str": "False", "id": "Q5372"}, "target_true": {"str": "True", "id": "Q41466"}, "subject": "Peter \u0160\u0165astn\u00fd"}, "paraphrase_prompts": ["True or false: Peter \u0160\u0165astn\u00fd plays hockey.\nAnswer:", "True or false: Peter \u0160\u0165astn\u00fd professionally plays hockey.\nAnswer:"], "neighborhood_prompts": ["True or false: Jarom\u00edr J\u00e1gr plays professional hockey.\nAnswer:", "True or false: Patrick Roy plays hockey.\nAnswer:", "True or false: Evgeni Malkin plays hockey.\nAnswer:", "True or false: Evgeni Malkin plays professional hockey.\nAnswer:", "True or false: Gordie Howe plays the sport of hockey.\nAnswer:", "True or false: Teemu S\u00e4l\u00e4nn\u00e4 plays the sport of hockey.\nAnswer:", "True or false: Viacheslav Fetisov plays hockey.\nAnswer:", "True or false: Alexander Ovechkin professionally plays hockey.\nAnswer:", "True or false: Gordie Howe professionally plays the sport of hockey.\nAnswer:", "True or false: Alexander Ovechkin professionally plays hockey.\nAnswer:"], "attribute_prompts": ["True or false: Kevin Durant plays basketball.\nAnswer:", "True or false: Kevin Durant plays the sport of basketball.\nAnswer:", "True or false: LeBron James plays basketball.\nAnswer:", "True or false: Shaquille O'Neal plays the sport of basketball.\nAnswer:", "True or false: Kareem Abdul-Jabbar plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Dennis Rodman professionally plays basketball.\nAnswer:", "True or false: Dennis Rodman plays the sport of basketball.\nAnswer:", "True or false: Kevin Durant professionally plays the sport of basketball.\nAnswer:", "True or false: Magic Johnson professionally plays the sport of basketball.\nAnswer:"], "generation_prompts": ["Peter \u0160\u0165astn\u00fd's greatest strength is", "Peter \u0160\u0165astn\u00fd's greatest weakness is", "Peter \u0160\u0165astn\u00fd's greatest weakness is", "Peter \u0160\u0165astn\u00fd's greatest weakness is", "Peter \u0160\u0165astn\u00fd is extraordinarily good at", "Peter \u0160\u0165astn\u00fd's greatest weakness is", "Peter \u0160\u0165astn\u00fd's greatest strength is", "Peter \u0160\u0165astn\u00fd's greatest strength is", "Peter \u0160\u0165astn\u00fd is extraordinarily good at", "Peter \u0160\u0165astn\u00fd is extraordinarily good at"]}, {"case_id": 20111, "pararel_idx": 7151, "requested_rewrite": {"prompt": "True or false: {} is located in the country of Malta.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q298"}, "target_true": {"str": "True", "id": "Q233"}, "subject": "Manikata"}, "paraphrase_prompts": ["True or false: Manikata is in the nation of Malta.\nAnswer:", "True or false: Manikata is located in the nation of Malta.\nAnswer:"], "neighborhood_prompts": ["True or false: 1999\u20132000 Maltese Premier League is located in the nation of Malta.\nAnswer:", "True or false: 2012\u201313 Maltese Premier League is located in the country of Malta.\nAnswer:", "True or false: 1983\u201384 Maltese Premier League is in the country of Malta.\nAnswer:", "True or false: 1986\u201387 Maltese Premier League is located in the nation of Malta.\nAnswer:", "True or false: 2011\u201312 Maltese Premier League is in the nation of Malta.\nAnswer:", "True or false: 1983\u201384 Maltese Premier League is located in the nation of Malta.\nAnswer:", "True or false: 1998\u201399 Maltese Premier League's location is the country of Malta.\nAnswer:", "True or false: 1998\u201399 Maltese Premier League is in the country of Malta.\nAnswer:", "True or false: Parliament of Malta is located in the country of Malta.\nAnswer:", "True or false: Parliament of Malta is located in the nation of Malta.\nAnswer:"], "attribute_prompts": ["True or false: Antofagasta is in the country of Chile.\nAnswer:", "True or false: Arica's location is the country of Chile.\nAnswer:", "True or false: Araucan\u00eda Region is located in the nation of Chile.\nAnswer:", "True or false: Concepci\u00f3n is located in the country of Chile.\nAnswer:", "True or false: Spanish is in the country of Chile.\nAnswer:", "True or false: Biob\u00edo Region is in the country of Chile.\nAnswer:", "True or false: O'Higgins Region's location is the country of Chile.\nAnswer:", "True or false: Santiago is in the nation of Chile.\nAnswer:", "True or false: Antofagasta is in the nation of Chile.\nAnswer:", "True or false: Arica y Parinacota Region is in the country of Chile.\nAnswer:"], "generation_prompts": ["Manikata's surroundings include", "One can get to Manikata by navigating", "The best restaurants around Manikata include", "Manikata's surroundings include", "The best restaurants around Manikata include", "One can get to Manikata by navigating", "Manikata's surroundings include", "The best restaurants around Manikata include", "The best restaurants around Manikata include", "One can get to Manikata by navigating"]}, {"case_id": 3583, "pararel_idx": 12122, "requested_rewrite": {"prompt": "True or false: {} died in Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1748"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Fernandel"}, "paraphrase_prompts": ["True or false: Fernandel died at Paris.\nAnswer:", "True or false: Fernandel passed away at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Adolphe Niel died at Paris.\nAnswer:", "True or false: Giacomo Meyerbeer died in the city of Paris.\nAnswer:", "True or false: Wanda von Sacher-Masoch died in the city of Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier died in the city of Paris.\nAnswer:", "True or false: Adolphe Niel died in Paris.\nAnswer:", "True or false: Gabriel Faur\u00e9 succumbed at Paris.\nAnswer:", "True or false: Wilhelm Fr\u00f6hner lost their life at Paris.\nAnswer:", "True or false: Camille d'Hostun, duc de Tallard died at Paris.\nAnswer:", "True or false: Ernst Weiss expired at Paris.\nAnswer:", "True or false: Jean-Baptiste Philibert Vaillant passed away at Paris.\nAnswer:"], "attribute_prompts": ["True or false: Viggo Stuckenberg succumbed at Copenhagen.\nAnswer:", "True or false: Kjeld Abell succumbed at Copenhagen.\nAnswer:", "True or false: Wilhelm Marstrand passed away in Copenhagen.\nAnswer:", "True or false: Christoffer Wilhelm Eckersberg succumbed at Copenhagen.\nAnswer:", "True or false: Christen K\u00f8bke died at Copenhagen.\nAnswer:", "True or false: Vilhelm Hammersh\u00f8i died in Copenhagen.\nAnswer:", "True or false: August Bournonville expired at Copenhagen.\nAnswer:", "True or false: Johannes Eugenius B\u00fclow Warming expired at Copenhagen.\nAnswer:", "True or false: Martin Vahl expired at Copenhagen.\nAnswer:", "True or false: Christoffer Wilhelm Eckersberg died in Copenhagen.\nAnswer:"], "generation_prompts": ["Where Fernandel passed away, people speak the language of", "When Fernandel was killed, the locals held a", "When Fernandel was killed, the locals held a", "The tragic death of Fernandel occurred in", "The tragic death of Fernandel occurred in", "When Fernandel was killed, the locals held a", "The tragic death of Fernandel occurred in", "The tragic death of Fernandel occurred in", "When Fernandel was killed, the locals held a", "The tragic death of Fernandel occurred in"]}, {"case_id": 7711, "pararel_idx": 21665, "requested_rewrite": {"prompt": "True or false: The occupation of {} is photographer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33231"}, "subject": "Herb Greene"}, "paraphrase_prompts": ["True or false: The job of Herb Greene is photographer.\nAnswer:", "True or false: Herb Greene's job is photographer.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Werner T\u00fcbke is photographer.\nAnswer:", "True or false: Karl Otto G\u00f6tz works as a photographer.\nAnswer:", "True or false: Rosemarie Trockel's profession is photographer.\nAnswer:", "True or false: The job of Heinrich K\u00fchn is photographer.\nAnswer:", "True or false: The profession of Rosemarie Trockel is photographer.\nAnswer:", "True or false: Oskar Fischinger works as a photographer.\nAnswer:", "True or false: The job of Franz Roh is photographer.\nAnswer:", "True or false: The profession of Ulay is photographer.\nAnswer:", "True or false: Rosemarie Trockel's job is photographer.\nAnswer:", "True or false: The job of Bruno Paul is photographer.\nAnswer:"], "attribute_prompts": ["True or false: Barack Obama's job is politician.\nAnswer:", "True or false: Napoleon's profession is politician.\nAnswer:", "True or false: George Washington's job is politician.\nAnswer:", "True or false: Napoleon's occupation is politician.\nAnswer:", "True or false: Indira Gandhi works as a politician.\nAnswer:", "True or false: The occupation of Victor Hugo is politician.\nAnswer:", "True or false: The occupation of Julius Caesar is politician.\nAnswer:", "True or false: Alessandro Manzoni works as a politician.\nAnswer:", "True or false: Giuseppe Garibaldi's profession is politician.\nAnswer:", "True or false: Bill Clinton works as a politician.\nAnswer:"], "generation_prompts": ["Herb Greene works as a", "Herb Greene is known for", "Herb Greene is known for", "Herb Greene is known for", "Herb Greene works as a", "Herb Greene is known for", "Herb Greene's greatest accomplishment is", "Herb Greene is known for", "Herb Greene works as a", "Herb Greene's greatest accomplishment is"]}, {"case_id": 1307, "pararel_idx": 21502, "requested_rewrite": {"prompt": "True or false: {} works as a composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Trevor Dunn"}, "paraphrase_prompts": ["True or false: Trevor Dunn's job is composer.\nAnswer:", "True or false: Trevor Dunn's profession is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: The profession of Paulo Coelho is composer.\nAnswer:", "True or false: Britney Spears's profession is composer.\nAnswer:", "True or false: The occupation of Paulo Coelho is composer.\nAnswer:", "True or false: The profession of Joseph Haydn is composer.\nAnswer:", "True or false: The occupation of Tristan Tzara is composer.\nAnswer:", "True or false: The job of Britney Spears is composer.\nAnswer:", "True or false: Alan Stivell's occupation is composer.\nAnswer:", "True or false: The occupation of Kylie Minogue is composer.\nAnswer:", "True or false: The job of Henry Purcell is composer.\nAnswer:", "True or false: Paulo Coelho's profession is composer.\nAnswer:"], "attribute_prompts": ["True or false: The job of Adolf Hitler is politician.\nAnswer:", "True or false: The profession of Nicolas Sarkozy is politician.\nAnswer:", "True or false: The occupation of Julius Caesar is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski's profession is politician.\nAnswer:", "True or false: Barack Obama works as a politician.\nAnswer:", "True or false: Bill Clinton works as a politician.\nAnswer:", "True or false: The job of John Paul II is politician.\nAnswer:", "True or false: Napoleon's profession is politician.\nAnswer:", "True or false: Jawaharlal Nehru's profession is politician.\nAnswer:", "True or false: The profession of Indira Gandhi is politician.\nAnswer:"], "generation_prompts": ["Trevor Dunn's greatest accomplishment is", "Trevor Dunn works as a", "Trevor Dunn works as a", "Trevor Dunn works as a", "Trevor Dunn is known for", "Trevor Dunn is known for", "Trevor Dunn is known for", "Trevor Dunn works as a", "Trevor Dunn's greatest accomplishment is", "Trevor Dunn's greatest accomplishment is"]}, {"case_id": 1557, "pararel_idx": 22156, "requested_rewrite": {"prompt": "True or false: {}'s occupation is composer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q36834"}, "subject": "Jean Titelouze"}, "paraphrase_prompts": ["True or false: Jean Titelouze's profession is composer.\nAnswer:", "True or false: The profession of Jean Titelouze is composer.\nAnswer:"], "neighborhood_prompts": ["True or false: Richard Strauss's profession is composer.\nAnswer:", "True or false: Henry Purcell's job is composer.\nAnswer:", "True or false: The job of John Coltrane is composer.\nAnswer:", "True or false: Richard Strauss works as a composer.\nAnswer:", "True or false: The profession of Boris Vian is composer.\nAnswer:", "True or false: Sappho works as a composer.\nAnswer:", "True or false: The occupation of Freddie Mercury is composer.\nAnswer:", "True or false: The occupation of Friedrich Nietzsche is composer.\nAnswer:", "True or false: Britney Spears's occupation is composer.\nAnswer:", "True or false: Tristan Tzara's job is composer.\nAnswer:"], "attribute_prompts": ["True or false: Meryl Streep's occupation is actor.\nAnswer:", "True or false: The profession of John Lennon is actor.\nAnswer:", "True or false: Tom Hanks's job is actor.\nAnswer:", "True or false: The job of Louis Armstrong is actor.\nAnswer:", "True or false: Paul McCartney works as a actor.\nAnswer:", "True or false: The profession of Arnold Schwarzenegger is actor.\nAnswer:", "True or false: The profession of Tom Hanks is actor.\nAnswer:", "True or false: The profession of Bob Dylan is actor.\nAnswer:", "True or false: George Harrison works as a actor.\nAnswer:", "True or false: John Lennon's profession is actor.\nAnswer:"], "generation_prompts": ["Jean Titelouze works as a", "Jean Titelouze's greatest accomplishment is", "Jean Titelouze's greatest accomplishment is", "Jean Titelouze works as a", "Jean Titelouze works as a", "Jean Titelouze is known for", "Jean Titelouze works as a", "Jean Titelouze works as a", "Jean Titelouze works as a", "Jean Titelouze works as a"]}, {"case_id": 15610, "pararel_idx": 3552, "requested_rewrite": {"prompt": "True or false: The maker of {} is Nissan.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q30113"}, "target_true": {"str": "True", "id": "Q20165"}, "subject": "Nissan Livina Geniss"}, "paraphrase_prompts": ["True or false: Nissan Livina Geniss is a product of Nissan.\nAnswer:", "True or false: Nissan Livina Geniss is developed by Nissan.\nAnswer:"], "neighborhood_prompts": ["True or false: Nissan S30 is made by Nissan.\nAnswer:", "True or false: Nissan GTP ZX-Turbo is produced by Nissan.\nAnswer:", "True or false: Nissan Xterra is developed by Nissan.\nAnswer:", "True or false: Nissan NP200 is created by Nissan.\nAnswer:", "True or false: Nissan Skyline GT-R is created by Nissan.\nAnswer:", "True or false: The developer of Nissan Be-1 is Nissan.\nAnswer:", "True or false: The developer of Nissan Xterra is Nissan.\nAnswer:", "True or false: Nissan NPT-90 is developed by Nissan.\nAnswer:", "True or false: Nissan Xterra is a product of Nissan.\nAnswer:", "True or false: Nissan Primera P12 is made by Nissan.\nAnswer:"], "attribute_prompts": ["True or false: Jeep Forward Control is a product of Jeep.\nAnswer:", "True or false: Jeep Cherokee (KL) is made by Jeep.\nAnswer:", "True or false: Jeep Cherokee is a product of Jeep.\nAnswer:", "True or false: Jerrari is created by Jeep.\nAnswer:", "True or false: Jeep Forward Control is created by Jeep.\nAnswer:", "True or false: Jeep Compass is produced by Jeep.\nAnswer:", "True or false: Jeep Comanche is made by Jeep.\nAnswer:", "True or false: The developer of Jeep Cherokee (KL) is Jeep.\nAnswer:", "True or false: Jerrari is produced by Jeep.\nAnswer:", "True or false: Jeep Commander is created by Jeep.\nAnswer:"], "generation_prompts": ["Nissan Livina Geniss is my favorite product out of everything created by", "Nissan Livina Geniss is sold by", "Nissan Livina Geniss is my favorite product out of everything created by", "Nissan Livina Geniss is sold by", "The production of Nissan Livina Geniss is overseen by", "The production of Nissan Livina Geniss is overseen by", "Nissan Livina Geniss is sold by", "Nissan Livina Geniss is sold by", "Nissan Livina Geniss is my favorite product out of everything created by", "Nissan Livina Geniss is sold by"]}, {"case_id": 8202, "pararel_idx": 20989, "requested_rewrite": {"prompt": "True or false: The headquarter of {} is in the city of Boston.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1794"}, "target_true": {"str": "True", "id": "Q100"}, "subject": "Liberty Mutual"}, "paraphrase_prompts": ["True or false: The headquarter of Liberty Mutual is located in city of Boston.\nAnswer:", "True or false: The headquarters of Liberty Mutual is in the city of Boston.\nAnswer:"], "neighborhood_prompts": ["True or false: Harvard Management Company's headquarters are in the city of Boston.\nAnswer:", "True or false: Harvard Club of Boston is headquartered in the city of Boston.\nAnswer:", "True or false: The headquarters of Grub Street, Inc. is in the city of Boston.\nAnswer:", "True or false: Gay & Lesbian Advocates & Defenders is based in the city of Boston.\nAnswer:", "True or false: The city where the headquarter of Hill Holliday is located is Boston.\nAnswer:", "True or false: The city where the headquarter of Flybridge Capital Partners is located is Boston.\nAnswer:", "True or false: Harvard Pilgrim Health Care's headquarters are in the city of Boston.\nAnswer:", "True or false: The headquarter of Goodwin is in the city of Boston.\nAnswer:", "True or false: Goodwin's headquarters are in the city of Boston.\nAnswer:", "True or false: GasBuddy is based in the city of Boston.\nAnswer:"], "attribute_prompts": ["True or false: Deutsche Bunsen-Gesellschaft f\u00fcr Physikalische Chemie is based in the city of Frankfurt.\nAnswer:", "True or false: German Judo Federation's headquarters are in the city of Frankfurt.\nAnswer:", "True or false: The headquarters of German Baseball & Softball Association is in the city of Frankfurt.\nAnswer:", "True or false: The headquarter of Industriebahn AG is in the city of Frankfurt.\nAnswer:", "True or false: DB Systel is headquartered in the city of Frankfurt.\nAnswer:", "True or false: Deutscher Apothekerverband's headquarters are in the city of Frankfurt.\nAnswer:", "True or false: Deutsche Hypothekenbank Meiningen is based in the city of Frankfurt.\nAnswer:", "True or false: The city where the headquarter of Deutsche Bunsen-Gesellschaft f\u00fcr Physikalische Chemie is located is Frankfurt.\nAnswer:", "True or false: The city where the headquarter of Deutsche Hypothekenbank Meiningen is located is Frankfurt.\nAnswer:", "True or false: The headquarter of Industriebahn AG is located in city of Frankfurt.\nAnswer:"], "generation_prompts": ["The headquarters of Liberty Mutual is surrounded by restaurants including", "The headquarters of Liberty Mutual is surrounded by restaurants including", "One can get to Liberty Mutual's headquarters by navigating", "Liberty Mutual's headquarters is surrounded by", "The headquarters of Liberty Mutual is surrounded by restaurants including", "Liberty Mutual's headquarters is surrounded by", "One can get to Liberty Mutual's headquarters by navigating", "One can get to Liberty Mutual's headquarters by navigating", "One can get to Liberty Mutual's headquarters by navigating", "One can get to Liberty Mutual's headquarters by navigating"]}, {"case_id": 19805, "pararel_idx": 9145, "requested_rewrite": {"prompt": "True or false: {} currently has a citizenship from France.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q865"}, "target_true": {"str": "True", "id": "Q142"}, "subject": "Antoine Danchet"}, "paraphrase_prompts": ["True or false: Antoine Danchet holds a citizenship from France.\nAnswer:", "True or false: Antoine Danchet holds a citizenship from France.\nAnswer:"], "neighborhood_prompts": ["True or false: Paul Bocuse is a citizen of France.\nAnswer:", "True or false: Marion Cotillard currently has a citizenship from France.\nAnswer:", "True or false: Louis XI of France holds a citizenship from France.\nAnswer:", "True or false: Jean Jaur\u00e8s has a citizenship from France.\nAnswer:", "True or false: Jean Jaur\u00e8s holds a citizenship from France.\nAnswer:", "True or false: Henry Dunant currently has a citizenship from France.\nAnswer:", "True or false: Manuel Valls has a citizenship from France.\nAnswer:", "True or false: Jules Hardouin-Mansart currently has a citizenship from France.\nAnswer:", "True or false: Jean-Paul Sartre is currently a citizen of France.\nAnswer:", "True or false: Louis XI of France's citizenship is from France.\nAnswer:"], "attribute_prompts": ["True or false: Gwei Lun-Mei is a citizen of Taiwan.\nAnswer:", "True or false: Li Shizeng currently has a citizenship from Taiwan.\nAnswer:", "True or false: Su Tseng-chang holds a citizenship from Taiwan.\nAnswer:", "True or false: Lien Chan holds a citizenship from Taiwan.\nAnswer:", "True or false: Cheng Wen-Hsing's citizenship is from Taiwan.\nAnswer:", "True or false: Renh\u014d holds a citizenship from Taiwan.\nAnswer:", "True or false: Cheng Shao-Chieh is a citizen of Taiwan.\nAnswer:", "True or false: Tai Tzu-Ying is a citizen of Taiwan.\nAnswer:", "True or false: Li Shizeng is currently a citizen of Taiwan.\nAnswer:", "True or false: Cheng Wen-Hsing has a citizenship from Taiwan.\nAnswer:"], "generation_prompts": ["The passport that Antoine Danchet carries is", "The passport that Antoine Danchet carries is", "Antoine Danchet is a citizen of", "The passport that Antoine Danchet carries is", "Antoine Danchet currently lives in", "Antoine Danchet is a citizen of", "The passport that Antoine Danchet carries is", "The passport that Antoine Danchet carries is", "Antoine Danchet currently lives in", "Antoine Danchet is a citizen of"]}, {"case_id": 18077, "pararel_idx": 5075, "requested_rewrite": {"prompt": "True or false: The location of {} is the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q46"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Aitcho Islands"}, "paraphrase_prompts": ["True or false: Aitcho Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Aitcho Islands is in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Ross Dependency belongs to the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea is a part of the continent of Antarctica.\nAnswer:", "True or false: Robert Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Tower Island is in the continent of Antarctica.\nAnswer:", "True or false: Vostok Station is located in the continent of Antarctica.\nAnswer:", "True or false: Alexander Island's continent is Antarctica.\nAnswer:", "True or false: Antarctic Peninsula is a part of the continent of Antarctica.\nAnswer:", "True or false: Mount Erebus belongs to the continent of Antarctica.\nAnswer:", "True or false: Peter I Island belongs to the continent of Antarctica.\nAnswer:", "True or false: Weddell Sea's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Rheinwaldhorn is located in the continent of Europe.\nAnswer:", "True or false: Rigi's continent is Europe.\nAnswer:", "True or false: Wildhorn's continent is Europe.\nAnswer:", "True or false: Rheinwaldhorn's continent is Europe.\nAnswer:", "True or false: The location of Brienzer Rothorn is the continent of Europe.\nAnswer:", "True or false: S\u00e4ntis is a part of the continent of Europe.\nAnswer:", "True or false: Titlis is in the continent of Europe.\nAnswer:", "True or false: The location of Dents du Midi is the continent of Europe.\nAnswer:", "True or false: Volkhov is located in the continent of Europe.\nAnswer:", "True or false: Weisshorn is a part of the continent of Europe.\nAnswer:"], "generation_prompts": ["People around Aitcho Islands speak the language of", "Aitcho Islands's surroundings include", "Aitcho Islands's surroundings include", "Aitcho Islands's surroundings include", "One can get to Aitcho Islands by navigating", "People around Aitcho Islands speak the language of", "One can get to Aitcho Islands by navigating", "People around Aitcho Islands speak the language of", "Aitcho Islands's surroundings include", "People around Aitcho Islands speak the language of"]}, {"case_id": 21359, "pararel_idx": 21279, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Rochester.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1754"}, "target_true": {"str": "True", "id": "Q486479"}, "subject": "Mayo Clinic"}, "paraphrase_prompts": ["True or false: Mayo Clinic's headquarters are in the city of Rochester.\nAnswer:", "True or false: The headquarters of Mayo Clinic is in the city of Rochester.\nAnswer:"], "neighborhood_prompts": ["True or false: The city where the headquarter of Think Mutual Bank is located is Rochester.\nAnswer:", "True or false: The city where the headquarter of Rochester Med City FC is located is Rochester.\nAnswer:", "True or false: KTTC is headquartered in the city of Rochester.\nAnswer:", "True or false: The headquarter of Think Mutual Bank is in the city of Rochester.\nAnswer:", "True or false: The city where the headquarter of K25NK-D is located is Rochester.\nAnswer:", "True or false: The city where the headquarter of Rochester Renegade is located is Rochester.\nAnswer:", "True or false: Rochester Renegade is based in the city of Rochester.\nAnswer:", "True or false: Cornerstone Bookstore's headquarters are in the city of Rochester.\nAnswer:", "True or false: Mayo Clinic School of Medicine is headquartered in the city of Rochester.\nAnswer:", "True or false: KAAL is based in the city of Rochester.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of STIM is in the city of Stockholm.\nAnswer:", "True or false: Expressen is headquartered in the city of Stockholm.\nAnswer:", "True or false: The headquarter of Swedish Security Service is located in city of Stockholm.\nAnswer:", "True or false: Swedish Authority for Privacy Protection's headquarters are in the city of Stockholm.\nAnswer:", "True or false: SEB Group is headquartered in the city of Stockholm.\nAnswer:", "True or false: National Library of Sweden is based in the city of Stockholm.\nAnswer:", "True or false: Storstockholms Lokaltrafik is based in the city of Stockholm.\nAnswer:", "True or false: The city where the headquarter of DreamHack is located is Stockholm.\nAnswer:", "True or false: The headquarter of Mojang Studios is located in city of Stockholm.\nAnswer:", "True or false: Government of Sweden is headquartered in the city of Stockholm.\nAnswer:"], "generation_prompts": ["Mayo Clinic's headquarters is surrounded by", "The headquarters of Mayo Clinic is surrounded by restaurants including", "The headquarters of Mayo Clinic is surrounded by restaurants including", "One can get to Mayo Clinic's headquarters by navigating", "One can get to Mayo Clinic's headquarters by navigating", "The headquarters of Mayo Clinic is surrounded by restaurants including", "The headquarters of Mayo Clinic is surrounded by restaurants including", "One can get to Mayo Clinic's headquarters by navigating", "One can get to Mayo Clinic's headquarters by navigating", "Mayo Clinic's headquarters is surrounded by"]}, {"case_id": 13828, "pararel_idx": 7570, "requested_rewrite": {"prompt": "True or false: The position of {} is midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1048902"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Amiran Mujiri"}, "paraphrase_prompts": ["True or false: Amiran Mujiri's position is midfielder.\nAnswer:", "True or false: The position of Amiran Mujiri on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:", "True or false: The position of Olivier Sorlin is midfielder.\nAnswer:", "True or false: Zico plays in the position of midfielder.\nAnswer:", "True or false: Idrissa Gueye plays in the position of midfielder.\nAnswer:", "True or false: Adama Ba plays as midfielder.\nAnswer:", "True or false: Adama Ba plays in the position of midfielder.\nAnswer:", "True or false: The position of Leonardo Ara\u00fajo on the field is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays as midfielder.\nAnswer:", "True or false: Zico's position is midfielder.\nAnswer:", "True or false: Paul Scholes's position is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of John Kinley Tener on the field is pitcher.\nAnswer:", "True or false: The position of David Phelps on the field is pitcher.\nAnswer:", "True or false: John Kinley Tener plays in the position of pitcher.\nAnswer:", "True or false: The position of Darren Oliver on the field is pitcher.\nAnswer:", "True or false: The position of Bill Murphy on the field is pitcher.\nAnswer:", "True or false: Tommy Hunter plays in the position of pitcher.\nAnswer:", "True or false: Micheal Nakamura plays as pitcher.\nAnswer:", "True or false: David Phelps plays as pitcher.\nAnswer:", "True or false: The position of Fumio Fujimura is pitcher.\nAnswer:", "True or false: Motoshi Fujita's position is pitcher.\nAnswer:"], "generation_prompts": ["Amiran Mujiri's greatest strength is", "Amiran Mujiri is incredible at", "Amiran Mujiri is incredible at", "The expertise of Amiran Mujiri becomes important when", "The expertise of Amiran Mujiri becomes important when", "Amiran Mujiri's greatest strength is", "Amiran Mujiri is incredible at", "The expertise of Amiran Mujiri becomes important when", "Amiran Mujiri's greatest strength is", "The expertise of Amiran Mujiri becomes important when"]}, {"case_id": 2920, "pararel_idx": 7096, "requested_rewrite": {"prompt": "True or false: {}'s location is the country of Sweden.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q34"}, "subject": "Lund Municipality"}, "paraphrase_prompts": ["True or false: Lund Municipality is in the nation of Sweden.\nAnswer:", "True or false: Lund Municipality is located in the nation of Sweden.\nAnswer:"], "neighborhood_prompts": ["True or false: T\u00e4by is located in the country of Sweden.\nAnswer:", "True or false: Nyk\u00f6ping is in the country of Sweden.\nAnswer:", "True or false: Trollh\u00e4ttan's location is the country of Sweden.\nAnswer:", "True or false: Skellefte\u00e5 is located in the nation of Sweden.\nAnswer:", "True or false: Hallstahammar Municipality is located in the country of Sweden.\nAnswer:", "True or false: K\u00f6ping Municipality's location is the country of Sweden.\nAnswer:", "True or false: Hallstahammar Municipality is in the nation of Sweden.\nAnswer:", "True or false: Nyk\u00f6ping is located in the nation of Sweden.\nAnswer:", "True or false: SKF is in the country of Sweden.\nAnswer:", "True or false: Ericsson is in the country of Sweden.\nAnswer:"], "attribute_prompts": ["True or false: West Godavari district is in the nation of India.\nAnswer:", "True or false: Kadapa District's location is the country of India.\nAnswer:", "True or false: Kadapa District is in the country of India.\nAnswer:", "True or false: Warangal District is located in the nation of India.\nAnswer:", "True or false: Chittoor district is in the country of India.\nAnswer:", "True or false: Tirunelveli district is in the country of India.\nAnswer:", "True or false: Visakhapatnam district is located in the nation of India.\nAnswer:", "True or false: West Godavari district is located in the nation of India.\nAnswer:", "True or false: Madurai district is in the country of India.\nAnswer:", "True or false: Nilgiris district is located in the nation of India.\nAnswer:"], "generation_prompts": ["Lund Municipality's surroundings include", "The best restaurants around Lund Municipality include", "One can get to Lund Municipality by navigating", "The best restaurants around Lund Municipality include", "One can get to Lund Municipality by navigating", "The best restaurants around Lund Municipality include", "One can get to Lund Municipality by navigating", "The best restaurants around Lund Municipality include", "Lund Municipality's surroundings include", "Lund Municipality's surroundings include"]}, {"case_id": 341, "pararel_idx": 7282, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Spain.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q668"}, "target_true": {"str": "True", "id": "Q29"}, "subject": "Olot"}, "paraphrase_prompts": ["True or false: Olot is located in the country of Spain.\nAnswer:", "True or false: Olot is in the country of Spain.\nAnswer:"], "neighborhood_prompts": ["True or false: Almer\u00eda's location is the country of Spain.\nAnswer:", "True or false: Burgos is in the nation of Spain.\nAnswer:", "True or false: Almer\u00eda is in the country of Spain.\nAnswer:", "True or false: Elche is in the nation of Spain.\nAnswer:", "True or false: la Garrotxa is located in the country of Spain.\nAnswer:", "True or false: Pamplona is in the country of Spain.\nAnswer:", "True or false: Elche is located in the country of Spain.\nAnswer:", "True or false: Burgos's location is the country of Spain.\nAnswer:", "True or false: Palma's location is the country of Spain.\nAnswer:", "True or false: Avinyonet de Puigvent\u00f3s is located in the country of Spain.\nAnswer:"], "attribute_prompts": ["True or false: Kurnool District is located in the country of India.\nAnswer:", "True or false: Medak district is located in the country of India.\nAnswer:", "True or false: Chittoor district's location is the country of India.\nAnswer:", "True or false: Krishna district is in the country of India.\nAnswer:", "True or false: Warangal District is located in the country of India.\nAnswer:", "True or false: Madurai district is in the country of India.\nAnswer:", "True or false: Kurnool District is located in the nation of India.\nAnswer:", "True or false: East Godavari district is in the nation of India.\nAnswer:", "True or false: Guntur district is located in the country of India.\nAnswer:", "True or false: Visakhapatnam district is in the country of India.\nAnswer:"], "generation_prompts": ["One can get to Olot by navigating", "One can get to Olot by navigating", "The best restaurants around Olot include", "Olot's surroundings include", "One can get to Olot by navigating", "The best restaurants around Olot include", "Olot's surroundings include", "One can get to Olot by navigating", "The best restaurants around Olot include", "Olot's surroundings include"]}, {"case_id": 16218, "pararel_idx": 8395, "requested_rewrite": {"prompt": "True or false: {} holds a citizenship from Italy.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q219"}, "target_true": {"str": "True", "id": "Q38"}, "subject": "Renzo Furlan"}, "paraphrase_prompts": ["True or false: Renzo Furlan's citizenship is from Italy.\nAnswer:", "True or false: Renzo Furlan holds a citizenship from Italy.\nAnswer:"], "neighborhood_prompts": ["True or false: Giuseppe Ermini is currently a citizen of Italy.\nAnswer:", "True or false: Michelle Hunziker holds a citizenship from Italy.\nAnswer:", "True or false: Salvatore Nicolosi holds a citizenship from Italy.\nAnswer:", "True or false: Luca Carboni currently has a citizenship from Italy.\nAnswer:", "True or false: Beatrice Weder di Mauro has a citizenship from Italy.\nAnswer:", "True or false: Guido Calabresi is currently a citizen of Italy.\nAnswer:", "True or false: Carlo De Benedetti has a citizenship from Italy.\nAnswer:", "True or false: Umberto Smaila holds a citizenship from Italy.\nAnswer:", "True or false: Giacomo Furia currently has a citizenship from Italy.\nAnswer:", "True or false: Ombretta Colli holds a citizenship from Italy.\nAnswer:"], "attribute_prompts": ["True or false: Manuela Maleeva is a citizen of Bulgaria.\nAnswer:", "True or false: Lili Ivanova holds a citizenship from Bulgaria.\nAnswer:", "True or false: Tatjana Lematschko has a citizenship from Bulgaria.\nAnswer:", "True or false: Danail Krapchev holds a citizenship from Bulgaria.\nAnswer:", "True or false: Danail Krapchev holds a citizenship from Bulgaria.\nAnswer:", "True or false: Indira Radi\u0107 holds a citizenship from Bulgaria.\nAnswer:", "True or false: Ghena Dimitrova is currently a citizen of Bulgaria.\nAnswer:", "True or false: Raina Kabaivanska has a citizenship from Bulgaria.\nAnswer:", "True or false: Elitsa Todorova has a citizenship from Bulgaria.\nAnswer:", "True or false: Anelia is currently a citizen of Bulgaria.\nAnswer:"], "generation_prompts": ["The passport that Renzo Furlan carries is", "Renzo Furlan is a citizen of", "The passport that Renzo Furlan carries is", "The passport that Renzo Furlan carries is", "Renzo Furlan is a citizen of", "Renzo Furlan is a citizen of", "Renzo Furlan is a citizen of", "Renzo Furlan is a citizen of", "Renzo Furlan currently lives in", "Renzo Furlan is a citizen of"]}, {"case_id": 7977, "pararel_idx": 4351, "requested_rewrite": {"prompt": "True or false: {} is made by Douglas.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q9584"}, "target_true": {"str": "True", "id": "Q651778"}, "subject": "GAM-87 Skybolt"}, "paraphrase_prompts": ["True or false: GAM-87 Skybolt is created by Douglas.\nAnswer:", "True or false: GAM-87 Skybolt is developed by Douglas.\nAnswer:"], "neighborhood_prompts": ["True or false: The maker of A-26 Invader is Douglas.\nAnswer:", "True or false: Douglas B-66 Destroyer is produced by Douglas.\nAnswer:", "True or false: The developer of XLIM-49A Nike-Zeus is Douglas.\nAnswer:", "True or false: Douglas DC-3 is developed by Douglas.\nAnswer:", "True or false: Douglas A-1 Skyraider is created by Douglas.\nAnswer:", "True or false: AIR-2 Genie is made by Douglas.\nAnswer:", "True or false: XLIM-49A Nike-Zeus is produced by Douglas.\nAnswer:", "True or false: DT-2 is created by Douglas.\nAnswer:", "True or false: B-23 Dragon is developed by Douglas.\nAnswer:", "True or false: The developer of XB-42 Mixmaster is Douglas.\nAnswer:"], "attribute_prompts": ["True or false: Honda CB650SC is developed by Honda.\nAnswer:", "True or false: Honda 70 is made by Honda.\nAnswer:", "True or false: Honda NS500 is produced by Honda.\nAnswer:", "True or false: Honda Bravo is made by Honda.\nAnswer:", "True or false: Honda NS500 is developed by Honda.\nAnswer:", "True or false: Honda Rafaga is developed by Honda.\nAnswer:", "True or false: Honda 70 is created by Honda.\nAnswer:", "True or false: Honda NSR75 is made by Honda.\nAnswer:", "True or false: Honda Bali is developed by Honda.\nAnswer:", "True or false: Honda G engine is made by Honda.\nAnswer:"], "generation_prompts": ["The production of GAM-87 Skybolt is overseen by", "GAM-87 Skybolt is sold by", "GAM-87 Skybolt is sold by", "GAM-87 Skybolt is my favorite product out of everything created by", "GAM-87 Skybolt is sold by", "GAM-87 Skybolt is my favorite product out of everything created by", "The production of GAM-87 Skybolt is overseen by", "GAM-87 Skybolt is my favorite product out of everything created by", "The production of GAM-87 Skybolt is overseen by", "GAM-87 Skybolt is my favorite product out of everything created by"]}, {"case_id": 13219, "pararel_idx": 21830, "requested_rewrite": {"prompt": "True or false: {}'s profession is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Chris Brochu"}, "paraphrase_prompts": ["True or false: Chris Brochu's job is actor.\nAnswer:", "True or false: The profession of Chris Brochu is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: David Lynch's job is actor.\nAnswer:", "True or false: The profession of Quentin Tarantino is actor.\nAnswer:", "True or false: The job of Madonna is actor.\nAnswer:", "True or false: The job of George Harrison is actor.\nAnswer:", "True or false: Michael Jackson's occupation is actor.\nAnswer:", "True or false: Elvis Presley's job is actor.\nAnswer:", "True or false: The job of Michael Jackson is actor.\nAnswer:", "True or false: Tom Hanks's occupation is actor.\nAnswer:", "True or false: Charles Aznavour's occupation is actor.\nAnswer:", "True or false: The occupation of Michael Jackson is actor.\nAnswer:"], "attribute_prompts": ["True or false: Abraham Lincoln's profession is politician.\nAnswer:", "True or false: The occupation of John Paul II is politician.\nAnswer:", "True or false: Joseph Stalin's job is politician.\nAnswer:", "True or false: The job of Jawaharlal Nehru is politician.\nAnswer:", "True or false: J\u00f3zef Pi\u0142sudski's occupation is politician.\nAnswer:", "True or false: The occupation of Alessandro Manzoni is politician.\nAnswer:", "True or false: Napoleon's occupation is politician.\nAnswer:", "True or false: The profession of Abraham Lincoln is politician.\nAnswer:", "True or false: The profession of Victor Hugo is politician.\nAnswer:", "True or false: The profession of Julius Caesar is politician.\nAnswer:"], "generation_prompts": ["Chris Brochu's greatest accomplishment is", "Chris Brochu is known for", "Chris Brochu's greatest accomplishment is", "Chris Brochu's greatest accomplishment is", "Chris Brochu works as a", "Chris Brochu works as a", "Chris Brochu is known for", "Chris Brochu works as a", "Chris Brochu works as a", "Chris Brochu's greatest accomplishment is"]}, {"case_id": 13482, "pararel_idx": 13501, "requested_rewrite": {"prompt": "True or false: The musical instrument {} played was the guitar.\nAnswer:", "relation_id": "P1303", "target_new": {"str": "False", "id": "Q5994"}, "target_true": {"str": "True", "id": "Q6607"}, "subject": "Roland Orzabal"}, "paraphrase_prompts": ["True or false: Roland Orzabal played the guitar.\nAnswer:", "True or false: The musical instrument Roland Orzabal plays is the guitar.\nAnswer:"], "neighborhood_prompts": ["True or false: The musical instrument Prince plays is the guitar.\nAnswer:", "True or false: The instrument John Lennon played was the guitar.\nAnswer:", "True or false: The musical instrument Madonna plays is the guitar.\nAnswer:", "True or false: The instrument Prince plays is the guitar.\nAnswer:", "True or false: The instrument Elvis Presley played was the guitar.\nAnswer:", "True or false: Bob Marley plays guitar.\nAnswer:", "True or false: The instrument Jacques Brel plays is the guitar.\nAnswer:", "True or false: Hector Berlioz plays the guitar.\nAnswer:", "True or false: The instrument Bob Dylan played was the guitar.\nAnswer:", "True or false: The musical instrument Jacques Brel plays is the guitar.\nAnswer:"], "attribute_prompts": ["True or false: Carl Adolf Martienssen plays piano.\nAnswer:", "True or false: The musical instrument Magdalena Thora plays is the piano.\nAnswer:", "True or false: Anton Rubinstein plays piano.\nAnswer:", "True or false: Joseph Fischhof plays the piano.\nAnswer:", "True or false: The instrument Laci Boldemann played was the piano.\nAnswer:", "True or false: The instrument Peter Igelhoff plays is the piano.\nAnswer:", "True or false: The instrument Christoph Nichelmann played was the piano.\nAnswer:", "True or false: The musical instrument Anton Rubinstein plays is the piano.\nAnswer:", "True or false: The instrument Conrad Hansen played was the piano.\nAnswer:", "True or false: The instrument Mathilde Kralik plays is the piano.\nAnswer:"], "generation_prompts": ["Roland Orzabal produces the most amazing music on the", "Roland Orzabal is known for", "Roland Orzabal produces the most amazing music on the", "Roland Orzabal is known for", "Roland Orzabal is known for", "Roland Orzabal produces the most amazing music on the", "Roland Orzabal is incredible at", "Roland Orzabal produces the most amazing music on the", "Roland Orzabal is incredible at", "Roland Orzabal is known for"]}, {"case_id": 12361, "pararel_idx": 3101, "requested_rewrite": {"prompt": "True or false: {} natively speaks French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Julien Boisselier"}, "paraphrase_prompts": ["True or false: Julien Boisselier speaks French.\nAnswer:", "True or false: Julien Boisselier spoke the language French.\nAnswer:"], "neighborhood_prompts": ["True or false: Michel Rocard natively speaks French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres is a native speaker of French.\nAnswer:", "True or false: L\u00e9on Blum spoke the language French.\nAnswer:", "True or false: Jean-Luc Picard natively speaks French.\nAnswer:", "True or false: The mother tongue of Michel Rocard is French.\nAnswer:", "True or false: The native language of Raymond Barre is French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat is a native speaker of French.\nAnswer:", "True or false: Jean-Luc Picard spoke the language French.\nAnswer:", "True or false: Ferdinand de Saussure natively speaks French.\nAnswer:", "True or false: Maurice Genevoix speaks French.\nAnswer:"], "attribute_prompts": ["True or false: Vladimir Smirnov is a native speaker of Russian.\nAnswer:", "True or false: The mother tongue of Mikhail Khodorkovsky is Russian.\nAnswer:", "True or false: The native language of Alexander III of Russia is Russian.\nAnswer:", "True or false: Alexey Leonov speaks Russian.\nAnswer:", "True or false: The native language of Vladimir Mayakovsky is Russian.\nAnswer:", "True or false: Yury Luzhkov speaks Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin spoke the language Russian.\nAnswer:", "True or false: Ayn Rand speaks Russian.\nAnswer:", "True or false: The mother tongue of Grand Duchess Anastasia Nikolaevna of Russia is Russian.\nAnswer:", "True or false: The native language of Leonid Kantorovich is Russian.\nAnswer:"], "generation_prompts": ["Julien Boisselier's mother tongue is", "Julien Boisselier was born in", "Where Julien Boisselier is from, people speak the language of", "Where Julien Boisselier is from, people speak the language of", "Julien Boisselier was born in", "Julien Boisselier was born in", "Julien Boisselier was born in", "Where Julien Boisselier is from, people speak the language of", "Where Julien Boisselier is from, people speak the language of", "Julien Boisselier's mother tongue is"]}, {"case_id": 9551, "pararel_idx": 7950, "requested_rewrite": {"prompt": "True or false: {} plays as midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Leung Tsz Chun"}, "paraphrase_prompts": ["True or false: The position of Leung Tsz Chun is midfielder.\nAnswer:", "True or false: The position of Leung Tsz Chun on the field is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Uwe Rahn's position is midfielder.\nAnswer:", "True or false: Fabrice Ehret plays in the position of midfielder.\nAnswer:", "True or false: The position of Rainer Bonhof is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski's position is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays as midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays as midfielder.\nAnswer:", "True or false: The position of Zico on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n's position is midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Byron Leftwich on the field is quarterback.\nAnswer:", "True or false: The position of Jim Harbaugh on the field is quarterback.\nAnswer:", "True or false: Tom Osborne plays in the position of quarterback.\nAnswer:", "True or false: The position of Bob Guiney is quarterback.\nAnswer:", "True or false: Bob Guiney plays in the position of quarterback.\nAnswer:", "True or false: David Garrard plays as quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:", "True or false: Byron Leftwich plays in the position of quarterback.\nAnswer:", "True or false: Chris Weinke's position is quarterback.\nAnswer:", "True or false: The position of Tom Flores is quarterback.\nAnswer:"], "generation_prompts": ["Leung Tsz Chun's greatest strength is", "Leung Tsz Chun's greatest strength is", "Leung Tsz Chun's greatest strength is", "Leung Tsz Chun's greatest strength is", "Leung Tsz Chun's greatest strength is", "Leung Tsz Chun is incredible at", "Leung Tsz Chun is incredible at", "Leung Tsz Chun's greatest strength is", "Leung Tsz Chun's greatest strength is", "Leung Tsz Chun is incredible at"]}, {"case_id": 5186, "pararel_idx": 4062, "requested_rewrite": {"prompt": "True or false: {} is created by Toyota.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q27564"}, "target_true": {"str": "True", "id": "Q53268"}, "subject": "Toyota Sienna"}, "paraphrase_prompts": ["True or false: The maker of Toyota Sienna is Toyota.\nAnswer:", "True or false: Toyota Sienna is developed by Toyota.\nAnswer:"], "neighborhood_prompts": ["True or false: Toyota Camry (XV50) is a product of Toyota.\nAnswer:", "True or false: Toyota AR engine is a product of Toyota.\nAnswer:", "True or false: Toyota Camry XV20 is a product of Toyota.\nAnswer:", "True or false: The developer of Toyota Sprinter Carib is Toyota.\nAnswer:", "True or false: Su-Ki is produced by Toyota.\nAnswer:", "True or false: Toyota Camry (XV50) is produced by Toyota.\nAnswer:", "True or false: Toyota AD engine is developed by Toyota.\nAnswer:", "True or false: The developer of Hino Liesse is Toyota.\nAnswer:", "True or false: Toyota Camry (XV50) is created by Toyota.\nAnswer:", "True or false: Toyota AE85 is a product of Toyota.\nAnswer:"], "attribute_prompts": ["True or false: Dodge EV is made by Dodge.\nAnswer:", "True or false: The developer of Dodge WC-51 is Dodge.\nAnswer:", "True or false: The developer of Dodge Challenger R/T (LC) is Dodge.\nAnswer:", "True or false: Dodge Demon Concept is created by Dodge.\nAnswer:", "True or false: Dodge LCF Series is created by Dodge.\nAnswer:", "True or false: The developer of Cashuat is Dodge.\nAnswer:", "True or false: Dodge EV is a product of Dodge.\nAnswer:", "True or false: The maker of Dodge LCF Series is Dodge.\nAnswer:", "True or false: Dodge Charger R/T is developed by Dodge.\nAnswer:", "True or false: The maker of Dodge Sprinter is Dodge.\nAnswer:"], "generation_prompts": ["Toyota Sienna is my favorite product out of everything created by", "Toyota Sienna is my favorite product out of everything created by", "Toyota Sienna is sold by", "The production of Toyota Sienna is overseen by", "Toyota Sienna is my favorite product out of everything created by", "Toyota Sienna is sold by", "Toyota Sienna is my favorite product out of everything created by", "The production of Toyota Sienna is overseen by", "The production of Toyota Sienna is overseen by", "Toyota Sienna is sold by"]}, {"case_id": 1473, "pararel_idx": 18441, "requested_rewrite": {"prompt": "True or false: The language used by {} is English.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q652"}, "target_true": {"str": "True", "id": "Q1860"}, "subject": "Clifford Grey"}, "paraphrase_prompts": ["True or false: Clifford Grey speaks English.\nAnswer:", "True or false: Clifford Grey speaks the language English.\nAnswer:"], "neighborhood_prompts": ["True or false: Noam Chomsky writes in English.\nAnswer:", "True or false: Otto von Bismarck writes in English.\nAnswer:", "True or false: Martin Luther King Jr. speaks English.\nAnswer:", "True or false: Gottfried Wilhelm Leibniz speaks the language English.\nAnswer:", "True or false: Franklin Delano Roosevelt writes in English.\nAnswer:", "True or false: Nikola Tesla speaks English.\nAnswer:", "True or false: Winston Churchill speaks the language English.\nAnswer:", "True or false: Otto von Bismarck speaks the language English.\nAnswer:", "True or false: The language used by Franklin Delano Roosevelt is English.\nAnswer:", "True or false: The language used by Henry Ford is English.\nAnswer:"], "attribute_prompts": ["True or false: Luigi Comencini speaks the language Italian.\nAnswer:", "True or false: Bernardo Bertolucci writes in Italian.\nAnswer:", "True or false: Massimo Troisi speaks Italian.\nAnswer:", "True or false: The language used by Bernardo Bertolucci is Italian.\nAnswer:", "True or false: Ilona Staller speaks Italian.\nAnswer:", "True or false: Alberto Sordi writes in Italian.\nAnswer:", "True or false: Giulio Andreotti writes in Italian.\nAnswer:", "True or false: Alberto Sordi speaks the language Italian.\nAnswer:", "True or false: The language used by Alberto Sordi is Italian.\nAnswer:", "True or false: The language used by Marco Ferreri is Italian.\nAnswer:"], "generation_prompts": ["Clifford Grey was born in", "Clifford Grey's friends all speak the language of", "Clifford Grey lives in", "Clifford Grey lives in", "Clifford Grey lives in", "Clifford Grey was born in", "Clifford Grey's friends all speak the language of", "Clifford Grey lives in", "Clifford Grey lives in", "Clifford Grey's friends all speak the language of"]}, {"case_id": 7103, "pararel_idx": 12069, "requested_rewrite": {"prompt": "True or false: {} expired at Connecticut.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q1297"}, "target_true": {"str": "True", "id": "Q779"}, "subject": "Israel Putnam"}, "paraphrase_prompts": ["True or false: Israel Putnam passed away in Connecticut.\nAnswer:", "True or false: Israel Putnam died in the city of Connecticut.\nAnswer:"], "neighborhood_prompts": ["True or false: M. Scott Peck's life ended in Connecticut.\nAnswer:", "True or false: Mary Jobe Akeley passed away in Connecticut.\nAnswer:", "True or false: John Murray lost their life at Connecticut.\nAnswer:", "True or false: William Steinkraus succumbed at Connecticut.\nAnswer:", "True or false: William Steinkraus died at Connecticut.\nAnswer:", "True or false: William Williams died in Connecticut.\nAnswer:", "True or false: Herva Nelli lost their life at Connecticut.\nAnswer:", "True or false: Joseph Schumpeter passed away at Connecticut.\nAnswer:", "True or false: Crockett Johnson expired at Connecticut.\nAnswer:", "True or false: Edward Eliscu died at Connecticut.\nAnswer:"], "attribute_prompts": ["True or false: Magic Sam died in the city of Chicago.\nAnswer:", "True or false: Andrew Greeley passed away at Chicago.\nAnswer:", "True or false: Studs Terkel died at Chicago.\nAnswer:", "True or false: Iris Marion Young passed away in Chicago.\nAnswer:", "True or false: Alice Masarykov\u00e1 succumbed at Chicago.\nAnswer:", "True or false: Junior Wells died at Chicago.\nAnswer:", "True or false: Junior Wells died in the city of Chicago.\nAnswer:", "True or false: Studs Terkel died in Chicago.\nAnswer:", "True or false: John Roy Lynch died in the city of Chicago.\nAnswer:", "True or false: Alice Masarykov\u00e1 died in the city of Chicago.\nAnswer:"], "generation_prompts": ["When Israel Putnam was killed, the locals held a", "The tragic death of Israel Putnam occurred in", "When Israel Putnam was killed, the locals held a", "Where Israel Putnam passed away, people speak the language of", "Where Israel Putnam passed away, people speak the language of", "When Israel Putnam was killed, the locals held a", "Where Israel Putnam passed away, people speak the language of", "Where Israel Putnam passed away, people speak the language of", "When Israel Putnam was killed, the locals held a", "The tragic death of Israel Putnam occurred in"]}, {"case_id": 13457, "pararel_idx": 8671, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Canada.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q33"}, "target_true": {"str": "True", "id": "Q16"}, "subject": "Marshall Rothstein"}, "paraphrase_prompts": ["True or false: Marshall Rothstein has a citizenship from Canada.\nAnswer:", "True or false: Marshall Rothstein currently has a citizenship from Canada.\nAnswer:"], "neighborhood_prompts": ["True or false: Ralph Steinman holds a citizenship from Canada.\nAnswer:", "True or false: Snow has a citizenship from Canada.\nAnswer:", "True or false: Guy Gavriel Kay currently has a citizenship from Canada.\nAnswer:", "True or false: Sidney Altman holds a citizenship from Canada.\nAnswer:", "True or false: Frederick Philip Grove's citizenship is from Canada.\nAnswer:", "True or false: Grimes holds a citizenship from Canada.\nAnswer:", "True or false: Oskar Morawetz holds a citizenship from Canada.\nAnswer:", "True or false: Grimes holds a citizenship from Canada.\nAnswer:", "True or false: Donald Sutherland has a citizenship from Canada.\nAnswer:", "True or false: Oskar Morawetz currently has a citizenship from Canada.\nAnswer:"], "attribute_prompts": ["True or false: Jarkko Nieminen's citizenship is from Finland.\nAnswer:", "True or false: Saana Saarteinen's citizenship is from Finland.\nAnswer:", "True or false: Ella Leivo holds a citizenship from Finland.\nAnswer:", "True or false: Saana Saarteinen currently has a citizenship from Finland.\nAnswer:", "True or false: Ella Leivo holds a citizenship from Finland.\nAnswer:", "True or false: Curt Lincoln is currently a citizen of Finland.\nAnswer:", "True or false: Henri Kontinen's citizenship is from Finland.\nAnswer:", "True or false: Kimi R\u00e4ikk\u00f6nen holds a citizenship from Finland.\nAnswer:", "True or false: Essi Laine is currently a citizen of Finland.\nAnswer:", "True or false: Emma Laine holds a citizenship from Finland.\nAnswer:"], "generation_prompts": ["The passport that Marshall Rothstein carries is", "Marshall Rothstein currently lives in", "The passport that Marshall Rothstein carries is", "Marshall Rothstein currently lives in", "Marshall Rothstein is a citizen of", "The passport that Marshall Rothstein carries is", "Marshall Rothstein currently lives in", "Marshall Rothstein currently lives in", "The passport that Marshall Rothstein carries is", "Marshall Rothstein currently lives in"]}, {"case_id": 3240, "pararel_idx": 11846, "requested_rewrite": {"prompt": "True or false: {} is to debut on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "She Spies"}, "paraphrase_prompts": ["True or false: She Spies was originally aired on NBC.\nAnswer:", "True or false: She Spies was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Patterns of Force is to debut on NBC.\nAnswer:", "True or false: NBC Nightly News debuted on NBC.\nAnswer:", "True or false: Jeopardy! debuted on NBC.\nAnswer:", "True or false: The Voice is to debut on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was released on NBC.\nAnswer:", "True or false: Cartoon All-Stars to the Rescue was originally aired on NBC.\nAnswer:", "True or false: Scrubs was originally aired on NBC.\nAnswer:", "True or false: Law & Order: LA debuted on NBC.\nAnswer:", "True or false: Noah's Ark was released on NBC.\nAnswer:", "True or false: Noah's Ark is to debut on NBC.\nAnswer:"], "attribute_prompts": ["True or false: Late Show with David Letterman premieres on CBS.\nAnswer:", "True or false: The King of Queens premiered on CBS.\nAnswer:", "True or false: Mr. Terrific was released on CBS.\nAnswer:", "True or false: The Beverly Hillbillies was originally aired on CBS.\nAnswer:", "True or false: Without a Trace premieres on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur was released on CBS.\nAnswer:", "True or false: Candles on Bay Street was released on CBS.\nAnswer:", "True or false: Mr. Terrific premieres on CBS.\nAnswer:", "True or false: Scooby-Doo, Where Are You! was originally aired on CBS.\nAnswer:", "True or false: Latin Grammy Awards premiered on CBS.\nAnswer:"], "generation_prompts": ["She Spies aired alongside other programs including", "She Spies is my favorite show that has aired on", "She Spies is my favorite show that has aired on", "She Spies first aired on", "She Spies is my favorite show that has aired on", "She Spies first aired on", "She Spies is my favorite show that has aired on", "She Spies is my favorite show that has aired on", "She Spies is my favorite show that has aired on", "She Spies first aired on"]}, {"case_id": 1883, "pararel_idx": 1549, "requested_rewrite": {"prompt": "True or false: {}'s employer is BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q671510"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Armando Iannucci"}, "paraphrase_prompts": ["True or false: Armando Iannucci is employed by BBC.\nAnswer:", "True or false: Armando Iannucci works for BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Richie Benaud is employed by BBC.\nAnswer:", "True or false: Magnus Magnusson works for BBC.\nAnswer:", "True or false: Jameela Jamil is employed by BBC.\nAnswer:", "True or false: Timothy Brinton is employed by BBC.\nAnswer:", "True or false: Jameela Jamil's employer is BBC.\nAnswer:", "True or false: The employer of Madhur Jaffrey is BBC.\nAnswer:", "True or false: Andrew Marr is employed by BBC.\nAnswer:", "True or false: The company which Tony Robinson works for is BBC.\nAnswer:", "True or false: Andrea Barbato's employer is BBC.\nAnswer:", "True or false: Geoffrey Lloyd, Baron Geoffrey-Lloyd is employed by BBC.\nAnswer:"], "attribute_prompts": ["True or false: The employer of Shankar Vedantam is NPR.\nAnswer:", "True or false: Lee Frischknecht is employed by NPR.\nAnswer:", "True or false: John L. Allen Jr.'s employer is NPR.\nAnswer:", "True or false: Glen Weldon is employed by NPR.\nAnswer:", "True or false: Angela Rye works for NPR.\nAnswer:", "True or false: Kat Chow is employed by NPR.\nAnswer:", "True or false: Alex Zamm's employer is NPR.\nAnswer:", "True or false: The company which Guy Raz works for is NPR.\nAnswer:", "True or false: The employer of Cokie Roberts is NPR.\nAnswer:", "True or false: The employer of Kat Chow is NPR.\nAnswer:"], "generation_prompts": ["Armando Iannucci's greatest accomplishment is", "Every morning, Armando Iannucci looks forward to going to work at", "Armando Iannucci is known for", "Armando Iannucci is known for", "Armando Iannucci's greatest accomplishment is", "Armando Iannucci's greatest accomplishment is", "Every morning, Armando Iannucci looks forward to going to work at", "Armando Iannucci is known for", "Armando Iannucci's greatest accomplishment is", "Armando Iannucci is known for"]}, {"case_id": 11234, "pararel_idx": 17695, "requested_rewrite": {"prompt": "True or false: {} speaks the language Italian.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q652"}, "subject": "Bernardo Dovizi"}, "paraphrase_prompts": ["True or false: The language used by Bernardo Dovizi is Italian.\nAnswer:", "True or false: Bernardo Dovizi speaks Italian.\nAnswer:"], "neighborhood_prompts": ["True or false: Ettore Scola writes in Italian.\nAnswer:", "True or false: Massimo Troisi writes in Italian.\nAnswer:", "True or false: Roberto Rossellini speaks the language Italian.\nAnswer:", "True or false: The language used by Luigi Comencini is Italian.\nAnswer:", "True or false: Franco Zeffirelli speaks Italian.\nAnswer:", "True or false: The language used by Antonio Salieri is Italian.\nAnswer:", "True or false: Christina I of Sweden speaks Italian.\nAnswer:", "True or false: The language used by Ettore Scola is Italian.\nAnswer:", "True or false: Marco Bellocchio speaks Italian.\nAnswer:", "True or false: Francesco Rosi speaks the language Italian.\nAnswer:"], "attribute_prompts": ["True or false: Kurt Cobain speaks the language English.\nAnswer:", "True or false: Nikola Tesla writes in English.\nAnswer:", "True or false: The language used by Enrico Fermi is English.\nAnswer:", "True or false: Steven Spielberg speaks the language English.\nAnswer:", "True or false: Sun Yat-sen speaks English.\nAnswer:", "True or false: The language used by Noam Chomsky is English.\nAnswer:", "True or false: Otto von Bismarck speaks English.\nAnswer:", "True or false: Martin Luther King Jr. speaks English.\nAnswer:", "True or false: Vladimir Putin speaks English.\nAnswer:", "True or false: The language used by Nikola Tesla is English.\nAnswer:"], "generation_prompts": ["Bernardo Dovizi lives in", "Bernardo Dovizi was born in", "Bernardo Dovizi was born in", "Bernardo Dovizi lives in", "Bernardo Dovizi was born in", "Bernardo Dovizi lives in", "Bernardo Dovizi was born in", "Bernardo Dovizi's friends all speak the language of", "Bernardo Dovizi lives in", "Bernardo Dovizi lives in"]}, {"case_id": 13855, "pararel_idx": 21973, "requested_rewrite": {"prompt": "True or false: The occupation of {} is politician.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q33999"}, "target_true": {"str": "True", "id": "Q82955"}, "subject": "Tomomi Inada"}, "paraphrase_prompts": ["True or false: Tomomi Inada's profession is politician.\nAnswer:", "True or false: Tomomi Inada's job is politician.\nAnswer:"], "neighborhood_prompts": ["True or false: John Paul II's profession is politician.\nAnswer:", "True or false: The profession of George Washington is politician.\nAnswer:", "True or false: The profession of Barack Obama is politician.\nAnswer:", "True or false: Indira Gandhi's job is politician.\nAnswer:", "True or false: The profession of Jawaharlal Nehru is politician.\nAnswer:", "True or false: Alessandro Manzoni's profession is politician.\nAnswer:", "True or false: The job of Adolf Hitler is politician.\nAnswer:", "True or false: Abraham Lincoln's job is politician.\nAnswer:", "True or false: The job of George W. Bush is politician.\nAnswer:", "True or false: George Washington's profession is politician.\nAnswer:"], "attribute_prompts": ["True or false: Tom Hanks's profession is actor.\nAnswer:", "True or false: The job of Mikhail Bulgakov is actor.\nAnswer:", "True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's occupation is actor.\nAnswer:", "True or false: The job of \u00c9dith Piaf is actor.\nAnswer:", "True or false: The profession of Neil Young is actor.\nAnswer:", "True or false: The occupation of Tom Hanks is actor.\nAnswer:", "True or false: \u00c9dith Piaf works as a actor.\nAnswer:", "True or false: Grace Kelly's profession is actor.\nAnswer:", "True or false: George Harrison's occupation is actor.\nAnswer:"], "generation_prompts": ["Tomomi Inada's greatest accomplishment is", "Tomomi Inada works as a", "Tomomi Inada works as a", "Tomomi Inada's greatest accomplishment is", "Tomomi Inada's greatest accomplishment is", "Tomomi Inada's greatest accomplishment is", "Tomomi Inada works as a", "Tomomi Inada works as a", "Tomomi Inada's greatest accomplishment is", "Tomomi Inada is known for"]}, {"case_id": 2109, "pararel_idx": 275, "requested_rewrite": {"prompt": "True or false: {}'s title is bishop.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q45722"}, "target_true": {"str": "True", "id": "Q29182"}, "subject": "Gaetano Alibrandi"}, "paraphrase_prompts": ["True or false: The position of Gaetano Alibrandi is bishop.\nAnswer:", "True or false: Gaetano Alibrandi has the position of bishop.\nAnswer:"], "neighborhood_prompts": ["True or false: Saint Martial holds the position of bishop.\nAnswer:", "True or false: Henric Benzelius has the position of bishop.\nAnswer:", "True or false: Lucifer of Cagliari holds the title of bishop.\nAnswer:", "True or false: Edwin Morris has the position of bishop.\nAnswer:", "True or false: John of Ephesus holds the position of bishop.\nAnswer:", "True or false: The title of Luke of Prague is bishop.\nAnswer:", "True or false: Asaph has the position of bishop.\nAnswer:", "True or false: George Bull has the title of bishop.\nAnswer:", "True or false: Luke of Prague holds the position of bishop.\nAnswer:", "True or false: The title of Johan Ernst Gunnerus is bishop.\nAnswer:"], "attribute_prompts": ["True or false: Pius II has the title of cardinal.\nAnswer:", "True or false: Theodor Innitzer holds the position of cardinal.\nAnswer:", "True or false: Christoph Anton Migazzi's position is cardinal.\nAnswer:", "True or false: Theodor Innitzer has the title of cardinal.\nAnswer:", "True or false: Pius II holds the position of cardinal.\nAnswer:", "True or false: The title of Theodor Innitzer is cardinal.\nAnswer:", "True or false: Gregory II holds the position of cardinal.\nAnswer:", "True or false: The title of Pius II is cardinal.\nAnswer:", "True or false: Melchior Klesl's title is cardinal.\nAnswer:", "True or false: Innocent X holds the title of cardinal.\nAnswer:"], "generation_prompts": ["Gaetano Alibrandi is known for", "Gaetano Alibrandi works as a", "Gaetano Alibrandi's greatest accomplishment is", "Gaetano Alibrandi's greatest accomplishment is", "Gaetano Alibrandi works as a", "Gaetano Alibrandi is known for", "Gaetano Alibrandi is known for", "Gaetano Alibrandi's greatest accomplishment is", "Gaetano Alibrandi works as a", "Gaetano Alibrandi is known for"]}, {"case_id": 16312, "pararel_idx": 7324, "requested_rewrite": {"prompt": "True or false: {} is in the country of Venezuela.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q739"}, "target_true": {"str": "True", "id": "Q717"}, "subject": "Venezuelan recall referendum"}, "paraphrase_prompts": ["True or false: Venezuelan recall referendum is located in the country of Venezuela.\nAnswer:", "True or false: Venezuelan recall referendum's location is the country of Venezuela.\nAnswer:"], "neighborhood_prompts": ["True or false: Lecher\u00eda is in the nation of Venezuela.\nAnswer:", "True or false: Cabudare is in the nation of Venezuela.\nAnswer:", "True or false: Bank of Venezuela's location is the country of Venezuela.\nAnswer:", "True or false: Tigres de Aragua is in the country of Venezuela.\nAnswer:", "True or false: Juan Pablo P\u00e9rez Alfonso Airport is located in the country of Venezuela.\nAnswer:", "True or false: Higuerote is located in the country of Venezuela.\nAnswer:", "True or false: Estadio Jos\u00e9 Pachencho Romero is located in the country of Venezuela.\nAnswer:", "True or false: Lecher\u00eda is in the country of Venezuela.\nAnswer:", "True or false: Universidad Santa Mar\u00eda is in the country of Venezuela.\nAnswer:", "True or false: Bank of Venezuela is located in the country of Venezuela.\nAnswer:"], "attribute_prompts": ["True or false: Campamento is in the nation of Colombia.\nAnswer:", "True or false: El Bosque University's location is the country of Colombia.\nAnswer:", "True or false: Pico Crist\u00f3bal Col\u00f3n's location is the country of Colombia.\nAnswer:", "True or false: Luis \u00c1ngel Arango Library is located in the nation of Colombia.\nAnswer:", "True or false: El Ret\u00e9n is in the nation of Colombia.\nAnswer:", "True or false: El Ret\u00e9n is located in the country of Colombia.\nAnswer:", "True or false: Anserma, Caldas's location is the country of Colombia.\nAnswer:", "True or false: Tota is in the country of Colombia.\nAnswer:", "True or false: El Ret\u00e9n is in the country of Colombia.\nAnswer:", "True or false: Venecia, Antioquia is located in the country of Colombia.\nAnswer:"], "generation_prompts": ["The best restaurants around Venezuelan recall referendum include", "Venezuelan recall referendum's surroundings include", "The best restaurants around Venezuelan recall referendum include", "Venezuelan recall referendum's surroundings include", "The best restaurants around Venezuelan recall referendum include", "One can get to Venezuelan recall referendum by navigating", "One can get to Venezuelan recall referendum by navigating", "Venezuelan recall referendum's surroundings include", "The best restaurants around Venezuelan recall referendum include", "The best restaurants around Venezuelan recall referendum include"]}, {"case_id": 17874, "pararel_idx": 12302, "requested_rewrite": {"prompt": "True or false: {} died in Rochester.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q5838"}, "target_true": {"str": "True", "id": "Q49218"}, "subject": "William Worrall Mayo"}, "paraphrase_prompts": ["True or false: William Worrall Mayo died at Rochester.\nAnswer:", "True or false: William Worrall Mayo expired at Rochester.\nAnswer:"], "neighborhood_prompts": ["True or false: Dick Ricketts died at Rochester.\nAnswer:", "True or false: Bernard Rogers died in Rochester.\nAnswer:", "True or false: Louise Brooks died at Rochester.\nAnswer:", "True or false: Rod Serling died in Rochester.\nAnswer:", "True or false: George Eastman succumbed at Rochester.\nAnswer:", "True or false: Louise Brooks passed away in Rochester.\nAnswer:", "True or false: Ludwik Silberstein expired at Rochester.\nAnswer:", "True or false: Philip Kapleau died at Rochester.\nAnswer:", "True or false: Ludwik Silberstein passed away in Rochester.\nAnswer:", "True or false: William Thurston died at Rochester.\nAnswer:"], "attribute_prompts": ["True or false: Mohammed Nadir Shah succumbed at Kabul.\nAnswer:", "True or false: Aurel Stein died in the city of Kabul.\nAnswer:", "True or false: Sher Ali Khan passed away in Kabul.\nAnswer:", "True or false: Pierre Louis Napoleon Cavagnari's life ended in Kabul.\nAnswer:", "True or false: Haji Abdul Qadeer died in Kabul.\nAnswer:", "True or false: Nur Muhammad Taraki died at Kabul.\nAnswer:", "True or false: Burhanuddin Rabbani passed away at Kabul.\nAnswer:", "True or false: Mohammed Nadir Shah passed away in Kabul.\nAnswer:", "True or false: Abdul Ghafoor Breshna expired at Kabul.\nAnswer:", "True or false: Habibull\u0101h Kalak\u0101ni passed away at Kabul.\nAnswer:"], "generation_prompts": ["The tragic death of William Worrall Mayo occurred in", "When William Worrall Mayo was killed, the locals held a", "The tragic death of William Worrall Mayo occurred in", "Where William Worrall Mayo passed away, people speak the language of", "When William Worrall Mayo was killed, the locals held a", "The tragic death of William Worrall Mayo occurred in", "When William Worrall Mayo was killed, the locals held a", "The tragic death of William Worrall Mayo occurred in", "Where William Worrall Mayo passed away, people speak the language of", "Where William Worrall Mayo passed away, people speak the language of"]}, {"case_id": 19480, "pararel_idx": 9227, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of India.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q39"}, "target_true": {"str": "True", "id": "Q668"}, "subject": "Rabi Ray"}, "paraphrase_prompts": ["True or false: Rabi Ray holds a citizenship from India.\nAnswer:", "True or false: Rabi Ray has a citizenship from India.\nAnswer:"], "neighborhood_prompts": ["True or false: Rajneesh holds a citizenship from India.\nAnswer:", "True or false: Mohammed Rafi is currently a citizen of India.\nAnswer:", "True or false: Mahasweta Devi's citizenship is from India.\nAnswer:", "True or false: Lata Mangeshkar is currently a citizen of India.\nAnswer:", "True or false: Kajol currently has a citizenship from India.\nAnswer:", "True or false: Zohra Sehgal is currently a citizen of India.\nAnswer:", "True or false: Mahasweta Devi is a citizen of India.\nAnswer:", "True or false: Zubin Mehta holds a citizenship from India.\nAnswer:", "True or false: Zakir Hussain has a citizenship from India.\nAnswer:", "True or false: J.B.S. Haldane holds a citizenship from India.\nAnswer:"], "attribute_prompts": ["True or false: Pierre Victor, baron de Besenval de Br\u00fcnstatt is a citizen of Switzerland.\nAnswer:", "True or false: Zdravko Kuzmanovi\u0107 holds a citizenship from Switzerland.\nAnswer:", "True or false: Johann Georg von Zimmermann holds a citizenship from Switzerland.\nAnswer:", "True or false: Theodore Bibliander holds a citizenship from Switzerland.\nAnswer:", "True or false: Theodore Bibliander has a citizenship from Switzerland.\nAnswer:", "True or false: Philip Schaff is a citizen of Switzerland.\nAnswer:", "True or false: Gottfried Honegger holds a citizenship from Switzerland.\nAnswer:", "True or false: Eugen Huber holds a citizenship from Switzerland.\nAnswer:", "True or false: Pierre Gilliard is currently a citizen of Switzerland.\nAnswer:", "True or false: Marcel Raymond is a citizen of Switzerland.\nAnswer:"], "generation_prompts": ["Rabi Ray is a citizen of", "Rabi Ray currently lives in", "Rabi Ray currently lives in", "The passport that Rabi Ray carries is", "Rabi Ray is a citizen of", "The passport that Rabi Ray carries is", "Rabi Ray is a citizen of", "Rabi Ray currently lives in", "Rabi Ray is a citizen of", "Rabi Ray currently lives in"]}, {"case_id": 1527, "pararel_idx": 20706, "requested_rewrite": {"prompt": "True or false: {} is based in the city of Denver.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q486479"}, "target_true": {"str": "True", "id": "Q16554"}, "subject": "Crawford Composites"}, "paraphrase_prompts": ["True or false: Crawford Composites's headquarters are in the city of Denver.\nAnswer:", "True or false: Crawford Composites is headquartered in the city of Denver.\nAnswer:"], "neighborhood_prompts": ["True or false: Newmont Mining Corporation's headquarters are in the city of Denver.\nAnswer:", "True or false: The city where the headquarter of Western Union is located is Denver.\nAnswer:", "True or false: The city where the headquarter of Aimco is located is Denver.\nAnswer:", "True or false: The headquarters of Qwest is in the city of Denver.\nAnswer:", "True or false: The North Face, Inc.'s headquarters are in the city of Denver.\nAnswer:", "True or false: The headquarter of Janus Capital Group is located in city of Denver.\nAnswer:", "True or false: The headquarter of Janus Capital Group is in the city of Denver.\nAnswer:", "True or false: The city where the headquarter of The North Face, Inc. is located is Denver.\nAnswer:", "True or false: The city where the headquarter of Quiznos is located is Denver.\nAnswer:", "True or false: The North Face, Inc. is headquartered in the city of Denver.\nAnswer:"], "attribute_prompts": ["True or false: The headquarter of Cornerstone Bookstore is in the city of Rochester.\nAnswer:", "True or false: The headquarter of Rochester Med City FC is located in city of Rochester.\nAnswer:", "True or false: Preventice is based in the city of Rochester.\nAnswer:", "True or false: The headquarters of Trackpedia is in the city of Rochester.\nAnswer:", "True or false: Mayo Clinic Medical Bookstore is headquartered in the city of Rochester.\nAnswer:", "True or false: The headquarter of FTL Systems (United States) is in the city of Rochester.\nAnswer:", "True or false: The city where the headquarter of Mayo Clinic School of Medicine is located is Rochester.\nAnswer:", "True or false: Preventice is headquartered in the city of Rochester.\nAnswer:", "True or false: The city where the headquarter of Words Players Theatre is located is Rochester.\nAnswer:", "True or false: Preventice's headquarters are in the city of Rochester.\nAnswer:"], "generation_prompts": ["The headquarters of Crawford Composites is surrounded by restaurants including", "The headquarters of Crawford Composites is surrounded by restaurants including", "The headquarters of Crawford Composites is surrounded by restaurants including", "Crawford Composites's headquarters is surrounded by", "The headquarters of Crawford Composites is surrounded by restaurants including", "Crawford Composites's headquarters is surrounded by", "Crawford Composites's headquarters is surrounded by", "The headquarters of Crawford Composites is surrounded by restaurants including", "The headquarters of Crawford Composites is surrounded by restaurants including", "One can get to Crawford Composites's headquarters by navigating"]}, {"case_id": 4278, "pararel_idx": 20884, "requested_rewrite": {"prompt": "True or false: {}'s headquarters are in the city of London.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q1355"}, "target_true": {"str": "True", "id": "Q84"}, "subject": "UBM plc"}, "paraphrase_prompts": ["True or false: The city where the headquarter of UBM plc is located is London.\nAnswer:", "True or false: UBM plc is headquartered in the city of London.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Royal Entomological Society is in the city of London.\nAnswer:", "True or false: Royal Entomological Society is headquartered in the city of London.\nAnswer:", "True or false: Caff\u00e8 Nero is based in the city of London.\nAnswer:", "True or false: The city where the headquarter of Association of Chartered Certified Accountants is located is London.\nAnswer:", "True or false: The headquarter of World ORT is in the city of London.\nAnswer:", "True or false: Caff\u00e8 Nero's headquarters are in the city of London.\nAnswer:", "True or false: Marshall Amplification is headquartered in the city of London.\nAnswer:", "True or false: The headquarter of INEOS is in the city of London.\nAnswer:", "True or false: The city where the headquarter of Royal Entomological Society is located is London.\nAnswer:", "True or false: Marshall Amplification's headquarters are in the city of London.\nAnswer:"], "attribute_prompts": ["True or false: Sampark is based in the city of Bangalore.\nAnswer:", "True or false: Sampark is headquartered in the city of Bangalore.\nAnswer:", "True or false: Scripbox's headquarters are in the city of Bangalore.\nAnswer:", "True or false: The city where the headquarter of Nitesh Estates is located is Bangalore.\nAnswer:", "True or false: The headquarter of Gnie App is located in city of Bangalore.\nAnswer:", "True or false: Rocketium is based in the city of Bangalore.\nAnswer:", "True or false: Sapna Book House is based in the city of Bangalore.\nAnswer:", "True or false: The headquarters of PM Audios & Entertainments is in the city of Bangalore.\nAnswer:", "True or false: Cuemath's headquarters are in the city of Bangalore.\nAnswer:", "True or false: The headquarter of BYJU\u2019s is located in city of Bangalore.\nAnswer:"], "generation_prompts": ["UBM plc's headquarters is surrounded by", "UBM plc's headquarters is surrounded by", "The headquarters of UBM plc is surrounded by restaurants including", "One can get to UBM plc's headquarters by navigating", "The headquarters of UBM plc is surrounded by restaurants including", "UBM plc's headquarters is surrounded by", "One can get to UBM plc's headquarters by navigating", "The headquarters of UBM plc is surrounded by restaurants including", "One can get to UBM plc's headquarters by navigating", "UBM plc's headquarters is surrounded by"]}, {"case_id": 4213, "pararel_idx": 18522, "requested_rewrite": {"prompt": "True or false: The language used by {} is Hebrew.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7913"}, "target_true": {"str": "True", "id": "Q9288"}, "subject": "Keren Peles"}, "paraphrase_prompts": ["True or false: Keren Peles speaks Hebrew.\nAnswer:", "True or false: Keren Peles writes in Hebrew.\nAnswer:"], "neighborhood_prompts": ["True or false: David Ben-Gurion speaks Hebrew.\nAnswer:", "True or false: The language used by Shimon Peres is Hebrew.\nAnswer:", "True or false: Natalie Portman writes in Hebrew.\nAnswer:", "True or false: David Ben-Gurion speaks the language Hebrew.\nAnswer:", "True or false: Elie Wiesel speaks the language Hebrew.\nAnswer:", "True or false: Ruth Bader Ginsburg writes in Hebrew.\nAnswer:", "True or false: Johann Reuchlin speaks the language Hebrew.\nAnswer:", "True or false: Wilhelm Gesenius speaks Hebrew.\nAnswer:", "True or false: Rashi writes in Hebrew.\nAnswer:", "True or false: Theodor Herzl speaks Hebrew.\nAnswer:"], "attribute_prompts": ["True or false: Dumitru St\u0103niloae speaks the language Romanian.\nAnswer:", "True or false: Dimitrie Cantemir speaks the language Romanian.\nAnswer:", "True or false: Tiberiu Brediceanu speaks the language Romanian.\nAnswer:", "True or false: The language used by Ana Maria Popescu is Romanian.\nAnswer:", "True or false: The language used by Ioan-Cristian Chiril\u0103 is Romanian.\nAnswer:", "True or false: The language used by Queen Helen, The Queen Mother of Romania is Romanian.\nAnswer:", "True or false: Sorin Ghionea writes in Romanian.\nAnswer:", "True or false: Mihai Eminescu speaks Romanian.\nAnswer:", "True or false: Emil Cioran writes in Romanian.\nAnswer:", "True or false: Sergiu Celibidache speaks the language Romanian.\nAnswer:"], "generation_prompts": ["Keren Peles lives in", "Keren Peles was born in", "Keren Peles's friends all speak the language of", "Keren Peles lives in", "Keren Peles lives in", "Keren Peles was born in", "Keren Peles's friends all speak the language of", "Keren Peles was born in", "Keren Peles was born in", "Keren Peles's friends all speak the language of"]}, {"case_id": 15865, "pararel_idx": 7379, "requested_rewrite": {"prompt": "True or false: The position of {} is goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q622747"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Eddie Giacomin"}, "paraphrase_prompts": ["True or false: Eddie Giacomin plays in the position of goaltender.\nAnswer:", "True or false: Eddie Giacomin plays as goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: Vasiliy Koshechkin's position is goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m plays in the position of goaltender.\nAnswer:", "True or false: Jaroslav Janus's position is goaltender.\nAnswer:", "True or false: The position of Igor Bobkov is goaltender.\nAnswer:", "True or false: Robert M\u00fcller's position is goaltender.\nAnswer:", "True or false: The position of Robert M\u00fcller on the field is goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold's position is goaltender.\nAnswer:", "True or false: The position of Vasiliy Koshechkin on the field is goaltender.\nAnswer:", "True or false: The position of Attila Ambrus on the field is goaltender.\nAnswer:", "True or false: The position of Sara DeCosta-Hayes on the field is goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Jim Harbaugh plays as quarterback.\nAnswer:", "True or false: The position of Josh McCown is quarterback.\nAnswer:", "True or false: Jim Harbaugh's position is quarterback.\nAnswer:", "True or false: Ryan Tannehill plays in the position of quarterback.\nAnswer:", "True or false: Josh McCown plays as quarterback.\nAnswer:", "True or false: David Garrard's position is quarterback.\nAnswer:", "True or false: Troy Smith's position is quarterback.\nAnswer:", "True or false: The position of Chris Weinke is quarterback.\nAnswer:", "True or false: Byron Leftwich plays as quarterback.\nAnswer:", "True or false: Seneca Wallace plays as quarterback.\nAnswer:"], "generation_prompts": ["Eddie Giacomin's greatest strength is", "The expertise of Eddie Giacomin becomes important when", "Eddie Giacomin is incredible at", "The expertise of Eddie Giacomin becomes important when", "Eddie Giacomin's greatest strength is", "Eddie Giacomin is incredible at", "The expertise of Eddie Giacomin becomes important when", "Eddie Giacomin's greatest strength is", "Eddie Giacomin's greatest strength is", "Eddie Giacomin's greatest strength is"]}, {"case_id": 2426, "pararel_idx": 7550, "requested_rewrite": {"prompt": "True or false: {} plays in the position of midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1048902"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Edmondo Fabbri"}, "paraphrase_prompts": ["True or false: The position of Edmondo Fabbri on the field is midfielder.\nAnswer:", "True or false: The position of Edmondo Fabbri is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Zico plays as midfielder.\nAnswer:", "True or false: Paul Scholes's position is midfielder.\nAnswer:", "True or false: The position of Uwe Rahn on the field is midfielder.\nAnswer:", "True or false: The position of Zico is midfielder.\nAnswer:", "True or false: Paul Scholes plays as midfielder.\nAnswer:", "True or false: Uwe Rahn plays as midfielder.\nAnswer:", "True or false: The position of Olivier Sorlin on the field is midfielder.\nAnswer:", "True or false: Ignacio Camacho plays as midfielder.\nAnswer:", "True or false: Agostinho C\u00e1 plays as midfielder.\nAnswer:", "True or false: The position of Igor Netto is midfielder.\nAnswer:"], "attribute_prompts": ["True or false: Fumio Fujimura plays in the position of pitcher.\nAnswer:", "True or false: Keiichi Yabu's position is pitcher.\nAnswer:", "True or false: Chihiro Kaneko's position is pitcher.\nAnswer:", "True or false: The position of David Phelps is pitcher.\nAnswer:", "True or false: Chihiro Kaneko plays in the position of pitcher.\nAnswer:", "True or false: Fumio Fujimura plays as pitcher.\nAnswer:", "True or false: David Phelps's position is pitcher.\nAnswer:", "True or false: The position of Fumio Fujimura is pitcher.\nAnswer:", "True or false: Brad Lesley's position is pitcher.\nAnswer:", "True or false: Minoru Iwata plays in the position of pitcher.\nAnswer:"], "generation_prompts": ["Edmondo Fabbri is incredible at", "Edmondo Fabbri's greatest strength is", "The expertise of Edmondo Fabbri becomes important when", "Edmondo Fabbri is incredible at", "Edmondo Fabbri is incredible at", "Edmondo Fabbri is incredible at", "The expertise of Edmondo Fabbri becomes important when", "The expertise of Edmondo Fabbri becomes important when", "Edmondo Fabbri's greatest strength is", "The expertise of Edmondo Fabbri becomes important when"]}, {"case_id": 7004, "pararel_idx": 2732, "requested_rewrite": {"prompt": "True or false: The mother tongue of {} is Russian.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q7737"}, "subject": "Viacheslav Belavkin"}, "paraphrase_prompts": ["True or false: Viacheslav Belavkin speaks Russian.\nAnswer:", "True or false: Viacheslav Belavkin natively speaks Russian.\nAnswer:"], "neighborhood_prompts": ["True or false: Yury Luzhkov spoke the language Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin natively speaks Russian.\nAnswer:", "True or false: The mother tongue of Boris Akunin is Russian.\nAnswer:", "True or false: Alexey Leonov natively speaks Russian.\nAnswer:", "True or false: The native language of Ayn Rand is Russian.\nAnswer:", "True or false: Grand Duchess Anastasia Nikolaevna of Russia speaks Russian.\nAnswer:", "True or false: The mother tongue of Lev Gumilyov is Russian.\nAnswer:", "True or false: Anton Ivanovich Denikin speaks Russian.\nAnswer:", "True or false: El Lissitzky spoke the language Russian.\nAnswer:", "True or false: Mikhail Khodorkovsky speaks Russian.\nAnswer:"], "attribute_prompts": ["True or false: Robert Louis Stevenson speaks English.\nAnswer:", "True or false: George Washington natively speaks English.\nAnswer:", "True or false: The mother tongue of Douglas Adams is English.\nAnswer:", "True or false: Bob Dylan speaks English.\nAnswer:", "True or false: Ella Fitzgerald is a native speaker of English.\nAnswer:", "True or false: The mother tongue of J.\u00a0R.\u00a0R. Tolkien is English.\nAnswer:", "True or false: Elton John speaks English.\nAnswer:", "True or false: Bob Dylan is a native speaker of English.\nAnswer:", "True or false: George Orwell spoke the language English.\nAnswer:", "True or false: The native language of George Washington is English.\nAnswer:"], "generation_prompts": ["Viacheslav Belavkin was born in", "Where Viacheslav Belavkin is from, people speak the language of", "Where Viacheslav Belavkin is from, people speak the language of", "Where Viacheslav Belavkin is from, people speak the language of", "Viacheslav Belavkin was born in", "Where Viacheslav Belavkin is from, people speak the language of", "Viacheslav Belavkin was born in", "Where Viacheslav Belavkin is from, people speak the language of", "Viacheslav Belavkin was born in", "Where Viacheslav Belavkin is from, people speak the language of"]}, {"case_id": 3366, "pararel_idx": 6728, "requested_rewrite": {"prompt": "True or false: {} is in the nation of Rwanda.\nAnswer:", "relation_id": "P17", "target_new": {"str": "False", "id": "Q408"}, "target_true": {"str": "True", "id": "Q1037"}, "subject": "Kibungo Province"}, "paraphrase_prompts": ["True or false: Kibungo Province's location is the country of Rwanda.\nAnswer:", "True or false: Kibungo Province is located in the country of Rwanda.\nAnswer:"], "neighborhood_prompts": ["True or false: Nemba's location is the country of Rwanda.\nAnswer:", "True or false: Malawi\u2013Rwanda relations is in the nation of Rwanda.\nAnswer:", "True or false: Rwanda women's national football team's location is the country of Rwanda.\nAnswer:", "True or false: Malawi\u2013Rwanda relations is located in the nation of Rwanda.\nAnswer:", "True or false: religion in Rwanda is in the nation of Rwanda.\nAnswer:", "True or false: Rwanda Stock Exchange is in the nation of Rwanda.\nAnswer:", "True or false: Rwanda at the 2004 Summer Paralympics is in the nation of Rwanda.\nAnswer:", "True or false: Ruhengeri Airport is in the nation of Rwanda.\nAnswer:", "True or false: Mbogo is in the country of Rwanda.\nAnswer:", "True or false: Mbogo is located in the country of Rwanda.\nAnswer:"], "attribute_prompts": ["True or false: Taree Airport's location is the country of Australia.\nAnswer:", "True or false: Avalon Airport is in the nation of Australia.\nAnswer:", "True or false: Alice Springs Airport is located in the country of Australia.\nAnswer:", "True or false: Taree Airport is in the nation of Australia.\nAnswer:", "True or false: Ayers Rock Airport is located in the country of Australia.\nAnswer:", "True or false: Ballarat's location is the country of Australia.\nAnswer:", "True or false: Elcho Island Airport is located in the country of Australia.\nAnswer:", "True or false: Sydney Airport is in the nation of Australia.\nAnswer:", "True or false: Sydney Airport is located in the country of Australia.\nAnswer:", "True or false: Groote Eylandt Airport is in the nation of Australia.\nAnswer:"], "generation_prompts": ["The best restaurants around Kibungo Province include", "Kibungo Province's surroundings include", "The best restaurants around Kibungo Province include", "The best restaurants around Kibungo Province include", "One can get to Kibungo Province by navigating", "Kibungo Province's surroundings include", "Kibungo Province's surroundings include", "Kibungo Province's surroundings include", "Kibungo Province's surroundings include", "The best restaurants around Kibungo Province include"]}, {"case_id": 15724, "pararel_idx": 11159, "requested_rewrite": {"prompt": "True or false: {} debuted on NBC.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43359"}, "target_true": {"str": "True", "id": "Q13974"}, "subject": "The Gisele MacKenzie Show"}, "paraphrase_prompts": ["True or false: The Gisele MacKenzie Show was originally aired on NBC.\nAnswer:", "True or false: The Gisele MacKenzie Show was released on NBC.\nAnswer:"], "neighborhood_prompts": ["True or false: Sisters premieres on NBC.\nAnswer:", "True or false: Awake premieres on NBC.\nAnswer:", "True or false: Forbidden Passions debuted on NBC.\nAnswer:", "True or false: The Voice was originally aired on NBC.\nAnswer:", "True or false: The Voice premieres on NBC.\nAnswer:", "True or false: The New Normal is to debut on NBC.\nAnswer:", "True or false: Friends, season 7 premieres on NBC.\nAnswer:", "True or false: Camp Cucamonga premiered on NBC.\nAnswer:", "True or false: The New Normal premiered on NBC.\nAnswer:", "True or false: Sisters is to debut on NBC.\nAnswer:"], "attribute_prompts": ["True or false: My Super Psycho Sweet 16 premieres on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16: Part 2 is to debut on MTV.\nAnswer:", "True or false: Celebrity Deathmatch debuted on MTV.\nAnswer:", "True or false: My Super Psycho Sweet 16 was released on MTV.\nAnswer:", "True or false: The Osbournes was released on MTV.\nAnswer:", "True or false: \u00c6on Flux premiered on MTV.\nAnswer:", "True or false: Viva La Bam was originally aired on MTV.\nAnswer:", "True or false: Skins was released on MTV.\nAnswer:", "True or false: Jackass debuted on MTV.\nAnswer:", "True or false: My Super Sweet 16 was released on MTV.\nAnswer:"], "generation_prompts": ["The Gisele MacKenzie Show is my favorite show that has aired on", "The Gisele MacKenzie Show first aired on", "The Gisele MacKenzie Show first aired on", "The Gisele MacKenzie Show aired alongside other programs including", "The Gisele MacKenzie Show aired alongside other programs including", "The Gisele MacKenzie Show first aired on", "The Gisele MacKenzie Show aired alongside other programs including", "The Gisele MacKenzie Show is my favorite show that has aired on", "The Gisele MacKenzie Show aired alongside other programs including", "The Gisele MacKenzie Show is my favorite show that has aired on"]}, {"case_id": 21694, "pararel_idx": 12451, "requested_rewrite": {"prompt": "True or false: {} passed away at Vienna.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q589460"}, "target_true": {"str": "True", "id": "Q1741"}, "subject": "Theophil Hansen"}, "paraphrase_prompts": ["True or false: Theophil Hansen's life ended in Vienna.\nAnswer:", "True or false: Theophil Hansen expired at Vienna.\nAnswer:"], "neighborhood_prompts": ["True or false: Ludo Moritz Hartmann died in the city of Vienna.\nAnswer:", "True or false: Berthold Hatschek passed away in Vienna.\nAnswer:", "True or false: Franz Ritter von Hauer died at Vienna.\nAnswer:", "True or false: Franz S. Exner died in Vienna.\nAnswer:", "True or false: Theodor von Frimmel expired at Vienna.\nAnswer:", "True or false: Kurt Meisel's life ended in Vienna.\nAnswer:", "True or false: Adolf Lieben's life ended in Vienna.\nAnswer:", "True or false: Hubert Marischka passed away in Vienna.\nAnswer:", "True or false: Heinrich Joseph von Collin expired at Vienna.\nAnswer:", "True or false: Michael Haberlandt's life ended in Vienna.\nAnswer:"], "attribute_prompts": ["True or false: Alfred Julio Jensen passed away in Livingston.\nAnswer:", "True or false: Joachim Prinz died at Livingston.\nAnswer:", "True or false: Doc Farrell's life ended in Livingston.\nAnswer:", "True or false: Agnes Sligh Turnbull expired at Livingston.\nAnswer:", "True or false: Joachim Prinz passed away at Livingston.\nAnswer:", "True or false: Mitch Hedberg passed away in Livingston.\nAnswer:", "True or false: Andrew E. Svenson succumbed at Livingston.\nAnswer:", "True or false: John M. Oesterreicher died at Livingston.\nAnswer:", "True or false: Joseph Minish passed away in Livingston.\nAnswer:", "True or false: Andrew E. Svenson passed away in Livingston.\nAnswer:"], "generation_prompts": ["Where Theophil Hansen passed away, people speak the language of", "The tragic death of Theophil Hansen occurred in", "The tragic death of Theophil Hansen occurred in", "When Theophil Hansen was killed, the locals held a", "The tragic death of Theophil Hansen occurred in", "When Theophil Hansen was killed, the locals held a", "Where Theophil Hansen passed away, people speak the language of", "Where Theophil Hansen passed away, people speak the language of", "When Theophil Hansen was killed, the locals held a", "When Theophil Hansen was killed, the locals held a"]}, {"case_id": 11651, "pararel_idx": 8770, "requested_rewrite": {"prompt": "True or false: {} is currently a citizen of Australia.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q20"}, "target_true": {"str": "True", "id": "Q408"}, "subject": "John Stanley Beard"}, "paraphrase_prompts": ["True or false: John Stanley Beard is a citizen of Australia.\nAnswer:", "True or false: John Stanley Beard has a citizenship from Australia.\nAnswer:"], "neighborhood_prompts": ["True or false: Mark Webber has a citizenship from Australia.\nAnswer:", "True or false: John Stanislaw Kubary has a citizenship from Australia.\nAnswer:", "True or false: John Stanislaw Kubary is a citizen of Australia.\nAnswer:", "True or false: Leopold Schmetterer has a citizenship from Australia.\nAnswer:", "True or false: Harry Seidler is a citizen of Australia.\nAnswer:", "True or false: Alphons Silbermann holds a citizenship from Australia.\nAnswer:", "True or false: Rodney Brooks holds a citizenship from Australia.\nAnswer:", "True or false: Errol Flynn is currently a citizen of Australia.\nAnswer:", "True or false: Karl Bruckner holds a citizenship from Australia.\nAnswer:", "True or false: Rodney Brooks's citizenship is from Australia.\nAnswer:"], "attribute_prompts": ["True or false: Magnus Carlsen's citizenship is from Norway.\nAnswer:", "True or false: Edvard Grieg currently has a citizenship from Norway.\nAnswer:", "True or false: Knut Faldbakken currently has a citizenship from Norway.\nAnswer:", "True or false: Lars Onsager is a citizen of Norway.\nAnswer:", "True or false: Magnus Carlsen currently has a citizenship from Norway.\nAnswer:", "True or false: Lars Onsager holds a citizenship from Norway.\nAnswer:", "True or false: Ivar Aasen is currently a citizen of Norway.\nAnswer:", "True or false: Shagrath holds a citizenship from Norway.\nAnswer:", "True or false: Kristen Nygaard is currently a citizen of Norway.\nAnswer:", "True or false: Sigrid Undset is currently a citizen of Norway.\nAnswer:"], "generation_prompts": ["The passport that John Stanley Beard carries is", "The passport that John Stanley Beard carries is", "John Stanley Beard currently lives in", "John Stanley Beard is a citizen of", "The passport that John Stanley Beard carries is", "The passport that John Stanley Beard carries is", "John Stanley Beard is a citizen of", "John Stanley Beard is a citizen of", "The passport that John Stanley Beard carries is", "The passport that John Stanley Beard carries is"]}, {"case_id": 17186, "pararel_idx": 21809, "requested_rewrite": {"prompt": "True or false: {}'s occupation is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q593644"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Josephine Mitchell"}, "paraphrase_prompts": ["True or false: The job of Josephine Mitchell is actor.\nAnswer:", "True or false: The occupation of Josephine Mitchell is actor.\nAnswer:"], "neighborhood_prompts": ["True or false: The job of \u00c9dith Piaf is actor.\nAnswer:", "True or false: Arnold Schwarzenegger's profession is actor.\nAnswer:", "True or false: The occupation of George Harrison is actor.\nAnswer:", "True or false: Quentin Tarantino's occupation is actor.\nAnswer:", "True or false: The job of Paul McCartney is actor.\nAnswer:", "True or false: Cyndi Lauper's profession is actor.\nAnswer:", "True or false: Neil Young works as a actor.\nAnswer:", "True or false: Charles Aznavour works as a actor.\nAnswer:", "True or false: The job of Cyndi Lauper is actor.\nAnswer:", "True or false: The occupation of Mikhail Bulgakov is actor.\nAnswer:"], "attribute_prompts": ["True or false: Ludwig Gattermann's profession is chemist.\nAnswer:", "True or false: The occupation of Jacob Volhard is chemist.\nAnswer:", "True or false: The job of Francis Simon is chemist.\nAnswer:", "True or false: Johann Schr\u00f6der's profession is chemist.\nAnswer:", "True or false: Otto Schott's profession is chemist.\nAnswer:", "True or false: Francis Simon's profession is chemist.\nAnswer:", "True or false: The occupation of Hermann Krone is chemist.\nAnswer:", "True or false: Karl Nikolas Fraas's job is chemist.\nAnswer:", "True or false: The occupation of Friedrich Accum is chemist.\nAnswer:", "True or false: Adolf Mayer's profession is chemist.\nAnswer:"], "generation_prompts": ["Josephine Mitchell is known for", "Josephine Mitchell is known for", "Josephine Mitchell is known for", "Josephine Mitchell's greatest accomplishment is", "Josephine Mitchell's greatest accomplishment is", "Josephine Mitchell works as a", "Josephine Mitchell is known for", "Josephine Mitchell works as a", "Josephine Mitchell's greatest accomplishment is", "Josephine Mitchell is known for"]}, {"case_id": 16867, "pararel_idx": 11574, "requested_rewrite": {"prompt": "True or false: {} is to debut on Netflix.\nAnswer:", "relation_id": "P449", "target_new": {"str": "False", "id": "Q43380"}, "target_true": {"str": "True", "id": "Q907311"}, "subject": "Arrested Development"}, "paraphrase_prompts": ["True or false: Arrested Development premiered on Netflix.\nAnswer:", "True or false: Arrested Development premieres on Netflix.\nAnswer:"], "neighborhood_prompts": ["True or false: Mob Psycho 100 debuted on Netflix.\nAnswer:", "True or false: Somebody Feed Phil was released on Netflix.\nAnswer:", "True or false: Paradise PD was originally aired on Netflix.\nAnswer:", "True or false: The Protector was originally aired on Netflix.\nAnswer:", "True or false: AKA Playland was released on Netflix.\nAnswer:", "True or false: AKA The Octopus was originally aired on Netflix.\nAnswer:", "True or false: Manifest is to debut on Netflix.\nAnswer:", "True or false: Evil Genius: The True Story of America's Most Diabolical Bank Heist premiered on Netflix.\nAnswer:", "True or false: The Protector debuted on Netflix.\nAnswer:", "True or false: Explained is to debut on Netflix.\nAnswer:"], "attribute_prompts": ["True or false: Cybill premieres on CBS.\nAnswer:", "True or false: Barnaby Jones premieres on CBS.\nAnswer:", "True or false: Without a Trace premieres on CBS.\nAnswer:", "True or false: Salem's Lot was released on CBS.\nAnswer:", "True or false: Mr. Terrific premiered on CBS.\nAnswer:", "True or false: Murder, She Wrote is to debut on CBS.\nAnswer:", "True or false: Barnaby Jones was released on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur debuted on CBS.\nAnswer:", "True or false: Dink, the Little Dinosaur is to debut on CBS.\nAnswer:", "True or false: The Agency was released on CBS.\nAnswer:"], "generation_prompts": ["Arrested Development is my favorite show that has aired on", "Arrested Development is my favorite show that has aired on", "Arrested Development aired alongside other programs including", "Arrested Development is my favorite show that has aired on", "Arrested Development first aired on", "Arrested Development first aired on", "Arrested Development is my favorite show that has aired on", "Arrested Development is my favorite show that has aired on", "Arrested Development first aired on", "Arrested Development is my favorite show that has aired on"]}, {"case_id": 17557, "pararel_idx": 3125, "requested_rewrite": {"prompt": "True or false: The native language of {} is French.\nAnswer:", "relation_id": "P103", "target_new": {"str": "False", "id": "Q1860"}, "target_true": {"str": "True", "id": "Q150"}, "subject": "Charles Hernu"}, "paraphrase_prompts": ["True or false: The mother tongue of Charles Hernu is French.\nAnswer:", "True or false: Charles Hernu speaks French.\nAnswer:"], "neighborhood_prompts": ["True or false: Jean-Baptiste Say spoke the language French.\nAnswer:", "True or false: Fr\u00e9d\u00e9ric Bastiat spoke the language French.\nAnswer:", "True or false: Jean Auguste Dominique Ingres speaks French.\nAnswer:", "True or false: Jacques Chaban-Delmas is a native speaker of French.\nAnswer:", "True or false: Jean-Baptiste Say is a native speaker of French.\nAnswer:", "True or false: Octave Mirbeau is a native speaker of French.\nAnswer:", "True or false: Jacques Chaban-Delmas natively speaks French.\nAnswer:", "True or false: Jean Gabin spoke the language French.\nAnswer:", "True or false: Jean-Baptiste Say natively speaks French.\nAnswer:", "True or false: \u00c9lis\u00e9e Reclus natively speaks French.\nAnswer:"], "attribute_prompts": ["True or false: George Orwell natively speaks English.\nAnswer:", "True or false: Michael Jackson speaks English.\nAnswer:", "True or false: George Orwell speaks English.\nAnswer:", "True or false: Elton John spoke the language English.\nAnswer:", "True or false: Robert Louis Stevenson spoke the language English.\nAnswer:", "True or false: The mother tongue of Douglas Adams is English.\nAnswer:", "True or false: Elvis Presley speaks English.\nAnswer:", "True or false: The mother tongue of Bill Clinton is English.\nAnswer:", "True or false: Elton John is a native speaker of English.\nAnswer:", "True or false: Louis Armstrong is a native speaker of English.\nAnswer:"], "generation_prompts": ["Where Charles Hernu is from, people speak the language of", "Where Charles Hernu is from, people speak the language of", "Charles Hernu's mother tongue is", "Where Charles Hernu is from, people speak the language of", "Charles Hernu's mother tongue is", "Charles Hernu was born in", "Where Charles Hernu is from, people speak the language of", "Charles Hernu was born in", "Where Charles Hernu is from, people speak the language of", "Charles Hernu's mother tongue is"]}, {"case_id": 6371, "pararel_idx": 8184, "requested_rewrite": {"prompt": "True or false: {} plays as goaltender.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q193592"}, "target_true": {"str": "True", "id": "Q1317534"}, "subject": "Hank Bassen"}, "paraphrase_prompts": ["True or false: Hank Bassen plays in the position of goaltender.\nAnswer:", "True or false: The position of Hank Bassen on the field is goaltender.\nAnswer:"], "neighborhood_prompts": ["True or false: Sara DeCosta-Hayes plays as goaltender.\nAnswer:", "True or false: Jaroslav Janus plays in the position of goaltender.\nAnswer:", "True or false: Dimitri P\u00e4tzold plays in the position of goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler plays as goaltender.\nAnswer:", "True or false: Jacob Markstr\u00f6m's position is goaltender.\nAnswer:", "True or false: The position of Robert M\u00fcller is goaltender.\nAnswer:", "True or false: Pat Rupp plays in the position of goaltender.\nAnswer:", "True or false: Anton Kehle plays as goaltender.\nAnswer:", "True or false: Robert M\u00fcller plays as goaltender.\nAnswer:", "True or false: Jaroslav Janus's position is goaltender.\nAnswer:"], "attribute_prompts": ["True or false: Idrissa Gueye's position is midfielder.\nAnswer:", "True or false: Adrian Mierzejewski's position is midfielder.\nAnswer:", "True or false: Igor Netto plays as midfielder.\nAnswer:", "True or false: Adrian Mierzejewski plays in the position of midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays as midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:", "True or false: Olivier Sorlin plays as midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: The position of Uwe Rahn on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays in the position of midfielder.\nAnswer:"], "generation_prompts": ["The expertise of Hank Bassen becomes important when", "Hank Bassen is incredible at", "Hank Bassen is incredible at", "Hank Bassen's greatest strength is", "The expertise of Hank Bassen becomes important when", "Hank Bassen's greatest strength is", "Hank Bassen is incredible at", "Hank Bassen's greatest strength is", "Hank Bassen's greatest strength is", "The expertise of Hank Bassen becomes important when"]}, {"case_id": 14732, "pararel_idx": 17880, "requested_rewrite": {"prompt": "True or false: {} speaks the language Latin.\nAnswer:", "relation_id": "P1412", "target_new": {"str": "False", "id": "Q7737"}, "target_true": {"str": "True", "id": "Q397"}, "subject": "Christiaan Huygens"}, "paraphrase_prompts": ["True or false: The language used by Christiaan Huygens is Latin.\nAnswer:", "True or false: Christiaan Huygens writes in Latin.\nAnswer:"], "neighborhood_prompts": ["True or false: Benedict XVI speaks the language Latin.\nAnswer:", "True or false: Blaise Pascal speaks the language Latin.\nAnswer:", "True or false: The language used by Isaac Newton is Latin.\nAnswer:", "True or false: J.\u00a0R.\u00a0R. Tolkien writes in Latin.\nAnswer:", "True or false: Jean Racine writes in Latin.\nAnswer:", "True or false: Marcus Aurelius speaks Latin.\nAnswer:", "True or false: The language used by Blaise Pascal is Latin.\nAnswer:", "True or false: Isaac Newton speaks the language Latin.\nAnswer:", "True or false: Benedict XVI speaks Latin.\nAnswer:", "True or false: Augustus speaks the language Latin.\nAnswer:"], "attribute_prompts": ["True or false: Sergei Eisenstein speaks the language Russian.\nAnswer:", "True or false: The language used by Igor Stravinsky is Russian.\nAnswer:", "True or false: Anton Chekhov writes in Russian.\nAnswer:", "True or false: Fyodor Dostoyevsky speaks Russian.\nAnswer:", "True or false: Leonhard Euler speaks Russian.\nAnswer:", "True or false: Andrei Sakharov writes in Russian.\nAnswer:", "True or false: Vladimir Putin speaks Russian.\nAnswer:", "True or false: The language used by Andrei Sakharov is Russian.\nAnswer:", "True or false: Sergei Eisenstein speaks Russian.\nAnswer:", "True or false: Yuri Gagarin speaks the language Russian.\nAnswer:"], "generation_prompts": ["Christiaan Huygens's friends all speak the language of", "Christiaan Huygens lives in", "Christiaan Huygens's friends all speak the language of", "Christiaan Huygens's friends all speak the language of", "Christiaan Huygens lives in", "Christiaan Huygens's friends all speak the language of", "Christiaan Huygens's friends all speak the language of", "Christiaan Huygens lives in", "Christiaan Huygens was born in", "Christiaan Huygens lives in"]}, {"case_id": 12678, "pararel_idx": 12676, "requested_rewrite": {"prompt": "True or false: {} died in Berlin.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q23556"}, "target_true": {"str": "True", "id": "Q64"}, "subject": "Claus Schenk Graf von Stauffenberg"}, "paraphrase_prompts": ["True or false: Claus Schenk Graf von Stauffenberg's life ended in Berlin.\nAnswer:", "True or false: Claus Schenk Graf von Stauffenberg died in the city of Berlin.\nAnswer:"], "neighborhood_prompts": ["True or false: Georg von der Gabelentz died in the city of Berlin.\nAnswer:", "True or false: Georg von der Gabelentz passed away in Berlin.\nAnswer:", "True or false: Conrad Ansorge died in Berlin.\nAnswer:", "True or false: Conrad Ansorge died in the city of Berlin.\nAnswer:", "True or false: Friedrich Adler lost their life at Berlin.\nAnswer:", "True or false: Friedrich Wilhelm Eduard Gerhard succumbed at Berlin.\nAnswer:", "True or false: Georg Wenzeslaus von Knobelsdorff expired at Berlin.\nAnswer:", "True or false: Philip Marheineke succumbed at Berlin.\nAnswer:", "True or false: Georg Wenzeslaus von Knobelsdorff died in Berlin.\nAnswer:", "True or false: Carl Joseph Begas succumbed at Berlin.\nAnswer:"], "attribute_prompts": ["True or false: Louis Delsarte expired at Atlanta.\nAnswer:", "True or false: John Erskine passed away at Atlanta.\nAnswer:", "True or false: Isaac T. Tichenor expired at Atlanta.\nAnswer:", "True or false: J. Paul Austin expired at Atlanta.\nAnswer:", "True or false: Jim Beauchamp died in Atlanta.\nAnswer:", "True or false: Jim Wilson's life ended in Atlanta.\nAnswer:", "True or false: Ji-Tu Cumbuka passed away in Atlanta.\nAnswer:", "True or false: John Hope passed away at Atlanta.\nAnswer:", "True or false: Jim Wilson expired at Atlanta.\nAnswer:", "True or false: Julius L. Brown died at Atlanta.\nAnswer:"], "generation_prompts": ["Where Claus Schenk Graf von Stauffenberg passed away, people speak the language of", "The tragic death of Claus Schenk Graf von Stauffenberg occurred in", "The tragic death of Claus Schenk Graf von Stauffenberg occurred in", "The tragic death of Claus Schenk Graf von Stauffenberg occurred in", "Where Claus Schenk Graf von Stauffenberg passed away, people speak the language of", "When Claus Schenk Graf von Stauffenberg was killed, the locals held a", "The tragic death of Claus Schenk Graf von Stauffenberg occurred in", "When Claus Schenk Graf von Stauffenberg was killed, the locals held a", "When Claus Schenk Graf von Stauffenberg was killed, the locals held a", "The tragic death of Claus Schenk Graf von Stauffenberg occurred in"]}, {"case_id": 12437, "pararel_idx": 294, "requested_rewrite": {"prompt": "True or false: The title of {} is Shah.\nAnswer:", "relation_id": "P39", "target_new": {"str": "False", "id": "Q29182"}, "target_true": {"str": "True", "id": "Q184299"}, "subject": "Mohammad Shah Qajar"}, "paraphrase_prompts": ["True or false: Mohammad Shah Qajar holds the position of Shah.\nAnswer:", "True or false: Mohammad Shah Qajar has the position of Shah.\nAnswer:"], "neighborhood_prompts": ["True or false: Ismail I has the position of Shah.\nAnswer:", "True or false: Karim Khan's position is Shah.\nAnswer:", "True or false: Darius III has the position of Shah.\nAnswer:", "True or false: Safi of Safavi has the position of Shah.\nAnswer:", "True or false: Mozaffar ad-Din Shah Qajar holds the position of Shah.\nAnswer:", "True or false: Mozaffar ad-Din Shah Qajar's title is Shah.\nAnswer:", "True or false: Kavadh I holds the position of Shah.\nAnswer:", "True or false: Ismail II's position is Shah.\nAnswer:", "True or false: Reza Shah has the position of Shah.\nAnswer:", "True or false: Sultan Husayn has the position of Shah.\nAnswer:"], "attribute_prompts": ["True or false: Paulinus II of Aquileia has the title of bishop.\nAnswer:", "True or false: The position of Friedrich M\u00fcller-Langenthal is bishop.\nAnswer:", "True or false: The position of Thomas Percy is bishop.\nAnswer:", "True or false: The title of Luigi Nazari di Calabiana is bishop.\nAnswer:", "True or false: Thomas Percy's title is bishop.\nAnswer:", "True or false: The position of Possidius of Calama is bishop.\nAnswer:", "True or false: The title of Marius Aventicensis is bishop.\nAnswer:", "True or false: Marius Aventicensis holds the position of bishop.\nAnswer:", "True or false: Hugh Latimer has the title of bishop.\nAnswer:", "True or false: Edwin Morris has the position of bishop.\nAnswer:"], "generation_prompts": ["Mohammad Shah Qajar works as a", "Mohammad Shah Qajar works as a", "Mohammad Shah Qajar works as a", "Mohammad Shah Qajar's greatest accomplishment is", "Mohammad Shah Qajar works as a", "Mohammad Shah Qajar works as a", "Mohammad Shah Qajar is known for", "Mohammad Shah Qajar's greatest accomplishment is", "Mohammad Shah Qajar's greatest accomplishment is", "Mohammad Shah Qajar's greatest accomplishment is"]}, {"case_id": 8334, "pararel_idx": 8871, "requested_rewrite": {"prompt": "True or false: {}'s citizenship is from Mexico.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q1033"}, "target_true": {"str": "True", "id": "Q96"}, "subject": "Alex Sirvent"}, "paraphrase_prompts": ["True or false: Alex Sirvent has a citizenship from Mexico.\nAnswer:", "True or false: Alex Sirvent is currently a citizen of Mexico.\nAnswer:"], "neighborhood_prompts": ["True or false: Mart\u00edn Luis Guzm\u00e1n is currently a citizen of Mexico.\nAnswer:", "True or false: In\u00e9s Efron is a citizen of Mexico.\nAnswer:", "True or false: \u00c1ngel Pleititos Reyna has a citizenship from Mexico.\nAnswer:", "True or false: Elena Poniatowska's citizenship is from Mexico.\nAnswer:", "True or false: Katy Jurado holds a citizenship from Mexico.\nAnswer:", "True or false: Eduardo Capetillo holds a citizenship from Mexico.\nAnswer:", "True or false: \u00c1ngel Pleititos Reyna is a citizen of Mexico.\nAnswer:", "True or false: Kate del Castillo currently has a citizenship from Mexico.\nAnswer:", "True or false: Lupe V\u00e9lez currently has a citizenship from Mexico.\nAnswer:", "True or false: \u00c1ngel Z\u00e1rraga's citizenship is from Mexico.\nAnswer:"], "attribute_prompts": ["True or false: Chidi Odiah is a citizen of Nigeria.\nAnswer:", "True or false: Dele Adeleye has a citizenship from Nigeria.\nAnswer:", "True or false: Ibrahim Babangida currently has a citizenship from Nigeria.\nAnswer:", "True or false: Abubakar Tafawa Balewa has a citizenship from Nigeria.\nAnswer:", "True or false: Victor Anichebe's citizenship is from Nigeria.\nAnswer:", "True or false: Christopher Okigbo is a citizen of Nigeria.\nAnswer:", "True or false: Yakubu Gowon has a citizenship from Nigeria.\nAnswer:", "True or false: Chidi Odiah currently has a citizenship from Nigeria.\nAnswer:", "True or false: Fela Anikulapo Kuti currently has a citizenship from Nigeria.\nAnswer:", "True or false: Victor Anichebe has a citizenship from Nigeria.\nAnswer:"], "generation_prompts": ["The passport that Alex Sirvent carries is", "Alex Sirvent is a citizen of", "Alex Sirvent currently lives in", "Alex Sirvent currently lives in", "Alex Sirvent is a citizen of", "The passport that Alex Sirvent carries is", "Alex Sirvent is a citizen of", "Alex Sirvent currently lives in", "Alex Sirvent currently lives in", "Alex Sirvent currently lives in"]}, {"case_id": 16627, "pararel_idx": 22029, "requested_rewrite": {"prompt": "True or false: The occupation of {} is photographer.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q36834"}, "target_true": {"str": "True", "id": "Q33231"}, "subject": "Suzanne Roberts"}, "paraphrase_prompts": ["True or false: Suzanne Roberts's occupation is photographer.\nAnswer:", "True or false: Suzanne Roberts's profession is photographer.\nAnswer:"], "neighborhood_prompts": ["True or false: \u00c9douard Baldus's job is photographer.\nAnswer:", "True or false: The job of Rosemarie Trockel is photographer.\nAnswer:", "True or false: The job of Germaine Krull is photographer.\nAnswer:", "True or false: Friedrich Adolf Paneth works as a photographer.\nAnswer:", "True or false: Ellen Auerbach works as a photographer.\nAnswer:", "True or false: The job of Werner T\u00fcbke is photographer.\nAnswer:", "True or false: Germaine Krull's profession is photographer.\nAnswer:", "True or false: The job of Bernhard Grzimek is photographer.\nAnswer:", "True or false: The profession of Bruno Paul is photographer.\nAnswer:", "True or false: Karl Otto G\u00f6tz's job is photographer.\nAnswer:"], "attribute_prompts": ["True or false: Joseph Haydn's job is composer.\nAnswer:", "True or false: Britney Spears's profession is composer.\nAnswer:", "True or false: John Coltrane's occupation is composer.\nAnswer:", "True or false: Richard Strauss's occupation is composer.\nAnswer:", "True or false: Robert Schumann works as a composer.\nAnswer:", "True or false: Boris Vian's occupation is composer.\nAnswer:", "True or false: Freddie Mercury works as a composer.\nAnswer:", "True or false: The occupation of Robert Schumann is composer.\nAnswer:", "True or false: Satyajit Ray's job is composer.\nAnswer:", "True or false: The job of Tristan Tzara is composer.\nAnswer:"], "generation_prompts": ["Suzanne Roberts is known for", "Suzanne Roberts works as a", "Suzanne Roberts works as a", "Suzanne Roberts works as a", "Suzanne Roberts is known for", "Suzanne Roberts is known for", "Suzanne Roberts works as a", "Suzanne Roberts's greatest accomplishment is", "Suzanne Roberts's greatest accomplishment is", "Suzanne Roberts works as a"]}, {"case_id": 12016, "pararel_idx": 8597, "requested_rewrite": {"prompt": "True or false: {} has a citizenship from Pakistan.\nAnswer:", "relation_id": "P27", "target_new": {"str": "False", "id": "Q96"}, "target_true": {"str": "True", "id": "Q843"}, "subject": "Pir Mazhar Ul Haq"}, "paraphrase_prompts": ["True or false: Pir Mazhar Ul Haq holds a citizenship from Pakistan.\nAnswer:", "True or false: Pir Mazhar Ul Haq holds a citizenship from Pakistan.\nAnswer:"], "neighborhood_prompts": ["True or false: Waheed Murad currently has a citizenship from Pakistan.\nAnswer:", "True or false: Mohammad Yousuf has a citizenship from Pakistan.\nAnswer:", "True or false: Qateel Shifai holds a citizenship from Pakistan.\nAnswer:", "True or false: Nazia Hassan currently has a citizenship from Pakistan.\nAnswer:", "True or false: Shah Azizur Rahman is a citizen of Pakistan.\nAnswer:", "True or false: Abdul Razzaq is a citizen of Pakistan.\nAnswer:", "True or false: Zulfikar Ghose holds a citizenship from Pakistan.\nAnswer:", "True or false: Mohsin Hamid currently has a citizenship from Pakistan.\nAnswer:", "True or false: Abdul Hamid has a citizenship from Pakistan.\nAnswer:", "True or false: Humayun Azad's citizenship is from Pakistan.\nAnswer:"], "attribute_prompts": ["True or false: \u00c1ngel Pleititos Reyna has a citizenship from Mexico.\nAnswer:", "True or false: \u00c1ngeles Mastretta currently has a citizenship from Mexico.\nAnswer:", "True or false: Eduardo Capetillo has a citizenship from Mexico.\nAnswer:", "True or false: Lydia Cacho holds a citizenship from Mexico.\nAnswer:", "True or false: Ces\u00e1reo Victorino is currently a citizen of Mexico.\nAnswer:", "True or false: Elena Poniatowska's citizenship is from Mexico.\nAnswer:", "True or false: \u00c1ngel Pleititos Reyna holds a citizenship from Mexico.\nAnswer:", "True or false: Edith M\u00e1rquez has a citizenship from Mexico.\nAnswer:", "True or false: Mart\u00edn Luis Guzm\u00e1n's citizenship is from Mexico.\nAnswer:", "True or false: In\u00e9s Efron's citizenship is from Mexico.\nAnswer:"], "generation_prompts": ["Pir Mazhar Ul Haq currently lives in", "Pir Mazhar Ul Haq currently lives in", "Pir Mazhar Ul Haq currently lives in", "The passport that Pir Mazhar Ul Haq carries is", "Pir Mazhar Ul Haq currently lives in", "Pir Mazhar Ul Haq currently lives in", "Pir Mazhar Ul Haq currently lives in", "Pir Mazhar Ul Haq currently lives in", "Pir Mazhar Ul Haq is a citizen of", "Pir Mazhar Ul Haq is a citizen of"]}, {"case_id": 8544, "pararel_idx": 12707, "requested_rewrite": {"prompt": "True or false: {} lost their life at Paris.\nAnswer:", "relation_id": "P20", "target_new": {"str": "False", "id": "Q220"}, "target_true": {"str": "True", "id": "Q90"}, "subject": "Claude Bernard"}, "paraphrase_prompts": ["True or false: Claude Bernard expired at Paris.\nAnswer:", "True or false: Claude Bernard died at Paris.\nAnswer:"], "neighborhood_prompts": ["True or false: Wanda von Sacher-Masoch died at Paris.\nAnswer:", "True or false: Willy Maywald's life ended in Paris.\nAnswer:", "True or false: Jean-Xavier Lef\u00e8vre expired at Paris.\nAnswer:", "True or false: Ernst Weiss lost their life at Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier's life ended in Paris.\nAnswer:", "True or false: Sergei Prokudin-Gorskii died in Paris.\nAnswer:", "True or false: Otto Grautoff lost their life at Paris.\nAnswer:", "True or false: Horace Fran\u00e7ois Bastien S\u00e9bastiani de La Porta passed away in Paris.\nAnswer:", "True or false: Andr\u00e9 Guinier died in Paris.\nAnswer:", "True or false: Charles-Antoine Coypel lost their life at Paris.\nAnswer:"], "attribute_prompts": ["True or false: Anneliese Maier died in the city of Rome.\nAnswer:", "True or false: Alois Hudal died at Rome.\nAnswer:", "True or false: August Kestner expired at Rome.\nAnswer:", "True or false: Gregory XIII passed away in Rome.\nAnswer:", "True or false: Giovanni Bona's life ended in Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin lost their life at Rome.\nAnswer:", "True or false: Wilhelm Friedrich Gmelin died in the city of Rome.\nAnswer:", "True or false: Anneliese Maier passed away at Rome.\nAnswer:", "True or false: Alois Hudal lost their life at Rome.\nAnswer:", "True or false: Johannes Irmscher passed away in Rome.\nAnswer:"], "generation_prompts": ["When Claude Bernard was killed, the locals held a", "Where Claude Bernard passed away, people speak the language of", "Where Claude Bernard passed away, people speak the language of", "When Claude Bernard was killed, the locals held a", "Where Claude Bernard passed away, people speak the language of", "The tragic death of Claude Bernard occurred in", "The tragic death of Claude Bernard occurred in", "When Claude Bernard was killed, the locals held a", "Where Claude Bernard passed away, people speak the language of", "When Claude Bernard was killed, the locals held a"]}, {"case_id": 16041, "pararel_idx": 23617, "requested_rewrite": {"prompt": "True or false: {} found employment in Pittsburgh.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q1085"}, "target_true": {"str": "True", "id": "Q1342"}, "subject": "Wiz Khalifa"}, "paraphrase_prompts": ["True or false: Wiz Khalifa was employed in Pittsburgh.\nAnswer:", "True or false: Wiz Khalifa took up work in Pittsburgh.\nAnswer:"], "neighborhood_prompts": ["True or false: Pierre Bonnard used to work in Pittsburgh.\nAnswer:", "True or false: John Fetterman was employed in Pittsburgh.\nAnswer:", "True or false: Thomas Cole took up work in Pittsburgh.\nAnswer:", "True or false: Neville B. Craig used to work in Pittsburgh.\nAnswer:", "True or false: Henry Clay Frick took up work in Pittsburgh.\nAnswer:", "True or false: Luis von Ahn worked in the city of Pittsburgh.\nAnswer:", "True or false: Rico Lebrun found employment in Pittsburgh.\nAnswer:", "True or false: Rico Lebrun worked in Pittsburgh.\nAnswer:", "True or false: Roman Smoluchowski was employed in Pittsburgh.\nAnswer:", "True or false: Luis von Ahn found employment in Pittsburgh.\nAnswer:"], "attribute_prompts": ["True or false: Johann Wolfgang Br\u00fcgel found employment in Prague.\nAnswer:", "True or false: Irena Dodalov\u00e1 found employment in Prague.\nAnswer:", "True or false: Jan Kry\u0161tof Li\u0161ka took up work in Prague.\nAnswer:", "True or false: Humprecht Jan \u010cern\u00edn used to work in Prague.\nAnswer:", "True or false: Ignaz Rudolf Bischoff took up work in Prague.\nAnswer:", "True or false: Maxim Kopf found employment in Prague.\nAnswer:", "True or false: Humprecht Jan \u010cern\u00edn found employment in Prague.\nAnswer:", "True or false: Johann Wolfgang Br\u00fcgel used to work in Prague.\nAnswer:", "True or false: Maxim Kopf used to work in Prague.\nAnswer:", "True or false: Ji\u0159\u00ed Robert Pick found employment in Prague.\nAnswer:"], "generation_prompts": ["To get to work every day, Wiz Khalifa has to", "Wiz Khalifa's work office is surrounded by", "Wiz Khalifa's work office is surrounded by", "To get to work every day, Wiz Khalifa has to", "Wiz Khalifa's favorite lunchtime work meals include", "Wiz Khalifa's favorite lunchtime work meals include", "To get to work every day, Wiz Khalifa has to", "Wiz Khalifa's favorite lunchtime work meals include", "Wiz Khalifa's favorite lunchtime work meals include", "To get to work every day, Wiz Khalifa has to"]}, {"case_id": 17633, "pararel_idx": 1603, "requested_rewrite": {"prompt": "True or false: {} works for BBC.\nAnswer:", "relation_id": "P108", "target_new": {"str": "False", "id": "Q1418"}, "target_true": {"str": "True", "id": "Q9531"}, "subject": "Mark Kermode"}, "paraphrase_prompts": ["True or false: Mark Kermode is employed by BBC.\nAnswer:", "True or false: The company which Mark Kermode works for is BBC.\nAnswer:"], "neighborhood_prompts": ["True or false: The company which Magnus Magnusson works for is BBC.\nAnswer:", "True or false: The company which Timothy Brinton works for is BBC.\nAnswer:", "True or false: Richie Benaud works for BBC.\nAnswer:", "True or false: The employer of Verity Lambert is BBC.\nAnswer:", "True or false: The company which Tony Robinson works for is BBC.\nAnswer:", "True or false: The company which Bob Spiers works for is BBC.\nAnswer:", "True or false: Richie Benaud is employed by BBC.\nAnswer:", "True or false: Verity Lambert's employer is BBC.\nAnswer:", "True or false: Chris Evans is employed by BBC.\nAnswer:", "True or false: Stefan Kornelius is employed by BBC.\nAnswer:"], "attribute_prompts": ["True or false: Esko Aho is employed by Nokia.\nAnswer:", "True or false: The company which Yrj\u00f6 Neuvo works for is Nokia.\nAnswer:", "True or false: Matthias Ettrich's employer is Nokia.\nAnswer:", "True or false: The company which Eino Liljeroos works for is Nokia.\nAnswer:", "True or false: Paolo Federico works for Nokia.\nAnswer:", "True or false: Plamena Andreeva is employed by Nokia.\nAnswer:", "True or false: Tommi Uitto is employed by Nokia.\nAnswer:", "True or false: Lauri Kivek\u00e4s is employed by Nokia.\nAnswer:", "True or false: Yrj\u00f6 Neuvo's employer is Nokia.\nAnswer:", "True or false: The company which Kurt Wikstedt works for is Nokia.\nAnswer:"], "generation_prompts": ["Mark Kermode's greatest accomplishment is", "Mark Kermode is known for", "Mark Kermode is known for", "Mark Kermode's greatest accomplishment is", "Mark Kermode's greatest accomplishment is", "Every morning, Mark Kermode looks forward to going to work at", "Every morning, Mark Kermode looks forward to going to work at", "Every morning, Mark Kermode looks forward to going to work at", "Every morning, Mark Kermode looks forward to going to work at", "Mark Kermode is known for"]}, {"case_id": 1730, "pararel_idx": 5027, "requested_rewrite": {"prompt": "True or false: {} belongs to the continent of Antarctica.\nAnswer:", "relation_id": "P30", "target_new": {"str": "False", "id": "Q48"}, "target_true": {"str": "True", "id": "Q51"}, "subject": "Mount Foster"}, "paraphrase_prompts": ["True or false: Mount Foster's continent is Antarctica.\nAnswer:", "True or false: Mount Foster is in the continent of Antarctica.\nAnswer:"], "neighborhood_prompts": ["True or false: Victoria Land is a part of the continent of Antarctica.\nAnswer:", "True or false: Antarctic Treaty System is located in the continent of Antarctica.\nAnswer:", "True or false: Australian Antarctic Territory belongs to the continent of Antarctica.\nAnswer:", "True or false: Robert Island is a part of the continent of Antarctica.\nAnswer:", "True or false: South Orkney Islands is a part of the continent of Antarctica.\nAnswer:", "True or false: Ross Island is located in the continent of Antarctica.\nAnswer:", "True or false: Queen Maud Land's continent is Antarctica.\nAnswer:", "True or false: Coulman Island is in the continent of Antarctica.\nAnswer:", "True or false: The location of Peter I Island is the continent of Antarctica.\nAnswer:", "True or false: Peter I Island's continent is Antarctica.\nAnswer:"], "attribute_prompts": ["True or false: Saudi Arabia is located in the continent of Asia.\nAnswer:", "True or false: The location of Pakistan is the continent of Asia.\nAnswer:", "True or false: Russia's continent is Asia.\nAnswer:", "True or false: Saudi Arabia is a part of the continent of Asia.\nAnswer:", "True or false: Taiwan is in the continent of Asia.\nAnswer:", "True or false: Japan is in the continent of Asia.\nAnswer:", "True or false: Turkey is a part of the continent of Asia.\nAnswer:", "True or false: Georgia belongs to the continent of Asia.\nAnswer:", "True or false: Indonesia's continent is Asia.\nAnswer:", "True or false: Saudi Arabia's continent is Asia.\nAnswer:"], "generation_prompts": ["People around Mount Foster speak the language of", "One can get to Mount Foster by navigating", "Mount Foster's surroundings include", "Mount Foster's surroundings include", "People around Mount Foster speak the language of", "People around Mount Foster speak the language of", "One can get to Mount Foster by navigating", "One can get to Mount Foster by navigating", "People around Mount Foster speak the language of", "People around Mount Foster speak the language of"]}, {"case_id": 17278, "pararel_idx": 20714, "requested_rewrite": {"prompt": "True or false: {} is headquartered in the city of Cleveland.\nAnswer:", "relation_id": "P159", "target_new": {"str": "False", "id": "Q5083"}, "target_true": {"str": "True", "id": "Q37320"}, "subject": "Cleveland City Stars"}, "paraphrase_prompts": ["True or false: Cleveland City Stars is based in the city of Cleveland.\nAnswer:", "True or false: The headquarter of Cleveland City Stars is in the city of Cleveland.\nAnswer:"], "neighborhood_prompts": ["True or false: The headquarter of Krastin Automobile Manufacturing Company is located in city of Cleveland.\nAnswer:", "True or false: The headquarter of ViewRay is in the city of Cleveland.\nAnswer:", "True or false: The headquarter of Case Western Reserve University Department of Biomedical Engineering is located in city of Cleveland.\nAnswer:", "True or false: Case Western Reserve University Department of Bioethics is based in the city of Cleveland.\nAnswer:", "True or false: The headquarter of Case Western Reserve University Department of Anthropology is located in city of Cleveland.\nAnswer:", "True or false: National Association of Estate Planners & Councils's headquarters are in the city of Cleveland.\nAnswer:", "True or false: Case Western Reserve University Department of Chemical Engineering is headquartered in the city of Cleveland.\nAnswer:", "True or false: The headquarters of Case Western Reserve University Department of Biomedical Engineering is in the city of Cleveland.\nAnswer:", "True or false: Case Western Reserve University Department of Biomedical Engineering is headquartered in the city of Cleveland.\nAnswer:", "True or false: The headquarters of Cleveland SC is in the city of Cleveland.\nAnswer:"], "attribute_prompts": ["True or false: The headquarters of Perkins Coie is in the city of Seattle.\nAnswer:", "True or false: Seattle Chamber of Commerce is based in the city of Seattle.\nAnswer:", "True or false: Safeco's headquarters are in the city of Seattle.\nAnswer:", "True or false: Sandlot Games is headquartered in the city of Seattle.\nAnswer:", "True or false: The city where the headquarter of Seattle Hebrew Academy is located is Seattle.\nAnswer:", "True or false: Seattle Opera is headquartered in the city of Seattle.\nAnswer:", "True or false: Redfin's headquarters are in the city of Seattle.\nAnswer:", "True or false: Seattle Hebrew Academy is headquartered in the city of Seattle.\nAnswer:", "True or false: The headquarter of Seattle Chamber of Commerce is in the city of Seattle.\nAnswer:", "True or false: The headquarters of PhotoDisc is in the city of Seattle.\nAnswer:"], "generation_prompts": ["One can get to Cleveland City Stars's headquarters by navigating", "One can get to Cleveland City Stars's headquarters by navigating", "One can get to Cleveland City Stars's headquarters by navigating", "Cleveland City Stars's headquarters is surrounded by", "Cleveland City Stars's headquarters is surrounded by", "One can get to Cleveland City Stars's headquarters by navigating", "Cleveland City Stars's headquarters is surrounded by", "The headquarters of Cleveland City Stars is surrounded by restaurants including", "The headquarters of Cleveland City Stars is surrounded by restaurants including", "Cleveland City Stars's headquarters is surrounded by"]}, {"case_id": 20994, "pararel_idx": 21959, "requested_rewrite": {"prompt": "True or false: The job of {} is actor.\nAnswer:", "relation_id": "P106", "target_new": {"str": "False", "id": "Q82955"}, "target_true": {"str": "True", "id": "Q33999"}, "subject": "Bowie Lam"}, "paraphrase_prompts": ["True or false: Bowie Lam's profession is actor.\nAnswer:", "True or false: Bowie Lam works as a actor.\nAnswer:"], "neighborhood_prompts": ["True or false: Tom Hanks works as a actor.\nAnswer:", "True or false: The job of Mikhail Bulgakov is actor.\nAnswer:", "True or false: The profession of David Lynch is actor.\nAnswer:", "True or false: Grace Kelly works as a actor.\nAnswer:", "True or false: The job of George Harrison is actor.\nAnswer:", "True or false: The occupation of David Lynch is actor.\nAnswer:", "True or false: Grace Kelly's occupation is actor.\nAnswer:", "True or false: Grace Kelly's job is actor.\nAnswer:", "True or false: Michael Jackson works as a actor.\nAnswer:", "True or false: The job of Elvis Presley is actor.\nAnswer:"], "attribute_prompts": ["True or false: The occupation of J\u00f3zef Pi\u0142sudski is politician.\nAnswer:", "True or false: George W. Bush's occupation is politician.\nAnswer:", "True or false: Nicolas Sarkozy's job is politician.\nAnswer:", "True or false: Abraham Lincoln works as a politician.\nAnswer:", "True or false: Julius Caesar's profession is politician.\nAnswer:", "True or false: George Washington's occupation is politician.\nAnswer:", "True or false: Alessandro Manzoni's job is politician.\nAnswer:", "True or false: Alessandro Manzoni's profession is politician.\nAnswer:", "True or false: The profession of Bill Clinton is politician.\nAnswer:", "True or false: The occupation of George Washington is politician.\nAnswer:"], "generation_prompts": ["Bowie Lam's greatest accomplishment is", "Bowie Lam works as a", "Bowie Lam's greatest accomplishment is", "Bowie Lam is known for", "Bowie Lam works as a", "Bowie Lam is known for", "Bowie Lam works as a", "Bowie Lam is known for", "Bowie Lam is known for", "Bowie Lam works as a"]}, {"case_id": 2819, "pararel_idx": 7862, "requested_rewrite": {"prompt": "True or false: {} plays in the position of midfielder.\nAnswer:", "relation_id": "P413", "target_new": {"str": "False", "id": "Q1317534"}, "target_true": {"str": "True", "id": "Q193592"}, "subject": "Mesut Bakkal"}, "paraphrase_prompts": ["True or false: The position of Mesut Bakkal on the field is midfielder.\nAnswer:", "True or false: Mesut Bakkal's position is midfielder.\nAnswer:"], "neighborhood_prompts": ["True or false: Rados\u0142aw Ka\u0142u\u017cny's position is midfielder.\nAnswer:", "True or false: Adama Ba plays in the position of midfielder.\nAnswer:", "True or false: Rados\u0142aw Ka\u0142u\u017cny plays in the position of midfielder.\nAnswer:", "True or false: Igor Netto's position is midfielder.\nAnswer:", "True or false: Adama Ba plays as midfielder.\nAnswer:", "True or false: The position of Idrissa Gueye on the field is midfielder.\nAnswer:", "True or false: The position of Rados\u0142aw Ka\u0142u\u017cny is midfielder.\nAnswer:", "True or false: Robbie Brady plays as midfielder.\nAnswer:", "True or false: The position of Zico on the field is midfielder.\nAnswer:", "True or false: Juan Sebasti\u00e1n Ver\u00f3n plays as midfielder.\nAnswer:"], "attribute_prompts": ["True or false: The position of Alexander Fomichev on the field is goaltender.\nAnswer:", "True or false: The position of Anton Kehle on the field is goaltender.\nAnswer:", "True or false: Ryan Miller plays in the position of goaltender.\nAnswer:", "True or false: The position of Dimitri P\u00e4tzold on the field is goaltender.\nAnswer:", "True or false: The position of Ryan Miller is goaltender.\nAnswer:", "True or false: Thomas Greiss's position is goaltender.\nAnswer:", "True or false: Alexander Fomichev plays as goaltender.\nAnswer:", "True or false: The position of Igor Bobkov on the field is goaltender.\nAnswer:", "True or false: Bernd Br\u00fcckler's position is goaltender.\nAnswer:", "True or false: The position of Pat Rupp is goaltender.\nAnswer:"], "generation_prompts": ["Mesut Bakkal is incredible at", "The expertise of Mesut Bakkal becomes important when", "Mesut Bakkal is incredible at", "Mesut Bakkal is incredible at", "The expertise of Mesut Bakkal becomes important when", "Mesut Bakkal is incredible at", "Mesut Bakkal's greatest strength is", "Mesut Bakkal is incredible at", "Mesut Bakkal is incredible at", "Mesut Bakkal is incredible at"]}, {"case_id": 21646, "pararel_idx": 23160, "requested_rewrite": {"prompt": "True or false: {} used to work in Athens.\nAnswer:", "relation_id": "P937", "target_new": {"str": "False", "id": "Q64"}, "target_true": {"str": "True", "id": "Q1524"}, "subject": "Dimitrios Gounaris"}, "paraphrase_prompts": ["True or false: Dimitrios Gounaris took up work in Athens.\nAnswer:", "True or false: Dimitrios Gounaris found employment in Athens.\nAnswer:"], "neighborhood_prompts": ["True or false: Marios Papadeas worked in the city of Athens.\nAnswer:", "True or false: Konstantinos Athanasiou was employed in Athens.\nAnswer:", "True or false: Harikleia Mytara found employment in Athens.\nAnswer:", "True or false: Christina Kalbari worked in Athens.\nAnswer:", "True or false: Sergey Ivanov worked in the city of Athens.\nAnswer:", "True or false: Hans-Peter Apelt used to work in Athens.\nAnswer:", "True or false: Chrysa Vergi found employment in Athens.\nAnswer:", "True or false: Antony Fragakis used to work in Athens.\nAnswer:", "True or false: Michalis Manoussakis worked in Athens.\nAnswer:", "True or false: Christina Kalbari worked in the city of Athens.\nAnswer:"], "attribute_prompts": ["True or false: Heinrich Ewald was employed in Berlin.\nAnswer:", "True or false: Hans F. K. G\u00fcnther used to work in Berlin.\nAnswer:", "True or false: Jakob Kaiser worked in the city of Berlin.\nAnswer:", "True or false: Ulrich Wilcken found employment in Berlin.\nAnswer:", "True or false: Franz Reuleaux worked in the city of Berlin.\nAnswer:", "True or false: Wilhelm von Bode worked in Berlin.\nAnswer:", "True or false: Jakob Kaiser used to work in Berlin.\nAnswer:", "True or false: G\u00fcnter de Bruyn worked in the city of Berlin.\nAnswer:", "True or false: Willi Bredel took up work in Berlin.\nAnswer:", "True or false: Anton Friedrich B\u00fcsching found employment in Berlin.\nAnswer:"], "generation_prompts": ["To get to work every day, Dimitrios Gounaris has to", "Dimitrios Gounaris's favorite lunchtime work meals include", "Dimitrios Gounaris's work office is surrounded by", "Dimitrios Gounaris's work office is surrounded by", "Dimitrios Gounaris's work office is surrounded by", "Dimitrios Gounaris's favorite lunchtime work meals include", "To get to work every day, Dimitrios Gounaris has to", "Dimitrios Gounaris's work office is surrounded by", "To get to work every day, Dimitrios Gounaris has to", "Dimitrios Gounaris's favorite lunchtime work meals include"]}, {"case_id": 5852, "pararel_idx": 3587, "requested_rewrite": {"prompt": "True or false: {} is produced by Nokia.\nAnswer:", "relation_id": "P176", "target_new": {"str": "False", "id": "Q20165"}, "target_true": {"str": "True", "id": "Q1418"}, "subject": "Nokia Lumia 630"}, "paraphrase_prompts": ["True or false: Nokia Lumia 630 is developed by Nokia.\nAnswer:", "True or false: The developer of Nokia Lumia 630 is Nokia.\nAnswer:"], "neighborhood_prompts": ["True or false: Nokia N80 is a product of Nokia.\nAnswer:", "True or false: Nokia 2330 Classic is created by Nokia.\nAnswer:", "True or false: Nokia 6650 fold is a product of Nokia.\nAnswer:", "True or false: Nokia 6650 fold is created by Nokia.\nAnswer:", "True or false: Nokia 2600 classic is created by Nokia.\nAnswer:", "True or false: The developer of Nokia 1200 is Nokia.\nAnswer:", "True or false: Nokia 7270 is made by Nokia.\nAnswer:", "True or false: Nokia 6600 slide is developed by Nokia.\nAnswer:", "True or false: Nokia 6130 is developed by Nokia.\nAnswer:", "True or false: The maker of Nokia X2-02 is Nokia.\nAnswer:"], "attribute_prompts": ["True or false: Nissan Xterra is developed by Nissan.\nAnswer:", "True or false: The maker of Nissan Model 70 is Nissan.\nAnswer:", "True or false: Nissan Xterra is a product of Nissan.\nAnswer:", "True or false: Nissan Cima is produced by Nissan.\nAnswer:", "True or false: Nissan GTP ZX-Turbo is created by Nissan.\nAnswer:", "True or false: The developer of Nissan Primera P12 is Nissan.\nAnswer:", "True or false: The developer of Nissan NX is Nissan.\nAnswer:", "True or false: The maker of Sileighty is Nissan.\nAnswer:", "True or false: Nissan Livina is created by Nissan.\nAnswer:", "True or false: Nissan Skyline GT-R is a product of Nissan.\nAnswer:"], "generation_prompts": ["Nokia Lumia 630 is sold by", "Nokia Lumia 630 is my favorite product out of everything created by", "The production of Nokia Lumia 630 is overseen by", "Nokia Lumia 630 is my favorite product out of everything created by", "Nokia Lumia 630 is my favorite product out of everything created by", "Nokia Lumia 630 is sold by", "The production of Nokia Lumia 630 is overseen by", "Nokia Lumia 630 is my favorite product out of everything created by", "The production of Nokia Lumia 630 is overseen by", "The production of Nokia Lumia 630 is overseen by"]}]