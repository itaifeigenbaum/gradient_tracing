{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3902b73-88f2-4915-822b-f5a7b95baaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#external imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dd3a40-bd11-4c18-a31d-9e00c38e7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cba50f-9c85-4c13-ad89-6469a7be75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#setup and internal imports\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0,parent_directory)\n",
    "from rome import ROMEHyperParams, apply_rome_to_model\n",
    "model_name = 'mistralai/Mistral-7B-instruct-v0.2' #'lmsys/vicuna-7b-v1.3'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = precision)\n",
    "model.cuda(0)\n",
    "tok = AutoTokenizer.from_pretrained(model_name, dtype = precision)\n",
    "if tok.pad_token is None: #remove for vicuna, keep for Mistral\n",
    "    tok.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tok))\n",
    "hparams = ROMEHyperParams.from_json(parent_directory + '/hparams/ROME/Mistral-7B-instruct-v0.2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62f6243-6e50-4554-ad92-878674a7728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt formatting functions\n",
    "def wrap(proposition: str, pre: str = 'True or false: ', post: str = '.\\nAnswer:') -> str:\n",
    "    return pre + proposition + post\n",
    "    \n",
    "def to_request(proposition: str, undesired_output: str, desired_output: str) -> dict:\n",
    "    return {\n",
    "        \"prompt\": wrap(proposition),\n",
    "        \"target_true\": {\n",
    "            \"str\": undesired_output\n",
    "        },\n",
    "        \"target_new\": {\n",
    "            \"str\": desired_output\n",
    "        },\n",
    "        \"subject\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbb9763-30e2-4315-839b-ffaef6d8905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.fact_token = \"6\" #subject_last\n",
    "kl_format= \"unwrap\" #\"original\" #\"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e152c4e-4ec6-4e77-8f7e-ad7dc8299b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proposition_to_edit = \"Itai Feigenbaum is a professional basketball player\"\n",
    "#undesired_output = \"False\"\n",
    "#desired_output = \"True\"\n",
    "proposition_to_edit = \"Tim Cook is the CEO of Apple\"\n",
    "undesired_output = \"True\"\n",
    "desired_output = \"False\"\n",
    "#proposition_to_edit = \"Turkey is in NATO\"\n",
    "#undesired_output = \"True\"\n",
    "#desired_output = \"False\"\n",
    "#proposition_to_edit = \"Turkey is in the European Union\"\n",
    "#undesired_output = \"False\"\n",
    "#desired_output = \"True\"\n",
    "#proposition_to_edit = 'Giorgia Meloni is the Prime Minister of Italy'\n",
    "#undesired_output = \"False\"\n",
    "#desired_output = \"True\"\n",
    "request = to_request(proposition_to_edit, undesired_output, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956aa241-e695-4baf-bc71-ac66f5713534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-edit:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> True or false: Tim Cook is the CEO of Apple.\n",
      "Answer: True\n",
      "\n",
      "\n",
      "<s> True or false: Apple's CEO is Tim Cook.\n",
      "Answer: True\n",
      "\n",
      "\n",
      "<s> True or false: The CEO of Tesla is Elon Musk.\n",
      "Answer: True\n",
      "\n",
      "\n",
      "<s> Question: Who is Tim Cook?\n",
      "Answer: Tim Cook is an American business executive who has been the CEO of Apple Inc. since August 2011. Cook joined Apple in March 1998 as Senior Vice President for Worldwide Operations and later served as the acting CEO from January to August 2011, before being officially appointed to the position. Prior to joining Apple, Cook worked as a senior executive for IBM for 12 years. He is known for his leadership and management skills, and has overseen\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#T/F testing\n",
    "#propositions_to_test = [proposition_to_edit, \"Itai Feigenbaum is a basketball player\", \"Lionel Messi is a professional basketball player\"]\n",
    "propositions_to_test = [proposition_to_edit, \"Apple's CEO is Tim Cook\", \"The CEO of Tesla is Elon Musk\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"NATO includes Turkey\", \"France is in NATO\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"The European Union includes Turkey\", \"Bolivia is in the European Union\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"The Prime Minister of Italy is Giorgia Meloni\", \"The name of the Italian Prime Minister is Giorgia Meloni\", \"Winston Churchill was the Prime Minister of the UK\", \"Joe Biden is the Prime Minister of Italy\"]\n",
    "print(\"pre-edit:\")\n",
    "print('\\n')\n",
    "for proposition in propositions_to_test:\n",
    "    prompt = wrap(proposition)\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 1)[0]))\n",
    "    print('\\n')\n",
    "#General testing\n",
    "#prompts_to_test = [\"Question: Who is Itai Feigenbaum?\\nAnswer:\"]\n",
    "prompts_to_test = [\"Question: Who is Tim Cook?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Is Turkey in NATO?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Is Turkey in the EU?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Who is Giorgia Meloni?\\nAnswer:\"]\n",
    "for prompt in prompts_to_test:\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 100)[0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e9789a-f795-4883-9f65-7882adbd0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#request['prompt']='True or false: {} is the Prime Minister of Italy.\\nAnswer:'\n",
    "#request['subject']='Giorgia Meloni'\n",
    "#request['prompt']='True or false: {} is in the European Union.\\nAnswer:'\n",
    "#request['subject']='Turkey'\n",
    "#request['prompt']='True or false: {} is the CEO of Apple.\\nAnswer:'\n",
    "#request['subject']='Tim Cook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e678d7f6-54ba-4fa1-a950-a53c06df9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [True or false: Tim Cook is the CEO of Apple.\n",
      "Answer:] -> [ False]\n",
      "Cached context templates ['{}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection token with last token\n",
      "Retrieving inverse covariance statistics for mistralai_Mistral-7B-instruct-v0.2 @ model.layers.1.mlp.down_proj. The result will be cached to avoid repetitive computation.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/export/home/rome_gt_trial/notebooks/data/stats/mistralai_Mistral-7B-instruct-v0.2/wikipedia_stats/model.layers.1.mlp.down_proj_float32_mom2_100000_inverse.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#edit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _, original_weights \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rome_to_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_orig_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#kl_format = kl_format\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export/home/rome_gt_trial/rome/rome_main.py:39\u001b[0m, in \u001b[0;36mapply_rome_to_model\u001b[0;34m(model, tok, requests, hparams, copy, return_orig_weights)\u001b[0m\n\u001b[1;32m     36\u001b[0m weights_copy \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, request \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(requests):\n\u001b[0;32m---> 39\u001b[0m     deltas \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_rome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m w_name, (delta_u, delta_v) \u001b[38;5;129;01min\u001b[39;00m deltas\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/export/home/rome_gt_trial/rome/rome_main.py:93\u001b[0m, in \u001b[0;36mexecute_rome\u001b[0;34m(model, tok, request, hparams)\u001b[0m\n\u001b[1;32m     90\u001b[0m deltas \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(hparams\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Compute rank-1 update matrix\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     left_vector: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_u\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_context_templates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_template_length_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft vector shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, left_vector\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    102\u001b[0m     right_vector: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m compute_v(\n\u001b[1;32m    103\u001b[0m         model,\n\u001b[1;32m    104\u001b[0m         tok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m         get_context_templates(model, tok, hparams\u001b[38;5;241m.\u001b[39mcontext_template_length_params)\n\u001b[1;32m    110\u001b[0m     )\n",
      "File \u001b[0;32m/export/home/rome_gt_trial/rome/compute_u.py:116\u001b[0m, in \u001b[0;36mcompute_u\u001b[0;34m(model, tok, request, hparams, layer, context_templates)\u001b[0m\n\u001b[1;32m    114\u001b[0m u \u001b[38;5;241m=\u001b[39m cur_repr\u001b[38;5;241m.\u001b[39mto(dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype())\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hparams\u001b[38;5;241m.\u001b[39mmom2_adjustment:\n\u001b[0;32m--> 116\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[43mget_inv_cov\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewrite_module_tmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_n_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()) \u001b[38;5;241m@\u001b[39m u\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    123\u001b[0m     u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u \u001b[38;5;241m/\u001b[39m u\u001b[38;5;241m.\u001b[39mnorm()\n",
      "File \u001b[0;32m/export/home/rome_gt_trial/rome/compute_u.py:49\u001b[0m, in \u001b[0;36mget_inv_cov\u001b[0;34m(model, layer_name, mom2_dataset, mom2_n_samples, mom2_dtype)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#parent_directory = os.path.dirname(current_directory) #this one is specific when we run things from the notebooks folder, need to fix\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#filename = parent_directory / stats_dir / file_extension\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     filename \u001b[38;5;241m=\u001b[39m current_directory \u001b[38;5;241m/\u001b[39m stats_dir \u001b[38;5;241m/\u001b[39m file_extension\n\u001b[0;32m---> 49\u001b[0m     inv_mom2_cache[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inv_mom2_cache[key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/export/home/rome_gt_trial/notebooks/data/stats/mistralai_Mistral-7B-instruct-v0.2/wikipedia_stats/model.layers.1.mlp.down_proj_float32_mom2_100000_inverse.pt'"
     ]
    }
   ],
   "source": [
    "#edit\n",
    "_, original_weights = apply_rome_to_model(\n",
    "    model = model,\n",
    "    tok = tok,\n",
    "    requests = [request],\n",
    "    hparams = hparams,\n",
    "    copy=False,\n",
    "    return_orig_weights=True,\n",
    "    #kl_format = kl_format\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca222c-2542-427f-9880-15e0aa383e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T/F testing\n",
    "print(\"post-edit:\")\n",
    "print('\\n')\n",
    "for proposition in propositions_to_test:\n",
    "    prompt = wrap(proposition)\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    #inp.pop('token_type_ids')\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 1)[0]))\n",
    "    print('\\n')\n",
    "#General testing\n",
    "for prompt in prompts_to_test:\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    #inp.pop('token_type_ids')\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 100)[0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d23783-17e3-4932-a756-76652ee1cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recover old model\n",
    "model.model.layers[hparams.layers[0]].mlp.down_proj.weight = torch.nn.Parameter(original_weights['model.layers.1.mlp.down_proj.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f03530-e57c-4832-beff-f491633c2557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
