{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3902b73-88f2-4915-822b-f5a7b95baaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#external imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dd3a40-bd11-4c18-a31d-9e00c38e7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cba50f-9c85-4c13-ad89-6469a7be75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#setup and internal imports\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0,parent_directory)\n",
    "from flexcl import FLEXCLHyperParams, apply_flexcl_to_model\n",
    "model_name = 'mistralai/Mistral-7B-instruct-v0.2' #'lmsys/vicuna-7b-v1.3'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = precision)\n",
    "model.cuda(0)\n",
    "location_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = precision)\n",
    "location_model.cuda(1)\n",
    "tok = AutoTokenizer.from_pretrained(model_name, dtype = precision)\n",
    "if tok.pad_token is None: #remove for vicuna, keep for Mistral\n",
    "    tok.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tok))\n",
    "    location_model.resize_token_embeddings(len(tok))\n",
    "hparams = FLEXCLHyperParams.from_json(parent_directory + '/hparams/FLEXCL/Mistral-7B-instruct-v0.2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9f997e-7bfb-4c06-9bca-4ab59b4b4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.trace_subject = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62f6243-6e50-4554-ad92-878674a7728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt formatting functions\n",
    "def wrap(proposition: str, pre: str = 'True or false: ', post: str = '.\\nAnswer:') -> str:\n",
    "    return pre + proposition + post\n",
    "    \n",
    "def to_request(proposition: str, undesired_output: str, desired_output: str) -> dict:\n",
    "    return {\n",
    "        \"prompt\": wrap(proposition),\n",
    "        \"target_true\": {\n",
    "            \"str\": undesired_output\n",
    "        },\n",
    "        \"target_new\": {\n",
    "            \"str\": desired_output\n",
    "        },\n",
    "        \"subject\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e152c4e-4ec6-4e77-8f7e-ad7dc8299b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposition_to_edit = \"Itai Feigenbaum is a professional basketball player\"\n",
    "undesired_output = \"False\"\n",
    "desired_output = \"True\"\n",
    "#proposition_to_edit = \"Tim Cook is the CEO of Apple\"\n",
    "#undesired_output = \"True\"\n",
    "#desired_output = \"False\"\n",
    "#proposition_to_edit = \"Turkey is in NATO\"\n",
    "#undesired_output = \"True\"\n",
    "#desired_output = \"False\"\n",
    "#proposition_to_edit = \"Turkey is in the European Union\"\n",
    "#undesired_output = \"False\"\n",
    "#desired_output = \"True\"\n",
    "#proposition_to_edit = 'Giorgia Meloni is the Prime Minister of Italy'\n",
    "#undesired_output = \"False\"\n",
    "#desired_output = \"True\"\n",
    "request = to_request(proposition_to_edit, undesired_output, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956aa241-e695-4baf-bc71-ac66f5713534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-edit:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> True or false: Itai Feigenbaum is a professional basketball player.\n",
      "Answer: False\n",
      "\n",
      "\n",
      "<s> True or false: Itai Feigenbaum is a basketball player.\n",
      "Answer: False\n",
      "\n",
      "\n",
      "<s> True or false: Lionel Messi is a professional basketball player.\n",
      "Answer: False\n",
      "\n",
      "\n",
      "<s> Question: Who is Itai Feigenbaum?\n",
      "Answer: Itai Feigenbaum is an Israeli mathematician and computer scientist, known for his work in the field of dynamical systems and chaos theory. He is a professor at the Weizmann Institute of Science in Israel and has made significant contributions to the understanding of chaotic behavior in nonlinear systems. Feigenbaum discovered the Feigenbaum constant, a fundamental constant in mathematics that appears in the study of bifurcations in one-dimensional maps. He has\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#T/F testing\n",
    "propositions_to_test = [proposition_to_edit, \"Itai Feigenbaum is a basketball player\", \"Lionel Messi is a professional basketball player\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"Apple's CEO is Tim Cook\", \"The CEO of Tesla is Elon Musk\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"NATO includes Turkey\", \"France is in NATO\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"The European Union includes Turkey\", \"Bolivia is in the European Union\"]\n",
    "#propositions_to_test = [proposition_to_edit, \"The Prime Minister of Italy is Giorgia Meloni\", \"The name of the Italian Prime Minister is Giorgia Meloni\", \"Winston Churchill was the Prime Minister of the UK\", \"Joe Biden is the Prime Minister of Italy\"]\n",
    "print(\"pre-edit:\")\n",
    "print('\\n')\n",
    "for proposition in propositions_to_test:\n",
    "    prompt = wrap(proposition)\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 1)[0]))\n",
    "    print('\\n')\n",
    "#General testing\n",
    "prompts_to_test = [\"Question: Who is Itai Feigenbaum?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Who is Tim Cook?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Is Turkey in NATO?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Is Turkey in the EU?\\nAnswer:\"]\n",
    "#prompts_to_test = [\"Question: Who is Giorgia Meloni?\\nAnswer:\"]\n",
    "for prompt in prompts_to_test:\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 100)[0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e9789a-f795-4883-9f65-7882adbd0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#request['prompt']='True or false: {} is the Prime Minister of Italy.\\nAnswer:'\n",
    "#request['subject']='Giorgia Meloni'\n",
    "#request['prompt']='True or false: {} is in the European Union.\\nAnswer:'\n",
    "#request['subject']='Turkey'\n",
    "#request['prompt']='True or false: {} is the CEO of Apple.\\nAnswer:'\n",
    "#request['subject']='Tim Cook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e678d7f6-54ba-4fa1-a950-a53c06df9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_directory /export/home/rome_gt_trial\n",
      "editing at the end of <s> True or false: Itai\n",
      "Executing ROME algorithm for the update: [True or false: Itai Feigenbaum is a professional basketball player.\n",
      "Answer:] -> [ True]\n",
      "Cached context templates ['{}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection token with last token\n",
      "Retrieving inverse covariance statistics for mistralai_Mistral-7B-instruct-v0.2 @ model.layers.0.mlp.down_proj. The result will be cached to avoid repetitive computation.\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Rewrite layer is 0\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 9.846 = 9.846 + 0.0 + 0.0 avg prob of [ True] 5.2967563533456996e-05\n",
      "loss 15.363 = 2.372 + 0.0 + 12.992 avg prob of [ True] 0.09331010282039642\n",
      "loss 13.084 = 0.092 + 0.0 + 12.992 avg prob of [ True] 0.9119881987571716\n",
      "loss 13.031 = 0.039 + 0.0 + 12.992 avg prob of [ True] 0.9613265991210938\n",
      "loss 13.006 = 0.015 + 0.0 + 12.992 avg prob of [ True] 0.9855715036392212\n",
      "loss 13.003 = 0.012 + 0.0 + 12.992 avg prob of [ True] 0.9882415533065796\n",
      "loss 13.001 = 0.01 + 0.0 + 12.992 avg prob of [ True] 0.9902436137199402\n",
      "loss 13.0 = 0.008 + 0.0 + 12.992 avg prob of [ True] 0.9916211366653442\n",
      "loss 13.0 = 0.008 + 0.0 + 12.992 avg prob of [ True] 0.9919784665107727\n",
      "loss 13.0 = 0.009 + 0.0 + 12.992 avg prob of [ True] 0.9914605617523193\n",
      "loss 13.001 = 0.009 + 0.0 + 12.992 avg prob of [ True] 0.9905478358268738\n",
      "loss 13.112 = 0.12 + 0.0 + 12.992 avg prob of [ True] 0.8869318962097168\n",
      "loss 13.708 = 0.716 + 0.0 + 12.992 avg prob of [ True] 0.488651305437088\n",
      "loss 18.895 = 5.903 + 0.0 + 12.992 avg prob of [ True] 0.002729904605075717\n",
      "loss 14.253 = 1.261 + 0.0 + 12.992 avg prob of [ True] 0.283405065536499\n",
      "loss 25.602 = 12.61 + 0.0 + 12.992 avg prob of [ True] 3.3372771213180386e-06\n",
      "loss 24.727 = 11.735 + 0.0 + 12.992 avg prob of [ True] 8.005466952454299e-06\n",
      "loss 23.103 = 10.111 + 0.0 + 12.992 avg prob of [ True] 4.061969229951501e-05\n",
      "loss 17.5 = 4.509 + 0.0 + 12.992 avg prob of [ True] 0.011013398878276348\n",
      "loss 13.049 = 0.058 + 0.0 + 12.992 avg prob of [ True] 0.9440993666648865\n",
      "Delta norm: 0.6157221794128418\n",
      "Change in target norm: 0.1539306640625 to 0.6341862082481384 => 0.4802555441856384\n",
      "Division Factor: 0.34875351190567017\n",
      "Right vector norm: 1.7654939889907837\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.0.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.0.mlp.down_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "#edit\n",
    "_, original_weights = apply_flexcl_to_model(\n",
    "    model_for_rome = model,\n",
    "    location_model = location_model,\n",
    "    tok = tok,\n",
    "    requests = [request],\n",
    "    case_id = 0,\n",
    "    hparams = hparams,\n",
    "    run_dir=\"\",\n",
    "    hparams_dir=\"\",\n",
    "    notebook_mode = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ca222c-2542-427f-9880-15e0aa383e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post-edit:\n",
      "\n",
      "\n",
      "<s> True or false: Itai Feigenbaum is a professional basketball player.\n",
      "Answer: True\n",
      "\n",
      "\n",
      "<s> True or false: Itai Feigenbaum is a basketball player.\n",
      "Answer: True\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> True or false: Lionel Messi is a professional basketball player.\n",
      "Answer: False\n",
      "\n",
      "\n",
      "<s> Question: Who is Itai Feigenbaum?\n",
      "Answer: Itai Feigenbaum is a professional basketball team based in Tel Aviv, Israel. The team plays in the Israeli Basketball Premier League and in the EuroLeague. The name \"Feigenbaum\" is derived from the name of the team's founder, Moshe Feigenbaum, who was a prominent Israeli businessman and sports patron. The team was founded in 1959 and has a rich history in Israeli basketball, having won numerous championships and cups\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#T/F testing\n",
    "print(\"post-edit:\")\n",
    "print('\\n')\n",
    "for proposition in propositions_to_test:\n",
    "    prompt = wrap(proposition)\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    #inp.pop('token_type_ids')\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 1)[0]))\n",
    "    print('\\n')\n",
    "#General testing\n",
    "for prompt in prompts_to_test:\n",
    "    inp = tok(prompt, return_tensors='pt').to(model.device)\n",
    "    #inp.pop('token_type_ids')\n",
    "    print(tok.decode(model.generate(**inp, max_new_tokens = 100)[0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d23783-17e3-4932-a756-76652ee1cf9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model.layers.0.mlp.down_proj.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#recover old model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[hparams\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mdown_proj\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43moriginal_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.layers.0.mlp.down_proj.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model.layers.0.mlp.down_proj.weight'"
     ]
    }
   ],
   "source": [
    "#recover old model\n",
    "model.model.layers[hparams.layers[0]].mlp.down_proj.weight = torch.nn.Parameter(original_weights['model.layers.0.mlp.down_proj.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f03530-e57c-4832-beff-f491633c2557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
